{
  "nodes": [
    {
      "content": "Tutorial: Create a pipeline with Copy Activity using Azure PowerShell",
      "pos": [
        28,
        97
      ]
    },
    {
      "content": "In this tutorial, you will create an Azure Data Factory pipeline with a Copy Activity by using Azure PowerShell.",
      "pos": [
        117,
        229
      ]
    },
    {
      "content": "Tutorial: Create and monitor a data factory using Azure PowerShell",
      "pos": [
        557,
        623
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        626,
        642
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        646,
        663
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        697,
        722
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        769,
        785
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        839,
        858
      ]
    },
    {
      "content": "The <bpt id=\"p1\">[</bpt>Get started with Azure Data Factory<ept id=\"p1\">][adf-get-started]</ept> tutorial shows you how to create and monitor an Azure data factory using the <bpt id=\"p2\">[</bpt>Azure Preview Portal<ept id=\"p2\">][azure-preview-portal]</ept>.",
      "pos": [
        900,
        1081
      ]
    },
    {
      "content": "In this tutorial, you will create and monitor an Azure data factory by using Azure PowerShell cmdlets.",
      "pos": [
        1083,
        1185
      ]
    },
    {
      "content": "The pipeline in the data factory you create in this tutorial copies data from an Azure blob to an Azure SQL database.",
      "pos": [
        1186,
        1303
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> This article does not cover all the Data Factory cmdlets.",
      "pos": [
        1314,
        1384
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Cmdlet Reference<ept id=\"p1\">][cmdlet-reference]</ept> for comprehensive documentation on Data Factory cmdlets.",
      "pos": [
        1385,
        1495
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        1500,
        1513
      ]
    },
    {
      "content": "Apart from prerequisites listed in the Tutorial Overview topic, you need to have Azure PowerShell installed on your computer.",
      "pos": [
        1514,
        1639
      ]
    },
    {
      "content": "If you do not have it already, download and install <bpt id=\"p1\">[</bpt>Azure PowerShell<ept id=\"p1\">][download-azure-powershell]</ept> on your computer.",
      "pos": [
        1640,
        1755
      ]
    },
    {
      "content": "In This Tutorial",
      "pos": [
        1759,
        1775
      ]
    },
    {
      "content": "The following table lists the steps you will perform as part of the tutorial and their descriptions.",
      "pos": [
        1776,
        1876
      ]
    },
    {
      "content": "Step",
      "pos": [
        1879,
        1883
      ]
    },
    {
      "content": "Description",
      "pos": [
        1886,
        1897
      ]
    },
    {
      "content": "Step 1: Create an Azure Data Factory",
      "pos": [
        1918,
        1954
      ]
    },
    {
      "pos": [
        1978,
        2066
      ],
      "content": "In this step, you will create an Azure data factory named <bpt id=\"p1\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Step 2: Create linked services",
      "pos": [
        2069,
        2099
      ]
    },
    {
      "content": "In this step, you will create two linked services: <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        2126,
        2232
      ]
    },
    {
      "content": "The StorageLinkedService links an Azure storage and AzureSqlLinkedService links an Azure SQL database to the ADFTutorialDataFactoryPSH.",
      "pos": [
        2233,
        2368
      ]
    },
    {
      "content": "Step 3: Create input and output datasets",
      "pos": [
        2370,
        2410
      ]
    },
    {
      "pos": [
        2445,
        2664
      ],
      "content": "In this step, you will define two data sets (<bpt id=\"p1\">**</bpt>EmpTableFromBlob<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>) that are used as input and output tables for the <bpt id=\"p3\">**</bpt>Copy Activity<ept id=\"p3\">**</ept> in the ADFTutorialPipeline that you will create in the next step."
    },
    {
      "content": "Step 4: Create and run a pipeline",
      "pos": [
        2666,
        2699
      ]
    },
    {
      "content": "In this step, you will create a pipeline named <bpt id=\"p1\">**</bpt>ADFTutorialPipeline<ept id=\"p1\">**</ept> in the data factory: <bpt id=\"p2\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p2\">**</ept>.",
      "pos": [
        2727,
        2849
      ]
    },
    {
      "content": ".",
      "pos": [
        2850,
        2851
      ]
    },
    {
      "content": "The pipeline will have a <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> that copies data from an Azure blob to an output Azure database table.",
      "pos": [
        2852,
        2965
      ]
    },
    {
      "content": "Step 5: Monitor data sets and pipeline",
      "pos": [
        2967,
        3005
      ]
    },
    {
      "content": "In this step, you will monitor the datasets and the pipeline using Azure PowerShell in this step.",
      "pos": [
        3038,
        3135
      ]
    },
    {
      "pos": [
        3140,
        3208
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateDataFactory\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 1: Create an Azure Data Factory"
    },
    {
      "pos": [
        3209,
        3320
      ],
      "content": "In this step, you use the Azure PowerShell to create an Azure data factory named <bpt id=\"p1\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept> and execute the following commands.",
      "pos": [
        3325,
        3388
      ]
    },
    {
      "content": "Keep the Azure PowerShell open until the end of this tutorial.",
      "pos": [
        3389,
        3451
      ]
    },
    {
      "content": "If you close and reopen, you need to run these commands again.",
      "pos": [
        3452,
        3514
      ]
    },
    {
      "pos": [
        3521,
        3636
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Add-AzureAccount<ept id=\"p1\">**</ept> and enter the  user name and password that you use to sign-in to the Azure Preview Portal."
    },
    {
      "pos": [
        3645,
        3722
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureSubscription<ept id=\"p1\">**</ept> to view all the subscriptions for this account."
    },
    {
      "content": "Run <bpt id=\"p1\">**</bpt>Select-AzureSubscription<ept id=\"p1\">**</ept> to select the subscription that you want to work with.",
      "pos": [
        3729,
        3816
      ]
    },
    {
      "content": "This subscription should be the same as the one you used in the Azure Preview Portal.",
      "pos": [
        3817,
        3902
      ]
    },
    {
      "pos": [
        3907,
        4008
      ],
      "content": "Switch to <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode as the Azure Data Factory cmdlets are available in this mode."
    },
    {
      "pos": [
        4060,
        4160
      ],
      "content": "Create an Azure resource group named: <bpt id=\"p1\">**</bpt>ADFTutorialResourceGroup<ept id=\"p1\">**</ept> by running the following command."
    },
    {
      "content": "Some of the steps in this tutorial assume that you use the resource group named <bpt id=\"p1\">**</bpt>ADFTutorialResourceGroup<ept id=\"p1\">**</ept>.",
      "pos": [
        4253,
        4362
      ]
    },
    {
      "content": "If you use a different resource group, you will need to use it in place of ADFTutorialResourceGroup in this tutorial.",
      "pos": [
        4363,
        4480
      ]
    },
    {
      "pos": [
        4485,
        4587
      ],
      "content": "Run the <bpt id=\"p1\">**</bpt>New-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet to create a data factory named: <bpt id=\"p2\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p2\">**</ept>."
    },
    {
      "content": "The name of the Azure data factory must be globally unique.",
      "pos": [
        4722,
        4781
      ]
    },
    {
      "content": "If you receive the error: <bpt id=\"p1\">**</bpt>Data factory name “ADFTutorialDataFactoryPSH” is not available<ept id=\"p1\">**</ept>, change the name (for example, yournameADFTutorialDataFactoryPSH).",
      "pos": [
        4782,
        4941
      ]
    },
    {
      "content": "Use this name in place of ADFTutorialFactoryPSH while performing steps in this tutorial.",
      "pos": [
        4942,
        5030
      ]
    },
    {
      "pos": [
        5035,
        5100
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateLinkedServices\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 2: Create linked services"
    },
    {
      "content": "Linked services link data stores or compute services to an Azure data factory.",
      "pos": [
        5101,
        5179
      ]
    },
    {
      "content": "A data store can be an Azure Storage, Azure SQL Database or an on-premises SQL Server database that contains input data or stores output data for a Data Factory pipeline.",
      "pos": [
        5180,
        5350
      ]
    },
    {
      "content": "A compute service is the service that processes  input data and produces output data.",
      "pos": [
        5351,
        5436
      ]
    },
    {
      "content": "In this step, you will create two linked services: <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        5439,
        5545
      ]
    },
    {
      "content": "StorageLinkedService linked service links an Azure Storage Account and AzureSqlLinkedService links an Azure SQL database to the data factory: <bpt id=\"p1\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p1\">**</ept>.",
      "pos": [
        5546,
        5718
      ]
    },
    {
      "content": "You will create a pipeline later in this tutorial that copies data from a blob container in StorageLinkedService to a SQL table in AzureSqlLinkedService.",
      "pos": [
        5719,
        5872
      ]
    },
    {
      "content": "Create a linked service for an Azure storage account",
      "pos": [
        5878,
        5930
      ]
    },
    {
      "content": "Create a JSON file named <bpt id=\"p1\">**</bpt>StorageLinkedService.json<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>C:\\ADFGetStartedPSH<ept id=\"p2\">**</ept> with the following content.",
      "pos": [
        5935,
        6048
      ]
    },
    {
      "content": "Create the folder ADFGetStartedPSH if it does not already exist.",
      "pos": [
        6049,
        6113
      ]
    },
    {
      "pos": [
        6419,
        6544
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>accountname<ept id=\"p1\">**</ept> with the name of your storage account and <bpt id=\"p2\">**</bpt>accountkey<ept id=\"p2\">**</ept> with the key for your Azure storage account."
    },
    {
      "pos": [
        6549,
        6620
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>, switch to the <bpt id=\"p2\">**</bpt>ADFGetStartedPSH<ept id=\"p2\">**</ept> folder."
    },
    {
      "content": "You can use the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">**</ept> cmdlet to create a linked service.",
      "pos": [
        6626,
        6714
      ]
    },
    {
      "content": "This cmdlet and other Data Factory cmdlets you use in this tutorial require you to pass values for the <bpt id=\"p1\">**</bpt>ResourceGroupName<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>DataFactoryName<ept id=\"p2\">**</ept> parameters.",
      "pos": [
        6715,
        6875
      ]
    },
    {
      "content": "Alternatively, you can use <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> to get a DataFactory object and pass the object without typing ResourceGroupName and DataFactoryName each time you run a cmdlet.",
      "pos": [
        6876,
        7056
      ]
    },
    {
      "content": "Run the following command to assign the output of the <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet to a variable: <bpt id=\"p2\">**</bpt>$df<ept id=\"p2\">**</ept>.",
      "pos": [
        7057,
        7166
      ]
    },
    {
      "pos": [
        7283,
        7396
      ],
      "content": "Now, run the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">**</ept> cmdlet to create the linked service: <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        7484,
        7684
      ],
      "content": "If you hadn't run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet and assigned the output to <bpt id=\"p2\">**</bpt>$df<ept id=\"p2\">**</ept> variable, you would have to specify values for the ResourceGroupName and DataFactoryName parameters as follows."
    },
    {
      "content": "If you close the Azure PowerShell in the middle of the tutorial, you will have run the Get-AzureDataFactory cmdlet next time you launch Azure PowerShell to complete the tutorial.",
      "pos": [
        7865,
        8043
      ]
    },
    {
      "content": "Create a linked service for an Azure SQL Database",
      "pos": [
        8049,
        8098
      ]
    },
    {
      "content": "Create a JSON file named AzureSqlLinkedService.json with the following content.",
      "pos": [
        8103,
        8182
      ]
    },
    {
      "pos": [
        8582,
        8741
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>servername<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>databasename<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>username@servername<ept id=\"p3\">**</ept>, and <bpt id=\"p4\">**</bpt>password<ept id=\"p4\">**</ept> with names of your Azure SQL server, database, user account, and  password."
    },
    {
      "content": "Run the following command to create a linked service.",
      "pos": [
        8747,
        8800
      ]
    },
    {
      "content": "Confirm that the <bpt id=\"p1\">**</bpt>Allow access to Azure services<ept id=\"p1\">**</ept> setting is turned ON for your Azure SQL server.",
      "pos": [
        8893,
        8992
      ]
    },
    {
      "content": "To verify and turn it on, do the following:",
      "pos": [
        8993,
        9036
      ]
    },
    {
      "pos": [
        9045,
        9104
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>BROWSE<ept id=\"p1\">**</ept> hub on the left and click <bpt id=\"p2\">**</bpt>SQL servers<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        9112,
        9179
      ],
      "content": "Select your server, and click <bpt id=\"p1\">**</bpt>SETTINGS<ept id=\"p1\">**</ept> on the SQL SERVER blade."
    },
    {
      "pos": [
        9187,
        9233
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>SETTINGS<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Firewall<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        9241,
        9330
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Firewalll settings<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>ON<ept id=\"p2\">**</ept> for <bpt id=\"p3\">**</bpt>Allow access to Azure services<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        9338,
        9423
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>ACTIVE<ept id=\"p1\">**</ept> hub on the left to switch to the <bpt id=\"p2\">**</bpt>Data Factory<ept id=\"p2\">**</ept> blade you were on."
    },
    {
      "pos": [
        9433,
        9514
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateInputAndOutputDataSets\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 3: Create input and output tables"
    },
    {
      "content": "In the previous step, you created linked services <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept> to link an Azure Storage account and Azure SQL database to the data factory: <bpt id=\"p3\">**</bpt>ADFTutorialDataFactoryPSH<ept id=\"p3\">**</ept>.",
      "pos": [
        9516,
        9728
      ]
    },
    {
      "content": "In this step, you will create tables that represent the input and output data for the Copy Activity in the pipeline you will be creating in the next step.",
      "pos": [
        9729,
        9883
      ]
    },
    {
      "content": "A table is a rectangular dataset and it is the only type of dataset that is supported at this time.",
      "pos": [
        9886,
        9985
      ]
    },
    {
      "content": "The input table in this tutorial refers to a blob container in the Azure Storage that StorageLinkedService points to and the output table refers to a SQL table in the Azure SQL database that AzureSqlLinkedService points to.",
      "pos": [
        9986,
        10209
      ]
    },
    {
      "content": "Prepare Azure Blob Storage and Azure SQL Database for the tutorial",
      "pos": [
        10217,
        10283
      ]
    },
    {
      "pos": [
        10284,
        10405
      ],
      "content": "Skip this step if you have gone through the tutorial from <bpt id=\"p1\">[</bpt>Get started with Azure Data Factory<ept id=\"p1\">][adf-get-started]</ept> article."
    },
    {
      "content": "You need to perform the following steps to prepare the Azure blob storage and Azure SQL database for this tutorial.",
      "pos": [
        10408,
        10523
      ]
    },
    {
      "pos": [
        10529,
        10641
      ],
      "content": "Create a blob container named <bpt id=\"p1\">**</bpt>adftutorial<ept id=\"p1\">**</ept> in the Azure blob storage that <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> points to."
    },
    {
      "pos": [
        10645,
        10732
      ],
      "content": "Create and upload a text file, <bpt id=\"p1\">**</bpt>emp.txt<ept id=\"p1\">**</ept>, as a blob to the <bpt id=\"p2\">**</bpt>adftutorial<ept id=\"p2\">**</ept> container."
    },
    {
      "pos": [
        10736,
        10858
      ],
      "content": "Create a table named <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> in the Azure SQL Database in the Azure SQL database that <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept> points to."
    },
    {
      "pos": [
        10864,
        10986
      ],
      "content": "Launch Notepad, paste the following text, and save it as <bpt id=\"p1\">**</bpt>emp.txt<ept id=\"p1\">**</ept> to <bpt id=\"p2\">**</bpt>C:\\ADFGetStartedPSH<ept id=\"p2\">**</ept> folder on your hard drive."
    },
    {
      "pos": [
        11045,
        11224
      ],
      "content": "Use tools such as <bpt id=\"p1\">[</bpt>Azure Storage Explorer<ept id=\"p1\">](https://azurestorageexplorer.codeplex.com/)</ept> to create the <bpt id=\"p2\">**</bpt>adftutorial<ept id=\"p2\">**</ept> container and to upload the <bpt id=\"p3\">**</bpt>emp.txt<ept id=\"p3\">**</ept> file to the container."
    },
    {
      "content": "Azure Storage Explorer",
      "pos": [
        11232,
        11254
      ]
    },
    {
      "pos": [
        11308,
        11392
      ],
      "content": "Use the following SQL script to create the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table in your Azure SQL Database."
    },
    {
      "content": "Create input table",
      "pos": [
        12517,
        12535
      ]
    },
    {
      "content": "A table is a rectangular dataset and has a schema.",
      "pos": [
        12537,
        12587
      ]
    },
    {
      "content": "In this step, you will create a table named <bpt id=\"p1\">**</bpt>EmpBlobTable<ept id=\"p1\">**</ept> that points to a blob container in the Azure Storage represented by the <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> linked service.",
      "pos": [
        12588,
        12761
      ]
    },
    {
      "content": "This blob container (<bpt id=\"p1\">**</bpt>adftutorial<ept id=\"p1\">**</ept>) contains the input data in the file: <bpt id=\"p2\">**</bpt>emp.txt<ept id=\"p2\">**</ept>.",
      "pos": [
        12762,
        12849
      ]
    },
    {
      "pos": [
        12856,
        12968
      ],
      "content": "Create a JSON file named <bpt id=\"p1\">**</bpt>EmpBlobTable.json<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>C:\\ADFGetStartedPSH<ept id=\"p2\">**</ept> folder with the following content:"
    },
    {
      "content": "Note the following:",
      "pos": [
        13764,
        13783
      ]
    },
    {
      "pos": [
        13796,
        13837
      ],
      "content": "dataset <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureBlob<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13844,
        13901
      ],
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept>."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> is set to the <bpt id=\"p2\">**</bpt>adftutorial<ept id=\"p2\">**</ept> container.",
      "pos": [
        13909,
        13964
      ]
    },
    {
      "content": "You can also specify the name of a blob within the folder.",
      "pos": [
        13965,
        14023
      ]
    },
    {
      "content": "Since you are not specifying the name of the blob, data from all blobs in the container is considered as an input data.",
      "pos": [
        14024,
        14143
      ]
    },
    {
      "pos": [
        14152,
        14192
      ],
      "content": "format <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>TextFormat<ept id=\"p2\">**</ept>"
    },
    {
      "pos": [
        14199,
        14324
      ],
      "content": "There are two fields in the text file – <bpt id=\"p1\">**</bpt>FirstName<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>LastName<ept id=\"p2\">**</ept> – separated by a comma character (<bpt id=\"p3\">**</bpt>columnDelimiter<ept id=\"p3\">**</ept>)"
    },
    {
      "pos": [
        14332,
        14581
      ],
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>hourly<ept id=\"p2\">**</ept> (<bpt id=\"p3\">**</bpt>frequency<ept id=\"p3\">**</ept> is set to <bpt id=\"p4\">**</bpt>hour<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>interval<ept id=\"p5\">**</ept> is set to <bpt id=\"p6\">**</bpt>1<ept id=\"p6\">**</ept> ), so the Data Factory service will look for input data every hour in the root folder in the blob container (<bpt id=\"p7\">**</bpt>adftutorial<ept id=\"p7\">**</ept>) you specified."
    },
    {
      "content": "if you don't specify a <bpt id=\"p1\">**</bpt>fileName<ept id=\"p1\">**</ept> for an <bpt id=\"p2\">**</bpt>input<ept id=\"p2\">**</ept> <bpt id=\"p3\">**</bpt>table<ept id=\"p3\">**</ept>, all files/blobs from the input folder (<bpt id=\"p4\">**</bpt>folderPath<ept id=\"p4\">**</ept>) are considered as inputs.",
      "pos": [
        14587,
        14731
      ]
    },
    {
      "content": "If you specify a fileName in the JSON, only the specified file/blob is considered asn input.",
      "pos": [
        14732,
        14824
      ]
    },
    {
      "content": "See the sample files in the <bpt id=\"p1\">[</bpt>tutorial<ept id=\"p1\">][adf-tutorial]</ept> for examples.",
      "pos": [
        14825,
        14891
      ]
    },
    {
      "pos": [
        14898,
        15112
      ],
      "content": "If you do not specify a <bpt id=\"p1\">**</bpt>fileName<ept id=\"p1\">**</ept> for an <bpt id=\"p2\">**</bpt>output table<ept id=\"p2\">**</ept>, the generated files in the <bpt id=\"p3\">**</bpt>folderPath<ept id=\"p3\">**</ept> are named in the following format: Data.&lt;Guid\\&gt;.txt (example: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.)."
    },
    {
      "content": "To set <bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>fileName<ept id=\"p2\">**</ept> dynamically based on the <bpt id=\"p3\">**</bpt>SliceStart<ept id=\"p3\">**</ept> time, use the <bpt id=\"p4\">**</bpt>partitionedBy<ept id=\"p4\">**</ept> property.",
      "pos": [
        15118,
        15238
      ]
    },
    {
      "content": "In the following example, folderPath uses Year, Month, and Day from from the SliceStart (start time of the slice being processed) and fileName uses Hour from the SliceStart.",
      "pos": [
        15239,
        15412
      ]
    },
    {
      "content": "For example, if a slice is being produced for 2014-10-20T08:00:00, the folderName is set to wikidatagateway/wikisampledataout/2014/10/20 and the fileName is set to 08.csv.",
      "pos": [
        15413,
        15584
      ]
    },
    {
      "pos": [
        16169,
        16281
      ],
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=516971)</ept> for details about JSON properties."
    },
    {
      "content": "Run the following command to create the Data Factory table.",
      "pos": [
        16287,
        16346
      ]
    },
    {
      "content": "Create output table",
      "pos": [
        16417,
        16436
      ]
    },
    {
      "content": "In this part of the step, you will create an output table named <bpt id=\"p1\">**</bpt>EmpSQLTable<ept id=\"p1\">**</ept> that points to a SQL table (<bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept>) in the Azure SQL database that is represented by the <bpt id=\"p3\">**</bpt>AzureSqlLinkedService<ept id=\"p3\">**</ept> linked service.",
      "pos": [
        16437,
        16648
      ]
    },
    {
      "content": "The pipeline copies data from the input blob to the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table.",
      "pos": [
        16649,
        16715
      ]
    },
    {
      "pos": [
        16722,
        16833
      ],
      "content": "Create a JSON file named <bpt id=\"p1\">**</bpt>EmpSQLTable.json<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>C:\\ADFGetStartedPSH<ept id=\"p2\">**</ept> folder with the following content."
    },
    {
      "content": "Note the following:",
      "pos": [
        17469,
        17488
      ]
    },
    {
      "pos": [
        17501,
        17546
      ],
      "content": "dataset <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureSqlTable<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        17553,
        17611
      ],
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        17618,
        17650
      ],
      "content": "<bpt id=\"p1\">**</bpt>tablename<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        17657,
        17853
      ],
      "content": "There are three columns – <bpt id=\"p1\">**</bpt>ID<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>FirstName<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>LastName<ept id=\"p3\">**</ept> – in the emp table in the database, but ID is an identity column, so you need to specify only <bpt id=\"p4\">**</bpt>FirstName<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>LastName<ept id=\"p5\">**</ept> here."
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>hourly<ept id=\"p2\">**</ept> (<bpt id=\"p3\">**</bpt>frequency<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>hour<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>interval<ept id=\"p5\">**</ept> set to <bpt id=\"p6\">**</bpt>1<ept id=\"p6\">**</ept>).",
      "pos": [
        17860,
        17964
      ]
    },
    {
      "content": "The Data Factory service will generate an output data slice every hour in the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table in the Azure SQL database.",
      "pos": [
        17966,
        18084
      ]
    },
    {
      "content": "Run the following command to create the Data Factory table.",
      "pos": [
        18090,
        18149
      ]
    },
    {
      "pos": [
        18224,
        18293
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateAndRunAPipeline\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 4: Create and run a pipeline"
    },
    {
      "pos": [
        18294,
        18425
      ],
      "content": "In this step, you create a pipeline with a <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> that uses <bpt id=\"p2\">**</bpt>EmpTableFromBlob<ept id=\"p2\">**</ept> as input and <bpt id=\"p3\">**</bpt>EmpSQLTable<ept id=\"p3\">**</ept> as output."
    },
    {
      "pos": [
        18431,
        18550
      ],
      "content": "Create a JSON file named <bpt id=\"p1\">**</bpt>ADFTutorialPipeline.json<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>C:\\ADFGetStartedPSH<ept id=\"p2\">**</ept> folder with the following content:"
    },
    {
      "content": "Note the following:",
      "pos": [
        19953,
        19972
      ]
    },
    {
      "pos": [
        19980,
        20068
      ],
      "content": "In the activities section, there is only one activity whose <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>Copy<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        20075,
        20183
      ],
      "content": "Input for the activity is set to <bpt id=\"p1\">**</bpt>EmpTableFromBlob<ept id=\"p1\">**</ept> and output for the activity is set to <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        20190,
        20318
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>transformation<ept id=\"p1\">**</ept> section, <bpt id=\"p2\">**</bpt>BlobSource<ept id=\"p2\">**</ept> is specified as the source type and <bpt id=\"p3\">**</bpt>SqlSink<ept id=\"p3\">**</ept> is specified as the sink type."
    },
    {
      "content": "Replace the value of the <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> property with the current day and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> value with the next day.",
      "pos": [
        20324,
        20425
      ]
    },
    {
      "content": "Both start and end datetimes must be in <bpt id=\"p1\">[</bpt>ISO format<ept id=\"p1\">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.",
      "pos": [
        20426,
        20518
      ]
    },
    {
      "content": "For example: 2014-10-14T16:32:41Z.",
      "pos": [
        20519,
        20553
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> time is optional, but we will use it in this tutorial.",
      "pos": [
        20554,
        20620
      ]
    },
    {
      "content": "If you do not specify value for the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> property, it is calculated as \"<bpt id=\"p2\">**</bpt>start + 48 hours<ept id=\"p2\">**</ept>\".",
      "pos": [
        20631,
        20728
      ]
    },
    {
      "content": "To run the pipeline indefinitely, specify <bpt id=\"p1\">**</bpt>9/9/9999<ept id=\"p1\">**</ept> as the value for the <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> property.",
      "pos": [
        20729,
        20822
      ]
    },
    {
      "content": "In the example above, there will be 24 data slices as each data slice is produced hourly.",
      "pos": [
        20832,
        20921
      ]
    },
    {
      "pos": [
        20931,
        21043
      ],
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=516971)</ept> for details about JSON properties."
    },
    {
      "content": "Run the following command to create the Data Factory table.",
      "pos": [
        21048,
        21107
      ]
    },
    {
      "pos": [
        21193,
        21334
      ],
      "content": "<bpt id=\"p1\">**</bpt>Congratulations!<ept id=\"p1\">**</ept> You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline."
    },
    {
      "pos": [
        21339,
        21421
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"MonitorDataSetsAndPipeline\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 5: Monitor the datasets and pipeline"
    },
    {
      "content": "In this step, you will use the Azure PowerShell to monitor what’s going on in an Azure data factory.",
      "pos": [
        21422,
        21522
      ]
    },
    {
      "pos": [
        21528,
        21597
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> and assign the output to a variable $df."
    },
    {
      "pos": [
        21714,
        21846
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactorySlice<ept id=\"p1\">**</ept> to get details about all slices of the <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>, which is the output table of the pipeline."
    },
    {
      "content": "Replace year, month, and date part of the <bpt id=\"p1\">**</bpt>StartDateTime<ept id=\"p1\">**</ept> parameter with the current year, month, and date.",
      "pos": [
        21951,
        22060
      ]
    },
    {
      "content": "This should match the <bpt id=\"p1\">**</bpt>Start<ept id=\"p1\">**</ept> value in the pipeline JSON.",
      "pos": [
        22061,
        22120
      ]
    },
    {
      "content": "You should see 24 slices, one for each hour from 12 AM of the current day to 12 AM of the next day.",
      "pos": [
        22127,
        22226
      ]
    },
    {
      "content": "First slice:",
      "pos": [
        22239,
        22251
      ]
    },
    {
      "content": "Last slice:",
      "pos": [
        22639,
        22650
      ]
    },
    {
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryRun<ept id=\"p1\">**</ept> to get the details of activity runs for a <bpt id=\"p2\">**</bpt>specific<ept id=\"p2\">**</ept> slice.",
      "pos": [
        23038,
        23131
      ]
    },
    {
      "content": "Change the value of the <bpt id=\"p1\">**</bpt>StartDateTime<ept id=\"p1\">**</ept> parameter to match the <bpt id=\"p2\">**</bpt>Start<ept id=\"p2\">**</ept> time of the slice from the output above.",
      "pos": [
        23132,
        23247
      ]
    },
    {
      "content": "The value of the <bpt id=\"p1\">**</bpt>StartDateTime<ept id=\"p1\">**</ept> must be in <bpt id=\"p2\">[</bpt>ISO format<ept id=\"p2\">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.",
      "pos": [
        23248,
        23346
      ]
    },
    {
      "content": "For example: 2014-03-03T22:00:00Z.",
      "pos": [
        23347,
        23381
      ]
    },
    {
      "content": "You should see output similar to the following:",
      "pos": [
        23482,
        23529
      ]
    },
    {
      "pos": [
        24359,
        24469
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Cmdlet Reference<ept id=\"p1\">][cmdlet-reference]</ept> for comprehensive documentation on Data Factory cmdlets."
    },
    {
      "content": "Send Feedback",
      "pos": [
        24475,
        24488
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        24489,
        24546
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-monitor-manage-using-powershell.md)</ept>.",
      "pos": [
        24547,
        24702
      ]
    },
    {
      "content": "test",
      "pos": [
        25555,
        25559
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Tutorial: Create a pipeline with Copy Activity using Azure PowerShell\" \n    description=\"In this tutorial, you will create an Azure Data Factory pipeline with a Copy Activity by using Azure PowerShell.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/27/2015\" \n    ms.author=\"spelluru\"/>\n\n# Tutorial: Create and monitor a data factory using Azure PowerShell\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-get-started.md)\n- [Using Data Factory Editor](data-factory-get-started-using-editor.md)\n- [Using PowerShell](data-factory-monitor-manage-using-powershell.md)\n- [Using Visual Studio](data-factory-get-started-using-vs.md)\n\n\nThe [Get started with Azure Data Factory][adf-get-started] tutorial shows you how to create and monitor an Azure data factory using the [Azure Preview Portal][azure-preview-portal]. \nIn this tutorial, you will create and monitor an Azure data factory by using Azure PowerShell cmdlets. The pipeline in the data factory you create in this tutorial copies data from an Azure blob to an Azure SQL database.       \n\n> [AZURE.NOTE] This article does not cover all the Data Factory cmdlets. See [Data Factory Cmdlet Reference][cmdlet-reference] for comprehensive documentation on Data Factory cmdlets. \n\n##Prerequisites\nApart from prerequisites listed in the Tutorial Overview topic, you need to have Azure PowerShell installed on your computer. If you do not have it already, download and install [Azure PowerShell][download-azure-powershell] on your computer.\n\n##In This Tutorial\nThe following table lists the steps you will perform as part of the tutorial and their descriptions. \n\nStep | Description\n-----| -----------\n[Step 1: Create an Azure Data Factory](#CreateDataFactory) | In this step, you will create an Azure data factory named **ADFTutorialDataFactoryPSH**. \n[Step 2: Create linked services](#CreateLinkedServices) | In this step, you will create two linked services: **StorageLinkedService** and **AzureSqlLinkedService**. The StorageLinkedService links an Azure storage and AzureSqlLinkedService links an Azure SQL database to the ADFTutorialDataFactoryPSH.\n[Step 3: Create input and output datasets](#CreateInputAndOutputDataSets) | In this step, you will define two data sets (**EmpTableFromBlob** and **EmpSQLTable**) that are used as input and output tables for the **Copy Activity** in the ADFTutorialPipeline that you will create in the next step.\n[Step 4: Create and run a pipeline](#CreateAndRunAPipeline) | In this step, you will create a pipeline named **ADFTutorialPipeline** in the data factory: **ADFTutorialDataFactoryPSH**. . The pipeline will have a **Copy Activity** that copies data from an Azure blob to an output Azure database table.\n[Step 5: Monitor data sets and pipeline](#MonitorDataSetsAndPipeline) | In this step, you will monitor the datasets and the pipeline using Azure PowerShell in this step.\n\n## <a name=\"CreateDataFactory\"></a>Step 1: Create an Azure Data Factory\nIn this step, you use the Azure PowerShell to create an Azure data factory named **ADFTutorialDataFactoryPSH**.\n\n1. Launch **Azure PowerShell** and execute the following commands. Keep the Azure PowerShell open until the end of this tutorial. If you close and reopen, you need to run these commands again.\n    - Run **Add-AzureAccount** and enter the  user name and password that you use to sign-in to the Azure Preview Portal.  \n    - Run **Get-AzureSubscription** to view all the subscriptions for this account.\n    - Run **Select-AzureSubscription** to select the subscription that you want to work with. This subscription should be the same as the one you used in the Azure Preview Portal. \n2. Switch to **AzureResourceManager** mode as the Azure Data Factory cmdlets are available in this mode.\n\n        Switch-AzureMode AzureResourceManager \n3. Create an Azure resource group named: **ADFTutorialResourceGroup** by running the following command.\n   \n        New-AzureResourceGroup -Name ADFTutorialResourceGroup  -Location \"West US\"\n\n    Some of the steps in this tutorial assume that you use the resource group named **ADFTutorialResourceGroup**. If you use a different resource group, you will need to use it in place of ADFTutorialResourceGroup in this tutorial. \n4. Run the **New-AzureDataFactory** cmdlet to create a data factory named: **ADFTutorialDataFactoryPSH**.  \n\n        New-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name ADFTutorialDataFactoryPSH –Location \"West US\"\n\n\n    The name of the Azure data factory must be globally unique. If you receive the error: **Data factory name “ADFTutorialDataFactoryPSH” is not available**, change the name (for example, yournameADFTutorialDataFactoryPSH). Use this name in place of ADFTutorialFactoryPSH while performing steps in this tutorial.\n\n## <a name=\"CreateLinkedServices\"></a>Step 2: Create linked services\nLinked services link data stores or compute services to an Azure data factory. A data store can be an Azure Storage, Azure SQL Database or an on-premises SQL Server database that contains input data or stores output data for a Data Factory pipeline. A compute service is the service that processes  input data and produces output data. \n\nIn this step, you will create two linked services: **StorageLinkedService** and **AzureSqlLinkedService**. StorageLinkedService linked service links an Azure Storage Account and AzureSqlLinkedService links an Azure SQL database to the data factory: **ADFTutorialDataFactoryPSH**. You will create a pipeline later in this tutorial that copies data from a blob container in StorageLinkedService to a SQL table in AzureSqlLinkedService.\n\n### Create a linked service for an Azure storage account\n1.  Create a JSON file named **StorageLinkedService.json** in the **C:\\ADFGetStartedPSH** with the following content. Create the folder ADFGetStartedPSH if it does not already exist.\n\n        {\n          \"name\": \"StorageLinkedService\",\n          \"properties\": {\n            \"type\": \"AzureStorage\",\n            \"typeProperties\": {\n              \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n            }\n          }\n        }\n\n    Replace **accountname** with the name of your storage account and **accountkey** with the key for your Azure storage account.\n2.  In the **Azure PowerShell**, switch to the **ADFGetStartedPSH** folder. \n3.  You can use the **New-AzureDataFactoryLinkedService** cmdlet to create a linked service. This cmdlet and other Data Factory cmdlets you use in this tutorial require you to pass values for the **ResourceGroupName** and **DataFactoryName** parameters. Alternatively, you can use **Get-AzureDataFactory** to get a DataFactory object and pass the object without typing ResourceGroupName and DataFactoryName each time you run a cmdlet. Run the following command to assign the output of the **Get-AzureDataFactory** cmdlet to a variable: **$df**. \n\n        $df=Get-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name ADFTutorialDataFactoryPSH\n\n4.  Now, run the **New-AzureDataFactoryLinkedService** cmdlet to create the linked service: **StorageLinkedService**. \n\n        New-AzureDataFactoryLinkedService $df -File .\\StorageLinkedService.json\n\n    If you hadn't run the **Get-AzureDataFactory** cmdlet and assigned the output to **$df** variable, you would have to specify values for the ResourceGroupName and DataFactoryName parameters as follows.   \n        \n        New-AzureDataFactoryLinkedService -ResourceGroupName ADFTutorialResourceGroup -DataFactoryName ADFTutorialDataFactoryPSH -File .\\StorageLinkedService.json\n\n    If you close the Azure PowerShell in the middle of the tutorial, you will have run the Get-AzureDataFactory cmdlet next time you launch Azure PowerShell to complete the tutorial.\n\n### Create a linked service for an Azure SQL Database\n1.  Create a JSON file named AzureSqlLinkedService.json with the following content.\n\n        {\n          \"name\": \"AzureSqlLinkedService\",\n          \"properties\": {\n            \"type\": \"AzureSqlDatabase\",\n            \"typeProperties\": {\n              \"connectionString\": \"Server=tcp:<server>.database.windows.net,1433;Database=<databasename>;User ID=user@server;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30\"\n            }\n          }\n        }\n\n    Replace **servername**, **databasename**, **username@servername**, and **password** with names of your Azure SQL server, database, user account, and  password.\n\n2.  Run the following command to create a linked service. \n    \n        New-AzureDataFactoryLinkedService $df -File .\\AzureSqlLinkedService.json\n\n    Confirm that the **Allow access to Azure services** setting is turned ON for your Azure SQL server. To verify and turn it on, do the following:\n\n    1. Click **BROWSE** hub on the left and click **SQL servers**.\n    2. Select your server, and click **SETTINGS** on the SQL SERVER blade.\n    3. In the **SETTINGS** blade, click **Firewall**.\n    4. In the **Firewalll settings** blade, click **ON** for **Allow access to Azure services**.\n    5. Click **ACTIVE** hub on the left to switch to the **Data Factory** blade you were on.\n    \n\n## <a name=\"CreateInputAndOutputDataSets\"></a>Step 3: Create input and output tables\n\nIn the previous step, you created linked services **StorageLinkedService** and **AzureSqlLinkedService** to link an Azure Storage account and Azure SQL database to the data factory: **ADFTutorialDataFactoryPSH**. In this step, you will create tables that represent the input and output data for the Copy Activity in the pipeline you will be creating in the next step. \n\nA table is a rectangular dataset and it is the only type of dataset that is supported at this time. The input table in this tutorial refers to a blob container in the Azure Storage that StorageLinkedService points to and the output table refers to a SQL table in the Azure SQL database that AzureSqlLinkedService points to.  \n\n### Prepare Azure Blob Storage and Azure SQL Database for the tutorial\nSkip this step if you have gone through the tutorial from [Get started with Azure Data Factory][adf-get-started] article. \n\nYou need to perform the following steps to prepare the Azure blob storage and Azure SQL database for this tutorial. \n \n* Create a blob container named **adftutorial** in the Azure blob storage that **StorageLinkedService** points to. \n* Create and upload a text file, **emp.txt**, as a blob to the **adftutorial** container. \n* Create a table named **emp** in the Azure SQL Database in the Azure SQL database that **AzureSqlLinkedService** points to.\n\n\n1. Launch Notepad, paste the following text, and save it as **emp.txt** to **C:\\ADFGetStartedPSH** folder on your hard drive. \n\n        John, Doe\n        Jane, Doe\n                \n2. Use tools such as [Azure Storage Explorer](https://azurestorageexplorer.codeplex.com/) to create the **adftutorial** container and to upload the **emp.txt** file to the container.\n\n    ![Azure Storage Explorer][image-data-factory-get-started-storage-explorer]\n3. Use the following SQL script to create the **emp** table in your Azure SQL Database.  \n\n\n        CREATE TABLE dbo.emp \n        (\n            ID int IDENTITY(1,1) NOT NULL,\n            FirstName varchar(50),\n            LastName varchar(50),\n        )\n        GO\n\n        CREATE CLUSTERED INDEX IX_emp_ID ON dbo.emp (ID); \n\n    If you have SQL Server 2014 installed on your computer: follow instructions from [Step 2: Connect to SQL Database of the Managing Azure SQL Database using SQL Server Management Studio][sql-management-studio] article to connect to your Azure SQL server and run the SQL script.\n\n    If you have Visual Studio 2013 installed on your computer: in the Azure Preview Portal ([http://portal.azure.com](http://portal.sazure.com)), click **BROWSE** hub on the left, click **SQL servers**, select your database, and click **Open in Visual Studio** button on toolbar to connect to your Azure SQL server and run the script. If your client is not allowed to access the Azure SQL server, you will need to configure firewall for your Azure SQL server to allow access from your machine (IP Address). See the article above for steps to configure the firewall for your Azure SQL server.\n        \n### Create input table \nA table is a rectangular dataset and has a schema. In this step, you will create a table named **EmpBlobTable** that points to a blob container in the Azure Storage represented by the **StorageLinkedService** linked service. This blob container (**adftutorial**) contains the input data in the file: **emp.txt**. \n\n1.  Create a JSON file named **EmpBlobTable.json** in the **C:\\ADFGetStartedPSH** folder with the following content:\n\n        {\n          \"name\": \"EmpTableFromBlob\",\n          \"properties\": {\n            \"structure\": [\n              {\n                \"name\": \"FirstName\",\n                \"type\": \"String\"\n              },\n              {\n                \"name\": \"LastName\",\n                \"type\": \"String\"\n              }\n            ],\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"AzureStorageLinkedService1\",\n            \"typeProperties\": {\n              \"folderPath\": \"adftutorial/\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"external\": true,\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            }\n          }\n        }\n    \n    Note the following: \n    \n    - dataset **type** is set to **AzureBlob**.\n    - **linkedServiceName** is set to **StorageLinkedService**. \n    - **folderPath** is set to the **adftutorial** container. You can also specify the name of a blob within the folder. Since you are not specifying the name of the blob, data from all blobs in the container is considered as an input data.  \n    - format **type** is set to **TextFormat**\n    - There are two fields in the text file – **FirstName** and **LastName** – separated by a comma character (**columnDelimiter**) \n    - The **availability** is set to **hourly** (**frequency** is set to **hour** and **interval** is set to **1** ), so the Data Factory service will look for input data every hour in the root folder in the blob container (**adftutorial**) you specified.\n\n    if you don't specify a **fileName** for an **input** **table**, all files/blobs from the input folder (**folderPath**) are considered as inputs. If you specify a fileName in the JSON, only the specified file/blob is considered asn input. See the sample files in the [tutorial][adf-tutorial] for examples.\n \n    If you do not specify a **fileName** for an **output table**, the generated files in the **folderPath** are named in the following format: Data.<Guid\\>.txt (example: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).\n\n    To set **folderPath** and **fileName** dynamically based on the **SliceStart** time, use the **partitionedBy** property. In the following example, folderPath uses Year, Month, and Day from from the SliceStart (start time of the slice being processed) and fileName uses Hour from the SliceStart. For example, if a slice is being produced for 2014-10-20T08:00:00, the folderName is set to wikidatagateway/wikisampledataout/2014/10/20 and the fileName is set to 08.csv. \n\n        \"folderPath\": \"wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}\",\n        \"fileName\": \"{Hour}.csv\",\n        \"partitionedBy\": \n        [\n            { \"name\": \"Year\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyy\" } },\n            { \"name\": \"Month\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"MM\" } }, \n            { \"name\": \"Day\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"dd\" } }, \n            { \"name\": \"Hour\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"hh\" } } \n        ],\n\n    See [JSON Scripting Reference](http://go.microsoft.com/fwlink/?LinkId=516971) for details about JSON properties.\n\n2.  Run the following command to create the Data Factory table.\n\n        New-AzureDataFactoryTable $df -File .\\EmpBlobTable.json\n\n### Create output table\nIn this part of the step, you will create an output table named **EmpSQLTable** that points to a SQL table (**emp**) in the Azure SQL database that is represented by the **AzureSqlLinkedService** linked service. The pipeline copies data from the input blob to the **emp** table. \n\n1.  Create a JSON file named **EmpSQLTable.json** in the **C:\\ADFGetStartedPSH** folder with the following content.\n        \n        {\n          \"name\": \"EmpSQLTable\",\n          \"properties\": {\n            \"structure\": [\n              {\n                \"name\": \"FirstName\",\n                \"type\": \"String\"\n              },\n              {\n                \"name\": \"LastName\",\n                \"type\": \"String\"\n              }\n            ],\n            \"type\": \"AzureSqlTable\",\n            \"linkedServiceName\": \"AzureSqlLinkedService1\",\n            \"typeProperties\": {\n              \"tableName\": \"emp\"\n            },\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            }\n          }\n        }\n\n     Note the following: \n    \n    * dataset **type** is set to **AzureSqlTable**.\n    * **linkedServiceName** is set to **AzureSqlLinkedService**.\n    * **tablename** is set to **emp**.\n    * There are three columns – **ID**, **FirstName**, and **LastName** – in the emp table in the database, but ID is an identity column, so you need to specify only **FirstName** and **LastName** here.\n    * The **availability** is set to **hourly** (**frequency** set to **hour** and **interval** set to **1**).  The Data Factory service will generate an output data slice every hour in the **emp** table in the Azure SQL database.\n\n2.  Run the following command to create the Data Factory table. \n    \n        New-AzureDataFactoryTable $df -File .\\EmpSQLTable.json\n\n\n## <a name=\"CreateAndRunAPipeline\"></a>Step 4: Create and run a pipeline\nIn this step, you create a pipeline with a **Copy Activity** that uses **EmpTableFromBlob** as input and **EmpSQLTable** as output.\n\n1.  Create a JSON file named **ADFTutorialPipeline.json** in the **C:\\ADFGetStartedPSH** folder with the following content: \n    \n         {\n          \"name\": \"ADFTutorialPipeline\",\n          \"properties\": {\n            \"description\": \"Copy data from a blob to Azure SQL table\",\n            \"activities\": [\n              {\n                \"name\": \"CopyFromBlobToSQL\",\n                \"description\": \"Push Regional Effectiveness Campaign data to Azure SQL database\",\n                \"type\": \"Copy\",\n                \"inputs\": [\n                  {\n                    \"name\": \"EmpTableFromBlob\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"EmpSQLTable\"\n                  }\n                ],\n                \"typeProperties\": {\n                  \"source\": {\n                    \"type\": \"BlobSource\"\n                  },\n                  \"sink\": {\n                    \"type\": \"SqlSink\",\n                    \"writeBatchSize\": 10000,\n                    \"writeBatchTimeout\": \"00:60:00\"\n                  }\n                },\n                \"Policy\": {\n                  \"concurrency\": 1,\n                  \"executionPriorityOrder\": \"NewestFirst\",\n                  \"style\": \"StartOfInterval\",\n                  \"retry\": 0,\n                  \"timeout\": \"01:00:00\"\n                }\n              }\n            ],\n            \"start\": \"2015-07-12T00:00:00Z\",\n            \"end\": \"2015-07-13T00:00:00Z\",\n            \"isPaused\": false\n          }\n        }\n\n    Note the following:\n\n    - In the activities section, there is only one activity whose **type** is set to **Copy**.\n    - Input for the activity is set to **EmpTableFromBlob** and output for the activity is set to **EmpSQLTable**.\n    - In the **transformation** section, **BlobSource** is specified as the source type and **SqlSink** is specified as the sink type.\n\n    Replace the value of the **start** property with the current day and **end** value with the next day. Both start and end datetimes must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-10-14T16:32:41Z. The **end** time is optional, but we will use it in this tutorial. \n    \n    If you do not specify value for the **end** property, it is calculated as \"**start + 48 hours**\". To run the pipeline indefinitely, specify **9/9/9999** as the value for the **end** property.\n    \n    In the example above, there will be 24 data slices as each data slice is produced hourly.\n    \n    See [JSON Scripting Reference](http://go.microsoft.com/fwlink/?LinkId=516971) for details about JSON properties.\n2.  Run the following command to create the Data Factory table. \n        \n        New-AzureDataFactoryPipeline $df -File .\\ADFTutorialPipeline.json\n\n**Congratulations!** You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline.\n\n## <a name=\"MonitorDataSetsAndPipeline\"></a>Step 5: Monitor the datasets and pipeline\nIn this step, you will use the Azure PowerShell to monitor what’s going on in an Azure data factory.\n\n1.  Run **Get-AzureDataFactory** and assign the output to a variable $df.\n\n        $df=Get-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name ADFTutorialDataFactoryPSH\n \n2.  Run **Get-AzureDataFactorySlice** to get details about all slices of the **EmpSQLTable**, which is the output table of the pipeline.  \n\n        Get-AzureDataFactorySlice $df -TableName EmpSQLTable -StartDateTime 2015-03-03T00:00:00\n\n    Replace year, month, and date part of the **StartDateTime** parameter with the current year, month, and date. This should match the **Start** value in the pipeline JSON. \n\n    You should see 24 slices, one for each hour from 12 AM of the current day to 12 AM of the next day. \n    \n    **First slice:**\n\n        ResourceGroupName : ADFTutorialResourceGroup\n        DataFactoryName   : ADFTutorialDataFactoryPSH\n        TableName         : EmpSQLTable\n        Start             : 3/3/2015 12:00:00 AM\n        End               : 3/3/2015 1:00:00 AM\n        RetryCount        : 0\n        Status            : PendingExecution\n        LatencyStatus     :\n        LongRetryCount    : 0\n\n    **Last slice:**\n\n        ResourceGroupName : ADFTutorialResourceGroup\n        DataFactoryName   : ADFTutorialDataFactoryPSH\n        TableName         : EmpSQLTable\n        Start             : 3/4/2015 11:00:00 PM\n        End               : 3/4/2015 12:00:00 AM\n        RetryCount        : 0\n        Status            : PendingExecution\n        LatencyStatus     : \n        LongRetryCount    : 0\n\n3.  Run **Get-AzureDataFactoryRun** to get the details of activity runs for a **specific** slice. Change the value of the **StartDateTime** parameter to match the **Start** time of the slice from the output above. The value of the **StartDateTime** must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-03-03T22:00:00Z.\n\n        Get-AzureDataFactoryRun $df -TableName EmpSQLTable -StartDateTime 2015-03-03T22:00:00\n\n    You should see output similar to the following:\n\n        Id                  : 3404c187-c889-4f88-933b-2a2f5cd84e90_635614488000000000_635614524000000000_EmpSQLTable\n        ResourceGroupName   : ADFTutorialResourceGroup\n        DataFactoryName     : ADFTutorialDataFactoryPSH\n        TableName           : EmpSQLTable\n        ProcessingStartTime : 3/3/2015 11:03:28 PM\n        ProcessingEndTime   : 3/3/2015 11:04:36 PM\n        PercentComplete     : 100\n        DataSliceStart      : 3/8/2015 10:00:00 PM\n        DataSliceEnd        : 3/8/2015 11:00:00 PM\n        Status              : Succeeded\n        Timestamp           : 3/8/2015 11:03:28 PM\n        RetryAttempt        : 0\n        Properties          : {}\n        ErrorMessage        :\n        ActivityName        : CopyFromBlobToSQL\n        PipelineName        : ADFTutorialPipeline\n        Type                : Copy\n\nSee [Data Factory Cmdlet Reference][cmdlet-reference] for comprehensive documentation on Data Factory cmdlets. \n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-monitor-manage-using-powershell.md). \n\n\n[adf-tutorial]: data-factory-tutorial.md\n[use-custom-activities]: data-factory-use-custom-activities.md\n[troubleshoot]: data-factory-troubleshoot.md\n[developer-reference]: http://go.microsoft.com/fwlink/?LinkId=516908\n\n[cmdlet-reference]: https://msdn.microsoft.com/library/dn820234.aspx\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n[data-factory-create-storage]: ../storage-create-storage-account.md\n\n[adf-get-started]: data-factory-get-started.md\n[azure-preview-portal]: http://portal.azure.com\n[download-azure-powershell]: ../powershell-install-configure.md\n[data-factory-introduction]: data-factory-introduction.md\n\n[image-data-factory-get-started-storage-explorer]: ./media/data-factory-monitor-manage-using-powershell/getstarted-storage-explorer.png\n\n[sql-management-studio]: ../sql-database-manage-azure-ssms.md#Step2\n \ntest\n"
}