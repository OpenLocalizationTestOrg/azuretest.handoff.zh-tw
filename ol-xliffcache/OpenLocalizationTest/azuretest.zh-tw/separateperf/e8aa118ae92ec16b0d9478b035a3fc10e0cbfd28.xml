{
  "nodes": [
    {
      "content": "NAMD with Microsoft HPC Pack on Linux VMs | Microsoft Azure",
      "pos": [
        24,
        83
      ]
    },
    {
      "content": "Deploy a Microsoft HPC Pack cluster on Azure and run a NAMD simulation with charmrun on multiple Linux compute nodes.",
      "pos": [
        99,
        216
      ]
    },
    {
      "content": "Run NAMD with Microsoft HPC Pack on Linux compute nodes in Azure",
      "pos": [
        525,
        589
      ]
    },
    {
      "pos": [
        591,
        877
      ],
      "content": "This article shows you how to deploy a Microsoft HPC Pack cluster on Azure and run a <bpt id=\"p1\">[</bpt>NAMD<ept id=\"p1\">](http://www.ks.uiuc.edu/Research/namd/)</ept> job with <bpt id=\"p2\">**</bpt>charmrun<ept id=\"p2\">**</ept> on multiple Linux compute nodes in a virtual cluster network to calculate and visualize the structure of a large biomolecular system."
    },
    {
      "content": "NAMD (for Nanoscale Molecular Dynamics program) is a parallel molecular dynamics package designed for high-performance simulation of large biomolecular systems containing up to millions of atoms, such as viruses, cell structures, and large proteins.",
      "pos": [
        879,
        1128
      ]
    },
    {
      "content": "NAMD scales to hundreds of cores for typical simulations and to more than 500,000 cores for the largest simulations.",
      "pos": [
        1129,
        1245
      ]
    },
    {
      "content": "Microsoft HPC Pack provides features to run a variety of large-scale HPC and parallel applications, including MPI applications, on clusters of Microsoft Azure virtual machines.",
      "pos": [
        1247,
        1423
      ]
    },
    {
      "content": "Starting in Microsoft HPC Pack 2012 R2, HPC Pack also supports running Linux HPC applications on Linux compute node VMs deployed in an HPC Pack cluster.",
      "pos": [
        1424,
        1576
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id=\"p1\">](virtual-machines-linux-cluster-hpcpack.md)</ept> for an introduction to using Linux compute nodes with HPC Pack.",
      "pos": [
        1577,
        1758
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        1764,
        1777
      ]
    },
    {
      "pos": [
        1781,
        2124
      ],
      "content": "<bpt id=\"p1\">**</bpt>HPC Pack cluster with Linux compute nodes<ept id=\"p1\">**</ept> - See <bpt id=\"p2\">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id=\"p2\">](virtual-machines-linux-cluster-hpcpack.md)</ept> for the prerequisites and steps to deploy an HPC Pack cluster with Linux compute nodes on Azure by using an Azure PowerShell script and HPC Pack images in the Azure Marketplace."
    },
    {
      "content": "Following is a sample XML configuration file you can use with the script to deploy an Azure-based HPC Pack cluster consisting of a Windows Server 2012 R2 head node and 4 size Large (A3) CentOS 6.6 compute nodes.",
      "pos": [
        2130,
        2341
      ]
    },
    {
      "content": "Substitute appropriate values for your subscription and service names.",
      "pos": [
        2342,
        2412
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>NAMD software and tutorial files<ept id=\"p1\">**</ept> - Download NAMD software for Linux from the <bpt id=\"p2\">[</bpt>NAMD<ept id=\"p2\">](http://www.ks.uiuc.edu/Research/namd/)</ept> site.",
      "pos": [
        3493,
        3625
      ]
    },
    {
      "content": "This article is based on NAMD version 2.10, and uses the <bpt id=\"p1\">[</bpt>Linux-x86_64 (64-bit Intel/AMD with Ethernet)<ept id=\"p1\">](http://www.ks.uiuc.edu/Development/Download/download.cgi?UserID=&amp;AccessCode=&amp;ArchiveID=1310)</ept> archive, which you'll use to run NAMD on multiple Linux compute nodes in a cluster network.",
      "pos": [
        3626,
        3915
      ]
    },
    {
      "content": "Also download the <bpt id=\"p1\">[</bpt>NAMD tutorial files<ept id=\"p1\">](http://www.ks.uiuc.edu/Training/Tutorials/#namd)</ept>.",
      "pos": [
        3916,
        4005
      ]
    },
    {
      "content": "Follow the instructions later in this article to extract the archive and the tutorial samples on the cluster head node.",
      "pos": [
        4006,
        4125
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>VMD<ept id=\"p1\">**</ept> (optional) - To see the results of your NAMD job, download and install the molecular visualization program <bpt id=\"p2\">[</bpt>VMD<ept id=\"p2\">](http://www.ks.uiuc.edu/Research/vmd/)</ept> on a computer of your choice.",
      "pos": [
        4129,
        4317
      ]
    },
    {
      "content": "The current version is 1.9.2.",
      "pos": [
        4318,
        4347
      ]
    },
    {
      "content": "See the VMD download site to get started.",
      "pos": [
        4348,
        4389
      ]
    },
    {
      "content": "Set up mutual trust between compute nodes",
      "pos": [
        4397,
        4438
      ]
    },
    {
      "content": "Running a cross-node job on multiple Linux nodes requires the nodes to trust each other (by <bpt id=\"p1\">**</bpt>rsh<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>ssh<ept id=\"p2\">**</ept>).",
      "pos": [
        4439,
        4551
      ]
    },
    {
      "content": "When you create the HPC Pack cluster with the Microsoft HPC Pack IaaS deployment script, the script automatically sets up permanent mutual trust for the administrator account you specify.",
      "pos": [
        4552,
        4739
      ]
    },
    {
      "content": "For non-administrator users you create in the cluster's domain, you have to set up temporary mutual trust among the nodes when a job is allocated to them, and destroy the relationship after the job is complete.",
      "pos": [
        4740,
        4950
      ]
    },
    {
      "content": "To do this for each user, provide an RSA key pair to the cluster which HPC Pack uses to establish the trust relationship.",
      "pos": [
        4951,
        5072
      ]
    },
    {
      "content": "Generate an RSA key pair",
      "pos": [
        5078,
        5102
      ]
    },
    {
      "pos": [
        5103,
        5233
      ],
      "content": "It's easy to generate an RSA key pair, which contains a public key and a private key, by running the Linux <bpt id=\"p1\">**</bpt>ssh-keygen<ept id=\"p1\">**</ept> command."
    },
    {
      "content": "Log on to a Linux computer.",
      "pos": [
        5239,
        5266
      ]
    },
    {
      "content": "Run the following command.",
      "pos": [
        5272,
        5298
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Press <bpt id=\"p1\">**</bpt>Enter<ept id=\"p1\">**</ept> to use the default settings until the command is completed.",
      "pos": [
        5344,
        5432
      ]
    },
    {
      "content": "Do not enter a passphrase here; when prompted for a password, just press <bpt id=\"p1\">**</bpt>Enter<ept id=\"p1\">**</ept>.",
      "pos": [
        5433,
        5516
      ]
    },
    {
      "content": "![Generate an RSA key pair][keygen]",
      "pos": [
        5522,
        5557
      ]
    },
    {
      "content": "Change directory to the ~/.ssh directory.",
      "pos": [
        5563,
        5604
      ]
    },
    {
      "content": "The private key is stored in id_rsa and the public key in id_rsa.pub.",
      "pos": [
        5605,
        5674
      ]
    },
    {
      "content": "![Private and public keys][keys]",
      "pos": [
        5680,
        5712
      ]
    },
    {
      "content": "Add the key pair to the HPC Pack cluster",
      "pos": [
        5718,
        5758
      ]
    },
    {
      "content": "Make a Remote Desktop connnection to your head node with your HPC Pack administrator account (the administrator account you set up when you ran the deployment script).",
      "pos": [
        5763,
        5930
      ]
    },
    {
      "content": "Use standard Windows Server procedures to create a domain user account in the cluster's Active Directory domain.",
      "pos": [
        5935,
        6047
      ]
    },
    {
      "content": "For example, use the Active Directory User and Computers tool on the head node.",
      "pos": [
        6048,
        6127
      ]
    },
    {
      "content": "The examples in this article assume you create a domain user named hpclab\\hpcuser.",
      "pos": [
        6128,
        6210
      ]
    },
    {
      "content": "Create a file named C:\\cred.xml and copy the RSA key data into it.",
      "pos": [
        6216,
        6282
      ]
    },
    {
      "content": "You can find an example of this file in the Appendix at the end of this article.",
      "pos": [
        6283,
        6363
      ]
    },
    {
      "content": "Open a Command window and enter the following command to set the credentials data for the hpclab\\hpcuser account.",
      "pos": [
        6560,
        6673
      ]
    },
    {
      "content": "You use the <bpt id=\"p1\">**</bpt>extendeddata<ept id=\"p1\">**</ept> parameter to pass the name of C:\\cred.xml file you created for the key data.",
      "pos": [
        6674,
        6779
      ]
    },
    {
      "content": "This command completes successfully without output.",
      "pos": [
        6895,
        6946
      ]
    },
    {
      "content": "After setting the credentials for the user accounts you need to run jobs, store the cred.xml file in a secure location, or delete it.",
      "pos": [
        6947,
        7080
      ]
    },
    {
      "content": "If you generated the RSA key pair on one of your Linux nodes, remember to delete the keys after you finish using them.",
      "pos": [
        7086,
        7204
      ]
    },
    {
      "content": "HPC Pack does not set up mutual trust if it finds an existing id_rsa file or id_rsa.pub file.",
      "pos": [
        7205,
        7298
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> We don’t recommend running a Linux job as a cluster administrator on a shared cluster, because a job submitted by an administrator runs under the root account on the Linux nodes.",
      "pos": [
        7301,
        7497
      ]
    },
    {
      "content": "A job submitted by a non-administrator user runs under a local Linux user account with the same name as the job user, and HPC Pack sets up mutual trust for this Linux user across all the nodes allocated to the job.",
      "pos": [
        7498,
        7712
      ]
    },
    {
      "content": "You can set up the Linux user manually on the Linux nodes before running the job, or HPC Pack creates the user automatically when the job is submitted.",
      "pos": [
        7713,
        7864
      ]
    },
    {
      "content": "If HPC Pack creates the user, HPC Pack deletes it after the job completes.",
      "pos": [
        7865,
        7939
      ]
    },
    {
      "content": "The keys are removed after job completion on the nodes to reduce security threats.",
      "pos": [
        7940,
        8022
      ]
    },
    {
      "content": "Set up a file share for Linux nodes",
      "pos": [
        8027,
        8062
      ]
    },
    {
      "content": "Now set up a standard SMB share on a folder on the head node, and mount the shared folder on all Linux nodes to allow the Linux nodes to access NAMD files with a common path.",
      "pos": [
        8064,
        8238
      ]
    },
    {
      "content": "See the file sharing options and steps in <bpt id=\"p1\">[</bpt>Get started with Linux compute nodes in an HPC Pack Cluster in Azure<ept id=\"p1\">](virtual-machines-linux-cluster-hpcpack.md)</ept>.",
      "pos": [
        8239,
        8395
      ]
    },
    {
      "content": "(We recommend mounting a shared folder on the head node in this article because CentOS 6.6 Linux nodes don’t currently support the Azure File service, which provides similar features.",
      "pos": [
        8396,
        8579
      ]
    },
    {
      "content": "For more about mounting an Azure File share, see <bpt id=\"p1\">[</bpt>Persisting connections to Microsoft Azure Files<ept id=\"p1\">](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/27/persisting-connections-to-microsoft-azure-files.aspx)</ept>.)",
      "pos": [
        8580,
        8797
      ]
    },
    {
      "content": "Create a folder on the head node, and share it to everyone by setting Read/Write privileges.",
      "pos": [
        8803,
        8895
      ]
    },
    {
      "content": "In this example, \\\\\\\\CentOS66HN\\Namd is the name of the folder, where CentOS66HN is the host name of the head node.",
      "pos": [
        8896,
        9011
      ]
    },
    {
      "content": "Extract the NAMD files in the folder by using a Windows version of <bpt id=\"p1\">**</bpt>tar<ept id=\"p1\">**</ept> or another Windows utility that operates on .tar archives.",
      "pos": [
        9016,
        9149
      ]
    },
    {
      "content": "Extract the NAMD tar archive to \\\\\\\\CentOS66HN\\Namd\\namd2, and extract the tutorial files under \\\\\\\\CentOS66HN\\Namd\\namd2\\namdsample.",
      "pos": [
        9150,
        9283
      ]
    },
    {
      "content": "Open a Windows PowerShell window and run the following commands to mount the shared folder.",
      "pos": [
        9289,
        9380
      ]
    },
    {
      "content": "The first command creates a folder named /namd2 on all nodes in the LinuxNodes group.",
      "pos": [
        9626,
        9711
      ]
    },
    {
      "content": "The second command mounts the shared folder //CentOS66HN/Namd/namd2 onto the folder with dir_mode and file_mode bits set to 777.",
      "pos": [
        9712,
        9840
      ]
    },
    {
      "content": "The <bpt id=\"p1\">*</bpt>username<ept id=\"p1\">*</ept> and <bpt id=\"p2\">*</bpt>password<ept id=\"p2\">*</ept> in the command should be the credentials of a user on the head node.",
      "pos": [
        9841,
        9939
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>The “\\`” symbol in the second command is an escape symbol for PowerShell.",
      "pos": [
        9942,
        10027
      ]
    },
    {
      "content": "“\\`,” means the “,” (comma character) is a part of the command.",
      "pos": [
        10028,
        10091
      ]
    },
    {
      "content": "Prepare to run a NAMD job",
      "pos": [
        10097,
        10122
      ]
    },
    {
      "content": "Your  NAMD job needs a <bpt id=\"p1\">*</bpt>nodelist<ept id=\"p1\">*</ept> file for <bpt id=\"p2\">**</bpt>charmrun<ept id=\"p2\">**</ept> to know the number of nodes to use when starting NAMD processes.",
      "pos": [
        10125,
        10245
      ]
    },
    {
      "content": "You'll write a Bash script that generates the nodelist file and runs <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept> with this nodelist file.",
      "pos": [
        10246,
        10352
      ]
    },
    {
      "content": "You can then submit a NAMD job in HPC Cluster Manager that calls this script.",
      "pos": [
        10353,
        10430
      ]
    },
    {
      "content": "Environment variables and nodelist file",
      "pos": [
        10436,
        10475
      ]
    },
    {
      "content": "Information about nodes and cores is in the $CCP_NODES_CORES environment variable, which is automatically set by the HPC Pack head node when the job is activated.",
      "pos": [
        10476,
        10638
      ]
    },
    {
      "content": "The format for the $CCP_NODES_CORES variable is as follows:",
      "pos": [
        10639,
        10698
      ]
    },
    {
      "content": "This lists the total number of nodes, node names, and number of cores on each node that are allocated to the job.",
      "pos": [
        10794,
        10907
      ]
    },
    {
      "content": "For example, if the job needs 10 cores to run, the value of $CCP_NODES_CORES will be similar to:",
      "pos": [
        10908,
        11004
      ]
    },
    {
      "content": "Following is the information in the nodelist file, which the script will generate:",
      "pos": [
        11065,
        11147
      ]
    },
    {
      "content": "For example:",
      "pos": [
        11261,
        11273
      ]
    },
    {
      "content": "Bash script to create a nodelist file",
      "pos": [
        11382,
        11419
      ]
    },
    {
      "content": "Using a text editor of your choice, create the following Bash script in the folder containing the NAMD program files and name it hpccharmrun.sh.",
      "pos": [
        11421,
        11565
      ]
    },
    {
      "content": "A complete sample of this file is in the Appendix of this article.",
      "pos": [
        11566,
        11632
      ]
    },
    {
      "content": "This bash script does the following things.",
      "pos": [
        11633,
        11676
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> Save your script as a text file with Linux line endings (LF only, not CR LF).",
      "pos": [
        11679,
        11768
      ]
    },
    {
      "content": "This ensures that it runs properly on the Linux nodes.",
      "pos": [
        11769,
        11823
      ]
    },
    {
      "content": "Define some variables.",
      "pos": [
        11829,
        11851
      ]
    },
    {
      "content": "Get node information from the environment variables.",
      "pos": [
        12133,
        12185
      ]
    },
    {
      "content": "$NODESCORES stores a list of split words from $CCP_NODES_CORES.",
      "pos": [
        12186,
        12249
      ]
    },
    {
      "content": "$COUNT is the size of $NODESCORES.",
      "pos": [
        12250,
        12284
      ]
    },
    {
      "content": "If the $CCP_NODES_CORES variable is not set, just start <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept> directly.",
      "pos": [
        12501,
        12579
      ]
    },
    {
      "content": "(This should only occur when you run this script directly on your Linux nodes.)",
      "pos": [
        12580,
        12659
      ]
    },
    {
      "pos": [
        12867,
        12910
      ],
      "content": "Or create a nodelist file for <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        13376,
        13480
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept> with the nodelist file, get its return status, and remove the nodelist file at the end."
    },
    {
      "content": "${CCP_NUMCPUS} is another environment variable set by the HPC Pack head node.",
      "pos": [
        13486,
        13563
      ]
    },
    {
      "content": "It stores the number of total cores allocated to this job.",
      "pos": [
        13564,
        13622
      ]
    },
    {
      "content": "We use it to specify the number of processes for charmrun.",
      "pos": [
        13623,
        13681
      ]
    },
    {
      "pos": [
        13956,
        13997
      ],
      "content": "Exit with the <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept> return status."
    },
    {
      "content": "Submit a NAMD job",
      "pos": [
        14038,
        14055
      ]
    },
    {
      "content": "Now you are ready to submit a NAMD job in HPC Cluster Manager.",
      "pos": [
        14057,
        14119
      ]
    },
    {
      "content": "Connect to your cluster head node and start HPC Cluster Manager.",
      "pos": [
        14125,
        14189
      ]
    },
    {
      "content": "In <bpt id=\"p1\">**</bpt>Node Management<ept id=\"p1\">**</ept>, ensure that the Linux compute nodes are in the <bpt id=\"p2\">**</bpt>Online<ept id=\"p2\">**</ept> state.",
      "pos": [
        14195,
        14283
      ]
    },
    {
      "content": "If they are not, select them and click <bpt id=\"p1\">**</bpt>Bring Online<ept id=\"p1\">**</ept>.",
      "pos": [
        14284,
        14340
      ]
    },
    {
      "pos": [
        14346,
        14387
      ],
      "content": "In <bpt id=\"p1\">**</bpt>Job Management<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New Job<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        14393,
        14436
      ],
      "content": "Enter a name for job such as <bpt id=\"p1\">*</bpt>hpccharmrun<ept id=\"p1\">*</ept>."
    },
    {
      "content": "![New HPC job][namd_job]",
      "pos": [
        14442,
        14466
      ]
    },
    {
      "content": "On the <bpt id=\"p1\">**</bpt>Job Details<ept id=\"p1\">**</ept> page, under <bpt id=\"p2\">**</bpt>Job Resources<ept id=\"p2\">**</ept>, select the type of resource as <bpt id=\"p3\">**</bpt>Node<ept id=\"p3\">**</ept> and set the <bpt id=\"p4\">**</bpt>Minimum<ept id=\"p4\">**</ept> to 3.",
      "pos": [
        14472,
        14595
      ]
    },
    {
      "content": "In this example we'll run the job on 3 Linux nodes and each node has 4 cores.",
      "pos": [
        14596,
        14673
      ]
    },
    {
      "content": "![Job resources][job_resources]",
      "pos": [
        14679,
        14710
      ]
    },
    {
      "pos": [
        14716,
        14821
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Task Details and I/O Redirection<ept id=\"p1\">**</ept> page, add a new task to the job and set the following values."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Command line<ept id=\"p1\">**</ept> -",
      "pos": [
        14829,
        14847
      ]
    },
    {
      "pos": [
        14986,
        15016
      ],
      "content": "<bpt id=\"p1\">**</bpt>Working directory<ept id=\"p1\">**</ept> - /namd2"
    },
    {
      "pos": [
        15024,
        15039
      ],
      "content": "<bpt id=\"p1\">**</bpt>Minimum<ept id=\"p1\">**</ept> - 3"
    },
    {
      "content": "![Task details][task_details]",
      "pos": [
        15045,
        15074
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You set the working directory here because <bpt id=\"p1\">**</bpt>charmrun<ept id=\"p1\">**</ept> tries to navigate to the same working directory on each node.",
      "pos": [
        15081,
        15211
      ]
    },
    {
      "content": "If the working directory isn't set, HPC Pack starts the command in a randomly named folder created on one of the Linux nodes.",
      "pos": [
        15212,
        15337
      ]
    },
    {
      "content": "This causes the following error on the other nodes:",
      "pos": [
        15338,
        15389
      ]
    },
    {
      "content": "<ph id=\"ph1\">`/bin/bash: line 37: cd: /tmp/nodemanager_task_94_0.mFlQSN: No such file or directory.`</ph> To avoid this, specify a folder path which can be accessed by all nodes as the working directory.",
      "pos": [
        15390,
        15575
      ]
    },
    {
      "pos": [
        15581,
        15614
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Submit<ept id=\"p1\">**</ept> to run this job."
    },
    {
      "content": "By default, HPC Pack submits the job as your current logged-on user account.",
      "pos": [
        15620,
        15696
      ]
    },
    {
      "content": "A dialog box might prompt you to enter the user name and password after you click <bpt id=\"p1\">**</bpt>Submit<ept id=\"p1\">**</ept>.",
      "pos": [
        15697,
        15790
      ]
    },
    {
      "content": "![Job credentials][creds]",
      "pos": [
        15796,
        15821
      ]
    },
    {
      "content": "Under some conditions HPC Pack remembers the user information you input before and won’t show this dialog box.",
      "pos": [
        15827,
        15937
      ]
    },
    {
      "content": "To make HPC Pack show it again, enter the following in a Command window and then submit the job.",
      "pos": [
        15938,
        16034
      ]
    },
    {
      "content": "The job takes several minutes to finish.",
      "pos": [
        16078,
        16118
      ]
    },
    {
      "content": "Find the job log at \\\\",
      "pos": [
        16124,
        16146
      ]
    },
    {
      "content": "\\Namd\\namd2\\namd2_hpccharmrun.log and the output files in \\\\",
      "pos": [
        16160,
        16220
      ]
    },
    {
      "content": "\\Namd\\namd2\\namdsample\\1-2-sphere\\.",
      "pos": [
        16230,
        16265
      ]
    },
    {
      "content": "Optionally, start VMD to view your job results.",
      "pos": [
        16271,
        16318
      ]
    },
    {
      "content": "The steps for visualizing the NAMD output files (in this case, a ubiquitin protein molecule in a water sphere) are beyond the scope of this article.",
      "pos": [
        16319,
        16467
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>NAMD Tutorial<ept id=\"p1\">](http://www.life.illinois.edu/emad/biop590c/namd-tutorial-unix-590C.pdf)</ept> for details.",
      "pos": [
        16468,
        16572
      ]
    },
    {
      "content": "![Job results][vmd_view]",
      "pos": [
        16578,
        16602
      ]
    },
    {
      "content": "Appendix",
      "pos": [
        16607,
        16615
      ]
    },
    {
      "content": "Sample hpccharmrun.sh script",
      "pos": [
        16621,
        16649
      ]
    },
    {
      "content": "Sample cred.xml file",
      "pos": [
        17871,
        17891
      ]
    }
  ],
  "content": "<properties\n pageTitle=\"NAMD with Microsoft HPC Pack on Linux VMs | Microsoft Azure\"\n description=\"Deploy a Microsoft HPC Pack cluster on Azure and run a NAMD simulation with charmrun on multiple Linux compute nodes.\"\n services=\"virtual-machines\"\n documentationCenter=\"\"\n authors=\"dlepow\"\n manager=\"timlt\"\n editor=\"\"\n tags=\"azure-service-management\"/>\n<tags\nms.service=\"virtual-machines\"\n ms.devlang=\"na\"\n ms.topic=\"article\"\n ms.tgt_pltfrm=\"vm-linux\"\n ms.workload=\"big-compute\"\n ms.date=\"09/02/2015\"\n ms.author=\"danlep\"/>\n\n# Run NAMD with Microsoft HPC Pack on Linux compute nodes in Azure\n\nThis article shows you how to deploy a Microsoft HPC Pack cluster on Azure and run a [NAMD](http://www.ks.uiuc.edu/Research/namd/) job with **charmrun** on multiple Linux compute nodes in a virtual cluster network to calculate and visualize the structure of a large biomolecular system.\n\nNAMD (for Nanoscale Molecular Dynamics program) is a parallel molecular dynamics package designed for high-performance simulation of large biomolecular systems containing up to millions of atoms, such as viruses, cell structures, and large proteins. NAMD scales to hundreds of cores for typical simulations and to more than 500,000 cores for the largest simulations.\n\nMicrosoft HPC Pack provides features to run a variety of large-scale HPC and parallel applications, including MPI applications, on clusters of Microsoft Azure virtual machines. Starting in Microsoft HPC Pack 2012 R2, HPC Pack also supports running Linux HPC applications on Linux compute node VMs deployed in an HPC Pack cluster. See [Get started with Linux compute nodes in an HPC Pack cluster in Azure](virtual-machines-linux-cluster-hpcpack.md) for an introduction to using Linux compute nodes with HPC Pack.\n\n\n## Prerequisites\n\n* **HPC Pack cluster with Linux compute nodes** - See [Get started with Linux compute nodes in an HPC Pack cluster in Azure](virtual-machines-linux-cluster-hpcpack.md) for the prerequisites and steps to deploy an HPC Pack cluster with Linux compute nodes on Azure by using an Azure PowerShell script and HPC Pack images in the Azure Marketplace.\n\n    Following is a sample XML configuration file you can use with the script to deploy an Azure-based HPC Pack cluster consisting of a Windows Server 2012 R2 head node and 4 size Large (A3) CentOS 6.6 compute nodes. Substitute appropriate values for your subscription and service names.\n\n    ```\n    <?xml version=\"1.0\" encoding=\"utf-8\" ?>\n    <IaaSClusterConfig>\n      <Subscription>\n        <SubscriptionName>Subscription-1</SubscriptionName>\n        <StorageAccount>mystorageaccount</StorageAccount>\n      </Subscription>\n      <Location>West US</Location>  \n      <VNet>\n        <VNetName>MyVNet</VNetName>\n        <SubnetName>Subnet-1</SubnetName>\n      </VNet>\n      <Domain>\n        <DCOption>HeadNodeAsDC</DCOption>\n        <DomainFQDN>hpclab.local</DomainFQDN>\n      </Domain>\n      <Database>\n        <DBOption>LocalDB</DBOption>\n      </Database>\n      <HeadNode>\n        <VMName>CentOS66HN</VMName>\n        <ServiceName>MyHPCService</ServiceName>\n        <VMSize>Large</VMSize>\n    <EnableRESTAPI />\n    <EnableWebPortal />\n  </HeadNode>\n  <LinuxComputeNodes>\n    <VMNamePattern>CentOS66LN-%00%</VMNamePattern>\n    <ServiceName>MyLnxCNService</ServiceName>\n    <VMSize>Large</VMSize>\n    <NodeCount>4</NodeCount>\n    <ImageName>5112500ae3b842c8b9c604889f8753c3__OpenLogic-CentOS-66-20150325</ImageName>\n  </LinuxComputeNodes>\n</IaaSClusterConfig>\n```\n\n\n* **NAMD software and tutorial files** - Download NAMD software for Linux from the [NAMD](http://www.ks.uiuc.edu/Research/namd/) site. This article is based on NAMD version 2.10, and uses the [Linux-x86_64 (64-bit Intel/AMD with Ethernet)](http://www.ks.uiuc.edu/Development/Download/download.cgi?UserID=&AccessCode=&ArchiveID=1310) archive, which you'll use to run NAMD on multiple Linux compute nodes in a cluster network. Also download the [NAMD tutorial files](http://www.ks.uiuc.edu/Training/Tutorials/#namd). Follow the instructions later in this article to extract the archive and the tutorial samples on the cluster head node.\n\n* **VMD** (optional) - To see the results of your NAMD job, download and install the molecular visualization program [VMD](http://www.ks.uiuc.edu/Research/vmd/) on a computer of your choice. The current version is 1.9.2. See the VMD download site to get started.  \n\n\n## Set up mutual trust between compute nodes\nRunning a cross-node job on multiple Linux nodes requires the nodes to trust each other (by **rsh** or **ssh**). When you create the HPC Pack cluster with the Microsoft HPC Pack IaaS deployment script, the script automatically sets up permanent mutual trust for the administrator account you specify. For non-administrator users you create in the cluster's domain, you have to set up temporary mutual trust among the nodes when a job is allocated to them, and destroy the relationship after the job is complete. To do this for each user, provide an RSA key pair to the cluster which HPC Pack uses to establish the trust relationship.\n\n### Generate an RSA key pair\nIt's easy to generate an RSA key pair, which contains a public key and a private key, by running the Linux **ssh-keygen** command.\n\n1.  Log on to a Linux computer.\n\n2.  Run the following command.\n\n    ```\n    ssh-keygen -t rsa\n    ```\n\n    >[AZURE.NOTE] Press **Enter** to use the default settings until the command is completed. Do not enter a passphrase here; when prompted for a password, just press **Enter**.\n\n    ![Generate an RSA key pair][keygen]\n\n3.  Change directory to the ~/.ssh directory. The private key is stored in id_rsa and the public key in id_rsa.pub.\n\n    ![Private and public keys][keys]\n\n### Add the key pair to the HPC Pack cluster\n1.  Make a Remote Desktop connnection to your head node with your HPC Pack administrator account (the administrator account you set up when you ran the deployment script).\n\n2. Use standard Windows Server procedures to create a domain user account in the cluster's Active Directory domain. For example, use the Active Directory User and Computers tool on the head node. The examples in this article assume you create a domain user named hpclab\\hpcuser.\n\n2.  Create a file named C:\\cred.xml and copy the RSA key data into it. You can find an example of this file in the Appendix at the end of this article.\n\n    ```\n    <ExtendedData>\n      <PrivateKey>Copy the contents of private key here</PrivateKey>\n      <PublicKey>Copy the contents of public key here</PublicKey>\n    </ExtendedData>\n    ```\n\n3.  Open a Command window and enter the following command to set the credentials data for the hpclab\\hpcuser account. You use the **extendeddata** parameter to pass the name of C:\\cred.xml file you created for the key data.\n\n    ```\n    hpccred setcreds /extendeddata:c:\\cred.xml /user:hpclab\\hpcuser /password:<UserPassword>\n    ```\n\n    This command completes successfully without output. After setting the credentials for the user accounts you need to run jobs, store the cred.xml file in a secure location, or delete it.\n\n5.  If you generated the RSA key pair on one of your Linux nodes, remember to delete the keys after you finish using them. HPC Pack does not set up mutual trust if it finds an existing id_rsa file or id_rsa.pub file.\n\n>[AZURE.IMPORTANT] We don’t recommend running a Linux job as a cluster administrator on a shared cluster, because a job submitted by an administrator runs under the root account on the Linux nodes. A job submitted by a non-administrator user runs under a local Linux user account with the same name as the job user, and HPC Pack sets up mutual trust for this Linux user across all the nodes allocated to the job. You can set up the Linux user manually on the Linux nodes before running the job, or HPC Pack creates the user automatically when the job is submitted. If HPC Pack creates the user, HPC Pack deletes it after the job completes. The keys are removed after job completion on the nodes to reduce security threats.\n\n## Set up a file share for Linux nodes\n\nNow set up a standard SMB share on a folder on the head node, and mount the shared folder on all Linux nodes to allow the Linux nodes to access NAMD files with a common path. See the file sharing options and steps in [Get started with Linux compute nodes in an HPC Pack Cluster in Azure](virtual-machines-linux-cluster-hpcpack.md). (We recommend mounting a shared folder on the head node in this article because CentOS 6.6 Linux nodes don’t currently support the Azure File service, which provides similar features. For more about mounting an Azure File share, see [Persisting connections to Microsoft Azure Files](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/27/persisting-connections-to-microsoft-azure-files.aspx).)\n\n1.  Create a folder on the head node, and share it to everyone by setting Read/Write privileges. In this example, \\\\\\\\CentOS66HN\\Namd is the name of the folder, where CentOS66HN is the host name of the head node.\n\n2. Extract the NAMD files in the folder by using a Windows version of **tar** or another Windows utility that operates on .tar archives. Extract the NAMD tar archive to \\\\\\\\CentOS66HN\\Namd\\namd2, and extract the tutorial files under \\\\\\\\CentOS66HN\\Namd\\namd2\\namdsample.\n\n2.  Open a Windows PowerShell window and run the following commands to mount the shared folder.\n\n    ```\n    PS > clusrun /nodegroup:LinuxNodes mkdir -p /namd2\n\n    PS > clusrun /nodegroup:LinuxNodes mount -t cifs //CentOS66HN/Namd/namd2 /namd2 -o vers=2.1`,username=<username>`,password='<password>'`,dir_mode=0777`,file_mode=0777\n    ```\n\nThe first command creates a folder named /namd2 on all nodes in the LinuxNodes group. The second command mounts the shared folder //CentOS66HN/Namd/namd2 onto the folder with dir_mode and file_mode bits set to 777. The *username* and *password* in the command should be the credentials of a user on the head node.\n\n>[AZURE.NOTE]The “\\`” symbol in the second command is an escape symbol for PowerShell. “\\`,” means the “,” (comma character) is a part of the command.\n\n\n## Prepare to run a NAMD job\n\n Your  NAMD job needs a *nodelist* file for **charmrun** to know the number of nodes to use when starting NAMD processes. You'll write a Bash script that generates the nodelist file and runs **charmrun** with this nodelist file. You can then submit a NAMD job in HPC Cluster Manager that calls this script.\n\n### Environment variables and nodelist file\nInformation about nodes and cores is in the $CCP_NODES_CORES environment variable, which is automatically set by the HPC Pack head node when the job is activated. The format for the $CCP_NODES_CORES variable is as follows:\n\n```\n<Number of nodes> <Name of node1> <Cores of node1> <Name of node2> <Cores of node2>…\n```\n\nThis lists the total number of nodes, node names, and number of cores on each node that are allocated to the job. For example, if the job needs 10 cores to run, the value of $CCP_NODES_CORES will be similar to:\n\n```\n3 CENTOS66LN-00 4 CENTOS66LN-01 4 CENTOS66LN-03 2\n```\n\nFollowing is the information in the nodelist file, which the script will generate:\n\n```\ngroup main\nhost <Name of node1> ++cpus <Cores of node1>\nhost <Name of node2> ++cpus <Cores of node2>\n…\n```\n\nFor example:\n\n```\ngroup main\nhost CENTOS66LN-00 ++cpus 4\nhost CENTOS66LN-01 ++cpus 4\nhost CENTOS66LN-03 ++cpus 2\n```\n### Bash script to create a nodelist file\n\nUsing a text editor of your choice, create the following Bash script in the folder containing the NAMD program files and name it hpccharmrun.sh. A complete sample of this file is in the Appendix of this article. This bash script does the following things.\n\n>[AZURE.TIP] Save your script as a text file with Linux line endings (LF only, not CR LF). This ensures that it runs properly on the Linux nodes.\n\n1.  Define some variables.\n\n    ```\n    #!/bin/bash\n\n    # The path of this script\n    SCRIPT_PATH=\"$( dirname \"${BASH_SOURCE[0]}\" )\"\n    # Charmrun command\n    CHARMRUN=${SCRIPT_PATH}/charmrun\n    # Argument of ++nodelist\n    NODELIST_OPT=\"++nodelist\"\n    # Argument of ++p\n    NUMPROCESS=\"+p\"\n    ```\n\n2.  Get node information from the environment variables. $NODESCORES stores a list of split words from $CCP_NODES_CORES. $COUNT is the size of $NODESCORES.\n\n    ```\n    # Get node information from the environment variables\n    # CCP_NODES_CORES=3 CENTOS66LN-00 4 CENTOS66LN-01 4 CENTOS66LN-03 4\n    NODESCORES=(${CCP_NODES_CORES})\n    COUNT=${#NODESCORES[@]}\n    ```\n\n3.  If the $CCP_NODES_CORES variable is not set, just start **charmrun** directly. (This should only occur when you run this script directly on your Linux nodes.)\n\n    ```\n    if [ ${COUNT} -eq 0 ]\n    then\n        # CCP_NODES is_CORES is not found or is empty, so just run charmrun without nodelist arg.\n        #echo ${CHARMRUN} $*\n        ${CHARMRUN} $*\n    ```\n\n4.  Or create a nodelist file for **charmrun**.\n\n    ```\n    else\n        # Create the nodelist file\n        NODELIST_PATH=${SCRIPT_PATH}/nodelist_$$\n\n        # Write the head line\n        echo \"group main\" > ${NODELIST_PATH}\n\n        # Get every node name and number of cores and write into the nodelist file\n        I=1\n        while [ ${I} -lt ${COUNT} ]\n        do\n            echo \"host ${NODESCORES[${I}]} ++cpus ${NODESCORES[$(($I+1))]}\" >> ${NODELIST_PATH}\n            let \"I=${I}+2\"\n        done\n```\n5.  Run **charmrun** with the nodelist file, get its return status, and remove the nodelist file at the end.\n\n    ${CCP_NUMCPUS} is another environment variable set by the HPC Pack head node. It stores the number of total cores allocated to this job. We use it to specify the number of processes for charmrun.\n\n    ```\n    # Run charmrun with nodelist arg\n    #echo ${CHARMRUN} ${NUMPROCESS}${CCP_NUMCPUS} ${NODELIST_OPT} ${NODELIST_PATH} $*\n    ${CHARMRUN} ${NUMPROCESS}${CCP_NUMCPUS} ${NODELIST_OPT} ${NODELIST_PATH} $*\n\n    RTNSTS=$?\n    rm -f ${NODELIST_PATH}\n    fi\n\n    ```\n6.  Exit with the **charmrun** return status.\n\n    ```\n    exit ${RTNSTS}\n    ```\n\n## Submit a NAMD job\n\nNow you are ready to submit a NAMD job in HPC Cluster Manager.\n\n1.  Connect to your cluster head node and start HPC Cluster Manager.\n\n2.  In **Node Management**, ensure that the Linux compute nodes are in the **Online** state. If they are not, select them and click **Bring Online**.\n\n2.  In **Job Management**, click **New Job**.\n\n3.  Enter a name for job such as *hpccharmrun*.\n\n    ![New HPC job][namd_job]\n\n4.  On the **Job Details** page, under **Job Resources**, select the type of resource as **Node** and set the **Minimum** to 3. In this example we'll run the job on 3 Linux nodes and each node has 4 cores.\n\n    ![Job resources][job_resources]\n\n5.  On the **Task Details and I/O Redirection** page, add a new task to the job and set the following values.\n\n    * **Command line** -\n`/namd2/hpccharmrun.sh ++remote-shell ssh /namd2/namd2 /namd2/namdsample/1-2-sphere/ubq_ws_eq.conf > /namd2/namd2_hpccharmrun.log`\n\n    * **Working directory** - /namd2\n\n    * **Minimum** - 3\n\n    ![Task details][task_details]\n\n    >[AZURE.NOTE] You set the working directory here because **charmrun** tries to navigate to the same working directory on each node. If the working directory isn't set, HPC Pack starts the command in a randomly named folder created on one of the Linux nodes. This causes the following error on the other nodes:\n`/bin/bash: line 37: cd: /tmp/nodemanager_task_94_0.mFlQSN: No such file or directory.` To avoid this, specify a folder path which can be accessed by all nodes as the working directory.\n\n5.  Click **Submit** to run this job.\n\n    By default, HPC Pack submits the job as your current logged-on user account. A dialog box might prompt you to enter the user name and password after you click **Submit**.\n\n    ![Job credentials][creds]\n\n    Under some conditions HPC Pack remembers the user information you input before and won’t show this dialog box. To make HPC Pack show it again, enter the following in a Command window and then submit the job.\n\n    ```\n    hpccred delcreds\n    ```\n\n6.  The job takes several minutes to finish.\n\n7.  Find the job log at \\\\<headnodeName>\\Namd\\namd2\\namd2_hpccharmrun.log and the output files in \\\\<headnode>\\Namd\\namd2\\namdsample\\1-2-sphere\\.\n\n8.  Optionally, start VMD to view your job results. The steps for visualizing the NAMD output files (in this case, a ubiquitin protein molecule in a water sphere) are beyond the scope of this article. See [NAMD Tutorial](http://www.life.illinois.edu/emad/biop590c/namd-tutorial-unix-590C.pdf) for details.\n\n    ![Job results][vmd_view]\n\n## Appendix\n\n### Sample hpccharmrun.sh script\n\n```\n#!/bin/bash\n\n# The path of this script\nSCRIPT_PATH=\"$( dirname \"${BASH_SOURCE[0]}\" )\"\n# Charmrun command\nCHARMRUN=${SCRIPT_PATH}/charmrun\n# Argument of ++nodelist\nNODELIST_OPT=\"++nodelist\"\n# Argument of ++p\nNUMPROCESS=\"+p\"\n\n# Get node information from ENVs\n# CCP_NODES_CORES=3 CENTOS66LN-00 4 CENTOS66LN-01 4 CENTOS66LN-03 4\nNODESCORES=(${CCP_NODES_CORES})\nCOUNT=${#NODESCORES[@]}\n\nif [ ${COUNT} -eq 0 ]\nthen\n    # If CCP_NODES_CORES is not found or is empty, just run the charmrun without nodelist arg.\n    #echo ${CHARMRUN} $*\n    ${CHARMRUN} $*\nelse\n    # Create the nodelist file\n    NODELIST_PATH=${SCRIPT_PATH}/nodelist_$$\n\n    # Write the head line\n    echo \"group main\" > ${NODELIST_PATH}\n\n    # Get every node name & cores and write into the nodelist file\n    I=1\n    while [ ${I} -lt ${COUNT} ]\n    do\n        echo \"host ${NODESCORES[${I}]} ++cpus ${NODESCORES[$(($I+1))]}\" >> ${NODELIST_PATH}\n        let \"I=${I}+2\"\n    done\n\n    # Run the charmrun with nodelist arg\n    #echo ${CHARMRUN} ${NUMPROCESS}${CCP_NUMCPUS} ${NODELIST_OPT} ${NODELIST_PATH} $*\n    ${CHARMRUN} ${NUMPROCESS}${CCP_NUMCPUS} ${NODELIST_OPT} ${NODELIST_PATH} $*\n\n    RTNSTS=$?\n    rm -f ${NODELIST_PATH}\nfi\n\nexit ${RTNSTS}\n```\n\n \n### Sample cred.xml file\n\n```\n<ExtendedData>\n  <PrivateKey>-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAxJKBABhnOsE9eneGHvsjdoXKooHUxpTHI1JVunAJkVmFy8JC\nqFt1pV98QCtKEHTC6kQ7tj1UT2N6nx1EY9BBHpZacnXmknpKdX4Nu0cNlSphLpru\nlscKPR3XVzkTwEF00OMiNJVknq8qXJF1T3lYx3rW5EnItn6C3nQm3gQPXP0ckYCF\nJdtu/6SSgzV9kaapctLGPNp1Vjf9KeDQMrJXsQNHxnQcfiICp21NiUCiXosDqJrR\nAfzePdl0XwsNngouy8t0fPlNSngZvsx+kPGh/AKakKIYS0cO9W3FmdYNW8Xehzkc\nVzrtJhU8x21hXGfSC7V0ZeD7dMeTL3tQCVxCmwIDAQABAoIBAQCve8Jh3Wc6koxZ\nqh43xicwhdwSGyliZisoozYZDC/ebDb/Ydq0BYIPMiDwADVMX5AqJuPPmwyLGtm6\n9hu5p46aycrQ5+QA299g6DlF+PZtNbowKuvX+rRvPxagrTmupkCswjglDUEYUHPW\n05wQaNoSqtzwS9Y85M/b24FfLeyxK0n8zjKFErJaHdhVxI6cxw7RdVlSmM9UHmah\nwTkW8HkblbOArilAHi6SlRTNZG4gTGeDzPb7fYZo3hzJyLbcaNfJscUuqnAJ+6pT\niY6NNp1E8PQgjvHe21yv3DRoVRM4egqQvNZgUbYAMUgr30T1UoxnUXwk2vqJMfg2\nNzw0ESGRAoGBAPkfXjjGfc4HryqPkdx0kjXs0bXC3js2g4IXItK9YUFeZzf+476y\nOTMQg/8DUbqd5rLv7PITIAqpGs39pkfnyohPjOe2zZzeoyaXurYIPV98hhH880uH\nZUhOxJYnlqHGxGT7p2PmmnAlmY4TSJrp12VnuiQVVVsXWOGPqHx4S4f9AoGBAMn/\nvuea7hsCgwIE25MJJ55FYCJodLkioQy6aGP4NgB89Azzg527WsQ6H5xhgVMKHWyu\nQ1snp+q8LyzD0i1veEvWb8EYifsMyTIPXOUTwZgzaTTCeJNHdc4gw1U22vd7OBYy\nnZCU7Tn8Pe6eIMNztnVduiv+2QHuiNPgN7M73/x3AoGBAOL0IcmFgy0EsR8MBq0Z\nge4gnniBXCYDptEINNBaeVStJUnNKzwab6PGwwm6w2VI3thbXbi3lbRAlMve7fKK\nB2ghWNPsJOtppKbPCek2Hnt0HUwb7qX7Zlj2cX/99uvRAjChVsDbYA0VJAxcIwQG\nTxXx5pFi4g0HexCa6LrkeKMdAoGAcvRIACX7OwPC6nM5QgQDt95jRzGKu5EpdcTf\ng4TNtplliblLPYhRrzokoyoaHteyxxak3ktDFCLj9eW6xoCZRQ9Tqd/9JhGwrfxw\nMS19DtCzHoNNewM/135tqyD8m7pTwM4tPQqDtmwGErWKj7BaNZARUlhFxwOoemsv\nR6DbZyECgYEAhjL2N3Pc+WW+8x2bbIBN3rJcMjBBIivB62AwgYZnA2D5wk5o0DKD\neesGSKS5l22ZMXJNShgzPKmv3HpH22CSVpO0sNZ6R+iG8a3oq4QkU61MT1CfGoMI\na8lxTKnZCsRXU1HexqZs+DSc+30tz50bNqLdido/l5B4EJnQP03ciO0=\n-----END RSA PRIVATE KEY-----</PrivateKey>\n  <PublicKey>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDEkoEAGGc6wT16d4Ye+yN2hcqigdTGlMcjUlW6cAmRWYXLwkKoW3WlX3xAK0oQdMLqRDu2PVRPY3qfHURj0EEellpydeaSekp1fg27Rw2VKmEumu6Wxwo9HddXORPAQXTQ4yI0lWSerypckXVPeVjHetbkSci2foLedCbeBA9c/RyRgIUl227/pJKDNX2Rpqly0sY82nVWN/0p4NAyslexA0fGdBx+IgKnbU2JQKJeiwOomtEB/N492XRfCw2eCi7Ly3R8+U1KeBm+zH6Q8aH8ApqQohhLRw71bcWZ1g1bxd6HORxXOu0mFTzHbWFcZ9ILtXRl4Pt0x5Mve1AJXEKb username@servername;</PublicKey>\n</ExtendedData>\n```\n\n\n\n\n<!--Image references-->\n[keygen]: ./media/virtual-machines-linux-cluster-hpcpack-namd/keygen.png\n[keys]: ./media/virtual-machines-linux-cluster-hpcpack-namd/keys.png\n[namd_job]: ./media/virtual-machines-linux-cluster-hpcpack-namd/namd_job.png\n[job_resources]: ./media/virtual-machines-linux-cluster-hpcpack-namd/job_resources.png\n[creds]: ./media/virtual-machines-linux-cluster-hpcpack-namd/creds.png\n[task_details]: ./media/virtual-machines-linux-cluster-hpcpack-namd/task_details.png\n[vmd_view]: ./media/virtual-machines-linux-cluster-hpcpack-namd/vmd_view.png\n"
}