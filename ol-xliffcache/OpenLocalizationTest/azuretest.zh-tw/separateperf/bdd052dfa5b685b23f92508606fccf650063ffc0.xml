{
  "nodes": [
    {
      "content": "Get started with a Hadoop emulator for HDInsight | Microsoft Azure",
      "pos": [
        27,
        93
      ]
    },
    {
      "content": "Use an installed emulator with a MapReduce tutorial and other samples to learn the Hadoop ecosystem.",
      "pos": [
        112,
        212
      ]
    },
    {
      "content": "HDInsight emulator works like a Hadoop sandbox.",
      "pos": [
        213,
        260
      ]
    },
    {
      "content": "Get started in the Hadoop ecosystem with the HDInsight Emulator, a Hadoop sandbox",
      "pos": [
        664,
        745
      ]
    },
    {
      "content": "This tutorial gets you started with Hadoop clusters in the Microsoft HDInsight Emulator for Azure (formerly HDInsight Server Developer Preview).",
      "pos": [
        747,
        891
      ]
    },
    {
      "content": "The HDInsight Emulator comes with the same components from the Hadoop ecosystem as Azure HDInsight.",
      "pos": [
        892,
        991
      ]
    },
    {
      "content": "For details, including information on the versions deployed, see <bpt id=\"p1\">[</bpt>What version of Hadoop is in Azure HDInsight?<ept id=\"p1\">](hdinsight-component-versioning.md)</ept>.",
      "pos": [
        992,
        1140
      ]
    },
    {
      "content": "Once the emulator is installed, you follow a MapReduce tutorial for word count and then run samples.",
      "pos": [
        1142,
        1242
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The HDInsight Emulator includes only a Hadoop cluster.",
      "pos": [
        1246,
        1313
      ]
    },
    {
      "content": "It does not include HBase or Storm.",
      "pos": [
        1314,
        1349
      ]
    },
    {
      "content": "The HDInsight Emulator provides a local development environment much like a Hadoop sandbox.",
      "pos": [
        1352,
        1443
      ]
    },
    {
      "content": "If you are familiar with Hadoop, you can get started with the HDInsight Emulator by using the Hadoop Distributed File System (HDFS).",
      "pos": [
        1444,
        1576
      ]
    },
    {
      "content": "In HDInsight, the default file system is Azure Blob storage.",
      "pos": [
        1577,
        1637
      ]
    },
    {
      "content": "So eventually, you will want to develop your jobs by using Azure Blob storage.",
      "pos": [
        1638,
        1716
      ]
    },
    {
      "content": "To use Azure Blob storage with the HDInsight Emulator, you must make changes to the configuration of the emulator.",
      "pos": [
        1717,
        1831
      ]
    },
    {
      "pos": [
        1835,
        1909
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The HDInsight Emulator can use only a single node deployment."
    },
    {
      "content": "Prerequisites",
      "pos": [
        1915,
        1928
      ]
    },
    {
      "content": "Before you begin this tutorial, you must have the following:",
      "pos": [
        1929,
        1989
      ]
    },
    {
      "content": "The HDInsight Emulator requires a 64-bit version of Windows.",
      "pos": [
        1993,
        2053
      ]
    },
    {
      "content": "One of the following requirements must be satisfied:",
      "pos": [
        2054,
        2106
      ]
    },
    {
      "content": "Windows 7 Service Pack 1",
      "pos": [
        2114,
        2138
      ]
    },
    {
      "content": "Windows Server 2008 R2 Service Pack 1",
      "pos": [
        2145,
        2182
      ]
    },
    {
      "content": "Windows 8",
      "pos": [
        2189,
        2198
      ]
    },
    {
      "content": "Windows Server 2012",
      "pos": [
        2205,
        2224
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>.",
      "pos": [
        2228,
        2249
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use Azure PowerShell<ept id=\"p1\">](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>.",
      "pos": [
        2250,
        2372
      ]
    },
    {
      "pos": [
        2377,
        2429
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Install the HDInsight Emulator"
    },
    {
      "content": "The Microsoft HDInsight Emulator is installable via the Microsoft Web Platform Installer.",
      "pos": [
        2431,
        2520
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The HDInsight Emulator currently supports only English operating systems.",
      "pos": [
        2526,
        2612
      ]
    },
    {
      "content": "If you have a previous version of the emulator installed, you must uninstall the following two components from Control Panel/Programs and Features before installing the latest version of the emulator:",
      "pos": [
        2613,
        2813
      ]
    },
    {
      "content": "Microsoft HDInsight Emulator for Azure or HDInsight Developer Preview, whichever is installed",
      "pos": [
        2824,
        2917
      ]
    },
    {
      "content": "Hortonworks Data Platform",
      "pos": [
        2927,
        2952
      ]
    },
    {
      "content": "To install the HDInsight Emulator",
      "pos": [
        2968,
        3001
      ]
    },
    {
      "pos": [
        3008,
        3142
      ],
      "content": "Open Internet Explorer, and then browse to the <bpt id=\"p1\">[</bpt>Microsoft HDInsight Emulator for Azure installation page<ept id=\"p1\">][hdinsight-emulator-install]</ept>."
    },
    {
      "pos": [
        3146,
        3168
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Install Now<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        3172,
        3264
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> when prompted for the installation of HDINSIGHT.exe at the bottom of the page."
    },
    {
      "content": "Click the <bpt id=\"p1\">**</bpt>Yes<ept id=\"p1\">**</ept> button in the <bpt id=\"p2\">**</bpt>User Account Control<ept id=\"p2\">**</ept> window that pops up to complete the installation.",
      "pos": [
        3268,
        3374
      ]
    },
    {
      "content": "The Web Platform Installer window appears.",
      "pos": [
        3375,
        3417
      ]
    },
    {
      "pos": [
        3421,
        3465
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Install<ept id=\"p1\">**</ept> on the bottom of the page."
    },
    {
      "pos": [
        3469,
        3520
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>I Accept<ept id=\"p1\">**</ept> to agree to the licensing terms."
    },
    {
      "pos": [
        3524,
        3651
      ],
      "content": "Verify that the Web Platform Installer shows <bpt id=\"p1\">**</bpt>The following products were successfully installed<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Finish<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        3655,
        3713
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Exit<ept id=\"p1\">**</ept> to close the Web Platform Installer window."
    },
    {
      "content": "To verify the HDInsight Emulator installation",
      "pos": [
        3717,
        3762
      ]
    },
    {
      "content": "The installation should have installed three icons on your desktop.",
      "pos": [
        3766,
        3833
      ]
    },
    {
      "content": "The three icons are linked as follows:",
      "pos": [
        3834,
        3872
      ]
    },
    {
      "pos": [
        3876,
        4002
      ],
      "content": "<bpt id=\"p1\">**</bpt>Hadoop Command Line<ept id=\"p1\">**</ept> - The Hadoop command prompt from which MapReduce, Pig and Hive jobs are run in the HDInsight Emulator."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Hadoop NameNode Status<ept id=\"p1\">**</ept> - The NameNode maintains a tree-based directory for all the files in HDFS.",
      "pos": [
        4006,
        4107
      ]
    },
    {
      "content": "It also keeps track of where the data for all the files are kept in a Hadoop cluster.",
      "pos": [
        4108,
        4193
      ]
    },
    {
      "content": "Clients communicate with the NameNode in order to figure out where the data nodes for all the files are stored.",
      "pos": [
        4194,
        4305
      ]
    },
    {
      "pos": [
        4309,
        4403
      ],
      "content": "<bpt id=\"p1\">**</bpt>Hadoop Yarn Status<ept id=\"p1\">**</ept> - The job tracker that allocates MapReduce tasks to nodes in a cluster."
    },
    {
      "content": "The installation should have also installed several local services.",
      "pos": [
        4405,
        4472
      ]
    },
    {
      "content": "The following is a screenshot of the Services window:",
      "pos": [
        4473,
        4526
      ]
    },
    {
      "content": "Hadoop ecosystem services listed in the emulator window.",
      "pos": [
        4530,
        4586
      ]
    },
    {
      "content": "The services related to the HDInsight Emulator are not started by default.",
      "pos": [
        4618,
        4692
      ]
    },
    {
      "content": "To start the services, from the Hadoop command line, run <bpt id=\"p1\">**</bpt>start\\_local\\_hdp_services.cmd<ept id=\"p1\">**</ept> under C:\\hdp (default location).",
      "pos": [
        4693,
        4817
      ]
    },
    {
      "content": "To automatically start the services after the computer restarts, run <bpt id=\"p1\">**</bpt>set-onebox-autostart.cmd<ept id=\"p1\">**</ept>.",
      "pos": [
        4818,
        4916
      ]
    },
    {
      "content": "For known issues with installing and running the HDInsight Emulator, see the <bpt id=\"p1\">[</bpt>HDInsight Emulator Release Notes<ept id=\"p1\">](hdinsight-emulator-release-notes.md)</ept>.",
      "pos": [
        4920,
        5069
      ]
    },
    {
      "content": "The installation log is located at <bpt id=\"p1\">**</bpt>C:\\HadoopFeaturePackSetup\\HadoopFeaturePackSetupTools\\gettingStarted.winpkg.install.log<ept id=\"p1\">**</ept>.",
      "pos": [
        5070,
        5197
      ]
    },
    {
      "pos": [
        5201,
        5274
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"vstools\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Use Emulator with HDInsight Tools for Visual Studio"
    },
    {
      "content": "You can use HDInsight tools for Visual Studio to connect to the HDInsight Emulator.",
      "pos": [
        5276,
        5359
      ]
    },
    {
      "content": "For information on how to use the Visual Studio tools with HDInsight clusters on Azure, see <bpt id=\"p1\">[</bpt>Get started using HDInsight Hadoop Tools for Visual Studio<ept id=\"p1\">](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md)</ept>.",
      "pos": [
        5360,
        5579
      ]
    },
    {
      "content": "Install the HDInsight tools for Emulator",
      "pos": [
        5585,
        5625
      ]
    },
    {
      "pos": [
        5627,
        5787
      ],
      "content": "For instructions on how to install the HDInsight Visual Studio tools, see <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md#installation)</ept>."
    },
    {
      "content": "Connect to the HDInsight Emulator",
      "pos": [
        5793,
        5826
      ]
    },
    {
      "content": "Open Visual Studio.",
      "pos": [
        5831,
        5850
      ]
    },
    {
      "pos": [
        5854,
        5939
      ],
      "content": "From the <bpt id=\"p1\">**</bpt>View<ept id=\"p1\">**</ept> menu, click <bpt id=\"p2\">**</bpt>Server Explorer<ept id=\"p2\">**</ept> to open the Server Explorer window."
    },
    {
      "pos": [
        5943,
        6037
      ],
      "content": "Expand <bpt id=\"p1\">**</bpt>Azure<ept id=\"p1\">**</ept>, right-click <bpt id=\"p2\">**</bpt>HDInsight<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Connect to HDInsight Emulator<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Visual Studio view: Connect to HDInsight emulator on menu.",
      "pos": [
        6046,
        6104
      ]
    },
    {
      "content": "In the Connect to HDInsight Emulator dialog box, verify the values for WebHCat, HiveServer2, and WebHDFS endpoints, and then click <bpt id=\"p1\">**</bpt>Next<ept id=\"p1\">**</ept>.",
      "pos": [
        6185,
        6325
      ]
    },
    {
      "content": "The values populated by default should work if you did not make any changes to the default configuration of the Emulator.",
      "pos": [
        6326,
        6447
      ]
    },
    {
      "content": "If you made any changes, update the values in the dialog box and then click Next.",
      "pos": [
        6448,
        6529
      ]
    },
    {
      "content": "Connect to HDInsight emulator dialog box with settings.",
      "pos": [
        6537,
        6592
      ]
    },
    {
      "content": "Once the connection is successfully established, click <bpt id=\"p1\">**</bpt>Finish<ept id=\"p1\">**</ept>.",
      "pos": [
        6680,
        6746
      ]
    },
    {
      "content": "You should now see the HDInsight Emulator in the Server Explorer.",
      "pos": [
        6747,
        6812
      ]
    },
    {
      "content": "Server Explorer showing HDInsight local emulator - a Hadoop sanbox - connected.",
      "pos": [
        6820,
        6899
      ]
    },
    {
      "content": "Once the connection is successfully established, you can use the HDInsight VS tools with Emulator, just like you would use it with an Azure HDInsight cluster.",
      "pos": [
        6979,
        7137
      ]
    },
    {
      "content": "For instructions on how to use VS tools with Azure HDInsight clusters, see <bpt id=\"p1\">[</bpt>Using HDInsight Hadoop Tools for Visual Studio<ept id=\"p1\">](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md)</ept>.",
      "pos": [
        7138,
        7328
      ]
    },
    {
      "content": "Troubleshoot: Connecting HDInsight Tools to the HDInsight Emulator",
      "pos": [
        7333,
        7399
      ]
    },
    {
      "content": "While connecting to the HDInsight Emulator, even though the dialog box shows that HiveServer2 connected successfully, you must manually set <bpt id=\"p1\">**</bpt>hive.security.authorization.enabled property<ept id=\"p1\">**</ept> to <bpt id=\"p2\">**</bpt>false<ept id=\"p2\">**</ept> in the Hive configuration file at C:\\hdp\\hive-<bpt id=\"p3\">*</bpt>version<ept id=\"p3\">*</ept>\\conf\\hive-site.xml, and then restart the local Emulator.",
      "pos": [
        7404,
        7718
      ]
    },
    {
      "content": "HDInsight Tools for Visual Studio connects to HiveServer2 only when you are previewing the top 100 rows of your table.",
      "pos": [
        7719,
        7837
      ]
    },
    {
      "content": "If you do not intend to use such a query, you can leave hive configuration as-is.",
      "pos": [
        7838,
        7919
      ]
    },
    {
      "content": "If you are using dynamic IP allocation (DHCP) on the computer running the HDInsight Emulator, you might need to update C:\\hdp\\hadoop-<bpt id=\"p1\">*</bpt>version<ept id=\"p1\">*</ept>\\etc\\hadoop\\core-site.xml and change the value of property <bpt id=\"p2\">**</bpt>hadoop.proxyuser.hadoop.hosts<ept id=\"p2\">**</ept> to (*).",
      "pos": [
        7924,
        8166
      ]
    },
    {
      "content": "This enables Hadoop user to connect from all hosts to impersonate the user you entered in Visual Studio.",
      "pos": [
        8167,
        8271
      ]
    },
    {
      "content": "You might get an error when Visual Studio tries to connect to WebHCat service (“error”: “Could not find job job_XXXX_0001”).",
      "pos": [
        8400,
        8524
      ]
    },
    {
      "content": "In this case, you must restart the WebHCat service and try again.",
      "pos": [
        8525,
        8590
      ]
    },
    {
      "content": "To restart the WebHCat service, start the <bpt id=\"p1\">**</bpt>Services<ept id=\"p1\">**</ept> MMC, right-click <bpt id=\"p2\">**</bpt>Apache Hadoop Templeton<ept id=\"p2\">**</ept> (this is the old name for WebHCat service), and click <bpt id=\"p3\">**</bpt>Restart<ept id=\"p3\">**</ept>.",
      "pos": [
        8591,
        8757
      ]
    },
    {
      "pos": [
        8761,
        8819
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"runwordcount\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>A word-count MapReduce tutorial"
    },
    {
      "content": "Now that you have the HDInsight Emulator configured on your workstation, try this MapReduce tutorial to test the installation.",
      "pos": [
        8821,
        8947
      ]
    },
    {
      "content": "You will first upload some data files to HDFS, and then run a word count MapReduce job to count the frequency of specific words in those files.",
      "pos": [
        8948,
        9091
      ]
    },
    {
      "content": "The word-counting MapReduce program has been packaged into <bpt id=\"p1\">*</bpt>hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar<ept id=\"p1\">*</ept>.",
      "pos": [
        9093,
        9203
      ]
    },
    {
      "content": "The jar file is located at the <bpt id=\"p1\">*</bpt>C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\hadoop\\mapreduce<ept id=\"p1\">*</ept> folder.",
      "pos": [
        9204,
        9300
      ]
    },
    {
      "content": "The MapReduce job to count words takes two arguments:",
      "pos": [
        9302,
        9355
      ]
    },
    {
      "content": "An input folder.",
      "pos": [
        9359,
        9375
      ]
    },
    {
      "content": "You will use <bpt id=\"p1\">*</bpt>hdfs://localhost/user/HDIUser<ept id=\"p1\">*</ept> as the input folder.",
      "pos": [
        9376,
        9441
      ]
    },
    {
      "content": "An output folder.",
      "pos": [
        9444,
        9461
      ]
    },
    {
      "content": "You will use <bpt id=\"p1\">*</bpt>hdfs://localhost/user/HDIUser/WordCount_Output<ept id=\"p1\">*</ept> as the output folder.",
      "pos": [
        9462,
        9545
      ]
    },
    {
      "content": "The output folder cannot be an existing folder, or the MapReduce job will fail.",
      "pos": [
        9546,
        9625
      ]
    },
    {
      "content": "If you want to run the MapReduce job for the second time, you must either specify a different output folder or delete the existing output folder.",
      "pos": [
        9626,
        9771
      ]
    },
    {
      "content": "To run the word-count MapReduce job",
      "pos": [
        9775,
        9810
      ]
    },
    {
      "content": "From the desktop, double-click <bpt id=\"p1\">**</bpt>Hadoop Command Line<ept id=\"p1\">**</ept> to open the Hadoop command-line window.",
      "pos": [
        9817,
        9911
      ]
    },
    {
      "content": "The current folder should be:",
      "pos": [
        9912,
        9941
      ]
    },
    {
      "content": "If not, run the following command:",
      "pos": [
        9989,
        10023
      ]
    },
    {
      "content": "Run the following Hadoop commands to make an HDFS folder for storing the input and output files:",
      "pos": [
        10054,
        10150
      ]
    },
    {
      "content": "Run the following Hadoop command to copy some local text files to HDFS:",
      "pos": [
        10226,
        10297
      ]
    },
    {
      "content": "Run the following command to list the files in the /user/HDIUser folder:",
      "pos": [
        10413,
        10485
      ]
    },
    {
      "content": "You should see the following files:",
      "pos": [
        10528,
        10563
      ]
    },
    {
      "content": "Run the following command to run the word-count MapReduce job:",
      "pos": [
        11022,
        11084
      ]
    },
    {
      "content": "Run the following command to list the number of words with \"windows\" in them from the output file:",
      "pos": [
        11308,
        11406
      ]
    },
    {
      "content": "The output should be:",
      "pos": [
        11500,
        11521
      ]
    },
    {
      "pos": [
        11714,
        11808
      ],
      "content": "For more information on Hadoop commands, see <bpt id=\"p1\">[</bpt>Hadoop commands manual<ept id=\"p1\">][hadoop-commands-manual]</ept>."
    },
    {
      "pos": [
        11812,
        11875
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"rungetstartedsamples\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Analyze sample web log data"
    },
    {
      "content": "The HDInsight Emulator installation provides some samples to get users started with learning Apache Hadoop-based services on Windows.",
      "pos": [
        11877,
        12010
      ]
    },
    {
      "content": "These samples cover some tasks that are typically needed when processing a big dataset.",
      "pos": [
        12011,
        12098
      ]
    },
    {
      "content": "Building on the MapReduce tutorial above, the the samples will help you become more familiar with the MapReduce programming model and its ecosystem.",
      "pos": [
        12099,
        12247
      ]
    },
    {
      "content": "The sample data is organized around processing IIS World Wide Web Consortium (W3C) log data.",
      "pos": [
        12249,
        12341
      ]
    },
    {
      "content": "A data generation tool is provided to create and import the datasets in various sizes to HDFS or Azure Blob storage.",
      "pos": [
        12342,
        12458
      ]
    },
    {
      "content": "(See <bpt id=\"p1\">[</bpt>Use Azure Blob storage for HDInsight<ept id=\"p1\">](../hdinsight-use-blob-storage.md)</ept> for more information).",
      "pos": [
        12459,
        12559
      ]
    },
    {
      "content": "MapReduce, Pig, or Hive jobs can then be run on the pages of data generated by the Azure PowerShell script.",
      "pos": [
        12560,
        12667
      ]
    },
    {
      "content": "Pig and Hive scripts are a layer of abstraction over MapReduce, and eventually compile to MapReduce programs.",
      "pos": [
        12668,
        12777
      ]
    },
    {
      "content": "You can run a series of jobs to observe the effects of using these different technologies and how the data size affects the execution of the processing tasks.",
      "pos": [
        12778,
        12936
      ]
    },
    {
      "content": "In this section",
      "pos": [
        12942,
        12957
      ]
    },
    {
      "content": "The IIS W3C log-data scenario",
      "pos": [
        12962,
        12991
      ]
    },
    {
      "content": "Load sample W3C log data",
      "pos": [
        13008,
        13032
      ]
    },
    {
      "content": "Run Java MapReduce job",
      "pos": [
        13048,
        13070
      ]
    },
    {
      "content": "Run Hive job",
      "pos": [
        13091,
        13103
      ]
    },
    {
      "content": "Run Pig job",
      "pos": [
        13115,
        13126
      ]
    },
    {
      "content": "Rebuild the samples",
      "pos": [
        13137,
        13156
      ]
    },
    {
      "pos": [
        13172,
        13226
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"scenarios\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>The IIS W3C log-data scenarios"
    },
    {
      "content": "The W3C scenario generates and imports IIS W3C log data in three sizes into HDFS or Azure Blob storage: 1MB (small), 500MB (medium), and 2GB (large).",
      "pos": [
        13228,
        13377
      ]
    },
    {
      "content": "It provides three job types and implements each of them in C#, Java, Pig and Hive.",
      "pos": [
        13378,
        13460
      ]
    },
    {
      "pos": [
        13464,
        13537
      ],
      "content": "<bpt id=\"p1\">**</bpt>totalhits<ept id=\"p1\">**</ept> - Calculates the total number of requests for a given page."
    },
    {
      "pos": [
        13540,
        13624
      ],
      "content": "<bpt id=\"p1\">**</bpt>avgtime<ept id=\"p1\">**</ept> - Calculates the average time taken (in seconds) for a request per page."
    },
    {
      "pos": [
        13627,
        13733
      ],
      "content": "<bpt id=\"p1\">**</bpt>errors<ept id=\"p1\">**</ept> - Calculates the number of errors per page, per hour, for requests whose status was 404 or 500."
    },
    {
      "content": "These samples and their documentation do not provide an in-depth study or full implementation of the key Hadoop technologies.",
      "pos": [
        13735,
        13860
      ]
    },
    {
      "content": "The cluster used has only a single node and so the effect of adding more nodes cannot, with this release, be observed.",
      "pos": [
        13861,
        13979
      ]
    },
    {
      "pos": [
        13984,
        14031
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"loaddata\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Load sample W3C log data"
    },
    {
      "content": "Generating and importing the data to HDFS is done via the Azure PowerShell script importdata.ps1.",
      "pos": [
        14033,
        14130
      ]
    },
    {
      "content": "To import sample W3C log data",
      "pos": [
        14134,
        14163
      ]
    },
    {
      "content": "Open a Hadoop command line from the desktop.",
      "pos": [
        14170,
        14214
      ]
    },
    {
      "pos": [
        14218,
        14268
      ],
      "content": "Change the directory to <bpt id=\"p1\">**</bpt>C:\\hdp\\GettingStarted<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to generate and import data to HDFS:",
      "pos": [
        14272,
        14334
      ]
    },
    {
      "pos": [
        14415,
        14523
      ],
      "content": "If you want to load data into Azure Blob storage instead, see <bpt id=\"p1\">[</bpt>Connect to Azure Blob storage<ept id=\"p1\">](#blobstorage)</ept>."
    },
    {
      "content": "Run the following command from the Hadoop command line to list the imported files on HDFS:",
      "pos": [
        14528,
        14618
      ]
    },
    {
      "content": "The output should be similar to the following:",
      "pos": [
        14655,
        14701
      ]
    },
    {
      "content": "If you want to verify the file contents, run the following command to display one of the data files to the console window:",
      "pos": [
        15387,
        15509
      ]
    },
    {
      "content": "You now have the data files created and imported to HDFS.",
      "pos": [
        15571,
        15628
      ]
    },
    {
      "content": "You can start running different Hadoop jobs.",
      "pos": [
        15629,
        15673
      ]
    },
    {
      "pos": [
        15678,
        15730
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"javamapreduce\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Run Java MapReduce jobs"
    },
    {
      "content": "MapReduce is the basic compute engine for Hadoop.",
      "pos": [
        15732,
        15781
      ]
    },
    {
      "content": "By default, it is implemented in Java, but there are also examples that leverage .NET and Hadoop Streaming that use C#.",
      "pos": [
        15782,
        15901
      ]
    },
    {
      "content": "The syntax for running a MapReduce job is:",
      "pos": [
        15902,
        15944
      ]
    },
    {
      "content": "The jar file and the source files are located in the C:\\Hadoop\\GettingStarted\\Java folder.",
      "pos": [
        16020,
        16110
      ]
    },
    {
      "content": "To run a MapReduce job for calculating webpage hits",
      "pos": [
        16114,
        16165
      ]
    },
    {
      "content": "Open the Hadoop command line.",
      "pos": [
        16172,
        16201
      ]
    },
    {
      "pos": [
        16205,
        16255
      ],
      "content": "Change the directory to <bpt id=\"p1\">**</bpt>C:\\hdp\\GettingStarted<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to remove the output directory in case the folder exists.",
      "pos": [
        16259,
        16342
      ]
    },
    {
      "content": "The MapReduce job will fail if the output folder already exists.",
      "pos": [
        16343,
        16407
      ]
    },
    {
      "content": "Run the following command:",
      "pos": [
        16450,
        16476
      ]
    },
    {
      "pos": [
        16619,
        17421
      ],
      "content": "The following table describes the elements of the command:\n <table border=\"1\">\n <tr><td>Parameter</td><td>Note</td></tr>\n <tr><td>w3c_scenarios.jar</td><td>The jar file is located in the C:\\hdp\\GettingStarted\\Java folder.</td></tr>\n <tr><td>microsoft.hadoop.w3c.TotalHitsForPage</td><td>The type can be substituted by one of the following:\n <ul>\n <li>microsoft.hadoop.w3c.AverageTimeTaken</li>\n <li>microsoft.hadoop.w3c.ErrorsByPage</li>\n </ul></td></tr>\n <tr><td>/w3c/input/small/data_w3c_small.txt</td><td>The input file can be substituted by the following:\n <ul>\n <li>/w3c/input/medium/data_w3c_medium.txt</li>\n <li>/w3c/input/large/data_w3c_large.txt</li>\n </ul></td></tr>\n <tr><td>/w3c/output</td><td>This is the output folder name.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "The following table describes the elements of the command:",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "Parameter",
          "pos": [
            88,
            97
          ]
        },
        {
          "content": "Note",
          "pos": [
            106,
            110
          ]
        },
        {
          "content": "w3c_scenarios.jar",
          "pos": [
            130,
            147
          ]
        },
        {
          "content": "The jar file is located in the C:\\hdp\\GettingStarted\\Java folder.",
          "pos": [
            156,
            221
          ]
        },
        {
          "content": "microsoft.hadoop.w3c.TotalHitsForPage",
          "pos": [
            241,
            278
          ]
        },
        {
          "content": "The type can be substituted by one of the following:",
          "pos": [
            287,
            339
          ]
        },
        {
          "content": "microsoft.hadoop.w3c.AverageTimeTaken",
          "pos": [
            351,
            388
          ]
        },
        {
          "content": "microsoft.hadoop.w3c.ErrorsByPage",
          "pos": [
            399,
            432
          ]
        },
        {
          "content": "/w3c/input/small/data_w3c_small.txt",
          "pos": [
            464,
            499
          ]
        },
        {
          "content": "The input file can be substituted by the following:",
          "pos": [
            508,
            559
          ]
        },
        {
          "content": "/w3c/input/medium/data_w3c_medium.txt",
          "pos": [
            571,
            608
          ]
        },
        {
          "content": "/w3c/input/large/data_w3c_large.txt",
          "pos": [
            619,
            654
          ]
        },
        {
          "content": "/w3c/output",
          "pos": [
            686,
            697
          ]
        },
        {
          "content": "This is the output folder name.",
          "pos": [
            706,
            737
          ]
        }
      ]
    },
    {
      "content": "Run the following command to display the output file:",
      "pos": [
        17426,
        17479
      ]
    },
    {
      "content": "The output shall be similar to:",
      "pos": [
        17532,
        17563
      ]
    },
    {
      "content": "The Default.aspx page gets 3360 hits and so on.",
      "pos": [
        17728,
        17775
      ]
    },
    {
      "content": "Try running the commands again by replacing the values as suggested in the table above and notice how the output changes based on the type of job and size of data.",
      "pos": [
        17776,
        17939
      ]
    },
    {
      "pos": [
        17945,
        17977
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"hive\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run Hive jobs"
    },
    {
      "content": "The Hive query engine might feel familiar to analysts with strong Structured Query Language (SQL) skills.",
      "pos": [
        17978,
        18083
      ]
    },
    {
      "content": "It provides a SQL-like interface and a relational data model for HDFS.",
      "pos": [
        18084,
        18154
      ]
    },
    {
      "content": "Hive uses a language called HiveQL, which is very similar to SQL.",
      "pos": [
        18155,
        18220
      ]
    },
    {
      "content": "Hive provides a layer of abstraction over the Java-based MapReduce framework, and the Hive queries are compiled to MapReduce at run time.",
      "pos": [
        18221,
        18358
      ]
    },
    {
      "content": "To run a Hive job",
      "pos": [
        18362,
        18379
      ]
    },
    {
      "content": "Open a Hadoop command line.",
      "pos": [
        18386,
        18413
      ]
    },
    {
      "pos": [
        18417,
        18467
      ],
      "content": "Change the directory to <bpt id=\"p1\">**</bpt>C:\\hdp\\GettingStarted<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to remove the <bpt id=\"p1\">**</bpt>/w3c/hive/input<ept id=\"p1\">**</ept> folder in case the folder exists.",
      "pos": [
        18471,
        18564
      ]
    },
    {
      "content": "The Hive job will fail if the folder exists.",
      "pos": [
        18565,
        18609
      ]
    },
    {
      "pos": [
        18654,
        18777
      ],
      "content": "Run the following command to create the <bpt id=\"p1\">**</bpt>/w3c/hive/input<ept id=\"p1\">**</ept> folders and then copy the data files to the /hive/input folder:"
    },
    {
      "content": "Run the following command to execute the <bpt id=\"p1\">**</bpt>w3ccreate.hql<ept id=\"p1\">**</ept> script file.",
      "pos": [
        18934,
        19005
      ]
    },
    {
      "content": "The script creates a Hive table, and loads data to the Hive table:",
      "pos": [
        19006,
        19072
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> At this stage, you can also use the HDInsight Visual Studio tools to run the Hive query.",
      "pos": [
        19080,
        19181
      ]
    },
    {
      "content": "Open Visual Studio, create a new Project, and from the HDInsight template, select <bpt id=\"p1\">**</bpt>Hive Application<ept id=\"p1\">**</ept>.",
      "pos": [
        19182,
        19285
      ]
    },
    {
      "content": "Once the project opens, add the query as a new item.",
      "pos": [
        19286,
        19338
      ]
    },
    {
      "content": "The query is available at <bpt id=\"p1\">**</bpt>C:/hdp/GettingStarted/Hive/w3c<ept id=\"p1\">**</ept>.",
      "pos": [
        19339,
        19400
      ]
    },
    {
      "content": "Once the query is added to the project, replace <bpt id=\"p1\">**</bpt>${hiveconf:input}<ept id=\"p1\">**</ept> with <bpt id=\"p2\">**</bpt>/w3c/hive/input<ept id=\"p2\">**</ept>, and then press <bpt id=\"p3\">**</bpt>Submit<ept id=\"p3\">**</ept>.",
      "pos": [
        19401,
        19523
      ]
    },
    {
      "content": "The output shall be similar to the following:",
      "pos": [
        19664,
        19709
      ]
    },
    {
      "pos": [
        20231,
        20314
      ],
      "content": "Run the following command to run the <bpt id=\"p1\">**</bpt>w3ctotalhitsbypage.hql<ept id=\"p1\">**</ept> HiveQL script file:"
    },
    {
      "pos": [
        20322,
        20428
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> As explained earlier, you can run this query using the HDInsight Visual Studio Tools as well."
    },
    {
      "pos": [
        20527,
        21063
      ],
      "content": "The following table describes the elements of the command:\n <table border=\"1\">\n <tr><td>File</td><td>Description</td></tr>\n <tr><td>C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd</td><td>The Hive command script.</td></tr>\n <tr><td>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3ctotalhitsbypage.hql</td><td> You can substitute the Hive script file with one of the following:\n <ul>\n <li>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3caveragetimetaken.hql</li>\n <li>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3cerrorsbypage.hql</li>\n </ul>\n </td></tr>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "The following table describes the elements of the command:",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "File",
          "pos": [
            88,
            92
          ]
        },
        {
          "content": "Description",
          "pos": [
            101,
            112
          ]
        },
        {
          "content": "C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd",
          "pos": [
            132,
            176
          ]
        },
        {
          "content": "The Hive command script.",
          "pos": [
            185,
            209
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Hive\\w3c\\w3ctotalhitsbypage.hql",
          "pos": [
            229,
            282
          ]
        },
        {
          "content": "You can substitute the Hive script file with one of the following:",
          "pos": [
            292,
            358
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Hive\\w3c\\w3caveragetimetaken.hql",
          "pos": [
            370,
            424
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Hive\\w3c\\w3cerrorsbypage.hql",
          "pos": [
            435,
            485
          ]
        }
      ]
    },
    {
      "content": "The w3ctotalhitsbypage.hql HiveQL script is:",
      "pos": [
        21083,
        21127
      ]
    },
    {
      "content": "The end of the output shall be similar to the following:",
      "pos": [
        21332,
        21388
      ]
    },
    {
      "content": "Note that as a first step in each of the jobs, a table will be created and data will be loaded into the table from the file created earlier.",
      "pos": [
        21847,
        21987
      ]
    },
    {
      "content": "You can browse the file that was created by looking under the /Hive node in HDFS, using the following command:",
      "pos": [
        21988,
        22098
      ]
    },
    {
      "pos": [
        22136,
        22166
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"pig\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run Pig jobs"
    },
    {
      "content": "Pig processing uses a data-flow language called <bpt id=\"p1\">*</bpt>Pig Latin<ept id=\"p1\">*</ept>.",
      "pos": [
        22168,
        22228
      ]
    },
    {
      "content": "Pig Latin abstractions provide richer data structures than MapReduce, and perform for Hadoop what SQL performs for relational database management systems.",
      "pos": [
        22229,
        22383
      ]
    },
    {
      "content": "To run the pig jobs",
      "pos": [
        22388,
        22407
      ]
    },
    {
      "content": "Open a Hadoop command line.",
      "pos": [
        22414,
        22441
      ]
    },
    {
      "pos": [
        22445,
        22506
      ],
      "content": "Change the directory to the <bpt id=\"p1\">**</bpt>C:\\hdp\\GettingStarted<ept id=\"p1\">**</ept> folder."
    },
    {
      "content": "Run the following command to submit a Pig job:",
      "pos": [
        22510,
        22556
      ]
    },
    {
      "pos": [
        22697,
        23330
      ],
      "content": "The following table shows the elements of the command:\n <table border=\"1\">\n <tr><td>File</td><td>Description</td></tr>\n <tr><td>C:\\hdp\\pig-0.12.1.2.1.3.0-1981\\bin\\pig.cmd</td><td>The Pig command script.</td></tr>\n <tr><td>C:\\hdp\\GettingStarted\\Pig\\w3c\\TotalHitsForPage.pig</td><td> You can substitute the Pig Latin script file with one of the following:\n <ul>\n <li>C:\\hdp\\GettingStarted\\Pig\\w3c\\AverageTimeTaken.pig</li>\n <li>C:\\hdp\\GettingStarted\\Pig\\w3c\\ErrorsByPage.pig</li>\n </ul>\n </td></tr>\n <tr><td>/w3c/input/small/data_w3c_small.txt</td><td> You can substitute the parameter with a larger file:",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "The following table shows the elements of the command:",
          "pos": [
            0,
            54
          ]
        },
        {
          "content": "File",
          "pos": [
            84,
            88
          ]
        },
        {
          "content": "Description",
          "pos": [
            97,
            108
          ]
        },
        {
          "content": "C:\\hdp\\pig-0.12.1.2.1.3.0-1981\\bin\\pig.cmd",
          "pos": [
            128,
            170
          ]
        },
        {
          "content": "The Pig command script.",
          "pos": [
            179,
            202
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Pig\\w3c\\TotalHitsForPage.pig",
          "pos": [
            222,
            272
          ]
        },
        {
          "content": "You can substitute the Pig Latin script file with one of the following:",
          "pos": [
            282,
            353
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Pig\\w3c\\AverageTimeTaken.pig",
          "pos": [
            365,
            415
          ]
        },
        {
          "content": "C:\\hdp\\GettingStarted\\Pig\\w3c\\ErrorsByPage.pig",
          "pos": [
            426,
            472
          ]
        },
        {
          "content": "/w3c/input/small/data_w3c_small.txt",
          "pos": [
            506,
            541
          ]
        },
        {
          "content": "You can substitute the parameter with a larger file:",
          "pos": [
            551,
            603
          ]
        }
      ]
    },
    {
      "pos": [
        23336,
        23450
      ],
      "content": "<ul>\n <li>/w3c/input/medium/data_w3c_medium.txt</li>\n <li>/w3c/input/large/data_w3c_large.txt</li>\n </ul>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "/w3c/input/medium/data_w3c_medium.txt",
          "pos": [
            10,
            47
          ]
        },
        {
          "content": "/w3c/input/large/data_w3c_large.txt",
          "pos": [
            58,
            93
          ]
        }
      ]
    },
    {
      "content": "The output should be similar to the following:",
      "pos": [
        23485,
        23531
      ]
    },
    {
      "content": "Note that since Pig scripts compile to MapReduce jobs, and potentially to more than one such job, you might see multiple MapReduce jobs executing in the course of processing a Pig job.",
      "pos": [
        23617,
        23801
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"blobstorage\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Connect to Azure Blob storage",
      "pos": [
        24254,
        24309
      ]
    },
    {
      "content": "The HDInsight Emulator uses HDFS as the default file system.",
      "pos": [
        24310,
        24370
      ]
    },
    {
      "content": "However, Azure HDInsight uses Azure Blob storage as the default file system.",
      "pos": [
        24371,
        24447
      ]
    },
    {
      "content": "It is possible to configure the HDInsight Emulator to use Azure Blob storage instead of local storage.",
      "pos": [
        24448,
        24550
      ]
    },
    {
      "content": "Follow the instructions below to create a storage container in Azure and to connect it to the HDInsight Emulator.",
      "pos": [
        24551,
        24664
      ]
    },
    {
      "pos": [
        24667,
        24821
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> For more information on how HDInsight uses Azure Blob storage, see <bpt id=\"p1\">[</bpt>Use Azure Blob storage with HDInsight<ept id=\"p1\">](../hdinsight-use-blob-storage.md)</ept>."
    },
    {
      "content": "Before you start with the instructions below, you must have created a storage account.",
      "pos": [
        24823,
        24909
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>How To Create a Storage Account<ept id=\"p1\">](../storage-create-storage-account.md)</ept>.",
      "pos": [
        24910,
        25004
      ]
    },
    {
      "content": "To create a container",
      "pos": [
        25008,
        25029
      ]
    },
    {
      "pos": [
        25036,
        25104
      ],
      "content": "Sign in to the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](https://ms.portal.azure.com/)</ept>."
    },
    {
      "pos": [
        25108,
        25188
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept> on the left, click <bpt id=\"p2\">**</bpt>Data + Storage<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Storage<ept id=\"p3\">**</ept>."
    },
    {
      "content": "In the Storage Account blade, configure the properties as shown in the screen capture below.",
      "pos": [
        25192,
        25284
      ]
    },
    {
      "content": "Create a storage account",
      "pos": [
        25296,
        25320
      ]
    },
    {
      "pos": [
        25406,
        25461
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Pin to Startboard<ept id=\"p1\">**</ept>, and the click <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        25465,
        25603
      ],
      "content": "Once the storage account is created, from the new storage account blade, click <bpt id=\"p1\">**</bpt>Containers<ept id=\"p1\">**</ept> to open the containers blade, click <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        25607,
        25665
      ],
      "content": "Enter the name of the container and then click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Create a container",
      "pos": [
        25673,
        25691
      ]
    },
    {
      "content": "Before you can access an Azure Storage account, you must add the account name and the account key to the configuration file.",
      "pos": [
        25775,
        25899
      ]
    },
    {
      "content": "To configure the connection to an Azure Storage account",
      "pos": [
        25903,
        25958
      ]
    },
    {
      "pos": [
        25965,
        26043
      ],
      "content": "Open <bpt id=\"p1\">**</bpt>C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\etc\\hadoop\\core-site.xml<ept id=\"p1\">**</ept> in Notepad."
    },
    {
      "content": "Add the following &lt;property\\&gt; tag next to the other &lt;property\\&gt; tags:",
      "pos": [
        26047,
        26116
      ]
    },
    {
      "content": "You must substitute &lt;StorageAccountName\\&gt; and &lt;StorageAccountKey\\&gt; with the values that match your Storage account information.",
      "pos": [
        26298,
        26425
      ]
    },
    {
      "content": "Save the change.",
      "pos": [
        26430,
        26446
      ]
    },
    {
      "content": "You don't need to restart the Hadoop services.",
      "pos": [
        26447,
        26493
      ]
    },
    {
      "content": "Use the following syntax to access the Storage account:",
      "pos": [
        26495,
        26550
      ]
    },
    {
      "content": "For example:",
      "pos": [
        26624,
        26636
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"powershell\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Run Azure PowerShell",
      "pos": [
        26712,
        26758
      ]
    },
    {
      "content": "Some of the Azure PowerShell cmdlets are also supported on the HDInsight Emulator.",
      "pos": [
        26759,
        26841
      ]
    },
    {
      "content": "These cmdlets include:",
      "pos": [
        26842,
        26864
      ]
    },
    {
      "content": "HDInsight job definition cmdlets",
      "pos": [
        26868,
        26900
      ]
    },
    {
      "content": "New-AzureHDInsightSqoopJobDefinition",
      "pos": [
        26908,
        26944
      ]
    },
    {
      "content": "New-AzureHDInsightStreamingMapReduceJobDefinition",
      "pos": [
        26951,
        27000
      ]
    },
    {
      "content": "New-AzureHDInsightPigJobDefinition",
      "pos": [
        27007,
        27041
      ]
    },
    {
      "content": "New-AzureHDInsightHiveJobDefinition",
      "pos": [
        27048,
        27083
      ]
    },
    {
      "content": "New-AzureHDInsightMapReduceJobDefinition",
      "pos": [
        27090,
        27130
      ]
    },
    {
      "content": "Start-AzureHDInsightJob",
      "pos": [
        27133,
        27156
      ]
    },
    {
      "content": "Get-AzureHDInsightJob",
      "pos": [
        27159,
        27180
      ]
    },
    {
      "content": "Wait-AzureHDInsightJob",
      "pos": [
        27183,
        27205
      ]
    },
    {
      "content": "Here is a sample for submitting a Hadoop job:",
      "pos": [
        27207,
        27252
      ]
    },
    {
      "content": "You will get a prompt when calling Get-Credential.",
      "pos": [
        27474,
        27524
      ]
    },
    {
      "content": "You must use <bpt id=\"p1\">**</bpt>hadoop<ept id=\"p1\">**</ept> as the user name.",
      "pos": [
        27525,
        27566
      ]
    },
    {
      "content": "The password can be any string.",
      "pos": [
        27567,
        27598
      ]
    },
    {
      "content": "The cluster name is always <bpt id=\"p1\">**</bpt>http://localhost:50111<ept id=\"p1\">**</ept>.",
      "pos": [
        27599,
        27653
      ]
    },
    {
      "content": "For more information about submitting Hadoop jobs, see <bpt id=\"p1\">[</bpt>Submit Hadoop jobs programmatically<ept id=\"p1\">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept>.",
      "pos": [
        27655,
        27798
      ]
    },
    {
      "content": "For more information about the Azure PowerShell cmdlets for HDInsight, see <bpt id=\"p1\">[</bpt>HDInsight cmdlet reference<ept id=\"p1\">][hdinsight-powershell-reference]</ept>.",
      "pos": [
        27799,
        27935
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"remove\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Remove the HDInsight Emulator",
      "pos": [
        27940,
        27991
      ]
    },
    {
      "content": "On the computer where you have the emulator installed, open Control Panel and under <bpt id=\"p1\">**</bpt>Programs<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Uninstall a Program<ept id=\"p2\">**</ept>.",
      "pos": [
        27992,
        28120
      ]
    },
    {
      "content": "From the list of installed programs, right-click <bpt id=\"p1\">**</bpt>Microsoft HDInsight Emulator for Azure<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Uninstall<ept id=\"p2\">**</ept>.",
      "pos": [
        28121,
        28243
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Next steps",
      "pos": [
        28248,
        28283
      ]
    },
    {
      "content": "In this MapReduce tutorial, you installed the HDInsight Emulator - a Hadoop sandbox - and ran some Hadoop jobs.",
      "pos": [
        28284,
        28395
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        28396,
        28438
      ]
    },
    {
      "content": "Get started using Azure HDInsight",
      "pos": [
        28443,
        28476
      ]
    },
    {
      "content": "Develop Java MapReduce programs for HDInsight",
      "pos": [
        28510,
        28555
      ]
    },
    {
      "content": "Develop C# Hadoop streaming MapReduce programs for HDInsight",
      "pos": [
        28604,
        28664
      ]
    },
    {
      "content": "HDInsight Emulator release notes",
      "pos": [
        28720,
        28752
      ]
    },
    {
      "content": "MSDN forum for discussing HDInsight",
      "pos": [
        28794,
        28829
      ]
    },
    {
      "content": "test",
      "pos": [
        30043,
        30047
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Get started with a Hadoop emulator for HDInsight | Microsoft Azure\"\n    description=\"Use an installed emulator with a MapReduce tutorial and other samples to learn the Hadoop ecosystem. HDInsight emulator works like a Hadoop sandbox.\"\n    keywords=\"emulator,hadoop ecosystem,hadoop sandbox,mapreduce tutorial\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    services=\"hdinsight\"\n    authors=\"nitinme\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\" \n    ms.date=\"08/07/2015\"\n    ms.author=\"nitinme\"/>\n\n# Get started in the Hadoop ecosystem with the HDInsight Emulator, a Hadoop sandbox\n\nThis tutorial gets you started with Hadoop clusters in the Microsoft HDInsight Emulator for Azure (formerly HDInsight Server Developer Preview). The HDInsight Emulator comes with the same components from the Hadoop ecosystem as Azure HDInsight. For details, including information on the versions deployed, see [What version of Hadoop is in Azure HDInsight?](hdinsight-component-versioning.md).\n\nOnce the emulator is installed, you follow a MapReduce tutorial for word count and then run samples.\n\n> [AZURE.NOTE] The HDInsight Emulator includes only a Hadoop cluster. It does not include HBase or Storm.\n\n\nThe HDInsight Emulator provides a local development environment much like a Hadoop sandbox. If you are familiar with Hadoop, you can get started with the HDInsight Emulator by using the Hadoop Distributed File System (HDFS). In HDInsight, the default file system is Azure Blob storage. So eventually, you will want to develop your jobs by using Azure Blob storage. To use Azure Blob storage with the HDInsight Emulator, you must make changes to the configuration of the emulator.\n\n> [AZURE.NOTE] The HDInsight Emulator can use only a single node deployment.\n\n\n## Prerequisites\nBefore you begin this tutorial, you must have the following:\n\n- The HDInsight Emulator requires a 64-bit version of Windows. One of the following requirements must be satisfied:\n\n    - Windows 7 Service Pack 1\n    - Windows Server 2008 R2 Service Pack 1\n    - Windows 8\n    - Windows Server 2012\n\n- **Azure PowerShell**. See [Install and use Azure PowerShell](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/).\n\n\n##<a name=\"install\"></a>Install the HDInsight Emulator\n\nThe Microsoft HDInsight Emulator is installable via the Microsoft Web Platform Installer.  \n\n> [AZURE.NOTE] The HDInsight Emulator currently supports only English operating systems. If you have a previous version of the emulator installed, you must uninstall the following two components from Control Panel/Programs and Features before installing the latest version of the emulator:\n><ul>\n<li>Microsoft HDInsight Emulator for Azure or HDInsight Developer Preview, whichever is installed</li>\n<li>Hortonworks Data Platform</li>\n</ul>\n\n\n**To install the HDInsight Emulator**\n\n1. Open Internet Explorer, and then browse to the [Microsoft HDInsight Emulator for Azure installation page][hdinsight-emulator-install].\n2. Click **Install Now**.\n3. Click **Run** when prompted for the installation of HDINSIGHT.exe at the bottom of the page.\n4. Click the **Yes** button in the **User Account Control** window that pops up to complete the installation. The Web Platform Installer window appears.\n6. Click **Install** on the bottom of the page.\n7. Click **I Accept** to agree to the licensing terms.\n8. Verify that the Web Platform Installer shows **The following products were successfully installed**, and then click **Finish**.\n9. Click **Exit** to close the Web Platform Installer window.\n\n**To verify the HDInsight Emulator installation**\n\nThe installation should have installed three icons on your desktop. The three icons are linked as follows:\n\n- **Hadoop Command Line** - The Hadoop command prompt from which MapReduce, Pig and Hive jobs are run in the HDInsight Emulator.\n\n- **Hadoop NameNode Status** - The NameNode maintains a tree-based directory for all the files in HDFS. It also keeps track of where the data for all the files are kept in a Hadoop cluster. Clients communicate with the NameNode in order to figure out where the data nodes for all the files are stored.\n\n- **Hadoop Yarn Status** - The job tracker that allocates MapReduce tasks to nodes in a cluster.\n\nThe installation should have also installed several local services. The following is a screenshot of the Services window:\n\n![Hadoop ecosystem services listed in the emulator window.][image-hdi-emulator-services]\n\nThe services related to the HDInsight Emulator are not started by default. To start the services, from the Hadoop command line, run **start\\_local\\_hdp_services.cmd** under C:\\hdp (default location). To automatically start the services after the computer restarts, run **set-onebox-autostart.cmd**.  \n\nFor known issues with installing and running the HDInsight Emulator, see the [HDInsight Emulator Release Notes](hdinsight-emulator-release-notes.md). The installation log is located at **C:\\HadoopFeaturePackSetup\\HadoopFeaturePackSetupTools\\gettingStarted.winpkg.install.log**.\n\n##<a name=\"vstools\"></a>Use Emulator with HDInsight Tools for Visual Studio\n\nYou can use HDInsight tools for Visual Studio to connect to the HDInsight Emulator. For information on how to use the Visual Studio tools with HDInsight clusters on Azure, see [Get started using HDInsight Hadoop Tools for Visual Studio](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md).\n\n### Install the HDInsight tools for Emulator\n\nFor instructions on how to install the HDInsight Visual Studio tools, see [here](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md#installation).\n\n### Connect to the HDInsight Emulator\n\n1. Open Visual Studio.\n2. From the **View** menu, click **Server Explorer** to open the Server Explorer window.\n3. Expand **Azure**, right-click **HDInsight**, and then click **Connect to HDInsight Emulator**.\n\n     ![Visual Studio view: Connect to HDInsight emulator on menu.](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.connect.vs.png)\n\n4. In the Connect to HDInsight Emulator dialog box, verify the values for WebHCat, HiveServer2, and WebHDFS endpoints, and then click **Next**. The values populated by default should work if you did not make any changes to the default configuration of the Emulator. If you made any changes, update the values in the dialog box and then click Next.\n\n    ![Connect to HDInsight emulator dialog box with settings.](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.connect.vs.dialog.png)\n\n5. Once the connection is successfully established, click **Finish**. You should now see the HDInsight Emulator in the Server Explorer.\n\n    ![Server Explorer showing HDInsight local emulator - a Hadoop sanbox - connected.](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.vs.connected.png)\n\nOnce the connection is successfully established, you can use the HDInsight VS tools with Emulator, just like you would use it with an Azure HDInsight cluster. For instructions on how to use VS tools with Azure HDInsight clusters, see [Using HDInsight Hadoop Tools for Visual Studio](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md).\n\n## Troubleshoot: Connecting HDInsight Tools to the HDInsight Emulator\n\n1. While connecting to the HDInsight Emulator, even though the dialog box shows that HiveServer2 connected successfully, you must manually set **hive.security.authorization.enabled property** to **false** in the Hive configuration file at C:\\hdp\\hive-*version*\\conf\\hive-site.xml, and then restart the local Emulator. HDInsight Tools for Visual Studio connects to HiveServer2 only when you are previewing the top 100 rows of your table. If you do not intend to use such a query, you can leave hive configuration as-is.\n\n2. If you are using dynamic IP allocation (DHCP) on the computer running the HDInsight Emulator, you might need to update C:\\hdp\\hadoop-*version*\\etc\\hadoop\\core-site.xml and change the value of property **hadoop.proxyuser.hadoop.hosts** to (*). This enables Hadoop user to connect from all hosts to impersonate the user you entered in Visual Studio.\n\n        <property>\n            <name>hadoop.proxyuser.hadoop.hosts</name>\n            <value>*</value>\n        </property>\n\n3. You might get an error when Visual Studio tries to connect to WebHCat service (“error”: “Could not find job job_XXXX_0001”). In this case, you must restart the WebHCat service and try again. To restart the WebHCat service, start the **Services** MMC, right-click **Apache Hadoop Templeton** (this is the old name for WebHCat service), and click **Restart**.\n\n##<a name=\"runwordcount\"></a>A word-count MapReduce tutorial\n\nNow that you have the HDInsight Emulator configured on your workstation, try this MapReduce tutorial to test the installation. You will first upload some data files to HDFS, and then run a word count MapReduce job to count the frequency of specific words in those files.\n\nThe word-counting MapReduce program has been packaged into *hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar*. The jar file is located at the *C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\hadoop\\mapreduce* folder.\n\nThe MapReduce job to count words takes two arguments:\n\n- An input folder. You will use *hdfs://localhost/user/HDIUser* as the input folder.\n- An output folder. You will use *hdfs://localhost/user/HDIUser/WordCount_Output* as the output folder. The output folder cannot be an existing folder, or the MapReduce job will fail. If you want to run the MapReduce job for the second time, you must either specify a different output folder or delete the existing output folder.\n\n**To run the word-count MapReduce job**\n\n1. From the desktop, double-click **Hadoop Command Line** to open the Hadoop command-line window. The current folder should be:\n\n        c:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\n\n    If not, run the following command:\n\n        cd %hadoop_home%\n\n2. Run the following Hadoop commands to make an HDFS folder for storing the input and output files:\n\n        hadoop fs -mkdir /user\n        hadoop fs -mkdir /user/HDIUser\n\n3. Run the following Hadoop command to copy some local text files to HDFS:\n\n        hadoop fs -copyFromLocal C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common\\*.txt /user/HDIUser\n\n4. Run the following command to list the files in the /user/HDIUser folder:\n\n        hadoop fs -ls /user/HDIUser\n\n    You should see the following files:\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop fs -ls /user/HDIUser\n        Found 4 items\n        -rw-r--r--   1 username hdfs     574261 2014-09-08 12:56 /user/HDIUser/CHANGES.txt\n        -rw-r--r--   1 username hdfs      15748 2014-09-08 12:56 /user/HDIUser/LICENSE.txt\n        -rw-r--r--   1 username hdfs        103 2014-09-08 12:56 /user/HDIUser/NOTICE.txt\n        -rw-r--r--   1 username hdfs       1397 2014-09-08 12:56 /user/HDIUser/README.txt\n\n5. Run the following command to run the word-count MapReduce job:\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop jar C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar wordcount /user/HDIUser/*.txt /user/HDIUser/WordCount_Output\n\n6. Run the following command to list the number of words with \"windows\" in them from the output file:\n\n        hadoop fs -cat /user/HDIUser/WordCount_Output/part-r-00000 | findstr \"windows\"\n\n    The output should be:\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop fs -cat /user/HDIUser/WordCount_Output/part-r-00000 | findstr \"windows\"\n        windows 4\n        windows.        2\n        windows/cygwin. 1\n\nFor more information on Hadoop commands, see [Hadoop commands manual][hadoop-commands-manual].\n\n##<a name=\"rungetstartedsamples\"></a> Analyze sample web log data\n\nThe HDInsight Emulator installation provides some samples to get users started with learning Apache Hadoop-based services on Windows. These samples cover some tasks that are typically needed when processing a big dataset. Building on the MapReduce tutorial above, the the samples will help you become more familiar with the MapReduce programming model and its ecosystem.\n\nThe sample data is organized around processing IIS World Wide Web Consortium (W3C) log data. A data generation tool is provided to create and import the datasets in various sizes to HDFS or Azure Blob storage. (See [Use Azure Blob storage for HDInsight](../hdinsight-use-blob-storage.md) for more information). MapReduce, Pig, or Hive jobs can then be run on the pages of data generated by the Azure PowerShell script. Pig and Hive scripts are a layer of abstraction over MapReduce, and eventually compile to MapReduce programs. You can run a series of jobs to observe the effects of using these different technologies and how the data size affects the execution of the processing tasks.\n\n### In this section\n\n- [The IIS W3C log-data scenario](#scenarios)\n- [Load sample W3C log data](#loaddata)\n- [Run Java MapReduce job](#javamapreduce)\n- [Run Hive job](#hive)\n- [Run Pig job](#pig)\n- [Rebuild the samples](#rebuild)\n\n###<a name=\"scenarios\"></a>The IIS W3C log-data scenarios\n\nThe W3C scenario generates and imports IIS W3C log data in three sizes into HDFS or Azure Blob storage: 1MB (small), 500MB (medium), and 2GB (large). It provides three job types and implements each of them in C#, Java, Pig and Hive.\n\n- **totalhits** - Calculates the total number of requests for a given page.\n- **avgtime** - Calculates the average time taken (in seconds) for a request per page.\n- **errors** - Calculates the number of errors per page, per hour, for requests whose status was 404 or 500.\n\nThese samples and their documentation do not provide an in-depth study or full implementation of the key Hadoop technologies. The cluster used has only a single node and so the effect of adding more nodes cannot, with this release, be observed.\n\n###<a name=\"loaddata\"></a>Load sample W3C log data\n\nGenerating and importing the data to HDFS is done via the Azure PowerShell script importdata.ps1.\n\n**To import sample W3C log data**\n\n1. Open a Hadoop command line from the desktop.\n2. Change the directory to **C:\\hdp\\GettingStarted**.\n3. Run the following command to generate and import data to HDFS:\n\n        powershell -File importdata.ps1 w3c -ExecutionPolicy unrestricted\n\n    If you want to load data into Azure Blob storage instead, see [Connect to Azure Blob storage](#blobstorage).\n\n4. Run the following command from the Hadoop command line to list the imported files on HDFS:\n\n        hadoop fs -ls -R /w3c\n\n    The output should be similar to the following:\n\n        C:\\hdp\\GettingStarted>hadoop fs -ls -R /w3c\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:40 /w3c/input\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:41 /w3c/input/large\n        -rw-r--r--   1 username hdfs  543683503 2014-09-08 15:41 /w3c/input/large/data_w3c_large.txt\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:40 /w3c/input/medium\n        -rw-r--r--   1 username hdfs  272435159 2014-09-08 15:40 /w3c/input/medium/data_w3c_medium.txt\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:39 /w3c/input/small\n        -rw-r--r--   1 username hdfs    1058423 2014-09-08 15:39 /w3c/input/small/data_w3c_small.txt\n\n5. If you want to verify the file contents, run the following command to display one of the data files to the console window:\n\n        hadoop fs -cat /w3c/input/small/data_w3c_small.txt\n\nYou now have the data files created and imported to HDFS. You can start running different Hadoop jobs.\n\n###<a name=\"javamapreduce\"></a> Run Java MapReduce jobs\n\nMapReduce is the basic compute engine for Hadoop. By default, it is implemented in Java, but there are also examples that leverage .NET and Hadoop Streaming that use C#. The syntax for running a MapReduce job is:\n\n    hadoop jar <jarFileName>.jar <className> <inputFiles> <outputFolder>\n\nThe jar file and the source files are located in the C:\\Hadoop\\GettingStarted\\Java folder.\n\n**To run a MapReduce job for calculating webpage hits**\n\n1. Open the Hadoop command line.\n2. Change the directory to **C:\\hdp\\GettingStarted**.\n3. Run the following command to remove the output directory in case the folder exists. The MapReduce job will fail if the output folder already exists.\n\n        hadoop fs -rm -r /w3c/output\n\n3. Run the following command:\n\n        hadoop jar .\\Java\\w3c_scenarios.jar \"microsoft.hadoop.w3c.TotalHitsForPage\" \"/w3c/input/small/data_w3c_small.txt\" \"/w3c/output\"\n\n    The following table describes the elements of the command:\n    <table border=\"1\">\n    <tr><td>Parameter</td><td>Note</td></tr>\n    <tr><td>w3c_scenarios.jar</td><td>The jar file is located in the C:\\hdp\\GettingStarted\\Java folder.</td></tr>\n    <tr><td>microsoft.hadoop.w3c.TotalHitsForPage</td><td>The type can be substituted by one of the following:\n    <ul>\n    <li>microsoft.hadoop.w3c.AverageTimeTaken</li>\n    <li>microsoft.hadoop.w3c.ErrorsByPage</li>\n    </ul></td></tr>\n    <tr><td>/w3c/input/small/data_w3c_small.txt</td><td>The input file can be substituted by the following:\n    <ul>\n    <li>/w3c/input/medium/data_w3c_medium.txt</li>\n    <li>/w3c/input/large/data_w3c_large.txt</li>\n    </ul></td></tr>\n    <tr><td>/w3c/output</td><td>This is the output folder name.</td></tr>\n    </table>\n\n4. Run the following command to display the output file:\n\n        hadoop fs -cat /w3c/output/part-00000\n\n    The output shall be similar to:\n\n        c:\\Hadoop\\GettingStarted>hadoop fs -cat /w3c/output/part-00000\n        /Default.aspx   3380\n        /Info.aspx      1135\n        /UserService    1126\n\n    The Default.aspx page gets 3360 hits and so on. Try running the commands again by replacing the values as suggested in the table above and notice how the output changes based on the type of job and size of data.\n\n### <a name=\"hive\"></a>Run Hive jobs\nThe Hive query engine might feel familiar to analysts with strong Structured Query Language (SQL) skills. It provides a SQL-like interface and a relational data model for HDFS. Hive uses a language called HiveQL, which is very similar to SQL. Hive provides a layer of abstraction over the Java-based MapReduce framework, and the Hive queries are compiled to MapReduce at run time.\n\n**To run a Hive job**\n\n1. Open a Hadoop command line.\n2. Change the directory to **C:\\hdp\\GettingStarted**.\n3. Run the following command to remove the **/w3c/hive/input** folder in case the folder exists. The Hive job will fail if the folder exists.\n\n        hadoop fs -rmr /w3c/hive/input\n\n4. Run the following command to create the **/w3c/hive/input** folders and then copy the data files to the /hive/input folder:\n\n        hadoop fs -mkdir /w3c/hive\n        hadoop fs -mkdir /w3c/hive/input\n\n        hadoop fs -cp /w3c/input/small/data_w3c_small.txt /w3c/hive/input\n\n5. Run the following command to execute the **w3ccreate.hql** script file. The script creates a Hive table, and loads data to the Hive table:\n\n    > [AZURE.NOTE] At this stage, you can also use the HDInsight Visual Studio tools to run the Hive query. Open Visual Studio, create a new Project, and from the HDInsight template, select **Hive Application**. Once the project opens, add the query as a new item. The query is available at **C:/hdp/GettingStarted/Hive/w3c**. Once the query is added to the project, replace **${hiveconf:input}** with **/w3c/hive/input**, and then press **Submit**.\n\n        C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd -f ./Hive/w3c/w3ccreate.hql -hiveconf \"input=/w3c/hive/input/data_w3c_small.txt\"\n\n    The output shall be similar to the following:\n\n        Logging initialized using configuration in file:/C:/hdp/hive-0.13.0.2.1.3.0-1981    /conf/hive-log4j.properties\n        OK\n        Time taken: 1.137 seconds\n        OK\n        Time taken: 4.403 seconds\n        Loading data to table default.w3c\n        Moved: 'hdfs://HDINSIGHT02:8020/hive/warehouse/w3c' to trash at: hdfs://HDINSIGHT02:8020/user/<username>/.Trash/Current\n        Table default.w3c stats: [numFiles=1, numRows=0, totalSize=1058423, rawDataSize=0]\n        OK\n        Time taken: 2.881 seconds\n\n6. Run the following command to run the **w3ctotalhitsbypage.hql** HiveQL script file:\n\n    > [AZURE.NOTE] As explained earlier, you can run this query using the HDInsight Visual Studio Tools as well.  \n\n        C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd -f ./Hive/w3c/w3ctotalhitsbypage.hql\n\n    The following table describes the elements of the command:\n    <table border=\"1\">\n    <tr><td>File</td><td>Description</td></tr>\n    <tr><td>C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd</td><td>The Hive command script.</td></tr>\n    <tr><td>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3ctotalhitsbypage.hql</td><td> You can substitute the Hive script file with one of the following:\n    <ul>\n    <li>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3caveragetimetaken.hql</li>\n    <li>C:\\hdp\\GettingStarted\\Hive\\w3c\\w3cerrorsbypage.hql</li>\n    </ul>\n    </td></tr>\n\n    </table>\n\n    The w3ctotalhitsbypage.hql HiveQL script is:\n\n        SELECT filtered.cs_uri_stem,COUNT(*)\n        FROM (\n          SELECT logdate,cs_uri_stem from w3c WHERE logdate NOT RLIKE '.*#.*'\n        ) filtered\n        GROUP BY (filtered.cs_uri_stem);\n\n    The end of the output shall be similar to the following:\n\n        MapReduce Total cumulative CPU time: 5 seconds 391 msec\n        Ended Job = job_1410201800143_0008\n        MapReduce Jobs Launched:\n        Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.391 sec   HDFS Read: 1058638 HDFS Write: 53 SUCCESS\n        Total MapReduce CPU Time Spent: 5 seconds 391 msec\n        OK\n        /Default.aspx   3380\n        /Info.aspx      1135\n        /UserService    1126\n        Time taken: 49.304 seconds, Fetched: 3 row(s)\n\nNote that as a first step in each of the jobs, a table will be created and data will be loaded into the table from the file created earlier. You can browse the file that was created by looking under the /Hive node in HDFS, using the following command:\n\n    hadoop fs -lsr /apps/hive/\n\n### <a name=\"pig\"></a>Run Pig jobs\n\nPig processing uses a data-flow language called *Pig Latin*. Pig Latin abstractions provide richer data structures than MapReduce, and perform for Hadoop what SQL performs for relational database management systems.\n\n\n**To run the pig jobs**\n\n1. Open a Hadoop command line.\n2. Change the directory to the **C:\\hdp\\GettingStarted** folder.\n3. Run the following command to submit a Pig job:\n\n        C:\\hdp\\pig-0.12.1.2.1.3.0-1981\\bin\\pig.cmd -f \".\\Pig\\w3c\\TotalHitsForPage.pig\" -p \"input=/w3c/input/small/data_w3c_small.txt\"\n\n    The following table shows the elements of the command:\n    <table border=\"1\">\n    <tr><td>File</td><td>Description</td></tr>\n    <tr><td>C:\\hdp\\pig-0.12.1.2.1.3.0-1981\\bin\\pig.cmd</td><td>The Pig command script.</td></tr>\n    <tr><td>C:\\hdp\\GettingStarted\\Pig\\w3c\\TotalHitsForPage.pig</td><td> You can substitute the Pig Latin script file with one of the following:\n    <ul>\n    <li>C:\\hdp\\GettingStarted\\Pig\\w3c\\AverageTimeTaken.pig</li>\n    <li>C:\\hdp\\GettingStarted\\Pig\\w3c\\ErrorsByPage.pig</li>\n    </ul>\n    </td></tr>\n    <tr><td>/w3c/input/small/data_w3c_small.txt</td><td> You can substitute the parameter with a larger file:\n\n    <ul>\n    <li>/w3c/input/medium/data_w3c_medium.txt</li>\n    <li>/w3c/input/large/data_w3c_large.txt</li>\n    </ul>\n\n    </td></tr>\n    </table>\n\n    The output should be similar to the following:\n\n        (/Info.aspx,1135)\n        (/UserService,1126)\n        (/Default.aspx,3380)\n\nNote that since Pig scripts compile to MapReduce jobs, and potentially to more than one such job, you might see multiple MapReduce jobs executing in the course of processing a Pig job.\n\n<!---\n### <a name=\"rebuild\"></a>Rebuild the samples\nThe samples currently contain all the required binaries, so building is not required. If you'd like to make changes to the Java or .NET samples, you can rebuild them by using either the Microsoft Build Engine (MSBuild) or the included Azure PowerShell script.\n\n\n**To rebuild the samples**\n\n1. Open a Hadoop command line.\n2. Run the following command:\n\n        powershell -F buildsamples.ps1\n--->\n\n##<a name=\"blobstorage\"></a>Connect to Azure Blob storage\nThe HDInsight Emulator uses HDFS as the default file system. However, Azure HDInsight uses Azure Blob storage as the default file system. It is possible to configure the HDInsight Emulator to use Azure Blob storage instead of local storage. Follow the instructions below to create a storage container in Azure and to connect it to the HDInsight Emulator.\n\n>[AZURE.NOTE] For more information on how HDInsight uses Azure Blob storage, see [Use Azure Blob storage with HDInsight](../hdinsight-use-blob-storage.md).\n\nBefore you start with the instructions below, you must have created a storage account. For instructions, see [How To Create a Storage Account](../storage-create-storage-account.md).\n\n**To create a container**\n\n1. Sign in to the [Azure Preview Portal](https://ms.portal.azure.com/).\n2. Click **NEW** on the left, click **Data + Storage**, and then click **Storage**.\n3. In the Storage Account blade, configure the properties as shown in the screen capture below.\n    \n    ![Create a storage account](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.create.storage.png)\n\n    Select **Pin to Startboard**, and the click **Create**.\n4. Once the storage account is created, from the new storage account blade, click **Containers** to open the containers blade, click **Add**.\n5. Enter the name of the container and then click **Select**.\n\n    ![Create a container](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.create.container.png)\n\nBefore you can access an Azure Storage account, you must add the account name and the account key to the configuration file.\n\n**To configure the connection to an Azure Storage account**\n\n1. Open **C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\etc\\hadoop\\core-site.xml** in Notepad.\n2. Add the following <property\\> tag next to the other <property\\> tags:\n\n        <property>\n            <name>fs.azure.account.key.<StorageAccountName>.blob.core.windows.net</name>\n            <value><StorageAccountKey></value>\n        </property>\n\n    You must substitute <StorageAccountName\\> and <StorageAccountKey\\> with the values that match your Storage account information.\n\n3. Save the change. You don't need to restart the Hadoop services.\n\nUse the following syntax to access the Storage account:\n\n    wasb://<ContainerName>@<StorageAccountName>.blob.core.windows.net/\n\nFor example:\n\n    hadoop fs -ls wasb://myContainer@myStorage.blob.core.windows.net/\n\n\n##<a name=\"powershell\"></a> Run Azure PowerShell\nSome of the Azure PowerShell cmdlets are also supported on the HDInsight Emulator. These cmdlets include:\n\n- HDInsight job definition cmdlets\n\n    - New-AzureHDInsightSqoopJobDefinition\n    - New-AzureHDInsightStreamingMapReduceJobDefinition\n    - New-AzureHDInsightPigJobDefinition\n    - New-AzureHDInsightHiveJobDefinition\n    - New-AzureHDInsightMapReduceJobDefinition\n- Start-AzureHDInsightJob\n- Get-AzureHDInsightJob\n- Wait-AzureHDInsightJob\n\nHere is a sample for submitting a Hadoop job:\n\n    $creds = Get-Credential (hadoop as username, password can be anything)\n    $hdinsightJob = <JobDefinition>\n    Start-AzureHDInsightJob -Cluster http://localhost:50111 -Credential $creds -JobDefinition $hdinsightJob\n\nYou will get a prompt when calling Get-Credential. You must use **hadoop** as the user name. The password can be any string. The cluster name is always **http://localhost:50111**.\n\nFor more information about submitting Hadoop jobs, see [Submit Hadoop jobs programmatically](hdinsight-submit-hadoop-jobs-programmatically.md). For more information about the Azure PowerShell cmdlets for HDInsight, see [HDInsight cmdlet reference][hdinsight-powershell-reference].\n\n\n##<a name=\"remove\"></a> Remove the HDInsight Emulator\nOn the computer where you have the emulator installed, open Control Panel and under **Programs**, click **Uninstall a Program**. From the list of installed programs, right-click **Microsoft HDInsight Emulator for Azure**, and then click **Uninstall**.\n\n\n##<a name=\"nextsteps\"></a> Next steps\nIn this MapReduce tutorial, you installed the HDInsight Emulator - a Hadoop sandbox - and ran some Hadoop jobs. To learn more, see the following articles:\n\n- [Get started using Azure HDInsight](../hdinsight-get-started.md)\n- [Develop Java MapReduce programs for HDInsight](hdinsight-develop-deploy-java-mapreduce.md)\n- [Develop C# Hadoop streaming MapReduce programs for HDInsight](hdinsight-hadoop-develop-deploy-streaming-jobs.md)\n- [HDInsight Emulator release notes](hdinsight-emulator-release-notes.md)\n- [MSDN forum for discussing HDInsight](http://social.msdn.microsoft.com/Forums/hdinsight)\n\n\n\n[azure-sdk]: http://azure.microsoft.com/downloads/\n[azure-create-storage-account]: ../storage-create-storage-account.md\n[azure-management-portal]: https://manage.windowsazure.com/\n[netstat-url]: http://technet.microsoft.com/library/ff961504.aspx\n\n[hdinsight-develop-mapreduce]: hdinsight-develop-deploy-java-mapreduce.md\n\n[hdinsight-emulator-install]: http://www.microsoft.com/web/gallery/install.aspx?appid=HDINSIGHT\n[hdinsight-emulator-release-notes]: hdinsight-emulator-release-notes.md\n\n[hdinsight-storage]: ../hdinsight-use-blob-storage.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-powershell-reference]: https://msdn.microsoft.com/library/dn858087.aspx\n[hdinsight-get-started]: ../hdinsight-get-started.md\n[hdinsight-develop-deploy-streaming]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n[hdinsight-versions]: hdinsight-component-versioning.md\n\n[Powershell-install-configure]: ../install-configure-powershell.md\n\n[hadoop-commands-manual]: http://hadoop.apache.org/docs/r1.1.1/commands_manual.html\n\n[image-hdi-emulator-services]: ./media/hdinsight-hadoop-emulator-get-started/HDI.Emulator.Services.png\n \ntest\n"
}