{
  "nodes": [
    {
      "content": "Migrate your data to SQL Data Warehouse | Microsoft Azure",
      "pos": [
        26,
        83
      ]
    },
    {
      "content": "Tips for migrating your data to Azure SQL Data Warehouse for developing solutions.",
      "pos": [
        101,
        183
      ]
    },
    {
      "content": "Migrate Your Data",
      "pos": [
        521,
        538
      ]
    },
    {
      "content": "The primary objective when migrating data is to populate your SQLDW database.",
      "pos": [
        539,
        616
      ]
    },
    {
      "content": "This process can be achieved in a number of ways.",
      "pos": [
        617,
        666
      ]
    },
    {
      "content": "ADF Copy, SSIS and bcp can all be used to achieve this goal.",
      "pos": [
        667,
        727
      ]
    },
    {
      "content": "However, as the amount of data increases you should think about breaking down the data migration process into steps.",
      "pos": [
        728,
        844
      ]
    },
    {
      "content": "This affords you the opportunity to optimize each step both for performance and for resilience to ensure a smooth data migration.",
      "pos": [
        845,
        974
      ]
    },
    {
      "content": "This article firstly discusses the simple migration scenarios of ADF Copy, SSIS and bcp.",
      "pos": [
        976,
        1064
      ]
    },
    {
      "content": "It then look a little deeper into how the migration can be optimized.",
      "pos": [
        1065,
        1134
      ]
    },
    {
      "content": "Azure Data Factory (ADF) copy",
      "pos": [
        1139,
        1168
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>ADF Copy<ept id=\"p1\">][]</ept> is part of <bpt id=\"p2\">[</bpt>Azure Data Factory<ept id=\"p2\">][]</ept>.",
      "pos": [
        1169,
        1216
      ]
    },
    {
      "content": "You can use ADF Copy to export your data to flat files residing on local storage, to remote flat files held in Azure blob storage or directly into SQL Data Warehouse.",
      "pos": [
        1217,
        1383
      ]
    },
    {
      "content": "If your data starts in flat files then you will first need to transfer it to Azure storage blob before initiating a load it into SQL Data Warehouse.",
      "pos": [
        1385,
        1533
      ]
    },
    {
      "content": "Once the data is transferred into Azure blob storage you can choose to use <bpt id=\"p1\">[</bpt>ADF Copy<ept id=\"p1\">][]</ept> again to push the data into SQL Data Warehouse.",
      "pos": [
        1534,
        1669
      ]
    },
    {
      "content": "PolyBase also provides a very high performance option for loading the data.",
      "pos": [
        1672,
        1747
      ]
    },
    {
      "content": "However, that does mean using two tools instead of one.",
      "pos": [
        1748,
        1803
      ]
    },
    {
      "content": "If you need the best performance then use PolyBase.",
      "pos": [
        1804,
        1855
      ]
    },
    {
      "content": "If you want a single tool experience (and the data is not massive) then ADF is your answer.",
      "pos": [
        1856,
        1947
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> PolyBase requires your data files to be in UTF-8.",
      "pos": [
        1951,
        2013
      ]
    },
    {
      "content": "This is ADF Copy's default encoding so there is nothing to change.",
      "pos": [
        2014,
        2080
      ]
    },
    {
      "content": "This is just a reminder to not change the default behavior of ADF Copy.",
      "pos": [
        2081,
        2152
      ]
    },
    {
      "pos": [
        2154,
        2224
      ],
      "content": "Head over to the following article for some great <bpt id=\"p1\">[</bpt><ept id=\"p1\">ADF Copy examples]</ept>."
    },
    {
      "content": "Integration Services",
      "pos": [
        2229,
        2249
      ]
    },
    {
      "content": "Integration Services (SSIS) is a powerful and flexible Extract Transform and Load (ETL) tool that supports complex workflows, data transformation, and several data loading options.",
      "pos": [
        2253,
        2433
      ]
    },
    {
      "content": "Use SSIS to simply transfer data to Azure or as part of a broader migration.",
      "pos": [
        2434,
        2510
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> SSIS can export to UTF-8 without the byte order mark in the file.",
      "pos": [
        2514,
        2592
      ]
    },
    {
      "content": "To configure this you must first use the derived column component to convert the character data in the data flow to use the 65001 UTF-8 code page.",
      "pos": [
        2593,
        2739
      ]
    },
    {
      "content": "Once the columns have been converted, write the data to the flat file destination adapter ensuring that 65001 has also been selected as the code page for the file.",
      "pos": [
        2740,
        2903
      ]
    },
    {
      "content": "SSIS connects to SQL Data Warehouse just as it would connect to a SQL Server deployment.",
      "pos": [
        2905,
        2993
      ]
    },
    {
      "content": "However, your connections will need to be using an ADO.NET connection manager.",
      "pos": [
        2994,
        3072
      ]
    },
    {
      "content": "You should also take care to configure the \"Use bulk insert when available\" setting to maximize throughput.",
      "pos": [
        3073,
        3180
      ]
    },
    {
      "content": "Please refer to the <bpt id=\"p1\">[</bpt>ADO.NET destination adapter<ept id=\"p1\">][]</ept> article to learn more about this property",
      "pos": [
        3181,
        3274
      ]
    },
    {
      "pos": [
        3278,
        3362
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Connecting to Azure SQL Data Warehouse by using OLEDB is not supported."
    },
    {
      "content": "In addition, there is always the possibility that a package might fail due to throttling or network issues.",
      "pos": [
        3364,
        3471
      ]
    },
    {
      "content": "Design packages so they can be resumed at the point of failure, without redoing work that completed before the failure.",
      "pos": [
        3472,
        3591
      ]
    },
    {
      "pos": [
        3593,
        3649
      ],
      "content": "For more information consult the <bpt id=\"p1\">[</bpt>SSIS documentation<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "bcp",
      "pos": [
        3654,
        3657
      ]
    },
    {
      "content": "bcp is a command-line utility that is designed for flat file data import and export.",
      "pos": [
        3658,
        3742
      ]
    },
    {
      "content": "Some transformation can can take place during data export.",
      "pos": [
        3743,
        3801
      ]
    },
    {
      "content": "To perform simple transformations use a query to select and transform the data.",
      "pos": [
        3802,
        3881
      ]
    },
    {
      "content": "Once exported, the flat files can then be loaded directly into the target the SQL Data Warehouse database.",
      "pos": [
        3882,
        3988
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> It is often a good idea to encapsulate the transformations used during data export in a view on the source system.",
      "pos": [
        3992,
        4119
      ]
    },
    {
      "content": "This ensures that the logic is retained and the process is repeatable.",
      "pos": [
        4120,
        4190
      ]
    },
    {
      "content": "Advantages of bcp are:",
      "pos": [
        4192,
        4214
      ]
    },
    {
      "content": "Simplicity.",
      "pos": [
        4218,
        4229
      ]
    },
    {
      "content": "bcp commands are simple to build and execute",
      "pos": [
        4230,
        4274
      ]
    },
    {
      "content": "Re-startable load process.",
      "pos": [
        4277,
        4303
      ]
    },
    {
      "content": "Once exported the load can be executed any number of times",
      "pos": [
        4304,
        4362
      ]
    },
    {
      "content": "Limitations of bcp are:",
      "pos": [
        4364,
        4387
      ]
    },
    {
      "content": "bcp works with tabulated flat files only.",
      "pos": [
        4391,
        4432
      ]
    },
    {
      "content": "It does not work with files such as xml or JSON",
      "pos": [
        4433,
        4480
      ]
    },
    {
      "content": "bcp does not support exporting to UTF-8.",
      "pos": [
        4483,
        4523
      ]
    },
    {
      "content": "This may prevent using PolyBase on bcp exported data",
      "pos": [
        4524,
        4576
      ]
    },
    {
      "content": "Data transformation capabilities are limited to the export stage only and are simple in nature",
      "pos": [
        4579,
        4673
      ]
    },
    {
      "content": "bcp has not been adapted to be robust when loading data over the internet.",
      "pos": [
        4676,
        4750
      ]
    },
    {
      "content": "Any network instability may cause a load error.",
      "pos": [
        4751,
        4798
      ]
    },
    {
      "content": "bcp relies on the schema being present in the target database prior to the load",
      "pos": [
        4801,
        4880
      ]
    },
    {
      "pos": [
        4882,
        4957
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Use bcp to load data into SQL Data Warehouse<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Optimizing data migration",
      "pos": [
        4962,
        4987
      ]
    },
    {
      "content": "A SQLDW data migration process can be effectively broken down into three discrete steps:",
      "pos": [
        4988,
        5076
      ]
    },
    {
      "content": "Export of source data",
      "pos": [
        5081,
        5102
      ]
    },
    {
      "content": "Transfer of data to Azure",
      "pos": [
        5106,
        5131
      ]
    },
    {
      "content": "Load into the target SQLDW database",
      "pos": [
        5135,
        5170
      ]
    },
    {
      "content": "Each step can be individually optimized to create a robust, re-startable and resilient migration process that maximises performance at each step.",
      "pos": [
        5172,
        5317
      ]
    },
    {
      "content": "Optimizing data load",
      "pos": [
        5322,
        5342
      ]
    },
    {
      "content": "Looking at these in reverse order for a moment; the fastest way to load data is via PolyBase.",
      "pos": [
        5343,
        5436
      ]
    },
    {
      "content": "Optimizing for a PolyBase load process places pre-quisites on the preceding steps so it's best to understand this upfront.",
      "pos": [
        5437,
        5559
      ]
    },
    {
      "content": "They are:",
      "pos": [
        5560,
        5569
      ]
    },
    {
      "content": "Encoding of data files",
      "pos": [
        5574,
        5596
      ]
    },
    {
      "content": "Format of data files",
      "pos": [
        5600,
        5620
      ]
    },
    {
      "content": "Location of data files",
      "pos": [
        5624,
        5646
      ]
    },
    {
      "content": "Encoding",
      "pos": [
        5652,
        5660
      ]
    },
    {
      "content": "PolyBase requires data files to be UTF-8 encoded.",
      "pos": [
        5661,
        5710
      ]
    },
    {
      "content": "This means that when you export your data it must conform to this requirement.",
      "pos": [
        5711,
        5789
      ]
    },
    {
      "content": "If your data only contains basic ASCII characters (not extended ASCII) then these map directly to the UTF-8 standard and you don't have to worry too much about the encoding.",
      "pos": [
        5790,
        5963
      ]
    },
    {
      "content": "However, if your data contains any special characters such as umlauts, accents or symbols or your data supports non-latin languages then you will have to ensure that your export files are properly UTF-8 encoded.",
      "pos": [
        5964,
        6175
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> bcp does not support exporting data to UTF-8.",
      "pos": [
        6179,
        6237
      ]
    },
    {
      "content": "Therefore your best option is to use either Integration Services or ADF Copy for the data export.",
      "pos": [
        6238,
        6335
      ]
    },
    {
      "content": "It is worth pointing out that the UTF-8 byte order mark (BOM) is not required in the data file.",
      "pos": [
        6336,
        6431
      ]
    },
    {
      "pos": [
        6433,
        6524
      ],
      "content": "Any files encoded using UTF-16 will need to be re-written <bpt id=\"p1\">***</bpt>prior<ept id=\"p1\">***</ept> to the data transfer."
    },
    {
      "content": "Format of data files",
      "pos": [
        6530,
        6550
      ]
    },
    {
      "content": "PolyBase mandates a fixed row terminator of \\n or newline.",
      "pos": [
        6551,
        6609
      ]
    },
    {
      "content": "Your data files must conform to this standard.",
      "pos": [
        6610,
        6656
      ]
    },
    {
      "content": "There aren't any restrictions on string or column terminators.",
      "pos": [
        6657,
        6719
      ]
    },
    {
      "content": "You will have to define every column in the file as part of your external table in PolyBase.",
      "pos": [
        6721,
        6813
      ]
    },
    {
      "content": "Make sure that all exported columns are required and that the types conform to the required standards.",
      "pos": [
        6814,
        6916
      ]
    },
    {
      "content": "Please refer back to the [migrate your schema] article for detail on supported data types.",
      "pos": [
        6918,
        7008
      ]
    },
    {
      "content": "Location of data files",
      "pos": [
        7014,
        7036
      ]
    },
    {
      "content": "SQL Data Warehouse uses PolyBase to load data from Azure Blob Storage exclusively.",
      "pos": [
        7037,
        7119
      ]
    },
    {
      "content": "Consequently, the data must have been first transferred into blob storage.",
      "pos": [
        7120,
        7194
      ]
    },
    {
      "content": "Optimizing data transfer",
      "pos": [
        7199,
        7223
      ]
    },
    {
      "content": "One of the slowest parts of data migration is the transfer of the data to Azure.",
      "pos": [
        7224,
        7304
      ]
    },
    {
      "content": "Not only can network bandwidth be an issue but also network reliability can seriously hamper progress.",
      "pos": [
        7305,
        7407
      ]
    },
    {
      "content": "By default migrating data to Azure is over the internet so the chances of transfer errors occurring are reasonably likely.",
      "pos": [
        7408,
        7530
      ]
    },
    {
      "content": "However, these errors may require data to be re-sent either in whole or in part.",
      "pos": [
        7531,
        7611
      ]
    },
    {
      "content": "Fortunately you have several options to improve the speed and resilience of this process:",
      "pos": [
        7613,
        7702
      ]
    },
    {
      "content": "ExpressRoute",
      "pos": [
        7709,
        7721
      ]
    },
    {
      "content": "You may want to consider using <bpt id=\"p1\">[</bpt>ExpressRoute<ept id=\"p1\">][]</ept> to speed up the transfer.",
      "pos": [
        7725,
        7798
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>ExpressRoute<ept id=\"p1\">][]</ept>provides you with an established private connection to Azure so the connection does not go over the public internet.",
      "pos": [
        7799,
        7931
      ]
    },
    {
      "content": "This is by no means a mandatory step.",
      "pos": [
        7932,
        7969
      ]
    },
    {
      "content": "However, it will improve throughput when pushing data to Azure from an on-premises or co-location facility.",
      "pos": [
        7970,
        8077
      ]
    },
    {
      "pos": [
        8079,
        8122
      ],
      "content": "The benefits of using <bpt id=\"p1\">[</bpt>ExpressRoute<ept id=\"p1\">][]</ept> are:"
    },
    {
      "content": "Increased reliability",
      "pos": [
        8127,
        8148
      ]
    },
    {
      "content": "Faster network speed",
      "pos": [
        8152,
        8172
      ]
    },
    {
      "content": "Lower network latency",
      "pos": [
        8176,
        8197
      ]
    },
    {
      "content": "higher network security",
      "pos": [
        8201,
        8224
      ]
    },
    {
      "pos": [
        8226,
        8307
      ],
      "content": "<bpt id=\"p1\">[</bpt>ExpressRoute<ept id=\"p1\">][]</ept> is beneficial for a number of scenarios; not just the migration."
    },
    {
      "content": "Interested?",
      "pos": [
        8309,
        8320
      ]
    },
    {
      "content": "For more information and pricing please visit the <bpt id=\"p1\">[</bpt>ExpressRoute documentation<ept id=\"p1\">][]</ept>.",
      "pos": [
        8321,
        8402
      ]
    },
    {
      "content": "Azure Import and Export Service",
      "pos": [
        8408,
        8439
      ]
    },
    {
      "content": "The Azure Import and Export Service is a data transfer process designed for large (GB++) to massive (TB++) transfers of data into Azure.",
      "pos": [
        8440,
        8576
      ]
    },
    {
      "content": "It involves writing your data to disks and shipping them to an Azure data center.",
      "pos": [
        8577,
        8658
      ]
    },
    {
      "content": "The disk contents will then be loaded into Azure Storage Blobss on your behalf.",
      "pos": [
        8659,
        8738
      ]
    },
    {
      "content": "A high level view of the import export process is as follows:",
      "pos": [
        8740,
        8801
      ]
    },
    {
      "content": "Configure an Azure Blob Storage container to receive the data",
      "pos": [
        8806,
        8867
      ]
    },
    {
      "content": "Export your data to local storage",
      "pos": [
        8871,
        8904
      ]
    },
    {
      "content": "Copy the data to 3.5 inch SATA II/III hard disk drives using the [Azure Import/Export Tool]",
      "pos": [
        8908,
        8999
      ]
    },
    {
      "content": "Create an Import Job using the Azure Import and Export Service providing the journal files produced by the [Azure Import/Export Tool]",
      "pos": [
        9003,
        9136
      ]
    },
    {
      "content": "Ship the disks your nominated Azure data center",
      "pos": [
        9140,
        9187
      ]
    },
    {
      "content": "Your data is transferred to your Azure Blob Storage container",
      "pos": [
        9191,
        9252
      ]
    },
    {
      "content": "Load the data into SQLDW using PolyBase",
      "pos": [
        9256,
        9295
      ]
    },
    {
      "pos": [
        9301,
        9319
      ],
      "content": "<bpt id=\"p1\">[</bpt>AZCopy<ept id=\"p1\">][]</ept> utility"
    },
    {
      "content": "The <bpt id=\"p1\">[</bpt>AZCopy<ept id=\"p1\">][]</ept> utility is a great tool for getting your data transferred into Azure Storage Blobs.",
      "pos": [
        9320,
        9418
      ]
    },
    {
      "content": "It is designed for small (MB++) to very large (GB++) data transfers.",
      "pos": [
        9419,
        9487
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt><ept id=\"p1\">AZCopy]</ept> has also been designed to provide good resilient throughput when transferring data to Azure and so is a great choice for the data transfer step.",
      "pos": [
        9488,
        9641
      ]
    },
    {
      "content": "Once transferred you can load the data using PolyBase into SQL Data Warehouse.",
      "pos": [
        9642,
        9720
      ]
    },
    {
      "content": "You can also incorporate AZCopy into your SSIS packages using an \"Execute Process\" task.",
      "pos": [
        9721,
        9809
      ]
    },
    {
      "content": "To use AZCopy you will first need to download and install it.",
      "pos": [
        9811,
        9872
      ]
    },
    {
      "content": "There is a <bpt id=\"p1\">[</bpt>production version<ept id=\"p1\">][]</ept> and a <bpt id=\"p2\">[</bpt>preview version<ept id=\"p2\">][]</ept> available.",
      "pos": [
        9873,
        9943
      ]
    },
    {
      "content": "To upload a file from your file system you will need a command like the one below:",
      "pos": [
        9945,
        10027
      ]
    },
    {
      "content": "A high level process summary could be:",
      "pos": [
        10153,
        10191
      ]
    },
    {
      "content": "Configure an Azure storage blob container to receive the data",
      "pos": [
        10196,
        10257
      ]
    },
    {
      "content": "Export your data to local storage",
      "pos": [
        10261,
        10294
      ]
    },
    {
      "content": "AZCopy your data in the Azure Blob Storage container",
      "pos": [
        10298,
        10350
      ]
    },
    {
      "content": "Load the data into SQL Data Warehouse using PolyBase",
      "pos": [
        10354,
        10406
      ]
    },
    {
      "pos": [
        10408,
        10449
      ],
      "content": "Full documentation available: <bpt id=\"p1\">[</bpt>AZCopy<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Optimizing data export",
      "pos": [
        10454,
        10476
      ]
    },
    {
      "content": "In addition to ensuring that the export conforms to the requirements laid out by PolyBase you can also seek to optimize the export of the data to improve the process further.",
      "pos": [
        10477,
        10651
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> As PolyBase requires the data to be in UTF-8 format it is unlikely you will use bcp to perform the data export.",
      "pos": [
        10655,
        10779
      ]
    },
    {
      "content": "bcp does not support outputting data files to  UTF-8.",
      "pos": [
        10780,
        10833
      ]
    },
    {
      "content": "SSIS or ADF Copy are much better suited to performing this kind of data export.",
      "pos": [
        10834,
        10913
      ]
    },
    {
      "content": "Data compression",
      "pos": [
        10919,
        10935
      ]
    },
    {
      "content": "PolyBase can read gzip compressed data.",
      "pos": [
        10936,
        10975
      ]
    },
    {
      "content": "If you are able to compress your data to gzip files then you will minimize the amount of data being pushed over the network.",
      "pos": [
        10976,
        11100
      ]
    },
    {
      "content": "Multiple files",
      "pos": [
        11106,
        11120
      ]
    },
    {
      "content": "Breaking up large tables into several files not only helps to improve export speed, it also helps with transfer re-startability, and the overall manageability of the data once in the Azure blob storage.",
      "pos": [
        11121,
        11323
      ]
    },
    {
      "content": "One of the many nice features of PolyBase is that it will read all the files inside a folder and treat it as one table.",
      "pos": [
        11324,
        11443
      ]
    },
    {
      "content": "It is therefore a good idea to isolate the files for each table into its own folder.",
      "pos": [
        11444,
        11528
      ]
    },
    {
      "content": "PolyBase also supports a feature known as \"recursive folder traversal\".",
      "pos": [
        11530,
        11601
      ]
    },
    {
      "content": "You can use this feature to further enhance the organization of your exported data to improve your data management.",
      "pos": [
        11602,
        11717
      ]
    },
    {
      "pos": [
        11720,
        11826
      ],
      "content": "To learn more about loading data with PolyBase, see <bpt id=\"p1\">[</bpt>Use PolyBase to load data into SQL Data Warehouse<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Next steps",
      "pos": [
        11832,
        11842
      ]
    },
    {
      "content": "For more about migration, see <bpt id=\"p1\">[</bpt>Migrate your solution to SQL Data Warehouse<ept id=\"p1\">][]</ept>.",
      "pos": [
        11843,
        11921
      ]
    },
    {
      "content": "For more development tips, see <bpt id=\"p1\">[</bpt>development overview<ept id=\"p1\">][]</ept>.",
      "pos": [
        11922,
        11978
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Migrate your data to SQL Data Warehouse | Microsoft Azure\"\n   description=\"Tips for migrating your data to Azure SQL Data Warehouse for developing solutions.\"\n   services=\"sql-data-warehouse\"\n   documentationCenter=\"NA\"\n   authors=\"jrowlandjones\"\n   manager=\"barbkess\"\n   editor=\"\"/>\n\n<tags\n   ms.service=\"sql-data-warehouse\"\n   ms.devlang=\"NA\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"NA\"\n   ms.workload=\"data-services\"\n   ms.date=\"06/25/2015\"\n   ms.author=\"JRJ@BigBangData.co.uk;barbkess\"/>\n\n# Migrate Your Data\nThe primary objective when migrating data is to populate your SQLDW database. This process can be achieved in a number of ways. ADF Copy, SSIS and bcp can all be used to achieve this goal. However, as the amount of data increases you should think about breaking down the data migration process into steps. This affords you the opportunity to optimize each step both for performance and for resilience to ensure a smooth data migration.\n\nThis article firstly discusses the simple migration scenarios of ADF Copy, SSIS and bcp. It then look a little deeper into how the migration can be optimized.\n\n## Azure Data Factory (ADF) copy\n[ADF Copy][] is part of [Azure Data Factory][]. You can use ADF Copy to export your data to flat files residing on local storage, to remote flat files held in Azure blob storage or directly into SQL Data Warehouse.\n\nIf your data starts in flat files then you will first need to transfer it to Azure storage blob before initiating a load it into SQL Data Warehouse. Once the data is transferred into Azure blob storage you can choose to use [ADF Copy][] again to push the data into SQL Data Warehouse. \n\nPolyBase also provides a very high performance option for loading the data. However, that does mean using two tools instead of one. If you need the best performance then use PolyBase. If you want a single tool experience (and the data is not massive) then ADF is your answer.\n\n> [AZURE.NOTE] PolyBase requires your data files to be in UTF-8. This is ADF Copy's default encoding so there is nothing to change. This is just a reminder to not change the default behavior of ADF Copy.\n\nHead over to the following article for some great [ADF Copy examples].\n\n## Integration Services ##\nIntegration Services (SSIS) is a powerful and flexible Extract Transform and Load (ETL) tool that supports complex workflows, data transformation, and several data loading options. Use SSIS to simply transfer data to Azure or as part of a broader migration.\n\n> [AZURE.NOTE] SSIS can export to UTF-8 without the byte order mark in the file. To configure this you must first use the derived column component to convert the character data in the data flow to use the 65001 UTF-8 code page. Once the columns have been converted, write the data to the flat file destination adapter ensuring that 65001 has also been selected as the code page for the file.\n\nSSIS connects to SQL Data Warehouse just as it would connect to a SQL Server deployment. However, your connections will need to be using an ADO.NET connection manager. You should also take care to configure the \"Use bulk insert when available\" setting to maximize throughput. Please refer to the [ADO.NET destination adapter][] article to learn more about this property\n\n> [AZURE.NOTE] Connecting to Azure SQL Data Warehouse by using OLEDB is not supported.\n\nIn addition, there is always the possibility that a package might fail due to throttling or network issues. Design packages so they can be resumed at the point of failure, without redoing work that completed before the failure.\n\nFor more information consult the [SSIS documentation][].\n\n## bcp\nbcp is a command-line utility that is designed for flat file data import and export. Some transformation can can take place during data export. To perform simple transformations use a query to select and transform the data. Once exported, the flat files can then be loaded directly into the target the SQL Data Warehouse database.\n\n> [AZURE.NOTE] It is often a good idea to encapsulate the transformations used during data export in a view on the source system. This ensures that the logic is retained and the process is repeatable.\n\nAdvantages of bcp are:\n\n- Simplicity. bcp commands are simple to build and execute\n- Re-startable load process. Once exported the load can be executed any number of times\n\nLimitations of bcp are:\n\n- bcp works with tabulated flat files only. It does not work with files such as xml or JSON\n- bcp does not support exporting to UTF-8. This may prevent using PolyBase on bcp exported data\n- Data transformation capabilities are limited to the export stage only and are simple in nature\n- bcp has not been adapted to be robust when loading data over the internet. Any network instability may cause a load error.\n- bcp relies on the schema being present in the target database prior to the load\n\nFor more information, see [Use bcp to load data into SQL Data Warehouse][].\n\n## Optimizing data migration\nA SQLDW data migration process can be effectively broken down into three discrete steps:\n\n1. Export of source data\n2. Transfer of data to Azure\n3. Load into the target SQLDW database\n\nEach step can be individually optimized to create a robust, re-startable and resilient migration process that maximises performance at each step.\n\n## Optimizing data load\nLooking at these in reverse order for a moment; the fastest way to load data is via PolyBase. Optimizing for a PolyBase load process places pre-quisites on the preceding steps so it's best to understand this upfront. They are:\n\n1. Encoding of data files\n2. Format of data files\n3. Location of data files\n\n### Encoding\nPolyBase requires data files to be UTF-8 encoded. This means that when you export your data it must conform to this requirement. If your data only contains basic ASCII characters (not extended ASCII) then these map directly to the UTF-8 standard and you don't have to worry too much about the encoding. However, if your data contains any special characters such as umlauts, accents or symbols or your data supports non-latin languages then you will have to ensure that your export files are properly UTF-8 encoded.\n\n> [AZURE.NOTE] bcp does not support exporting data to UTF-8. Therefore your best option is to use either Integration Services or ADF Copy for the data export. It is worth pointing out that the UTF-8 byte order mark (BOM) is not required in the data file.\n\nAny files encoded using UTF-16 will need to be re-written ***prior*** to the data transfer.\n\n### Format of data files\nPolyBase mandates a fixed row terminator of \\n or newline. Your data files must conform to this standard. There aren't any restrictions on string or column terminators.\n\nYou will have to define every column in the file as part of your external table in PolyBase. Make sure that all exported columns are required and that the types conform to the required standards.\n\nPlease refer back to the [migrate your schema] article for detail on supported data types.\n\n### Location of data files\nSQL Data Warehouse uses PolyBase to load data from Azure Blob Storage exclusively. Consequently, the data must have been first transferred into blob storage.\n\n## Optimizing data transfer\nOne of the slowest parts of data migration is the transfer of the data to Azure. Not only can network bandwidth be an issue but also network reliability can seriously hamper progress. By default migrating data to Azure is over the internet so the chances of transfer errors occurring are reasonably likely. However, these errors may require data to be re-sent either in whole or in part.\n\nFortunately you have several options to improve the speed and resilience of this process:\n\n### [ExpressRoute][]\nYou may want to consider using [ExpressRoute][] to speed up the transfer. [ExpressRoute][]provides you with an established private connection to Azure so the connection does not go over the public internet. This is by no means a mandatory step. However, it will improve throughput when pushing data to Azure from an on-premises or co-location facility.\n\nThe benefits of using [ExpressRoute][] are:\n\n1. Increased reliability\n2. Faster network speed\n3. Lower network latency\n4. higher network security\n\n[ExpressRoute][] is beneficial for a number of scenarios; not just the migration.\n\nInterested? For more information and pricing please visit the [ExpressRoute documentation][].\n\n### Azure Import and Export Service\nThe Azure Import and Export Service is a data transfer process designed for large (GB++) to massive (TB++) transfers of data into Azure. It involves writing your data to disks and shipping them to an Azure data center. The disk contents will then be loaded into Azure Storage Blobss on your behalf.\n\nA high level view of the import export process is as follows:\n\n1. Configure an Azure Blob Storage container to receive the data\n2. Export your data to local storage\n2. Copy the data to 3.5 inch SATA II/III hard disk drives using the [Azure Import/Export Tool]\n3. Create an Import Job using the Azure Import and Export Service providing the journal files produced by the [Azure Import/Export Tool]\n4. Ship the disks your nominated Azure data center\n5. Your data is transferred to your Azure Blob Storage container\n6. Load the data into SQLDW using PolyBase\n\n### [AZCopy][] utility\nThe [AZCopy][] utility is a great tool for getting your data transferred into Azure Storage Blobs. It is designed for small (MB++) to very large (GB++) data transfers. [AZCopy] has also been designed to provide good resilient throughput when transferring data to Azure and so is a great choice for the data transfer step. Once transferred you can load the data using PolyBase into SQL Data Warehouse. You can also incorporate AZCopy into your SSIS packages using an \"Execute Process\" task.\n\nTo use AZCopy you will first need to download and install it. There is a [production version][] and a [preview version][] available.\n\nTo upload a file from your file system you will need a command like the one below:\n\n```\nAzCopy /Source:C:\\myfolder /Dest:https://myaccount.blob.core.windows.net/mycontainer /DestKey:key /Pattern:abc.txt\n```\n\nA high level process summary could be:\n\n1. Configure an Azure storage blob container to receive the data\n2. Export your data to local storage\n3. AZCopy your data in the Azure Blob Storage container\n4. Load the data into SQL Data Warehouse using PolyBase\n\nFull documentation available: [AZCopy][].\n\n## Optimizing data export\nIn addition to ensuring that the export conforms to the requirements laid out by PolyBase you can also seek to optimize the export of the data to improve the process further.\n\n> [AZURE.NOTE] As PolyBase requires the data to be in UTF-8 format it is unlikely you will use bcp to perform the data export. bcp does not support outputting data files to  UTF-8. SSIS or ADF Copy are much better suited to performing this kind of data export.\n\n### Data compression\nPolyBase can read gzip compressed data. If you are able to compress your data to gzip files then you will minimize the amount of data being pushed over the network.\n\n### Multiple files\nBreaking up large tables into several files not only helps to improve export speed, it also helps with transfer re-startability, and the overall manageability of the data once in the Azure blob storage. One of the many nice features of PolyBase is that it will read all the files inside a folder and treat it as one table. It is therefore a good idea to isolate the files for each table into its own folder.\n\nPolyBase also supports a feature known as \"recursive folder traversal\". You can use this feature to further enhance the organization of your exported data to improve your data management. \n\nTo learn more about loading data with PolyBase, see [Use PolyBase to load data into SQL Data Warehouse][].\n\n\n## Next steps\nFor more about migration, see [Migrate your solution to SQL Data Warehouse][].\nFor more development tips, see [development overview][].\n\n<!--Image references-->\n\n<!--Article references-->\n[AZCopy]: ../storage/storage-use-azcopy.md\n[ADF Copy]: ../data-factory/data-factory-copy-activity.md\n[ADF Copy examples]: ../data-factory/data-factory-copy-activity-examples.md\n[development overview]: sql-data-warehouse-develop-overview.md\n[Migrate your solution to SQL Data Warehouse]: sql-data-warehouse-overview-migrate.md\n[SQL Data Warehouse development overview]: sql-data-warehouse-overview-develop.md\n[Use bcp to load data into SQL Data Warehouse]: sql-data-warehouse-load-with-bcp.md\n[Use PolyBase to load data into SQL Data Warehouse]: sql-data-warehouse-load-with-polybase.md\n\n\n<!--MSDN references-->\n\n<!--Other Web references-->\n[Azure Data Factory]: http://azure.microsoft.com/services/data-factory/\n[ExpressRoute]: http://azure.microsoft.com/services/expressroute/\n[ExpressRoute documentation]: http://azure.microsoft.com/documentation/services/expressroute/\n\n[production version]: http://aka.ms/downloadazcopy/\n[preview version]: http://aka.ms/downloadazcopypr/\n[ADO.NET destination adapter]: https://msdn.microsoft.com/en-us/library/bb934041.aspx\n[SSIS documentation]: https://msdn.microsoft.com/en-us/library/ms141026.aspx\n\n"
}