<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure</source>
          <target state="new">Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job.</source>
          <target state="new">Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Run Pig jobs on a Linux-based cluster with the Pig command (SSH)</source>
          <target state="new">Run Pig jobs on a Linux-based cluster with the Pig command (SSH)</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job.</source>
          <target state="new">In this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>The Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output.</source>
          <target state="new">The Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id="p1">[</bpt>Linux-based HDInsight Tips<ept id="p1">](hdinsight-hadoop-linux-information.md)</ept>.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id="p1">[</bpt>Linux-based HDInsight Tips<ept id="p1">](hdinsight-hadoop-linux-information.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="prereq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</source>
          <target state="new"><ph id="ph1">&lt;a id="prereq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>To complete the steps in this article, you will need the following.</source>
          <target state="new">To complete the steps in this article, you will need the following.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>A Linux-based HDInsight (Hadoop on HDInsight) cluster.</source>
          <target state="new">A Linux-based HDInsight (Hadoop on HDInsight) cluster.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>An SSH client.</source>
          <target state="new">An SSH client.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Linux, Unix, and Mac OS should come with an SSH client.</source>
          <target state="new">Linux, Unix, and Mac OS should come with an SSH client.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Windows users must download a client, such as <bpt id="p1">[</bpt>PuTTY<ept id="p1">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">Windows users must download a client, such as <bpt id="p1">[</bpt>PuTTY<ept id="p1">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="ssh"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Connect with SSH</source>
          <target state="new"><ph id="ph1">&lt;a id="ssh"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Connect with SSH</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</source>
          <target state="new">Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The FQDN will be the name you gave the cluster, then <bpt id="p1">**</bpt>.azurehdinsight.net<ept id="p1">**</ept>.</source>
          <target state="new">The FQDN will be the name you gave the cluster, then <bpt id="p1">**</bpt>.azurehdinsight.net<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>For example, the following would connect to a cluster named <bpt id="p1">**</bpt>myhdinsight<ept id="p1">**</ept>.</source>
          <target state="new">For example, the following would connect to a cluster named <bpt id="p1">**</bpt>myhdinsight<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>If you provided a certificate key for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system.</source>
          <target state="new"><bpt id="p1">**</bpt>If you provided a certificate key for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>If you provided a password for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you will need to provide the password when prompted.</source>
          <target state="new"><bpt id="p1">**</bpt>If you provided a password for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you will need to provide the password when prompted.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>For more information on using SSH with HDInsight, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</source>
          <target state="new">For more information on using SSH with HDInsight, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>PuTTY (Windows-based clients)</source>
          <target state="new">PuTTY (Windows-based clients)</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Windows does not provide a built-in SSH client.</source>
          <target state="new">Windows does not provide a built-in SSH client.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>We recommend using <bpt id="p1">**</bpt>PuTTY<ept id="p1">**</ept>, which can be downloaded from <bpt id="p2">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p2">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">We recommend using <bpt id="p1">**</bpt>PuTTY<ept id="p1">**</ept>, which can be downloaded from <bpt id="p2">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p2">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>For more information on using PuTTY, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</source>
          <target state="new">For more information on using PuTTY, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="pig"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use the Pig command</source>
          <target state="new"><ph id="ph1">&lt;a id="pig"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use the Pig command</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Once connected, start the Pig command-line interface (CLI) by using the following command.</source>
          <target state="new">Once connected, start the Pig command-line interface (CLI) by using the following command.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>After a moment, you should see a <ph id="ph1">`grunt&gt;`</ph> prompt.</source>
          <target state="new">After a moment, you should see a <ph id="ph1">`grunt&gt;`</ph> prompt.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Enter the following statement.</source>
          <target state="new">Enter the following statement.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>This command loads the contents of the sample.log file into LOGS.</source>
          <target state="new">This command loads the contents of the sample.log file into LOGS.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>You can view the contents of the file by using the following.</source>
          <target state="new">You can view the contents of the file by using the following.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following.</source>
          <target state="new">Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>You can use <bpt id="p1">**</bpt>DUMP<ept id="p1">**</ept> to view the data after the transformation.</source>
          <target state="new">You can use <bpt id="p1">**</bpt>DUMP<ept id="p1">**</ept> to view the data after the transformation.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>In this case, use <ph id="ph1">`DUMP LEVELS;`</ph>.</source>
          <target state="new">In this case, use <ph id="ph1">`DUMP LEVELS;`</ph>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Continue applying transformations by using the following statements.</source>
          <target state="new">Continue applying transformations by using the following statements.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Use <ph id="ph1">`DUMP`</ph> to view the result of the transformation after each step.</source>
          <target state="new">Use <ph id="ph1">`DUMP`</ph> to view the result of the transformation after each step.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Statement</source>
          <target state="new">Statement</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>What it does</source>
          <target state="new">What it does</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</source>
          <target state="new">FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</source>
          <target state="new">Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</source>
          <target state="new">GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Groups the rows by log level and stores the results into GROUPEDLEVELS.</source>
          <target state="new">Groups the rows by log level and stores the results into GROUPEDLEVELS.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</source>
          <target state="new">FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Creates a new set of data that contains each unique log level value and how many times it occurs.</source>
          <target state="new">Creates a new set of data that contains each unique log level value and how many times it occurs.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>This is stored into FREQUENCIES.</source>
          <target state="new">This is stored into FREQUENCIES.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>RESULT = order FREQUENCIES by COUNT desc;</source>
          <target state="new">RESULT = order FREQUENCIES by COUNT desc;</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Orders the log levels by count (descending) and stores into RESULT.</source>
          <target state="new">Orders the log levels by count (descending) and stores into RESULT.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>You can also save the results of a transformation by using the <ph id="ph1">`STORE`</ph> statement.</source>
          <target state="new">You can also save the results of a transformation by using the <ph id="ph1">`STORE`</ph> statement.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>For example, the following saves the <ph id="ph1">`RESULT`</ph> to the <bpt id="p1">**</bpt>/example/data/pigout<ept id="p1">**</ept> directory on the default storage container for your cluster.</source>
          <target state="new">For example, the following saves the <ph id="ph1">`RESULT`</ph> to the <bpt id="p1">**</bpt>/example/data/pigout<ept id="p1">**</ept> directory on the default storage container for your cluster.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The data is stored in the specified directory in files named <bpt id="p1">**</bpt>part-nnnnn<ept id="p1">**</ept>.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The data is stored in the specified directory in files named <bpt id="p1">**</bpt>part-nnnnn<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>If the directory already exists, you will receive an error.</source>
          <target state="new">If the directory already exists, you will receive an error.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>To exit the grunt prompt, enter the following statement.</source>
          <target state="new">To exit the grunt prompt, enter the following statement.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Pig Latin batch files</source>
          <target state="new">Pig Latin batch files</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>You can also use the Pig command to run Pig Latin contained in a file.</source>
          <target state="new">You can also use the Pig command to run Pig Latin contained in a file.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>After exiting the grunt prompt, use the following command to pipe STDIN into a file named <bpt id="p1">**</bpt>pigbatch.pig<ept id="p1">**</ept>.</source>
          <target state="new">After exiting the grunt prompt, use the following command to pipe STDIN into a file named <bpt id="p1">**</bpt>pigbatch.pig<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>This file will be created in the home directory for the account you are logged in to for the SSH session.</source>
          <target state="new">This file will be created in the home directory for the account you are logged in to for the SSH session.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Type or paste the following lines, and then use Ctrl+D when finished.</source>
          <target state="new">Type or paste the following lines, and then use Ctrl+D when finished.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Use the following to run the <bpt id="p1">**</bpt>pigbatch.pig<ept id="p1">**</ept> file by using the Pig command.</source>
          <target state="new">Use the following to run the <bpt id="p1">**</bpt>pigbatch.pig<ept id="p1">**</ept> file by using the Pig command.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Once the batch job finishes, you should see the following output, which should be the same as when you used <ph id="ph1">`DUMP RESULT;`</ph> in the previous steps.</source>
          <target state="new">Once the batch job finishes, you should see the following output, which should be the same as when you used <ph id="ph1">`DUMP RESULT;`</ph> in the previous steps.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</source>
          <target state="new"><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>As you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file.</source>
          <target state="new">As you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</source>
          <target state="new"><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>For general information on Pig in HDInsight.</source>
          <target state="new">For general information on Pig in HDInsight.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Use Pig with Hadoop on HDInsight</source>
          <target state="new">Use Pig with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>For information on other ways you can work with Hadoop on HDInsight.</source>
          <target state="new">For information on other ways you can work with Hadoop on HDInsight.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Use Hive with Hadoop on HDInsight</source>
          <target state="new">Use Hive with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Use MapReduce with Hadoop on HDInsight</source>
          <target state="new">Use MapReduce with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">d7c5dae08ea1e6a1b226af4da0bebbffcec77611</xliffext:olfilehash>
  </header>
</xliff>