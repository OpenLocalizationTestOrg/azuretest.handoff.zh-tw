{
  "nodes": [
    {
      "content": "Pig Activity",
      "pos": [
        28,
        40
      ]
    },
    {
      "content": "Learn how you can use the Pig Activity in an Azure data factory to run Pig scripts on an on-demand/your own HDInsight cluster.",
      "pos": [
        60,
        186
      ]
    },
    {
      "content": "Pig Activity",
      "pos": [
        514,
        526
      ]
    },
    {
      "content": "The HDInsight Pig activity in a Data Factory <bpt id=\"p1\">[</bpt>pipeline<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> executes Pig queries on <bpt id=\"p2\">[</bpt>your own<ept id=\"p2\">](data-factory-compute-linked-services.md#azure-hdinsight-linked-service)</ept> or <bpt id=\"p3\">[</bpt>on-demand<ept id=\"p3\">](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service)</ept> HDInsight cluster..",
      "pos": [
        528,
        841
      ]
    },
    {
      "content": "This article builds on the <bpt id=\"p1\">[</bpt>data transformation activities<ept id=\"p1\">](data-factory-data-transformation-activities.md)</ept> article which presents a general overview of data transformation and the supported transformation activities.",
      "pos": [
        842,
        1059
      ]
    },
    {
      "content": "Syntax",
      "pos": [
        1064,
        1070
      ]
    },
    {
      "content": "Syntax details",
      "pos": [
        2102,
        2116
      ]
    },
    {
      "content": "Property",
      "pos": [
        2118,
        2126
      ]
    },
    {
      "content": "Description",
      "pos": [
        2129,
        2140
      ]
    },
    {
      "content": "Required",
      "pos": [
        2143,
        2151
      ]
    },
    {
      "content": "name",
      "pos": [
        2186,
        2190
      ]
    },
    {
      "content": "Name of the activity",
      "pos": [
        2193,
        2213
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2216,
        2219
      ]
    },
    {
      "content": "description",
      "pos": [
        2220,
        2231
      ]
    },
    {
      "content": "Text describing what the activity is used for",
      "pos": [
        2234,
        2279
      ]
    },
    {
      "content": "No",
      "pos": [
        2282,
        2284
      ]
    },
    {
      "content": "type",
      "pos": [
        2285,
        2289
      ]
    },
    {
      "content": "HDinsightPig",
      "pos": [
        2292,
        2304
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2307,
        2310
      ]
    },
    {
      "content": "inputs",
      "pos": [
        2311,
        2317
      ]
    },
    {
      "content": "Input(s) consumed by the Pig activity",
      "pos": [
        2320,
        2357
      ]
    },
    {
      "content": "No",
      "pos": [
        2360,
        2362
      ]
    },
    {
      "content": "outputs",
      "pos": [
        2363,
        2370
      ]
    },
    {
      "content": "Output(s) produced by the Pig activity",
      "pos": [
        2373,
        2411
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2414,
        2417
      ]
    },
    {
      "content": "linkedServiceName",
      "pos": [
        2418,
        2435
      ]
    },
    {
      "content": "Reference to the HDInsight cluster registered as a linked service in Data Factory",
      "pos": [
        2438,
        2519
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2522,
        2525
      ]
    },
    {
      "content": "script",
      "pos": [
        2526,
        2532
      ]
    },
    {
      "content": "Specify the Pig script inline",
      "pos": [
        2535,
        2564
      ]
    },
    {
      "content": "No",
      "pos": [
        2567,
        2569
      ]
    },
    {
      "content": "script path",
      "pos": [
        2570,
        2581
      ]
    },
    {
      "content": "Store the Pig script in an Azure blob storage and provide the path to the file.",
      "pos": [
        2584,
        2663
      ]
    },
    {
      "content": "Use 'script' or 'scriptPath' property.",
      "pos": [
        2664,
        2702
      ]
    },
    {
      "content": "Both cannot be used together",
      "pos": [
        2703,
        2731
      ]
    },
    {
      "content": "No",
      "pos": [
        2734,
        2736
      ]
    },
    {
      "content": "defines",
      "pos": [
        2737,
        2744
      ]
    },
    {
      "content": "Specify parameters as key/value pairs for referencing within the Pig script",
      "pos": [
        2747,
        2822
      ]
    },
    {
      "content": "No",
      "pos": [
        2825,
        2827
      ]
    },
    {
      "content": "Example",
      "pos": [
        2832,
        2839
      ]
    },
    {
      "content": "Let’s consider an example of game logs analytics where you want to identify the time spent by users playing games launched by your company.",
      "pos": [
        2841,
        2980
      ]
    },
    {
      "content": "Below is a sample game log, which is comma (,) separated and contains the following fields – ProfileID, SessionStart, Duration, SrcIPAddress, and GameType.",
      "pos": [
        2983,
        3138
      ]
    },
    {
      "pos": [
        3411,
        3467
      ],
      "content": "The <bpt id=\"p1\">**</bpt>Pig script<ept id=\"p1\">**</ept> to process this data looks like this:"
    },
    {
      "content": "To execute this Pig script in a Data Factory pipeline, you need to the do the following:",
      "pos": [
        3969,
        4057
      ]
    },
    {
      "content": "Create a linked service to register <bpt id=\"p1\">[</bpt>your own HDInsight compute cluster<ept id=\"p1\">](data-factory-compute-linked-services.md#azure-hdinsight-linked-service)</ept> or configure <bpt id=\"p2\">[</bpt>on-demand HDInsight compute cluster<ept id=\"p2\">](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service)</ept>.",
      "pos": [
        4062,
        4340
      ]
    },
    {
      "content": "Let’s call this linked service “HDInsightLinkedService”.",
      "pos": [
        4341,
        4397
      ]
    },
    {
      "content": "Create a <bpt id=\"p1\">[</bpt>linked service<ept id=\"p1\">](data-factory-azure-storage-connector.md)</ept> to configure the connection to Azure Blob storage hosting the data.",
      "pos": [
        4402,
        4536
      ]
    },
    {
      "content": "Let’s call this linked service “StorageLinkedService”.",
      "pos": [
        4537,
        4591
      ]
    },
    {
      "content": "Create <bpt id=\"p1\">[</bpt>datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept> pointing to the input and the output data.",
      "pos": [
        4596,
        4689
      ]
    },
    {
      "content": "Let’s call the input dataset “PigSampleIn” and the output dataset “PigSampleOut”.",
      "pos": [
        4690,
        4771
      ]
    },
    {
      "content": "Copy the Pig query in a file the Azure Blob Storage configured in step #2 above.",
      "pos": [
        4776,
        4856
      ]
    },
    {
      "content": "if the linked service for hosting the data is different from the one hosting this query file, create a separate Azure Storage linked service and refer to it in the activity configuration.",
      "pos": [
        4857,
        5044
      ]
    },
    {
      "content": "Use **scriptPath **to specify the path to pig script file and <bpt id=\"p1\">**</bpt>scriptLinkedService<ept id=\"p1\">**</ept> to specify the Azure storage that contains the script file.",
      "pos": [
        5045,
        5190
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You can also provide the Pig script inline in the activity definition by using the <bpt id=\"p1\">**</bpt>script<ept id=\"p1\">**</ept> property but this is not recommended as all special characters in the script within the JSON document needs to be escaped and may cause debugging issues.",
      "pos": [
        5203,
        5463
      ]
    },
    {
      "content": "The best practice is to follow step #4.",
      "pos": [
        5464,
        5503
      ]
    },
    {
      "content": "Create the below pipeline with the HDInsightPig activity to process the data.",
      "pos": [
        5507,
        5584
      ]
    },
    {
      "content": "Deploy the pipeline.",
      "pos": [
        6494,
        6514
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Creating pipelines<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> article for details.",
      "pos": [
        6515,
        6594
      ]
    },
    {
      "content": "Monitor the pipeline using the data factory monitoring and management views.",
      "pos": [
        6599,
        6675
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Monitoring and manage Data Factory pipelines<ept id=\"p1\">](data-factory-monitor-manage-pipelines.md)</ept> article for details.",
      "pos": [
        6676,
        6789
      ]
    },
    {
      "content": "Specifying parameters for a Pig script using the defines element",
      "pos": [
        6794,
        6858
      ]
    },
    {
      "content": "Consider the example where the game logs are ingested daily into Azure Blob Storage and stored in a folder partitioned with date and time.",
      "pos": [
        6860,
        6998
      ]
    },
    {
      "content": "You want to parameterize the Pig script and pass the input folder location dynamically during runtime and also produce the output partitioned with date and time.",
      "pos": [
        6999,
        7160
      ]
    },
    {
      "content": "To use parameterize Pig script, do the following:",
      "pos": [
        7163,
        7212
      ]
    },
    {
      "pos": [
        7216,
        7253
      ],
      "content": "Define the parameters in <bpt id=\"p1\">**</bpt>defines<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        8725,
        8829
      ],
      "content": "In the Pig Script, refer to the parameters using '<bpt id=\"p1\">**</bpt>$parameterName<ept id=\"p1\">**</ept>' as shown in the following example:"
    },
    {
      "content": "Send Feedback",
      "pos": [
        9236,
        9249
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        9250,
        9307
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-pig-activity.md)</ept>.",
      "pos": [
        9308,
        9444
      ]
    },
    {
      "content": "test",
      "pos": [
        9447,
        9451
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Pig Activity\" \n    description=\"Learn how you can use the Pig Activity in an Azure data factory to run Pig scripts on an on-demand/your own HDInsight cluster.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/26/2015\" \n    ms.author=\"spelluru\"/>\n\n# Pig Activity\n\nThe HDInsight Pig activity in a Data Factory [pipeline](data-factory-create-pipelines.md) executes Pig queries on [your own](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) or [on-demand](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service) HDInsight cluster.. This article builds on the [data transformation activities](data-factory-data-transformation-activities.md) article which presents a general overview of data transformation and the supported transformation activities.\n\n## Syntax\n\n    {\n        \"name\": \"HiveActivitySamplePipeline\",\n        \"properties\": {\n        \"activities\": [\n            {\n                \"name\": \"Pig Activity\",\n                \"description\": \"description\",\n                \"type\": \"HDInsightPig\",\n                \"inputs\": [\n                    {\n                        \"name\": \"input tables\"\n                    }\n                ],\n                \"outputs\": [\n                    {\n                        \"name\": \"output tables\"\n                    }\n                ],\n                \"linkedServiceName\": \"MyHDInsightLinkedService\",\n                \"typeProperties\": {\n                    \"script\": \"Pig script\",\n                    \"scriptPath\": \"<pathtothePigscriptfileinAzureblobstorage>\",\n                    \"defines\": {\n                        \"param1\": \"param1Value\"\n                    }\n                },\n                \"scheduler\": {\n                    \"frequency\": \"Day\",\n                    \"interval\": 1\n                }\n            }\n        ]\n      }\n    }\n\n## Syntax details\n\nProperty | Description | Required\n-------- | ----------- | --------\nname | Name of the activity | Yes\ndescription | Text describing what the activity is used for | No\ntype | HDinsightPig | Yes\ninputs | Input(s) consumed by the Pig activity | No\noutputs | Output(s) produced by the Pig activity | Yes\nlinkedServiceName | Reference to the HDInsight cluster registered as a linked service in Data Factory | Yes\nscript | Specify the Pig script inline | No\nscript path | Store the Pig script in an Azure blob storage and provide the path to the file. Use 'script' or 'scriptPath' property. Both cannot be used together | No\ndefines | Specify parameters as key/value pairs for referencing within the Pig script | No\n\n## Example\n\nLet’s consider an example of game logs analytics where you want to identify the time spent by users playing games launched by your company.\n \nBelow is a sample game log, which is comma (,) separated and contains the following fields – ProfileID, SessionStart, Duration, SrcIPAddress, and GameType.\n\n    1809,2014-05-04 12:04:25.3470000,14,221.117.223.75,CaptureFlag\n    1703,2014-05-04 06:05:06.0090000,16,12.49.178.247,KingHill\n    1703,2014-05-04 10:21:57.3290000,10,199.118.18.179,CaptureFlag\n    1809,2014-05-04 05:24:22.2100000,23,192.84.66.141,KingHill\n    .....\n\nThe **Pig script** to process this data looks like this:\n\n    PigSampleIn = LOAD 'wasb://adfwalkthrough@anandsub14.blob.core.windows.net/samplein/' USING PigStorage(',') AS (ProfileID:chararray, SessionStart:chararray, Duration:int, SrcIPAddress:chararray, GameType:chararray);\n    \n    GroupProfile = Group PigSampleIn all;\n    \n    PigSampleOut = Foreach GroupProfile Generate PigSampleIn.ProfileID, SUM(PigSampleIn.Duration);\n    \n    Store PigSampleOut into 'wasb://adfwalkthrough@anandsub14.blob.core.windows.net/sampleoutpig/' USING PigStorage (',');\n\nTo execute this Pig script in a Data Factory pipeline, you need to the do the following:\n\n1. Create a linked service to register [your own HDInsight compute cluster](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) or configure [on-demand HDInsight compute cluster](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service). Let’s call this linked service “HDInsightLinkedService”.\n2.  Create a [linked service](data-factory-azure-storage-connector.md) to configure the connection to Azure Blob storage hosting the data. Let’s call this linked service “StorageLinkedService”.\n3.  Create [datasets](data-factory-create-datasets.md) pointing to the input and the output data. Let’s call the input dataset “PigSampleIn” and the output dataset “PigSampleOut”.\n4.  Copy the Pig query in a file the Azure Blob Storage configured in step #2 above. if the linked service for hosting the data is different from the one hosting this query file, create a separate Azure Storage linked service and refer to it in the activity configuration. Use **scriptPath **to specify the path to pig script file and **scriptLinkedService** to specify the Azure storage that contains the script file. \n    \n    > [AZURE.NOTE] You can also provide the Pig script inline in the activity definition by using the **script** property but this is not recommended as all special characters in the script within the JSON document needs to be escaped and may cause debugging issues. The best practice is to follow step #4.\n5. Create the below pipeline with the HDInsightPig activity to process the data.\n\n        {\n          \"name\": \"PigActivitySamplePipeline\",\n          \"properties\": {\n            \"activities\": [\n              {\n                \"name\": \"PigActivitySample\",\n                \"type\": \"HDInsightPig\",\n                \"inputs\": [\n                  {\n                    \"name\": \"PigSampleIn\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"PigSampleOut\"\n                  }\n                ],\n                \"linkedServiceName\": \"HDInsightLinkedService\",\n                \"typeproperties\": {\n                  \"scriptPath\": \"adfwalkthrough\\\\scripts\\\\enrichlogs.pig\",\n                  \"scriptLinkedService\": \"StorageLinkedService\"\n                },\n                \"scheduler\": {\n                    \"frequency\": \"Day\",\n                    \"interval\": 1\n                }\n              }\n            ]\n          }\n        } \n6. Deploy the pipeline. See [Creating pipelines](data-factory-create-pipelines.md) article for details. \n7. Monitor the pipeline using the data factory monitoring and management views. See [Monitoring and manage Data Factory pipelines](data-factory-monitor-manage-pipelines.md) article for details.\n\n## Specifying parameters for a Pig script using the defines element\n\nConsider the example where the game logs are ingested daily into Azure Blob Storage and stored in a folder partitioned with date and time. You want to parameterize the Pig script and pass the input folder location dynamically during runtime and also produce the output partitioned with date and time.\n \nTo use parameterize Pig script, do the following:\n\n- Define the parameters in **defines**.\n\n        {\n            \"name\": \"PigActivitySamplePipeline\",\n            \"properties\": {\n            \"activities\": [\n                {\n                    \"name\": \"PigActivitySample\",\n                    \"type\": \"HDInsightPig\",\n                    \"inputs\": [\n                        {\n                            \"name\": \"PigSampleIn\"\n                        }\n                    ],\n                    \"outputs\": [\n                        {\n                            \"name\": \"PigSampleOut\"\n                        }\n                    ],\n                    \"linkedServiceName\": \"HDInsightLinkedService\",\n                    \"typeproperties\": {\n                        \"scriptPath\": \"adfwalkthrough\\\\scripts\\\\samplepig.hql\",\n                        \"scriptLinkedService\": \"StorageLinkedService\",\n                        \"defines\": {\n                            \"Input\": \"$$Text.Format('wasb: //adfwalkthrough@<storageaccountname>.blob.core.windows.net/samplein/yearno={0: yyyy}/monthno={0: %M}/dayno={0: %d}/',SliceStart)\",\n                            \"Output\": \"$$Text.Format('wasb://adfwalkthrough@<storageaccountname>.blob.core.windows.net/sampleout/yearno={0:yyyy}/monthno={0:%M}/dayno={0:%d}/', SliceStart)\"\n                        }\n                    },\n                    \"scheduler\": {\n                        \"frequency\": \"Day\",\n                        \"interval\": 1\n                    }\n                }\n            ]\n          }\n        }  \n      \n- In the Pig Script, refer to the parameters using '**$parameterName**' as shown in the following example:\n\n        PigSampleIn = LOAD '$Input' USING PigStorage(',') AS (ProfileID:chararray, SessionStart:chararray, Duration:int, SrcIPAddress:chararray, GameType:chararray);   \n        GroupProfile = Group PigSampleIn all;       \n        PigSampleOut = Foreach GroupProfile Generate PigSampleIn.ProfileID, SUM(PigSampleIn.Duration);      \n        Store PigSampleOut into '$Output' USING PigStorage (','); \n\n\n \n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-pig-activity.md).\n\n\ntest\n"
}