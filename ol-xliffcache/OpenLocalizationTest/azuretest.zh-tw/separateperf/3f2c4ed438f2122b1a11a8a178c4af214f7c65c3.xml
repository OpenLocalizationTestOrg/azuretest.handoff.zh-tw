{
  "nodes": [
    {
      "content": "Build your first pipeline using Azure Data Factory",
      "pos": [
        27,
        77
      ]
    },
    {
      "content": "This tutorial shows you how to create a sample data pipeline that transforms data using Azure HDInsight.",
      "pos": [
        96,
        200
      ]
    },
    {
      "content": "Build your first pipeline using Azure Data Factory",
      "pos": [
        529,
        579
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        582,
        598
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        602,
        619
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        667,
        692
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        753,
        769
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        834,
        853
      ]
    },
    {
      "content": "This article helps you get started with building your first pipeline, and deploy it to the Azure Data Factory.",
      "pos": [
        908,
        1018
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> This article does not provide a conceptual overview of the Azure Data Factory service.",
      "pos": [
        1023,
        1122
      ]
    },
    {
      "content": "For a detailed overview of the service, see the <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">](data-factory-introduction.md)</ept> article.",
      "pos": [
        1123,
        1246
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        1251,
        1268
      ]
    },
    {
      "content": "This tutorial takes you through the steps needed to get your first pipeline up and running.",
      "pos": [
        1269,
        1360
      ]
    },
    {
      "content": "You will be creating pipelines, and specifying all the resources needed from scratch.",
      "pos": [
        1361,
        1446
      ]
    },
    {
      "content": "If you want to explore the various capabilities of the data factory quickly, without creating one from scratch, you can use the samples that we provide in the Azure Preview Portal.",
      "pos": [
        1448,
        1628
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Azure Data Factory Update: Simplified sample deployment<ept id=\"p1\">](http://azure.microsoft.com/blog/2015/04/24/azure-data-factory-update-simplified-sample-deployment/)</ept> on how to deploy an use-case based sample using the Azure Preview Portal.",
      "pos": [
        1629,
        1864
      ]
    },
    {
      "content": "Pre-requisites",
      "pos": [
        1869,
        1883
      ]
    },
    {
      "content": "Before you begin this tutorial, you must have the following pre-requisites:",
      "pos": [
        1884,
        1959
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure subscription<ept id=\"p1\">**</ept> - If you don't have an Azure subscription, you can create a free trial account in just a couple of minutes.",
      "pos": [
        1965,
        2095
      ]
    },
    {
      "content": "See the <bpt id=\"p1\">[</bpt>Free Trial<ept id=\"p1\">](http://azure.microsoft.com/pricing/free-trial/)</ept> article on how you can obtain a free trial account.",
      "pos": [
        2096,
        2216
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Storage<ept id=\"p1\">**</ept> – You will use an Azure storage account for storing the data in this tutorial.",
      "pos": [
        2222,
        2318
      ]
    },
    {
      "content": "If you don't have an Azure storage account, see the <bpt id=\"p1\">[</bpt>Create a storage account<ept id=\"p1\">](../storage-create-storage-account/#create-a-storage-account)</ept> article.",
      "pos": [
        2319,
        2467
      ]
    },
    {
      "content": "After you have created the storage account, you will need to obtain the account key used to access the storage.",
      "pos": [
        2468,
        2579
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>View, copy and regenerate storage access keys<ept id=\"p1\">](../storage-create-storage-account/#view-copy-and-regenerate-storage-access-keys)</ept>.",
      "pos": [
        2580,
        2713
      ]
    },
    {
      "content": "What’s covered in this tutorial?",
      "pos": [
        2718,
        2750
      ]
    },
    {
      "content": "Azure Data Factory enables you to compose data movement and data processing tasks as a data driven workflow.",
      "pos": [
        2752,
        2860
      ]
    },
    {
      "content": "You will learn how to build your first pipeline that uses HDInsight to transform and analyze web logs on a monthly basis.",
      "pos": [
        2861,
        2982
      ]
    },
    {
      "content": "In this tutorial, you will be performing the following steps:",
      "pos": [
        2986,
        3047
      ]
    },
    {
      "content": "Create a data factory.",
      "pos": [
        3053,
        3075
      ]
    },
    {
      "content": "Create following linked services:",
      "pos": [
        3080,
        3113
      ]
    },
    {
      "pos": [
        3122,
        3244
      ],
      "content": "<bpt id=\"p1\">**</bpt>Azure Storage Account<ept id=\"p1\">**</ept> – The Azure storage account will be used to store files used by the on-demand HDInsight cluster."
    },
    {
      "pos": [
        3253,
        3367
      ],
      "content": "<bpt id=\"p1\">**</bpt>On-Demand HDInsight Cluster<ept id=\"p1\">**</ept> – A HDInsight cluster will be started on-demand to transform and analyze the data."
    },
    {
      "content": "Create  the output dataset",
      "pos": [
        3372,
        3398
      ]
    },
    {
      "content": "Create the pipeline that runs a Hive script, and stores the result in the output dataset.",
      "pos": [
        3404,
        3493
      ]
    },
    {
      "content": "The Hive script first creates an external table, referencing the raw web log data stored in Azure blob storage.",
      "pos": [
        3494,
        3605
      ]
    },
    {
      "content": "The next step in the Hive script then partitions the raw data by year and month.",
      "pos": [
        3606,
        3686
      ]
    },
    {
      "pos": [
        3688,
        3906
      ],
      "content": "Your first pipeline, called <bpt id=\"p1\">**</bpt>MyFirstPipeline<ept id=\"p1\">**</ept>, uses a Hive activity to transform and analyze web logs that are deployed as part of the HDInsight cluster, and stored in <bpt id=\"p2\">**</bpt>/HdiSamples/WebsiteLogSampleData/SampleLog/<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Diagram View",
      "pos": [
        3911,
        3923
      ]
    },
    {
      "pos": [
        3991,
        4111
      ],
      "content": "After the Hive script executes, the results will be stored in an Azure blob storage container: <bpt id=\"p1\">**</bpt>data/partitioneddata<ept id=\"p1\">**</ept>."
    },
    {
      "content": "The availability defined on the <bpt id=\"p1\">**</bpt>AzureBlobOutput<ept id=\"p1\">**</ept> dataset determines how often the Hive activity is run.",
      "pos": [
        4113,
        4219
      ]
    },
    {
      "content": "In this tutorial, this is set to monthly.",
      "pos": [
        4220,
        4261
      ]
    },
    {
      "content": "Prepare Azure Storage for the tutorial",
      "pos": [
        4266,
        4304
      ]
    },
    {
      "content": "Before starting the tutorial, you need to prepare the Azure storage with files needed for the tutorial.",
      "pos": [
        4305,
        4408
      ]
    },
    {
      "content": "Launch notepad, paste the following text, and save it as <bpt id=\"p1\">**</bpt>partitionweblogs.hql<ept id=\"p1\">**</ept> in the C:\\adfgettingstarted folder on your hard drive.",
      "pos": [
        4413,
        4549
      ]
    },
    {
      "content": "This Hive scripts creates two external tables: <bpt id=\"p1\">**</bpt>WebLogsRaw<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>WebLogsPartitioned<ept id=\"p2\">**</ept>.",
      "pos": [
        4550,
        4639
      ]
    },
    {
      "pos": [
        4647,
        4751
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Replace <bpt id=\"p1\">**</bpt>storageaccountname<ept id=\"p1\">**</ept> in the last line with the name of your storage account."
    },
    {
      "content": "To prepare the Azure storage for the tutorial:",
      "pos": [
        6898,
        6944
      ]
    },
    {
      "content": "Download the <bpt id=\"p1\">[</bpt>latest version of <bpt id=\"p2\">**</bpt>AzCopy<ept id=\"p2\">**</ept><ept id=\"p1\">](http://aka.ms/downloadazcopy)</ept>, or the <bpt id=\"p3\">[</bpt>latest preview version<ept id=\"p3\">](http://aka.ms/downloadazcopypr)</ept>.",
      "pos": [
        6952,
        7091
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>How to use AzCopy<ept id=\"p1\">](../storage/storage-use-azcopy.md)</ept> article for instructions on using the utility.",
      "pos": [
        7092,
        7196
      ]
    },
    {
      "content": "After AzCopy has been installed, you can add it to the system path by running the following command at a command prompt.",
      "pos": [
        7204,
        7324
      ]
    },
    {
      "content": "Navigate to the c:\\adfgettingstarted folder, and run the following command to upload the Hive .HQL file to the storage account.",
      "pos": [
        7423,
        7550
      ]
    },
    {
      "content": "Replace <bpt id=\"p1\">**</bpt>&lt;StorageAccountName\\&gt;<ept id=\"p1\">**</ept> with the name of your storage account, and <bpt id=\"p2\">**</bpt>&lt;Storage Key\\&gt;<ept id=\"p2\">**</ept> with the storage account key.",
      "pos": [
        7551,
        7676
      ]
    },
    {
      "content": "After the file has been successfully uploaded, you will see the following output from AzCopy.",
      "pos": [
        7801,
        7894
      ]
    },
    {
      "content": "Do the following:",
      "pos": [
        8231,
        8248
      ]
    },
    {
      "pos": [
        8252,
        8446
      ],
      "content": "Click <bpt id=\"p1\">[</bpt>Using Data Factory Editor<ept id=\"p1\">](data-factory-build-your-first-pipeline-using-editor.md)</ept> link at the top to perform the tutorial by using Data Factory Editor, which is part of the Azure Portal."
    },
    {
      "pos": [
        8449,
        8600
      ],
      "content": "Click <bpt id=\"p1\">[</bpt>Using PowerShell<ept id=\"p1\">](data-factory-build-your-first-pipeline-using-powershell.md)</ept> link at the top to perform the tutorial by using Azure PowerShell."
    },
    {
      "pos": [
        8603,
        8746
      ],
      "content": "Click <bpt id=\"p1\">[</bpt>Using Visual Studio<ept id=\"p1\">](data-factory-build-your-first-pipeline-using-vs.md)</ept> link at the top to perform the tutorial by using Visual Studio."
    },
    {
      "content": "Send Feedback",
      "pos": [
        8752,
        8765
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        8766,
        8823
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline.md)</ept>.",
      "pos": [
        8824,
        8973
      ]
    },
    {
      "content": "test",
      "pos": [
        8975,
        8979
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Build your first pipeline using Azure Data Factory\"\n    description=\"This tutorial shows you how to create a sample data pipeline that transforms data using Azure HDInsight.\"\n    services=\"data-factory\"\n    documentationCenter=\"\"\n    authors=\"spelluru\"\n    manager=\"jhubbard\"\n    editor=\"monicar\"/>\n\n<tags\n    ms.service=\"data-factory\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"get-started-article\" \n    ms.date=\"07/27/2015\"\n    ms.author=\"spelluru\"/>\n\n# Build your first pipeline using Azure Data Factory\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-build-your-first-pipeline.md)\n- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)\n- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)\n- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)\n\nThis article helps you get started with building your first pipeline, and deploy it to the Azure Data Factory. \n\n> [AZURE.NOTE] This article does not provide a conceptual overview of the Azure Data Factory service. For a detailed overview of the service, see the [Introduction to Azure Data Factory](data-factory-introduction.md) article.\n\n## Tutorial Overview\nThis tutorial takes you through the steps needed to get your first pipeline up and running. You will be creating pipelines, and specifying all the resources needed from scratch.\n\nIf you want to explore the various capabilities of the data factory quickly, without creating one from scratch, you can use the samples that we provide in the Azure Preview Portal. See [Azure Data Factory Update: Simplified sample deployment](http://azure.microsoft.com/blog/2015/04/24/azure-data-factory-update-simplified-sample-deployment/) on how to deploy an use-case based sample using the Azure Preview Portal.\n\n## Pre-requisites\nBefore you begin this tutorial, you must have the following pre-requisites:\n\n1.  **Azure subscription** - If you don't have an Azure subscription, you can create a free trial account in just a couple of minutes. See the [Free Trial](http://azure.microsoft.com/pricing/free-trial/) article on how you can obtain a free trial account.\n\n2.  **Azure Storage** – You will use an Azure storage account for storing the data in this tutorial. If you don't have an Azure storage account, see the [Create a storage account](../storage-create-storage-account/#create-a-storage-account) article. After you have created the storage account, you will need to obtain the account key used to access the storage. See [View, copy and regenerate storage access keys](../storage-create-storage-account/#view-copy-and-regenerate-storage-access-keys).\n\n## What’s covered in this tutorial? \nAzure Data Factory enables you to compose data movement and data processing tasks as a data driven workflow. You will learn how to build your first pipeline that uses HDInsight to transform and analyze web logs on a monthly basis.  \n\nIn this tutorial, you will be performing the following steps:\n\n1.  Create a data factory.\n2.  Create following linked services:\n    1.  **Azure Storage Account** – The Azure storage account will be used to store files used by the on-demand HDInsight cluster.\n    2.  **On-Demand HDInsight Cluster** – A HDInsight cluster will be started on-demand to transform and analyze the data.\n3.  Create  the output dataset \n4.  Create the pipeline that runs a Hive script, and stores the result in the output dataset. The Hive script first creates an external table, referencing the raw web log data stored in Azure blob storage. The next step in the Hive script then partitions the raw data by year and month.\n\nYour first pipeline, called **MyFirstPipeline**, uses a Hive activity to transform and analyze web logs that are deployed as part of the HDInsight cluster, and stored in **/HdiSamples/WebsiteLogSampleData/SampleLog/**. \n\n![Diagram View](./media/data-factory-build-your-first-pipeline/diagram-view.png)\n\nAfter the Hive script executes, the results will be stored in an Azure blob storage container: **data/partitioneddata**.\n\nThe availability defined on the **AzureBlobOutput** dataset determines how often the Hive activity is run. In this tutorial, this is set to monthly.\n\n## Prepare Azure Storage for the tutorial\nBefore starting the tutorial, you need to prepare the Azure storage with files needed for the tutorial.\n\n1. Launch notepad, paste the following text, and save it as **partitionweblogs.hql** in the C:\\adfgettingstarted folder on your hard drive. This Hive scripts creates two external tables: **WebLogsRaw** and **WebLogsPartitioned**.\n\n    > [AZURE.IMPORTANT] Replace **storageaccountname** in the last line with the name of your storage account. \n\n        set hive.exec.dynamic.partition.mode=nonstrict;\n\n        DROP TABLE IF EXISTS WebLogsRaw; \n        CREATE EXTERNAL TABLE WebLogsRaw (\n          date  date,\n          time  string,\n          ssitename string,\n          csmethod  string,\n          csuristem  string,\n          csuriquery string,\n          sport int,\n          susername string,\n          cipcsUserAgent string,\n          csCookie string,\n          csReferer string,\n          cshost  string,\n          scstatus  int,\n          scsubstatus  int,\n          scwin32status  int,\n          scbytes int,\n          csbytes int,\n          timetaken int\n        )\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n        LINES TERMINATED BY '\\n' \n        LOCATION '/HdiSamples/WebsiteLogSampleData/SampleLog/'\n        tblproperties (\"skip.header.line.count\"=\"2\");\n        \n        DROP TABLE IF EXISTS WebLogsPartitioned ; \n        create external table WebLogsPartitioned (  \n          date  date,\n          time  string,\n          ssitename string,\n          csmethod  string,\n          csuristem  string,\n          csuriquery string,\n          sport int,\n          susername string,\n          cipcsUserAgent string,\n          csCookie string,\n          csReferer string,\n          cshost  string,\n          scstatus  int,\n          scsubstatus  int,\n          scwin32status  int,\n          scbytes int,\n          csbytes int,\n          timetaken int\n        )\n        partitioned by ( year int, month int)\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n        STORED AS TEXTFILE \n        LOCATION '${hiveconf:partitionedtable}';\n\n        INSERT INTO TABLE WebLogsPartitioned  PARTITION( year , month) \n        SELECT\n          date,\n          time,\n          ssitename,\n          csmethod,\n          csuristem,\n          csuriquery,\n          sport,\n          susername,\n          cipcsUserAgent,\n          csCookie,\n          csReferer,\n          cshost,\n          scstatus,\n          scsubstatus,\n          scwin32status,\n          scbytes,\n          csbytes,\n          timetaken,\n          year(date),\n          month(date)\n        FROM WebLogsRaw\n\n     \n \n2. To prepare the Azure storage for the tutorial:\n    1. Download the [latest version of **AzCopy**](http://aka.ms/downloadazcopy), or the [latest preview version](http://aka.ms/downloadazcopypr). See [How to use AzCopy](../storage/storage-use-azcopy.md) article for instructions on using the utility.\n    2. After AzCopy has been installed, you can add it to the system path by running the following command at a command prompt. \n    \n            set path=%path%;C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\n    \n\n    3. Navigate to the c:\\adfgettingstarted folder, and run the following command to upload the Hive .HQL file to the storage account. Replace **<StorageAccountName\\>** with the name of your storage account, and **<Storage Key\\>** with the storage account key.\n\n            AzCopy /Source:. /Dest:https://<StorageAccountName>.blob.core.windows.net/script /DestKey:<Storage Key>\n    4. After the file has been successfully uploaded, you will see the following output from AzCopy.\n    \n            Finished 1 of total 1 file(s).\n            [2015/06/15 15:47:13] Transfer summary:\n            -----------------\n            Total files transferred: 1\n            Transfer successfully:   1\n            Transfer skipped:        0\n            Transfer failed:         0\n            Elapsed time:            00.00:00:01\n\nDo the following:\n\n- Click [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md) link at the top to perform the tutorial by using Data Factory Editor, which is part of the Azure Portal.\n- Click [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) link at the top to perform the tutorial by using Azure PowerShell.\n- Click [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) link at the top to perform the tutorial by using Visual Studio. \n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline.md). \ntest\n"
}