{
  "nodes": [
    {
      "content": "Script Action development with HDInsight | Microsoft Azure",
      "pos": [
        27,
        85
      ]
    },
    {
      "content": "Learn how to customize Hadoop clusters with Script Action.",
      "pos": [
        104,
        162
      ]
    },
    {
      "content": "Develop Script Action scripts for HDInsight",
      "pos": [
        486,
        529
      ]
    },
    {
      "content": "Learn how to write Script Action scripts for HDInsight.",
      "pos": [
        531,
        586
      ]
    },
    {
      "content": "For information on using Script Action scripts, see <bpt id=\"p1\">[</bpt>Customize HDInsight clusters using Script Action<ept id=\"p1\">](hdinsight-hadoop-customize-cluster.md)</ept>.",
      "pos": [
        587,
        729
      ]
    },
    {
      "content": "For the same article written for the HDInsight cluster on Linux operating system, see <bpt id=\"p1\">[</bpt>Develop Script Action scripts for HDInsight<ept id=\"p1\">](hdinsight-hadoop-script-actions-linux.md)</ept>.",
      "pos": [
        730,
        904
      ]
    },
    {
      "content": "Script Action can be used to install additional software running on a Hadoop cluster or to change the configuration of applications installed on a cluster.",
      "pos": [
        906,
        1061
      ]
    },
    {
      "content": "Script actions are scripts that run on the cluster nodes when HDInsight clusters are deployed, and they are executed once nodes in the cluster complete HDInsight configuration.",
      "pos": [
        1062,
        1238
      ]
    },
    {
      "content": "A script action is executed under system admin account privileges and provides full access rights to the cluster nodes.",
      "pos": [
        1239,
        1358
      ]
    },
    {
      "content": "Each cluster can be provided with a list of script actions to be executed in the order in which they are specified.",
      "pos": [
        1359,
        1474
      ]
    },
    {
      "pos": [
        1479,
        1538
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you experience the following error message:"
    },
    {
      "content": "It is because you didn't include the helper methods.",
      "pos": [
        1842,
        1894
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Helper methods for custom scripts<ept id=\"p1\">](hdinsight-hadoop-script-actions.md#helper-methods-for-custom-scripts)</ept>.",
      "pos": [
        1896,
        2006
      ]
    },
    {
      "content": "Sample scripts",
      "pos": [
        2011,
        2025
      ]
    },
    {
      "content": "For provisioning HDInsight clusters on Windows operating system, the Script Action is Azure PowerShell script.The following is a sample script for configure the site configuration files:",
      "pos": [
        2027,
        2213
      ]
    },
    {
      "content": "The script takes four parameters, the configuration file name, the property you want to modify, the value you want to set, and a description.",
      "pos": [
        3712,
        3853
      ]
    },
    {
      "content": "For example:",
      "pos": [
        3854,
        3866
      ]
    },
    {
      "content": "These parameters will set the hive.metastore.client.socket.timeout value to 90 in the hive-site.xml file.",
      "pos": [
        3928,
        4033
      ]
    },
    {
      "content": "The default value is 60 seconds.",
      "pos": [
        4035,
        4067
      ]
    },
    {
      "pos": [
        4069,
        4276
      ],
      "content": "This sample script can also be found at <bpt id=\"p1\">[</bpt>https://hditutorialdata.blob.core.windows.net/customizecluster/editSiteConfig.ps1<ept id=\"p1\">](https://hditutorialdata.blob.core.windows.net/customizecluster/editSiteConfig.ps1)</ept>."
    },
    {
      "content": "HDInsight provides several scripts to install additional components on HDInsight clusters:",
      "pos": [
        4279,
        4369
      ]
    },
    {
      "content": "Name",
      "pos": [
        4371,
        4375
      ]
    },
    {
      "content": "Script",
      "pos": [
        4378,
        4384
      ]
    },
    {
      "content": "Install Spark",
      "pos": [
        4401,
        4414
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1.",
      "pos": [
        4419,
        4511
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use Spark on HDInsight clusters<ept id=\"p1\">][hdinsight-install-spark]</ept>.",
      "pos": [
        4512,
        4587
      ]
    },
    {
      "content": "Install R",
      "pos": [
        4590,
        4599
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/rconfigactionv02/r-installer-v02.ps1.",
      "pos": [
        4604,
        4688
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use R on HDInsight clusters<ept id=\"p1\">][hdinsight-r-scripts]</ept>.",
      "pos": [
        4689,
        4756
      ]
    },
    {
      "content": "Install Solr",
      "pos": [
        4759,
        4771
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/solrconfigactionv01/solr-installer-v01.ps1.",
      "pos": [
        4776,
        4866
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use Solr on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-solr-install.md)</ept>.",
      "pos": [
        4867,
        4950
      ]
    },
    {
      "pos": [
        4951,
        4971
      ],
      "content": "- <bpt id=\"p1\">**</bpt>Install Giraph<ept id=\"p1\">**</ept>"
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/giraphconfigactionv01/giraph-installer-v01.ps1.",
      "pos": [
        4974,
        5068
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use Giraph on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-giraph-install.md)</ept>.",
      "pos": [
        5069,
        5156
      ]
    },
    {
      "content": "Script Action can be deployed from the Azure preview portal, Azure PowerShell or by using the HDInsight .NET SDK.",
      "pos": [
        5158,
        5271
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Customize HDInsight clusters using Script Action<ept id=\"p1\">][hdinsight-cluster-customize]</ept>.",
      "pos": [
        5273,
        5379
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The sample scripts work only with HDInsight cluster version 3.1 or above.",
      "pos": [
        5383,
        5469
      ]
    },
    {
      "content": "For more information on HDInsight cluster versions, see <bpt id=\"p1\">[</bpt>HDInsight cluster versions<ept id=\"p1\">](../hdinsight-component-versioning/)</ept>.",
      "pos": [
        5470,
        5591
      ]
    },
    {
      "content": "Helper methods for custom scripts",
      "pos": [
        5600,
        5633
      ]
    },
    {
      "content": "Script Action helper methods are utilities that you can use while writing custom scripts.",
      "pos": [
        5635,
        5724
      ]
    },
    {
      "content": "These are defined in <bpt id=\"p1\">[</bpt>https://hdiconfigactions.blob.core.windows.net/configactionmodulev05/HDInsightUtilities-v05.psm1<ept id=\"p1\">](https://hdiconfigactions.blob.core.windows.net/configactionmodulev05/HDInsightUtilities-v05.psm1)</ept>, and can be included in your scripts using the following:",
      "pos": [
        5725,
        6000
      ]
    },
    {
      "content": "Here are the helper methods that are provided by this script:",
      "pos": [
        6681,
        6742
      ]
    },
    {
      "content": "Helper method",
      "pos": [
        6744,
        6757
      ]
    },
    {
      "content": "Description",
      "pos": [
        6760,
        6771
      ]
    },
    {
      "content": "Save-HDIFile",
      "pos": [
        6803,
        6815
      ]
    },
    {
      "content": "Download a file from the specified Uniform Resource Identifier (URI) to a location on the local disk that is associated with the Azure VM node assigned to the cluster.",
      "pos": [
        6820,
        6987
      ]
    },
    {
      "content": "Expand-HDIZippedFile",
      "pos": [
        6990,
        7010
      ]
    },
    {
      "content": "Unzip a zipped file.",
      "pos": [
        7015,
        7035
      ]
    },
    {
      "content": "Invoke-HDICmdScript",
      "pos": [
        7038,
        7057
      ]
    },
    {
      "content": "Run a script from cmd.exe.",
      "pos": [
        7062,
        7088
      ]
    },
    {
      "content": "Write-HDILog",
      "pos": [
        7091,
        7103
      ]
    },
    {
      "content": "Write output from the custom script used for a script action.",
      "pos": [
        7108,
        7169
      ]
    },
    {
      "content": "Get-Services",
      "pos": [
        7172,
        7184
      ]
    },
    {
      "content": "Get a list of services running on the machine where the script executes.",
      "pos": [
        7189,
        7261
      ]
    },
    {
      "content": "Get-Service",
      "pos": [
        7264,
        7275
      ]
    },
    {
      "content": "With the specific service name as input, get detailed information for a specific service (service name, process ID, state, etc.) on the machine where the script executes.",
      "pos": [
        7280,
        7450
      ]
    },
    {
      "content": "Get-HDIServices",
      "pos": [
        7453,
        7468
      ]
    },
    {
      "content": "Get a list of HDInsight services running on the computer where the script executes.",
      "pos": [
        7473,
        7556
      ]
    },
    {
      "content": "Get-HDIService",
      "pos": [
        7559,
        7573
      ]
    },
    {
      "content": "With the specific HDInsight service name as input, get detailed information for a specific service (service name, process ID, state, etc.) on the machine where the script executes.",
      "pos": [
        7578,
        7758
      ]
    },
    {
      "content": "Get-ServicesRunning",
      "pos": [
        7761,
        7780
      ]
    },
    {
      "content": "Get a list of services that are running on the computer where the script executes.",
      "pos": [
        7785,
        7867
      ]
    },
    {
      "content": "Get-ServiceRunning",
      "pos": [
        7870,
        7888
      ]
    },
    {
      "content": "Check if a specific service (by name) is running on the computer where the script executes.",
      "pos": [
        7893,
        7984
      ]
    },
    {
      "content": "Get-HDIServicesRunning",
      "pos": [
        7987,
        8009
      ]
    },
    {
      "content": "Get a list of HDInsight services running on the computer where the script executes.",
      "pos": [
        8014,
        8097
      ]
    },
    {
      "content": "Get-HDIServiceRunning",
      "pos": [
        8100,
        8121
      ]
    },
    {
      "content": "Check if a specific HDInsight service (by name) is running on the computer where the script executes.",
      "pos": [
        8126,
        8227
      ]
    },
    {
      "content": "Get-HDIHadoopVersion",
      "pos": [
        8230,
        8250
      ]
    },
    {
      "content": "Get the version of Hadoop installed on the computer where the script executes.",
      "pos": [
        8255,
        8333
      ]
    },
    {
      "content": "Test-IsHDIHeadNode",
      "pos": [
        8336,
        8354
      ]
    },
    {
      "content": "Check if the computer where the script executes is a head node.",
      "pos": [
        8359,
        8422
      ]
    },
    {
      "content": "Test-IsActiveHDIHeadNode",
      "pos": [
        8425,
        8449
      ]
    },
    {
      "content": "Check if the computer where the script executes is an active head node.",
      "pos": [
        8454,
        8525
      ]
    },
    {
      "content": "Test-IsHDIDataNode",
      "pos": [
        8528,
        8546
      ]
    },
    {
      "content": "Check if the computer where the script executes is a data node.",
      "pos": [
        8551,
        8614
      ]
    },
    {
      "content": "Edit-HDIConfigFile",
      "pos": [
        8617,
        8635
      ]
    },
    {
      "content": "Edit the config files hive-site.xml, core-site.xml, hdfs-site.xml, mapred-site.xml, or yarn-site.xml.",
      "pos": [
        8640,
        8741
      ]
    },
    {
      "content": "Best practices for script development",
      "pos": [
        8747,
        8784
      ]
    },
    {
      "content": "When you develop a custom script for an HDInsight cluster, there are several best practices to keep in mind:",
      "pos": [
        8786,
        8894
      ]
    },
    {
      "content": "Check for the Hadoop version",
      "pos": [
        8898,
        8926
      ]
    },
    {
      "content": "Only HDInsight version 3.1 (Hadoop 2.4) and above support using Script Action to install custom components on a cluster.",
      "pos": [
        8932,
        9052
      ]
    },
    {
      "content": "In your custom script, you must use the <bpt id=\"p1\">**</bpt>Get-HDIHadoopVersion<ept id=\"p1\">**</ept> helper method to check the Hadoop version before proceeding with performing other tasks in the script.",
      "pos": [
        9053,
        9220
      ]
    },
    {
      "content": "Provide stable links to script resources",
      "pos": [
        9225,
        9265
      ]
    },
    {
      "pos": [
        9271,
        10041
      ],
      "content": "Users should make sure that all of the scripts and other artifacts used in the customization of a cluster remain available throughout the lifetime of the cluster and that the versions of these files do not change for the duration. These resources are required if the re-imaging of nodes in the cluster is required. The best practice is to download and archive everything in a Storage account that the user controls. This can be the default Storage account or any of the additional Storage accounts specified at the time of deployment for a customized cluster.\n  In the Spark and R customized cluster samples provided in the documentation, for example, we have made a local copy of the resources in this Storage account: https://hdiconfigactions.blob.core.windows.net/.",
      "leadings": [
        "",
        "  "
      ],
      "nodes": [
        {
          "content": "Users should make sure that all of the scripts and other artifacts used in the customization of a cluster remain available throughout the lifetime of the cluster and that the versions of these files do not change for the duration. These resources are required if the re-imaging of nodes in the cluster is required. The best practice is to download and archive everything in a Storage account that the user controls. This can be the default Storage account or any of the additional Storage accounts specified at the time of deployment for a customized cluster.",
          "pos": [
            0,
            559
          ],
          "nodes": [
            {
              "content": "Users should make sure that all of the scripts and other artifacts used in the customization of a cluster remain available throughout the lifetime of the cluster and that the versions of these files do not change for the duration.",
              "pos": [
                0,
                230
              ]
            },
            {
              "content": "These resources are required if the re-imaging of nodes in the cluster is required.",
              "pos": [
                231,
                314
              ]
            },
            {
              "content": "The best practice is to download and archive everything in a Storage account that the user controls.",
              "pos": [
                315,
                415
              ]
            },
            {
              "content": "This can be the default Storage account or any of the additional Storage accounts specified at the time of deployment for a customized cluster.",
              "pos": [
                416,
                559
              ]
            }
          ]
        },
        {
          "content": "In the Spark and R customized cluster samples provided in the documentation, for example, we have made a local copy of the resources in this Storage account: https://hdiconfigactions.blob.core.windows.net/.",
          "pos": [
            562,
            768
          ]
        }
      ]
    },
    {
      "content": "Ensure that the cluster customization script is idempotent",
      "pos": [
        10046,
        10104
      ]
    },
    {
      "content": "You must expect that the nodes of an HDInsight cluster will be re-imaged during the cluster lifetime.",
      "pos": [
        10110,
        10211
      ]
    },
    {
      "content": "The cluster customization script is run whenever a cluster is re-imaged.",
      "pos": [
        10212,
        10284
      ]
    },
    {
      "content": "This script must be designed to be idempotent in the sense that upon re-imaging, the script should ensure that the cluster is returned to the same customized state that it was in just after the script ran for the first time when the cluster was initially created.",
      "pos": [
        10285,
        10548
      ]
    },
    {
      "content": "For example, if a custom script installed an application at D:\\AppLocation on its first run, then on each subsequent run, upon re-imaging, the script should check whether the application exists at the D:\\AppLocation location before proceeding with other steps in the script.",
      "pos": [
        10549,
        10823
      ]
    },
    {
      "content": "Install custom components in the optimal location",
      "pos": [
        10828,
        10877
      ]
    },
    {
      "content": "When cluster nodes are re-imaged, the C:\\ resource drive and D:\\ system drive can be re-formatted, resulting in the loss of data and applications that had been installed on those drives.",
      "pos": [
        10883,
        11069
      ]
    },
    {
      "content": "This could also happen if an Azure virtual machine (VM) node that is part of the cluster goes down and is replaced by a new node.",
      "pos": [
        11070,
        11199
      ]
    },
    {
      "content": "You can install components on the D:\\ drive or in the C:\\apps location on the cluster.",
      "pos": [
        11200,
        11286
      ]
    },
    {
      "content": "All other locations on the C:\\ drive are reserved.",
      "pos": [
        11287,
        11337
      ]
    },
    {
      "content": "Specify the location where applications or libraries are to be installed in the cluster customization script.",
      "pos": [
        11338,
        11447
      ]
    },
    {
      "content": "Ensure high availability of the cluster architecture",
      "pos": [
        11452,
        11504
      ]
    },
    {
      "content": "HDInsight has an active-passive architecture for high availability, in which one head node is in active mode (where the HDInsight services are running) and the other head node is in standby mode (in which HDInsight services are not running).",
      "pos": [
        11510,
        11751
      ]
    },
    {
      "content": "The nodes switch active and passive modes if HDInsight services are interrupted.",
      "pos": [
        11752,
        11832
      ]
    },
    {
      "content": "If a script action is used to install services on both head nodes for high availability, note that the HDInsight failover mechanism will not be able to automatically fail over these user-installed services.",
      "pos": [
        11833,
        12039
      ]
    },
    {
      "content": "So user-installed services on HDInsight head nodes that are expected to be highly available must either have their own failover mechanism if in active-passive mode or be in active-active mode.",
      "pos": [
        12040,
        12232
      ]
    },
    {
      "content": "An HDInsight Script Action command runs on both head nodes when the head-node role is specified as a value in the <bpt id=\"p1\">*</bpt>ClusterRoleCollection<ept id=\"p1\">*</ept> parameter (documented below in the section <bpt id=\"p2\">[</bpt>How to run a script action<ept id=\"p2\">](#runScriptAction)</ept>).",
      "pos": [
        12238,
        12467
      ]
    },
    {
      "content": "So when you design a custom script, make sure that your script is aware of this setup.",
      "pos": [
        12468,
        12554
      ]
    },
    {
      "content": "You should not run into problems where the same services are installed and started on both of the head nodes and they end up competing with each other.",
      "pos": [
        12555,
        12706
      ]
    },
    {
      "content": "Also, be aware that data will be lost during re-imaging, so software installed via Script Action has to be resilient to such events.",
      "pos": [
        12707,
        12839
      ]
    },
    {
      "content": "Applications should be designed to work with highly available data that is distributed across many nodes.",
      "pos": [
        12840,
        12945
      ]
    },
    {
      "content": "Note that as many as 1/5 of the nodes in a cluster can be re-imaged at the same time.",
      "pos": [
        12946,
        13031
      ]
    },
    {
      "content": "Configure the custom components to use Azure Blob storage",
      "pos": [
        13036,
        13093
      ]
    },
    {
      "content": "The custom components that you install on the cluster nodes might have a default configuration to use Hadoop Distributed File System (HDFS) storage.",
      "pos": [
        13099,
        13247
      ]
    },
    {
      "content": "You should change the configuration to use Azure Blob storage instead.",
      "pos": [
        13248,
        13318
      ]
    },
    {
      "content": "On a cluster re-image, the HDFS file system gets formatted and you would lose any data that is stored there.",
      "pos": [
        13319,
        13427
      ]
    },
    {
      "content": "Using Azure Blob storage instead ensures that your data will be retained.",
      "pos": [
        13428,
        13501
      ]
    },
    {
      "content": "Common usage patterns",
      "pos": [
        13506,
        13527
      ]
    },
    {
      "content": "This section provides guidance on implementing some of the common usage patterns that you might run into while writing your own custom script.",
      "pos": [
        13529,
        13671
      ]
    },
    {
      "content": "Configure environment variables",
      "pos": [
        13677,
        13708
      ]
    },
    {
      "content": "Often in script action development, you will feel the need to set environment variables.",
      "pos": [
        13710,
        13798
      ]
    },
    {
      "content": "For instance, a most likely scenario is when you download a binary from an external site, install it on the cluster, and add the location of where it is installed to your ‘PATH’ environment variable.",
      "pos": [
        13799,
        13998
      ]
    },
    {
      "content": "The following snippet shows you how to set environment variables in the custom script.",
      "pos": [
        13999,
        14085
      ]
    },
    {
      "content": "This statement sets the environment variable <bpt id=\"p1\">**</bpt>MDS_RUNNER_CUSTOM_CLUSTER<ept id=\"p1\">**</ept> to the value 'true' and also sets the scope of this variable to be machine-wide.",
      "pos": [
        14253,
        14408
      ]
    },
    {
      "content": "At times it is important that environment variables are set at the appropriate scope – machine or user.",
      "pos": [
        14409,
        14512
      ]
    },
    {
      "content": "Refer [here][1] for more information on setting environment variables.",
      "pos": [
        14513,
        14583
      ]
    },
    {
      "content": "Access to locations where the custom scripts are stored",
      "pos": [
        14589,
        14644
      ]
    },
    {
      "content": "Scripts used to customize a cluster needs to either be in the default storage account for the cluster or in a public read-only container on any other storage account.",
      "pos": [
        14646,
        14812
      ]
    },
    {
      "content": "If your script accesses resources located elsewhere these need to be in a publicly accessible (at least public read-only).",
      "pos": [
        14813,
        14935
      ]
    },
    {
      "content": "For instance you might want to access a file and save it using the SaveFile-HDI command.",
      "pos": [
        14936,
        15024
      ]
    },
    {
      "content": "In this example, you must ensure that the container 'somecontainer' in storage account 'somestorageaccount' is publicly accessible.",
      "pos": [
        15219,
        15350
      ]
    },
    {
      "content": "Otherwise, the script will throw a ‘Not Found’ exception and fail.",
      "pos": [
        15351,
        15417
      ]
    },
    {
      "content": "Pass parameters to the Add-AzureHDInsightScriptAction cmdlet",
      "pos": [
        15423,
        15483
      ]
    },
    {
      "content": "To pass multiple parameters to the Add-AzureHDInsightScriptAction cmdlet, you need to format the string value to contain all parameters for the script.",
      "pos": [
        15485,
        15636
      ]
    },
    {
      "content": "For example:",
      "pos": [
        15637,
        15649
      ]
    },
    {
      "content": "or",
      "pos": [
        15746,
        15748
      ]
    },
    {
      "content": "Throw exception for failed cluster deployment",
      "pos": [
        15864,
        15909
      ]
    },
    {
      "content": "If you want to get accurately notified of the fact that cluster customization did not succeed as expected, it is important to throw an exception and fail the cluster provisioning.",
      "pos": [
        15911,
        16090
      ]
    },
    {
      "content": "For instance, you might want to process a file if it exists and handle the error case where the file does not exist.",
      "pos": [
        16091,
        16207
      ]
    },
    {
      "content": "This would ensure that the script exits gracefully and the state of the cluster is correctly known.",
      "pos": [
        16208,
        16307
      ]
    },
    {
      "content": "The following snippet gives an example of how to achieve this:",
      "pos": [
        16308,
        16370
      ]
    },
    {
      "content": "In this snippet, if the file did not exist, it would lead to a state where the script actually exits gracefully after printing the error message, and the cluster reaches running state assuming it \"successfully\" completed cluster customization process.",
      "pos": [
        16545,
        16796
      ]
    },
    {
      "content": "If you want to be accurately notified of the fact that cluster customization essentially did not succeed as expected because of a missing file, it is more appropriate to throw an exception and fail the cluster customization step.",
      "pos": [
        16797,
        17026
      ]
    },
    {
      "content": "To achieve this you must use the following sample code snippet instead.",
      "pos": [
        17027,
        17098
      ]
    },
    {
      "content": "Checklist for deploying a script action",
      "pos": [
        17278,
        17317
      ]
    },
    {
      "content": "Here are the steps we took when preparing to deploy these scripts:",
      "pos": [
        17318,
        17384
      ]
    },
    {
      "content": "Put the files that contain the custom scripts in a place that is accessible by the cluster nodes during deployment.",
      "pos": [
        17389,
        17504
      ]
    },
    {
      "content": "This can be any of the default or additional Storage accounts specified at the time of cluster deployment, or any other publicly accessible storage container.",
      "pos": [
        17505,
        17663
      ]
    },
    {
      "content": "Add checks into scripts to make sure that they execute idempotently, so that the script can be executed multiple times on the same node.",
      "pos": [
        17667,
        17803
      ]
    },
    {
      "content": "Use the <bpt id=\"p1\">**</bpt>Write-Output<ept id=\"p1\">**</ept> Azure PowerShell cmdlet to print to STDOUT as well as STDERR.",
      "pos": [
        17807,
        17893
      ]
    },
    {
      "content": "Do not use <bpt id=\"p1\">**</bpt>Write-Host<ept id=\"p1\">**</ept>.",
      "pos": [
        17894,
        17920
      ]
    },
    {
      "content": "Use a temporary file folder, such as $env:TEMP, to keep the downloaded file used by the scripts and then clean them up after scripts have executed.",
      "pos": [
        17924,
        18071
      ]
    },
    {
      "content": "Install custom software only at D:\\ or C:\\apps.",
      "pos": [
        18075,
        18122
      ]
    },
    {
      "content": "Other locations on the C: drive should not be used as they are reserved.",
      "pos": [
        18123,
        18195
      ]
    },
    {
      "content": "Note that installing files on the C: drive outside of the C:\\apps folder may result in setup failures during re-images of the node.",
      "pos": [
        18196,
        18327
      ]
    },
    {
      "content": "In the event that OS-level settings or Hadoop service configuration files were changed, you may want to restart HDInsight services so that they can pick up any OS-level settings, such as the environment variables set in the scripts.",
      "pos": [
        18331,
        18563
      ]
    },
    {
      "content": "Test custom scripts with the HDInsight Emulator",
      "pos": [
        18570,
        18617
      ]
    },
    {
      "content": "A straight-forward way to test a custom script before using it in the HDInsight Script Action command is to run it on the HDInsight Emulator.",
      "pos": [
        18619,
        18760
      ]
    },
    {
      "content": "You can install the HDInsight Emulator locally or on an Azure infrastructure as a service (IaaS) Windows Server 2012 R2 VM or on a local machine and observe if the script behaves correctly.",
      "pos": [
        18761,
        18950
      ]
    },
    {
      "content": "Note that the Windows Server 2012 R2 VM is the same VM that HDInsight uses for its nodes.",
      "pos": [
        18951,
        19040
      ]
    },
    {
      "content": "This section outlines the procedure for using the HDInsight Emulator locally for testing purposes, but the procedure for using a VM is similar.",
      "pos": [
        19042,
        19185
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Install the HDInsight Emulator<ept id=\"p1\">**</ept> - To run Script Action locally, you must have the HDInsight Emulator installed.",
      "pos": [
        19187,
        19301
      ]
    },
    {
      "content": "For instructions on how to install it, see <bpt id=\"p1\">[</bpt>Get started with the HDInsight Emulator<ept id=\"p1\">](../hdinsight-get-started-emulator/)</ept>.",
      "pos": [
        19302,
        19423
      ]
    },
    {
      "pos": [
        19425,
        19619
      ],
      "content": "<bpt id=\"p1\">**</bpt>Set the execution policy for Azure PowerShell<ept id=\"p1\">**</ept> - Open Azure PowerShell and run (as administrator) the following command to set the execution policy to <bpt id=\"p2\">*</bpt>LocalMachine<ept id=\"p2\">*</ept> and to be <bpt id=\"p3\">*</bpt>Unrestricted<ept id=\"p3\">*</ept>:"
    },
    {
      "content": "We need this policy to be unrestricted as scripts are not signed.",
      "pos": [
        19679,
        19744
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Download the script action<ept id=\"p1\">**</ept> that you want to run to a local destination.",
      "pos": [
        19746,
        19821
      ]
    },
    {
      "content": "The following sample scripts are available to download from the following locations:",
      "pos": [
        19822,
        19906
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Spark<ept id=\"p1\">**</ept>.",
      "pos": [
        19910,
        19920
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv02/spark-installer-v02.ps1",
      "pos": [
        19921,
        20012
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>R<ept id=\"p1\">**</ept>.",
      "pos": [
        20015,
        20021
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/rconfigactionv02/r-installer-v02.ps1",
      "pos": [
        20022,
        20105
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Solr<ept id=\"p1\">**</ept>.",
      "pos": [
        20108,
        20117
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/solrconfigactionv01/solr-installer-v01.ps1",
      "pos": [
        20118,
        20207
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Giraph<ept id=\"p1\">**</ept>.",
      "pos": [
        20210,
        20221
      ]
    },
    {
      "content": "https://hdiconfigactions.blob.core.windows.net/giraphconfigactionv01/giraph-installer-v01.ps1",
      "pos": [
        20222,
        20315
      ]
    },
    {
      "pos": [
        20317,
        20483
      ],
      "content": "<bpt id=\"p1\">**</bpt>Run the script action<ept id=\"p1\">**</ept> - Open a new Azure PowerShell window in admin mode and run the Spark or R installation script from the local location where they were saved."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Usage examples<ept id=\"p1\">**</ept>",
      "pos": [
        20485,
        20503
      ]
    },
    {
      "content": "When you're using the Spark and R clusters, data files needed may not be present in the HDInsight Emulator.",
      "pos": [
        20504,
        20611
      ]
    },
    {
      "content": "So you may need to upload relevant .txt files that contain data to a path in HDFS and then use that path to access the data.",
      "pos": [
        20612,
        20736
      ]
    },
    {
      "content": "For example:",
      "pos": [
        20737,
        20749
      ]
    },
    {
      "content": "Note that in some cases a custom script may actually depend on HDInsight components, such as detecting whether certain Hadoop services are up.",
      "pos": [
        20819,
        20961
      ]
    },
    {
      "content": "In this case, you will have to test your custom scripts by deploying them on an actual HDInsight cluster.",
      "pos": [
        20962,
        21067
      ]
    },
    {
      "content": "Debug custom scripts",
      "pos": [
        21073,
        21093
      ]
    },
    {
      "content": "The script error logs are stored, along with other output, in the default Storage account that you specified for the cluster at its creation.",
      "pos": [
        21095,
        21236
      ]
    },
    {
      "content": "The logs are stored in a table with the name <bpt id=\"p1\">*</bpt>u&lt;\\cluster-name-fragment&gt;&lt;\\time-stamp&gt;setuplog<ept id=\"p1\">*</ept>.",
      "pos": [
        21237,
        21331
      ]
    },
    {
      "content": "These are aggregated logs that have records from all of the nodes (head node and worker nodes) on which the script runs in the cluster.",
      "pos": [
        21332,
        21467
      ]
    },
    {
      "content": "An easy way to check the logs is to use HDInsight Tools for Visual Studio.",
      "pos": [
        21468,
        21542
      ]
    },
    {
      "content": "For installing the tools, see <bpt id=\"p1\">[</bpt>Get started using Visual Studio Hadoop tools for HDInsight<ept id=\"p1\">](hdinsight-hadoop-visual-studio-tools-get-started.md#install-hdinsight-tools-for-visual-studio)</ept>",
      "pos": [
        21543,
        21728
      ]
    },
    {
      "content": "To check the log using Visual Studio",
      "pos": [
        21732,
        21768
      ]
    },
    {
      "content": "Open Visual Studio.",
      "pos": [
        21775,
        21794
      ]
    },
    {
      "pos": [
        21798,
        21849
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>View<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Server Explorer<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        21853,
        21958
      ],
      "content": "Right-click \"Azure\", click Connect to <bpt id=\"p1\">**</bpt>Microsoft Azure Subscriptions<ept id=\"p1\">**</ept>, and then enter your credentials."
    },
    {
      "pos": [
        21962,
        22104
      ],
      "content": "Expand <bpt id=\"p1\">**</bpt>Storage<ept id=\"p1\">**</ept>, expand the Azure storage account used as the default file system, expand <bpt id=\"p2\">**</bpt>Tables<ept id=\"p2\">**</ept>, and then double-click the table name."
    },
    {
      "content": "You can also remote into the cluster nodes to see the both STDOUT and STDERR for custom scripts.",
      "pos": [
        22107,
        22203
      ]
    },
    {
      "content": "The logs on each node are specific only to that node and are logged into <bpt id=\"p1\">**</bpt>C:\\HDInsightLogs\\DeploymentAgent.log<ept id=\"p1\">**</ept>.",
      "pos": [
        22204,
        22318
      ]
    },
    {
      "content": "These log files record all outputs from the custom script.",
      "pos": [
        22319,
        22377
      ]
    },
    {
      "content": "An example log snippet for a Spark script action looks like this:",
      "pos": [
        22378,
        22443
      ]
    },
    {
      "content": "In this log, it is clear that the Spark script action has been executed on the VM named HEADNODE0 and that no exceptions were thrown during the execution.",
      "pos": [
        23738,
        23892
      ]
    },
    {
      "content": "In the event that an execution failure occurs, the output describing it will also be contained in this log file.",
      "pos": [
        23894,
        24006
      ]
    },
    {
      "content": "The information provided in these logs should be helpful in debugging script problems that may arise.",
      "pos": [
        24007,
        24108
      ]
    },
    {
      "content": "See also",
      "pos": [
        24114,
        24122
      ]
    },
    {
      "content": "Customize HDInsight clusters using Script Action",
      "pos": [
        24127,
        24175
      ]
    },
    {
      "content": "Install and use Spark on HDInsight clusters",
      "pos": [
        24209,
        24252
      ]
    },
    {
      "content": "Install and use R on HDInsight clusters",
      "pos": [
        24282,
        24321
      ]
    },
    {
      "pos": [
        24346,
        24425
      ],
      "content": "<bpt id=\"p1\">[</bpt>Install and use Solr on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-solr-install.md)</ept>."
    },
    {
      "pos": [
        24428,
        24511
      ],
      "content": "<bpt id=\"p1\">[</bpt>Install and use Giraph on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-giraph-install.md)</ept>."
    },
    {
      "content": "test",
      "pos": [
        24919,
        24923
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Script Action development with HDInsight | Microsoft Azure\"\n    description=\"Learn how to customize Hadoop clusters with Script Action.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"09/03/2015\"\n    ms.author=\"jgao\"/>\n\n# Develop Script Action scripts for HDInsight\n\nLearn how to write Script Action scripts for HDInsight. For information on using Script Action scripts, see [Customize HDInsight clusters using Script Action](hdinsight-hadoop-customize-cluster.md). For the same article written for the HDInsight cluster on Linux operating system, see [Develop Script Action scripts for HDInsight](hdinsight-hadoop-script-actions-linux.md).\n\nScript Action can be used to install additional software running on a Hadoop cluster or to change the configuration of applications installed on a cluster. Script actions are scripts that run on the cluster nodes when HDInsight clusters are deployed, and they are executed once nodes in the cluster complete HDInsight configuration. A script action is executed under system admin account privileges and provides full access rights to the cluster nodes. Each cluster can be provided with a list of script actions to be executed in the order in which they are specified. \n\n> [AZURE.NOTE] If you experience the following error message: \n> \n>     System.Management.Automation.CommandNotFoundException; ExceptionMessage : The term 'Save-HDIFile' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\n> It is because you didn't include the helper methods.  See [Helper methods for custom scripts](hdinsight-hadoop-script-actions.md#helper-methods-for-custom-scripts).\n\n## Sample scripts\n\nFor provisioning HDInsight clusters on Windows operating system, the Script Action is Azure PowerShell script.The following is a sample script for configure the site configuration files:\n\n    param (\n        [parameter(Mandatory)][string] $ConfigFileName,\n        [parameter(Mandatory)][string] $Name,\n        [parameter(Mandatory)][string] $Value,\n        [parameter()][string] $Description\n    )\n\n    if (!$Description) {\n        $Description = \"\"\n    }\n\n    $hdiConfigFiles = @{\n        \"hive-site.xml\" = \"$env:HIVE_HOME\\conf\\hive-site.xml\";\n        \"core-site.xml\" = \"$env:HADOOP_HOME\\etc\\hadoop\\core-site.xml\";\n        \"hdfs-site.xml\" = \"$env:HADOOP_HOME\\etc\\hadoop\\hdfs-site.xml\";\n        \"mapred-site.xml\" = \"$env:HADOOP_HOME\\etc\\hadoop\\mapred-site.xml\";\n        \"yarn-site.xml\" = \"$env:HADOOP_HOME\\etc\\hadoop\\yarn-site.xml\"\n    }\n\n    if (!($hdiConfigFiles[$ConfigFileName])) {\n        Write-HDILog \"Unable to configure $ConfigFileName because it is not part of the HDI configuration files.\"\n        return\n    }\n\n    [xml]$configFile = Get-Content $hdiConfigFiles[$ConfigFileName]\n\n    $existingproperty = $configFile.configuration.property | where {$_.Name -eq $Name}\n\n    if ($existingproperty) {\n        $existingproperty.Value = $Value\n        $existingproperty.Description = $Description\n    } else {\n        $newproperty = @($configFile.configuration.property)[0].Clone()\n        $newproperty.Name = $Name\n        $newproperty.Value = $Value\n        $newproperty.Description = $Description\n        $configFile.configuration.AppendChild($newproperty)\n    }\n\n    $configFile.Save($hdiConfigFiles[$ConfigFileName])\n\n    Write-HDILog \"$configFileName has been configured.\"\n\nThe script takes four parameters, the configuration file name, the property you want to modify, the value you want to set, and a description. For example:\n\n    hive-site.xml hive.metastore.client.socket.timeout 90 \n\nThese parameters will set the hive.metastore.client.socket.timeout value to 90 in the hive-site.xml file.  The default value is 60 seconds.\n\nThis sample script can also be found at [https://hditutorialdata.blob.core.windows.net/customizecluster/editSiteConfig.ps1](https://hditutorialdata.blob.core.windows.net/customizecluster/editSiteConfig.ps1). \n\nHDInsight provides several scripts to install additional components on HDInsight clusters:\n\nName | Script\n----- | -----\n**Install Spark** | https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1. See [Install and use Spark on HDInsight clusters][hdinsight-install-spark].\n**Install R** | https://hdiconfigactions.blob.core.windows.net/rconfigactionv02/r-installer-v02.ps1. See [Install and use R on HDInsight clusters][hdinsight-r-scripts].\n**Install Solr** | https://hdiconfigactions.blob.core.windows.net/solrconfigactionv01/solr-installer-v01.ps1. See [Install and use Solr on HDInsight clusters](hdinsight-hadoop-solr-install.md).\n- **Install Giraph** | https://hdiconfigactions.blob.core.windows.net/giraphconfigactionv01/giraph-installer-v01.ps1. See [Install and use Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install.md).\n\nScript Action can be deployed from the Azure preview portal, Azure PowerShell or by using the HDInsight .NET SDK.  For more information, see [Customize HDInsight clusters using Script Action][hdinsight-cluster-customize].\n\n> [AZURE.NOTE] The sample scripts work only with HDInsight cluster version 3.1 or above. For more information on HDInsight cluster versions, see [HDInsight cluster versions](../hdinsight-component-versioning/).\n\n\n\n\n\n## Helper methods for custom scripts\n\nScript Action helper methods are utilities that you can use while writing custom scripts. These are defined in [https://hdiconfigactions.blob.core.windows.net/configactionmodulev05/HDInsightUtilities-v05.psm1](https://hdiconfigactions.blob.core.windows.net/configactionmodulev05/HDInsightUtilities-v05.psm1), and can be included in your scripts using the following:\n\n    # Download config action module from a well-known directory.\n    $CONFIGACTIONURI = \"https://hdiconfigactions.blob.core.windows.net/configactionmodulev05/HDInsightUtilities-v05.psm1\";\n    $CONFIGACTIONMODULE = \"C:\\apps\\dist\\HDInsightUtilities.psm1\";\n    $webclient = New-Object System.Net.WebClient;\n    $webclient.DownloadFile($CONFIGACTIONURI, $CONFIGACTIONMODULE);\n    \n    # (TIP) Import config action helper method module to make writing config action easy.\n    if (Test-Path ($CONFIGACTIONMODULE))\n    { \n        Import-Module $CONFIGACTIONMODULE;\n    } \n    else\n    {\n        Write-Output \"Failed to load HDInsightUtilities module, exiting ...\";\n        exit;\n    }\n\nHere are the helper methods that are provided by this script:\n\nHelper method | Description\n-------------- | -----------\n**Save-HDIFile** | Download a file from the specified Uniform Resource Identifier (URI) to a location on the local disk that is associated with the Azure VM node assigned to the cluster.\n**Expand-HDIZippedFile** | Unzip a zipped file.\n**Invoke-HDICmdScript** | Run a script from cmd.exe.\n**Write-HDILog** | Write output from the custom script used for a script action.\n**Get-Services** | Get a list of services running on the machine where the script executes.\n**Get-Service** | With the specific service name as input, get detailed information for a specific service (service name, process ID, state, etc.) on the machine where the script executes.\n**Get-HDIServices** | Get a list of HDInsight services running on the computer where the script executes.\n**Get-HDIService** | With the specific HDInsight service name as input, get detailed information for a specific service (service name, process ID, state, etc.) on the machine where the script executes.\n**Get-ServicesRunning** | Get a list of services that are running on the computer where the script executes.\n**Get-ServiceRunning** | Check if a specific service (by name) is running on the computer where the script executes.\n**Get-HDIServicesRunning** | Get a list of HDInsight services running on the computer where the script executes.\n**Get-HDIServiceRunning** | Check if a specific HDInsight service (by name) is running on the computer where the script executes.\n**Get-HDIHadoopVersion** | Get the version of Hadoop installed on the computer where the script executes.\n**Test-IsHDIHeadNode** | Check if the computer where the script executes is a head node.\n**Test-IsActiveHDIHeadNode** | Check if the computer where the script executes is an active head node.\n**Test-IsHDIDataNode** | Check if the computer where the script executes is a data node.\n**Edit-HDIConfigFile** | Edit the config files hive-site.xml, core-site.xml, hdfs-site.xml, mapred-site.xml, or yarn-site.xml.\n\n\n## Best practices for script development\n\nWhen you develop a custom script for an HDInsight cluster, there are several best practices to keep in mind:\n\n- Check for the Hadoop version\n\n    Only HDInsight version 3.1 (Hadoop 2.4) and above support using Script Action to install custom components on a cluster. In your custom script, you must use the **Get-HDIHadoopVersion** helper method to check the Hadoop version before proceeding with performing other tasks in the script.\n\n\n- Provide stable links to script resources\n\n    Users should make sure that all of the scripts and other artifacts used in the customization of a cluster remain available throughout the lifetime of the cluster and that the versions of these files do not change for the duration. These resources are required if the re-imaging of nodes in the cluster is required. The best practice is to download and archive everything in a Storage account that the user controls. This can be the default Storage account or any of the additional Storage accounts specified at the time of deployment for a customized cluster.\n    In the Spark and R customized cluster samples provided in the documentation, for example, we have made a local copy of the resources in this Storage account: https://hdiconfigactions.blob.core.windows.net/.\n\n\n- Ensure that the cluster customization script is idempotent\n\n    You must expect that the nodes of an HDInsight cluster will be re-imaged during the cluster lifetime. The cluster customization script is run whenever a cluster is re-imaged. This script must be designed to be idempotent in the sense that upon re-imaging, the script should ensure that the cluster is returned to the same customized state that it was in just after the script ran for the first time when the cluster was initially created. For example, if a custom script installed an application at D:\\AppLocation on its first run, then on each subsequent run, upon re-imaging, the script should check whether the application exists at the D:\\AppLocation location before proceeding with other steps in the script.\n\n\n- Install custom components in the optimal location\n\n    When cluster nodes are re-imaged, the C:\\ resource drive and D:\\ system drive can be re-formatted, resulting in the loss of data and applications that had been installed on those drives. This could also happen if an Azure virtual machine (VM) node that is part of the cluster goes down and is replaced by a new node. You can install components on the D:\\ drive or in the C:\\apps location on the cluster. All other locations on the C:\\ drive are reserved. Specify the location where applications or libraries are to be installed in the cluster customization script.\n\n\n- Ensure high availability of the cluster architecture\n\n    HDInsight has an active-passive architecture for high availability, in which one head node is in active mode (where the HDInsight services are running) and the other head node is in standby mode (in which HDInsight services are not running). The nodes switch active and passive modes if HDInsight services are interrupted. If a script action is used to install services on both head nodes for high availability, note that the HDInsight failover mechanism will not be able to automatically fail over these user-installed services. So user-installed services on HDInsight head nodes that are expected to be highly available must either have their own failover mechanism if in active-passive mode or be in active-active mode.\n\n    An HDInsight Script Action command runs on both head nodes when the head-node role is specified as a value in the *ClusterRoleCollection* parameter (documented below in the section [How to run a script action](#runScriptAction)). So when you design a custom script, make sure that your script is aware of this setup. You should not run into problems where the same services are installed and started on both of the head nodes and they end up competing with each other. Also, be aware that data will be lost during re-imaging, so software installed via Script Action has to be resilient to such events. Applications should be designed to work with highly available data that is distributed across many nodes. Note that as many as 1/5 of the nodes in a cluster can be re-imaged at the same time.\n\n\n- Configure the custom components to use Azure Blob storage\n\n    The custom components that you install on the cluster nodes might have a default configuration to use Hadoop Distributed File System (HDFS) storage. You should change the configuration to use Azure Blob storage instead. On a cluster re-image, the HDFS file system gets formatted and you would lose any data that is stored there. Using Azure Blob storage instead ensures that your data will be retained.\n\n## Common usage patterns\n\nThis section provides guidance on implementing some of the common usage patterns that you might run into while writing your own custom script.\n\n### Configure environment variables\n\nOften in script action development, you will feel the need to set environment variables. For instance, a most likely scenario is when you download a binary from an external site, install it on the cluster, and add the location of where it is installed to your ‘PATH’ environment variable. The following snippet shows you how to set environment variables in the custom script.\n\n    Write-HDILog \"Starting environment variable setting at: $(Get-Date)\";\n    [Environment]::SetEnvironmentVariable('MDS_RUNNER_CUSTOM_CLUSTER', 'true', 'Machine');\n\nThis statement sets the environment variable **MDS_RUNNER_CUSTOM_CLUSTER** to the value 'true' and also sets the scope of this variable to be machine-wide. At times it is important that environment variables are set at the appropriate scope – machine or user. Refer [here][1] for more information on setting environment variables.\n\n### Access to locations where the custom scripts are stored\n\nScripts used to customize a cluster needs to either be in the default storage account for the cluster or in a public read-only container on any other storage account. If your script accesses resources located elsewhere these need to be in a publicly accessible (at least public read-only). For instance you might want to access a file and save it using the SaveFile-HDI command.\n\n    Save-HDIFile -SrcUri 'https://somestorageaccount.blob.core.windows.net/somecontainer/some-file.jar' -DestFile 'C:\\apps\\dist\\hadoop-2.4.0.2.1.9.0-2196\\share\\hadoop\\mapreduce\\some-file.jar'\n\nIn this example, you must ensure that the container 'somecontainer' in storage account 'somestorageaccount' is publicly accessible. Otherwise, the script will throw a ‘Not Found’ exception and fail.\n\n### Pass parameters to the Add-AzureHDInsightScriptAction cmdlet\n\nTo pass multiple parameters to the Add-AzureHDInsightScriptAction cmdlet, you need to format the string value to contain all parameters for the script. For example:\n\n    \"-CertifcateUri wasb:///abc.pfx -CertificatePassword 123456 -InstallFolderName MyFolder\"\n \nor\n\n    $parameters = '-Parameters \"{0};{1};{2}\"' -f $CertificateName,$certUriWithSasToken,$CertificatePassword\n\n\n### Throw exception for failed cluster deployment\n\nIf you want to get accurately notified of the fact that cluster customization did not succeed as expected, it is important to throw an exception and fail the cluster provisioning. For instance, you might want to process a file if it exists and handle the error case where the file does not exist. This would ensure that the script exits gracefully and the state of the cluster is correctly known. The following snippet gives an example of how to achieve this:\n\n    If(Test-Path($SomePath)) {\n        #Process file in some way\n    } else {\n        # File does not exist; handle error case\n        # Print error message\n    exit\n    }\n\nIn this snippet, if the file did not exist, it would lead to a state where the script actually exits gracefully after printing the error message, and the cluster reaches running state assuming it \"successfully\" completed cluster customization process. If you want to be accurately notified of the fact that cluster customization essentially did not succeed as expected because of a missing file, it is more appropriate to throw an exception and fail the cluster customization step. To achieve this you must use the following sample code snippet instead.\n\n    If(Test-Path($SomePath)) {\n        #Process file in some way\n    } else {\n        # File does not exist; handle error case\n        # Print error message\n    throw\n    }\n\n\n## Checklist for deploying a script action\nHere are the steps we took when preparing to deploy these scripts:\n\n1. Put the files that contain the custom scripts in a place that is accessible by the cluster nodes during deployment. This can be any of the default or additional Storage accounts specified at the time of cluster deployment, or any other publicly accessible storage container.\n2. Add checks into scripts to make sure that they execute idempotently, so that the script can be executed multiple times on the same node.\n3. Use the **Write-Output** Azure PowerShell cmdlet to print to STDOUT as well as STDERR. Do not use **Write-Host**.\n4. Use a temporary file folder, such as $env:TEMP, to keep the downloaded file used by the scripts and then clean them up after scripts have executed.\n5. Install custom software only at D:\\ or C:\\apps. Other locations on the C: drive should not be used as they are reserved. Note that installing files on the C: drive outside of the C:\\apps folder may result in setup failures during re-images of the node.\n6. In the event that OS-level settings or Hadoop service configuration files were changed, you may want to restart HDInsight services so that they can pick up any OS-level settings, such as the environment variables set in the scripts.\n\n\n\n## Test custom scripts with the HDInsight Emulator\n\nA straight-forward way to test a custom script before using it in the HDInsight Script Action command is to run it on the HDInsight Emulator. You can install the HDInsight Emulator locally or on an Azure infrastructure as a service (IaaS) Windows Server 2012 R2 VM or on a local machine and observe if the script behaves correctly. Note that the Windows Server 2012 R2 VM is the same VM that HDInsight uses for its nodes.\n\nThis section outlines the procedure for using the HDInsight Emulator locally for testing purposes, but the procedure for using a VM is similar.\n\n**Install the HDInsight Emulator** - To run Script Action locally, you must have the HDInsight Emulator installed. For instructions on how to install it, see [Get started with the HDInsight Emulator](../hdinsight-get-started-emulator/).\n\n**Set the execution policy for Azure PowerShell** - Open Azure PowerShell and run (as administrator) the following command to set the execution policy to *LocalMachine* and to be *Unrestricted*:\n\n    Set-ExecutionPolicy Unrestricted –Scope LocalMachine\n\nWe need this policy to be unrestricted as scripts are not signed.\n\n**Download the script action** that you want to run to a local destination. The following sample scripts are available to download from the following locations:\n\n* **Spark**. https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv02/spark-installer-v02.ps1\n* **R**. https://hdiconfigactions.blob.core.windows.net/rconfigactionv02/r-installer-v02.ps1\n* **Solr**. https://hdiconfigactions.blob.core.windows.net/solrconfigactionv01/solr-installer-v01.ps1\n* **Giraph**. https://hdiconfigactions.blob.core.windows.net/giraphconfigactionv01/giraph-installer-v01.ps1\n\n**Run the script action** - Open a new Azure PowerShell window in admin mode and run the Spark or R installation script from the local location where they were saved.\n\n**Usage examples**\nWhen you're using the Spark and R clusters, data files needed may not be present in the HDInsight Emulator. So you may need to upload relevant .txt files that contain data to a path in HDFS and then use that path to access the data. For example:\n\n    val file = sc.textFile(\"/example/data/gutenberg/davinci.txt\")\n\n\nNote that in some cases a custom script may actually depend on HDInsight components, such as detecting whether certain Hadoop services are up. In this case, you will have to test your custom scripts by deploying them on an actual HDInsight cluster.\n\n\n## Debug custom scripts\n\nThe script error logs are stored, along with other output, in the default Storage account that you specified for the cluster at its creation. The logs are stored in a table with the name *u<\\cluster-name-fragment><\\time-stamp>setuplog*. These are aggregated logs that have records from all of the nodes (head node and worker nodes) on which the script runs in the cluster.\nAn easy way to check the logs is to use HDInsight Tools for Visual Studio. For installing the tools, see [Get started using Visual Studio Hadoop tools for HDInsight](hdinsight-hadoop-visual-studio-tools-get-started.md#install-hdinsight-tools-for-visual-studio)\n\n**To check the log using Visual Studio**\n\n1. Open Visual Studio.\n2. Click **View**, and then click **Server Explorer**.\n3. Right-click \"Azure\", click Connect to **Microsoft Azure Subscriptions**, and then enter your credentials.\n4. Expand **Storage**, expand the Azure storage account used as the default file system, expand **Tables**, and then double-click the table name.\n\n\nYou can also remote into the cluster nodes to see the both STDOUT and STDERR for custom scripts. The logs on each node are specific only to that node and are logged into **C:\\HDInsightLogs\\DeploymentAgent.log**. These log files record all outputs from the custom script. An example log snippet for a Spark script action looks like this:\n\n    Microsoft.Hadoop.Deployment.Engine.CustomPowershellScriptCommand; Details : BEGIN: Invoking powershell script https://configactions.blob.core.windows.net/sparkconfigactions/spark-installer.ps1.;\n    Version : 2.1.0.0;\n    ActivityId : 739e61f5-aa22-4254-aafc-9faf56fc2692;\n    AzureVMName : HEADNODE0;\n    IsException : False;\n    ExceptionType : ;\n    ExceptionMessage : ;\n    InnerExceptionType : ;\n    InnerExceptionMessage : ;\n    Exception : ;\n    ...\n\n    Starting Spark installation at: 09/04/2014 21:46:02 Done with Spark installation at: 09/04/2014 21:46:38;\n\n    Version : 2.1.0.0;\n    ActivityId : 739e61f5-aa22-4254-aafc-9faf56fc2692;\n    AzureVMName : HEADNODE0;\n    IsException : False;\n    ExceptionType : ;\n    ExceptionMessage : ;\n    InnerExceptionType : ;\n    InnerExceptionMessage : ;\n    Exception : ;\n    ...\n\n    Microsoft.Hadoop.Deployment.Engine.CustomPowershellScriptCommand;\n    Details : END: Invoking powershell script https://configactions.blob.core.windows.net/sparkconfigactions/spark-installer.ps1.;\n    Version : 2.1.0.0;\n    ActivityId : 739e61f5-aa22-4254-aafc-9faf56fc2692;\n    AzureVMName : HEADNODE0;\n    IsException : False;\n    ExceptionType : ;\n    ExceptionMessage : ;\n    InnerExceptionType : ;\n    InnerExceptionMessage : ;\n    Exception : ;\n\n\nIn this log, it is clear that the Spark script action has been executed on the VM named HEADNODE0 and that no exceptions were thrown during the execution.\n\nIn the event that an execution failure occurs, the output describing it will also be contained in this log file. The information provided in these logs should be helpful in debugging script problems that may arise.\n\n\n## See also\n\n- [Customize HDInsight clusters using Script Action][hdinsight-cluster-customize]\n- [Install and use Spark on HDInsight clusters][hdinsight-install-spark]\n- [Install and use R on HDInsight clusters][hdinsight-r-scripts]\n- [Install and use Solr on HDInsight clusters](hdinsight-hadoop-solr-install.md).\n- [Install and use Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install.md).\n\n[hdinsight-provision]: ../hdinsight-provision-clusters/\n[hdinsight-cluster-customize]: ../hdinsight-hadoop-customize-cluster\n[hdinsight-install-spark]: ../hdinsight-hadoop-spark-install/\n[hdinsight-r-scripts]: ../hdinsight-hadoop-r-scripts/\n[powershell-install-configure]: ../install-configure-powershell/\n\n<!--Reference links in article-->\n[1]: https://msdn.microsoft.com/library/96xafkes(v=vs.110).aspx\n\ntest\n"
}