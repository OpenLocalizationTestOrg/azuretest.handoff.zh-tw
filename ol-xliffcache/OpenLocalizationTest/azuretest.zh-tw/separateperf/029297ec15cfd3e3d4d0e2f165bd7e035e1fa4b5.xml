{
  "nodes": [
    {
      "content": "Compression support",
      "pos": [
        4,
        23
      ]
    },
    {
      "content": "Processing large data sets can cause I/O and network bottlenecks.",
      "pos": [
        26,
        91
      ]
    },
    {
      "content": "Therefore, compressed data in stores can not only speed up data transfer across the network and save disk space, but also bring significant performance improvements in processing big data.",
      "pos": [
        92,
        280
      ]
    },
    {
      "content": "At this time, compression is supported for file-based data stores such as Azure Blob or On-premises File System.",
      "pos": [
        281,
        393
      ]
    },
    {
      "pos": [
        397,
        516
      ],
      "content": "To specify compression for a dataset, use the <bpt id=\"p1\">**</bpt>compression<ept id=\"p1\">**</ept> property in the dataset JSON as in the following example:"
    },
    {
      "pos": [
        1116,
        1173
      ],
      "content": "Note that the <bpt id=\"p1\">**</bpt>compression<ept id=\"p1\">**</ept> section has two properties:"
    },
    {
      "pos": [
        1181,
        1262
      ],
      "content": "<bpt id=\"p1\">**</bpt>Type:<ept id=\"p1\">**</ept> the compression codec, which can be <bpt id=\"p2\">**</bpt>GZIP<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>Deflate<ept id=\"p3\">**</ept> or <bpt id=\"p4\">**</bpt>BZIP2<ept id=\"p4\">**</ept>."
    },
    {
      "pos": [
        1267,
        1341
      ],
      "content": "<bpt id=\"p1\">**</bpt>Level:<ept id=\"p1\">**</ept> the compression ratio, which can be <bpt id=\"p2\">**</bpt>Optimal<ept id=\"p2\">**</ept> or <bpt id=\"p3\">**</bpt>Fastest<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        1349,
        1483
      ],
      "content": "<bpt id=\"p1\">**</bpt>Fastest:<ept id=\"p1\">**</ept> The compression operation should complete as quickly as possible, even if the resulting file is not optimally compressed."
    },
    {
      "pos": [
        1491,
        1616
      ],
      "content": "<bpt id=\"p1\">**</bpt>Optimal<ept id=\"p1\">**</ept>: The compression operation should be optimally compressed, even if the operation takes a longer time to complete."
    },
    {
      "pos": [
        1627,
        1758
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Compression Level<ept id=\"p1\">](https://msdn.microsoft.com/library/system.io.compression.compressionlevel.aspx)</ept> topic for more information."
    },
    {
      "content": "Suppose the above sample dataset is used as the output of a copy activity, the copy activity will compresses the output data with GZIP codec using optimal ratio and then write the compressed data into a file named pagecounts.csv.gz in the Azure Blob Storage.",
      "pos": [
        1761,
        2019
      ]
    },
    {
      "content": "When you specify compression property in an input dataset JSON, the pipeline can read compressed data from the source and when you specify the property in an output dataset JSON, the copy activity can write compressed data to the destination.",
      "pos": [
        2024,
        2266
      ]
    },
    {
      "content": "Here are a few sample scenarios:",
      "pos": [
        2267,
        2299
      ]
    },
    {
      "content": "Read GZIP compressed data from an Azure blob, decompress it, and write result data to an Azure SQL database.",
      "pos": [
        2304,
        2412
      ]
    },
    {
      "content": "You define the input Azure Blob dataset with the compression JSON property in this case.",
      "pos": [
        2413,
        2501
      ]
    },
    {
      "content": "Read data from a plain-text file from on-premises File System, compress it using GZip format, and write the compressed data to an Azure blob.",
      "pos": [
        2505,
        2646
      ]
    },
    {
      "content": "You define an output Azure Blob dataset with the compression JSON property in this case.",
      "pos": [
        2647,
        2735
      ]
    },
    {
      "content": "Read a GZIP-compressed data from an Azure blob, decompress it, compress it using BZIP2, and write result data to an Azure blob.",
      "pos": [
        2740,
        2867
      ]
    },
    {
      "content": "You define the input Azure Blob dataset with compression type set to GZIP and the output dataset with compression type set to BZIP2 in this case.",
      "pos": [
        2868,
        3013
      ]
    }
  ],
  "content": "### Compression support  \nProcessing large data sets can cause I/O and network bottlenecks. Therefore, compressed data in stores can not only speed up data transfer across the network and save disk space, but also bring significant performance improvements in processing big data. At this time, compression is supported for file-based data stores such as Azure Blob or On-premises File System.  \n\nTo specify compression for a dataset, use the **compression** property in the dataset JSON as in the following example:   \n\n    {  \n        \"name\": \"AzureBlobDataSet\",  \n        \"properties\": {  \n            \"availability\": {  \n                \"frequency\": \"Day\",  \n                \"interval\": 1  \n            },  \n            \"type\": \"AzureBlob\",  \n            \"linkedServiceName\": \"StorageLinkedService\",  \n            \"typeProperties\": {  \n                \"fileName\": \"pagecounts.csv.gz\",  \n                \"folderPath\": \"compression/file/\",  \n                \"compression\": {  \n                    \"type\": \"GZip\",  \n                    \"level\": \"Optimal\"  \n                }  \n            }  \n        }  \n    }  \n \nNote that the **compression** section has two properties:  \n  \n- **Type:** the compression codec, which can be **GZIP**, **Deflate** or **BZIP2**.  \n- **Level:** the compression ratio, which can be **Optimal** or **Fastest**. \n    - **Fastest:** The compression operation should complete as quickly as possible, even if the resulting file is not optimally compressed. \n    - **Optimal**: The compression operation should be optimally compressed, even if the operation takes a longer time to complete. \n    \n    See [Compression Level](https://msdn.microsoft.com/library/system.io.compression.compressionlevel.aspx) topic for more information. \n\nSuppose the above sample dataset is used as the output of a copy activity, the copy activity will compresses the output data with GZIP codec using optimal ratio and then write the compressed data into a file named pagecounts.csv.gz in the Azure Blob Storage.   \n\nWhen you specify compression property in an input dataset JSON, the pipeline can read compressed data from the source and when you specify the property in an output dataset JSON, the copy activity can write compressed data to the destination. Here are a few sample scenarios: \n\n- Read GZIP compressed data from an Azure blob, decompress it, and write result data to an Azure SQL database. You define the input Azure Blob dataset with the compression JSON property in this case. \n- Read data from a plain-text file from on-premises File System, compress it using GZip format, and write the compressed data to an Azure blob. You define an output Azure Blob dataset with the compression JSON property in this case.  \n- Read a GZIP-compressed data from an Azure blob, decompress it, compress it using BZIP2, and write result data to an Azure blob. You define the input Azure Blob dataset with compression type set to GZIP and the output dataset with compression type set to BZIP2 in this case.   \n"
}