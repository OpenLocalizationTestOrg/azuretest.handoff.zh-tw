{
  "nodes": [
    {
      "content": "Create Stream Analytics Inputs | Microsoft Azure",
      "pos": [
        28,
        76
      ]
    },
    {
      "content": "Learn how to connect to and configure the input sources for Stream Analytics solutions.",
      "pos": [
        96,
        183
      ]
    },
    {
      "content": "Create Stream Analytics inputs",
      "pos": [
        521,
        551
      ]
    },
    {
      "content": "Understanding Stream Analytics inputs",
      "pos": [
        556,
        593
      ]
    },
    {
      "content": "Stream Analytics inputs are defined as a connection to a data source.",
      "pos": [
        598,
        667
      ]
    },
    {
      "content": "Stream Analytics has first class integration with the Azure sources Event Hub and Blob storage from within and outside of the job subscription.",
      "pos": [
        668,
        811
      ]
    },
    {
      "content": "As data is sent to that data source, it is consumed by the Stream Analytics job and processed in real time.",
      "pos": [
        812,
        919
      ]
    },
    {
      "content": "Inputs are divided into two distinct types: data stream inputs and reference data inputs.",
      "pos": [
        920,
        1009
      ]
    },
    {
      "content": "Data stream inputs",
      "pos": [
        1014,
        1032
      ]
    },
    {
      "content": "Stream Analytics jobs must include at least one data stream input to be consumed and transformed by the job.",
      "pos": [
        1037,
        1145
      ]
    },
    {
      "content": "Azure Blob storage and Azure Event Hubs are supported as data stream input sources.",
      "pos": [
        1146,
        1229
      ]
    },
    {
      "content": "Azure Event Hubs are used to collect event streams from multiple devices and services, such as social media activity feeds, stock trade information or data from sensors.",
      "pos": [
        1230,
        1399
      ]
    },
    {
      "content": "Alternately, Azure Blob storage can be used as an input source for ingesting bulk data.",
      "pos": [
        1400,
        1487
      ]
    },
    {
      "content": "Reference data inputs",
      "pos": [
        1492,
        1513
      ]
    },
    {
      "content": "Stream Analytics supports a second type of input known as reference data.",
      "pos": [
        1518,
        1591
      ]
    },
    {
      "content": "This is auxiliary data which is typically used for performing correlation and look-ups, and the data here is usually static or infrequently changed.",
      "pos": [
        1592,
        1740
      ]
    },
    {
      "content": "Azure Blob storage is currently the only supported input source for reference data.",
      "pos": [
        1741,
        1824
      ]
    },
    {
      "content": "Reference data source blobs are limited to 50MB in size.",
      "pos": [
        1825,
        1881
      ]
    },
    {
      "content": "Creating an Event Hub data input stream",
      "pos": [
        1886,
        1925
      ]
    },
    {
      "content": "Overview of Event Hubs",
      "pos": [
        1934,
        1956
      ]
    },
    {
      "content": "Event Hubs are a highly scalable event ingestor, and are the most common method of data ingestion to a Stream Analytics job.",
      "pos": [
        1957,
        2081
      ]
    },
    {
      "content": "Event Hubs and Stream Analytics together provide customers an end to end solution for real time analytics -- Event Hubs allow customers to feed events into Azure in real time, and Stream Analytics jobs can process them in real time.",
      "pos": [
        2082,
        2314
      ]
    },
    {
      "content": "For example, customers can send web clicks, sensor readings, online log events to Event Hubs, and create Stream Analytics jobs to use Event Hubs as the input data streams for real time filtering, aggregating and joining.",
      "pos": [
        2315,
        2535
      ]
    },
    {
      "content": "Event Hubs can be used for data egress also.",
      "pos": [
        2536,
        2580
      ]
    },
    {
      "content": "For further details on Event Hubs see the <bpt id=\"p1\">[</bpt>Event Hubs<ept id=\"p1\">]</ept><bpt id=\"p2\">(https://azure.microsoft.com/services/event-hubs/ \"</bpt>Event Hubs<ept id=\"p2\">\")</ept> documentation.",
      "pos": [
        2581,
        2713
      ]
    },
    {
      "content": "Consumer groups",
      "pos": [
        2719,
        2734
      ]
    },
    {
      "content": "Each Stream Analytics Event Hub input should be configured to have its own consumer group.",
      "pos": [
        2735,
        2825
      ]
    },
    {
      "content": "When a job contains a self-join or multiple inputs, some input may be read by more than one reader downstream, which impacts the number of readers in a single consumer group.",
      "pos": [
        2826,
        3000
      ]
    },
    {
      "content": "To avoid exceeding Event Hub limit of 5 readers per consumer group per partition, it is a best practice to designate a consumer group for each Stream Analytics job.",
      "pos": [
        3002,
        3166
      ]
    },
    {
      "content": "Note that there is also a limit of 20 consumer groups per Event Hub.",
      "pos": [
        3167,
        3235
      ]
    },
    {
      "content": "For details, see the <bpt id=\"p1\">[</bpt>Event Hubs Programming Guide<ept id=\"p1\">]</ept><bpt id=\"p2\">(https://msdn.microsoft.com/library/azure/dn789972.aspx \"</bpt>Event Hubs Programming Guide<ept id=\"p2\">\")</ept>.",
      "pos": [
        3236,
        3375
      ]
    },
    {
      "content": "Creating an Event Hub input data stream",
      "pos": [
        3380,
        3419
      ]
    },
    {
      "content": "Adding an Event Hub as a data stream input",
      "pos": [
        3428,
        3470
      ]
    },
    {
      "pos": [
        3480,
        3627
      ],
      "content": "On the inputs tab of the Stream Analytics job, click <bpt id=\"p1\">**</bpt>ADD INPUT<ept id=\"p1\">**</ept> and then select the default option, <bpt id=\"p2\">**</bpt>Data stream<ept id=\"p2\">**</ept>, and click the right button."
    },
    {
      "content": "image1",
      "pos": [
        3635,
        3641
      ]
    },
    {
      "pos": [
        3739,
        3765
      ],
      "content": "Next select <bpt id=\"p1\">**</bpt>Event Hub<ept id=\"p1\">**</ept>."
    },
    {
      "content": "image6",
      "pos": [
        3773,
        3779
      ]
    },
    {
      "content": "Type or select the following fields and click the right button:",
      "pos": [
        3877,
        3940
      ]
    },
    {
      "pos": [
        3948,
        4039
      ],
      "content": "<bpt id=\"p1\">**</bpt>Input Alias<ept id=\"p1\">**</ept>: A friendly name that will be used in the job query to reference this input"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Service Bus Namespace<ept id=\"p1\">**</ept>: A Service Bus namespace is a container for a set of messaging entities.",
      "pos": [
        4048,
        4146
      ]
    },
    {
      "content": "When you created a new Event Hub, you also created a Service Bus namespace.",
      "pos": [
        4147,
        4222
      ]
    },
    {
      "pos": [
        4231,
        4278
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Hub<ept id=\"p1\">**</ept>: The name of your Event Hub input"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Event Hub Policy Name<ept id=\"p1\">**</ept>: The shared access policy, which can be created on the Event Hub Configure tab.",
      "pos": [
        4287,
        4392
      ]
    },
    {
      "content": "Each shared access policy will have a name, permissions that you set, and access keys.",
      "pos": [
        4393,
        4479
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Event Hub Consumer Group<ept id=\"p1\">**</ept> (Optional): The Consumer Group to ingest data from the Event Hub.",
      "pos": [
        4488,
        4582
      ]
    },
    {
      "content": "If not specified, Stream Analytics jobs will use the Default Consumer Group to ingest data from the Event Hub.",
      "pos": [
        4583,
        4693
      ]
    },
    {
      "content": "It is recommended to use a distinct consumer Group for each Stream Analytics job.",
      "pos": [
        4694,
        4775
      ]
    },
    {
      "content": "image7",
      "pos": [
        4785,
        4791
      ]
    },
    {
      "content": "Specify the following settings:",
      "pos": [
        4889,
        4920
      ]
    },
    {
      "pos": [
        4928,
        5129
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Serialization Format<ept id=\"p1\">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams."
    },
    {
      "pos": [
        5138,
        5209
      ],
      "content": "<bpt id=\"p1\">**</bpt>Encoding<ept id=\"p1\">**</ept>: UTF-8 is the only supported encoding format at this time."
    },
    {
      "content": "image8",
      "pos": [
        5219,
        5225
      ]
    },
    {
      "content": "Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Event Hub.",
      "pos": [
        5323,
        5444
      ]
    },
    {
      "content": "Creating a Blob storage data stream input",
      "pos": [
        5449,
        5490
      ]
    },
    {
      "content": "For scenarios with large amounts of unstructured data to store in the cloud, Blob storage offers a cost-effective and scalable solution.",
      "pos": [
        5495,
        5631
      ]
    },
    {
      "content": "Data in Blob storage is generally considered data “at rest” but it can be processed as a data stream by Stream Analytics.",
      "pos": [
        5632,
        5753
      ]
    },
    {
      "content": "One common scenario for Blob storage inputs with Stream Analytics is log processing, where telemetry is captured from a system and needs to be parsed and processed to extract meaningful data.",
      "pos": [
        5754,
        5945
      ]
    },
    {
      "content": "It is important to note that the default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was created.",
      "pos": [
        5947,
        6084
      ]
    },
    {
      "content": "To process the data as a stream using a timestamp in the event payload, the <bpt id=\"p1\">[</bpt>TIMESTAMP BY<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn834998.aspx)</ept> keyword must be used.",
      "pos": [
        6085,
        6253
      ]
    },
    {
      "content": "For further information on Blob storage see the <bpt id=\"p1\">[</bpt>Blob storage<ept id=\"p1\">](http://azure.microsoft.com/services/storage/blobs/)</ept> documentation.",
      "pos": [
        6254,
        6383
      ]
    },
    {
      "content": "Adding Blob storage as a data stream input",
      "pos": [
        6389,
        6431
      ]
    },
    {
      "pos": [
        6441,
        6588
      ],
      "content": "On the inputs tab of the Stream Analytics job, click <bpt id=\"p1\">**</bpt>ADD INPUT<ept id=\"p1\">**</ept> and then select the default option, <bpt id=\"p2\">**</bpt>Data stream<ept id=\"p2\">**</ept>, and click the right button."
    },
    {
      "content": "image1",
      "pos": [
        6596,
        6602
      ]
    },
    {
      "pos": [
        6700,
        6751
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Blob storage<ept id=\"p1\">**</ept> and click the right button."
    },
    {
      "content": "image2",
      "pos": [
        6759,
        6765
      ]
    },
    {
      "content": "Type or select the following fields:",
      "pos": [
        6863,
        6899
      ]
    },
    {
      "pos": [
        6907,
        6999
      ],
      "content": "<bpt id=\"p1\">**</bpt>Input Alias<ept id=\"p1\">**</ept>: A friendly name that  will be used in the job query to reference this input"
    },
    {
      "pos": [
        7008,
        7172
      ],
      "content": "<bpt id=\"p1\">**</bpt>Storage Account<ept id=\"p1\">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Storage Container<ept id=\"p1\">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.",
      "pos": [
        7181,
        7295
      ]
    },
    {
      "content": "When you upload a blob to the Blob service, you must specify a container for that blob.",
      "pos": [
        7296,
        7383
      ]
    },
    {
      "content": "image3",
      "pos": [
        7393,
        7399
      ]
    },
    {
      "content": "Click the <bpt id=\"p1\">**</bpt>Configure Advanced Settings<ept id=\"p1\">**</ept> box for the option to configure the Path Prefix Pattern for readings blobs in a customized path.",
      "pos": [
        7497,
        7635
      ]
    },
    {
      "content": "If this field is not specified, Stream Analytics will read all blobs in the container.",
      "pos": [
        7636,
        7722
      ]
    },
    {
      "content": "image4",
      "pos": [
        7730,
        7736
      ]
    },
    {
      "content": "Choose the following settings:",
      "pos": [
        7834,
        7864
      ]
    },
    {
      "pos": [
        7872,
        8073
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Serialization Format<ept id=\"p1\">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams."
    },
    {
      "pos": [
        8082,
        8153
      ],
      "content": "<bpt id=\"p1\">**</bpt>Encoding<ept id=\"p1\">**</ept>: UTF-8 is the only supported encoding format at this time."
    },
    {
      "content": "Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.",
      "pos": [
        8268,
        8400
      ]
    },
    {
      "content": "Creating a Blob storage reference data",
      "pos": [
        8405,
        8443
      ]
    },
    {
      "content": "Blob storage can be used to define reference data for a Stream Analytics job.",
      "pos": [
        8448,
        8525
      ]
    },
    {
      "content": "This is static or slow-changing data that is used for performing lookups or correlating data.",
      "pos": [
        8527,
        8620
      ]
    },
    {
      "content": "Support for refreshing reference data can be enabled by specifying a path pattern in the input configuration using the {date} and {time} tokens.",
      "pos": [
        8621,
        8765
      ]
    },
    {
      "content": "Stream Analytics will update reference data definitions based on this path pattern.",
      "pos": [
        8766,
        8849
      ]
    },
    {
      "content": "For example, a pattern of <ph id=\"ph1\">`\"/sample/{date}/{time}/products.csv\"`</ph> with a date format of “YYYY-MM-DD” and a time format of “HH:mm” tells Stream Analytics to pick up the updated blob <ph id=\"ph2\">`\"/sample/2015-04-16/17:30/products.csv\"`</ph> at 5:30 PM on April 16th 2015 UTC time zone .",
      "pos": [
        8850,
        9117
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Currently Stream Analytics jobs look for reference blob refresh data only when the time coincides with the time encoded in the blob name: e.g. jobs look for /sample/2015-04-16/17:30/products.csv between 5:30 PM and 5:30:59.999999999PM on April 16th 2015 UTC time zone.",
      "pos": [
        9121,
        9402
      ]
    },
    {
      "content": "When the clock strikes 5:31PM it stops looking for /sample/2015-04-16/17:30/products.csv and starts looking for /sample/2015-04-16/17:31/products.csv.",
      "pos": [
        9403,
        9553
      ]
    },
    {
      "content": "The only time previous reference data blobs are considered is when the job starts.",
      "pos": [
        9555,
        9637
      ]
    },
    {
      "content": "At that time the job is looking for the blob which has a latest date/time encoded in its name with a value before than the job start time (the newest reference data blob from before the job start time).",
      "pos": [
        9638,
        9840
      ]
    },
    {
      "content": "This is done to ensure there is a non-empty reference data set at the start of the job.",
      "pos": [
        9841,
        9928
      ]
    },
    {
      "content": "If one cannot be found, the job will fail and display a diagnostic notice to the user:",
      "pos": [
        9929,
        10015
      ]
    },
    {
      "content": "Adding Blob storage as reference data",
      "pos": [
        10021,
        10058
      ]
    },
    {
      "pos": [
        10068,
        10197
      ],
      "content": "On the inputs tab of the Stream Analytics job, click <bpt id=\"p1\">**</bpt>ADD INPUT<ept id=\"p1\">**</ept> and then select <bpt id=\"p2\">**</bpt>Reference data<ept id=\"p2\">**</ept> and click the right button."
    },
    {
      "content": "image9",
      "pos": [
        10205,
        10211
      ]
    },
    {
      "content": "Type or select the following fields:",
      "pos": [
        10310,
        10346
      ]
    },
    {
      "pos": [
        10354,
        10446
      ],
      "content": "<bpt id=\"p1\">**</bpt>Input Alias<ept id=\"p1\">**</ept>: A friendly name that  will be used in the job query to reference this input"
    },
    {
      "pos": [
        10455,
        10619
      ],
      "content": "<bpt id=\"p1\">**</bpt>Storage Account<ept id=\"p1\">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Storage Container<ept id=\"p1\">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.",
      "pos": [
        10628,
        10742
      ]
    },
    {
      "content": "When you upload a blob to the Blob service, you must specify a container for that blob.",
      "pos": [
        10743,
        10830
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Path Pattern<ept id=\"p1\">**</ept>: The file path used to locate your blobs within the specified container.",
      "pos": [
        10839,
        10928
      ]
    },
    {
      "content": "Within the path, you may choose to specify one or more instances of the following 2 variables: {date}, {time}",
      "pos": [
        10929,
        11038
      ]
    },
    {
      "content": "image10",
      "pos": [
        11048,
        11055
      ]
    },
    {
      "content": "Choose the following settings:",
      "pos": [
        11153,
        11183
      ]
    },
    {
      "pos": [
        11191,
        11392
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Serialization Format<ept id=\"p1\">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams."
    },
    {
      "pos": [
        11401,
        11472
      ],
      "content": "<bpt id=\"p1\">**</bpt>Encoding<ept id=\"p1\">**</ept>: UTF-8 is the only supported encoding format at this time."
    },
    {
      "content": "image12",
      "pos": [
        11482,
        11489
      ]
    },
    {
      "content": "Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.",
      "pos": [
        11588,
        11720
      ]
    },
    {
      "content": "image11",
      "pos": [
        11728,
        11735
      ]
    },
    {
      "content": "Get help",
      "pos": [
        11834,
        11842
      ]
    },
    {
      "pos": [
        11843,
        11985
      ],
      "content": "For further assistance, try our <bpt id=\"p1\">[</bpt>Azure Stream Analytics forum<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept>"
    },
    {
      "content": "Next steps",
      "pos": [
        11990,
        12000
      ]
    },
    {
      "content": "Introduction to Azure Stream Analytics",
      "pos": [
        12005,
        12043
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics",
      "pos": [
        12082,
        12122
      ]
    },
    {
      "content": "Scale Azure Stream Analytics jobs",
      "pos": [
        12160,
        12193
      ]
    },
    {
      "content": "Azure Stream Analytics Query Language Reference",
      "pos": [
        12230,
        12277
      ]
    },
    {
      "content": "Azure Stream Analytics Management REST API Reference",
      "pos": [
        12338,
        12390
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Create Stream Analytics Inputs | Microsoft Azure\" \n    description=\"Learn how to connect to and configure the input sources for Stream Analytics solutions.\"\n    documentationCenter=\"\"\n    services=\"stream-analytics\"\n    authors=\"jeffstokes72\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"stream-analytics\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.tgt_pltfrm=\"na\" \n    ms.workload=\"data-services\" \n    ms.date=\"08/19/2015\" \n    ms.author=\"jeffstok\"/>\n\n# Create Stream Analytics inputs\n\n## Understanding Stream Analytics inputs\n---\nStream Analytics inputs are defined as a connection to a data source. Stream Analytics has first class integration with the Azure sources Event Hub and Blob storage from within and outside of the job subscription. As data is sent to that data source, it is consumed by the Stream Analytics job and processed in real time. Inputs are divided into two distinct types: data stream inputs and reference data inputs.\n\n## Data stream inputs\n---\nStream Analytics jobs must include at least one data stream input to be consumed and transformed by the job. Azure Blob storage and Azure Event Hubs are supported as data stream input sources. Azure Event Hubs are used to collect event streams from multiple devices and services, such as social media activity feeds, stock trade information or data from sensors.\nAlternately, Azure Blob storage can be used as an input source for ingesting bulk data.\n\n## Reference data inputs\n---\nStream Analytics supports a second type of input known as reference data. This is auxiliary data which is typically used for performing correlation and look-ups, and the data here is usually static or infrequently changed. Azure Blob storage is currently the only supported input source for reference data. Reference data source blobs are limited to 50MB in size.\n\n## Creating an Event Hub data input stream\n---\n### Overview of Event Hubs\nEvent Hubs are a highly scalable event ingestor, and are the most common method of data ingestion to a Stream Analytics job. Event Hubs and Stream Analytics together provide customers an end to end solution for real time analytics -- Event Hubs allow customers to feed events into Azure in real time, and Stream Analytics jobs can process them in real time. For example, customers can send web clicks, sensor readings, online log events to Event Hubs, and create Stream Analytics jobs to use Event Hubs as the input data streams for real time filtering, aggregating and joining. Event Hubs can be used for data egress also. For further details on Event Hubs see the [Event Hubs](https://azure.microsoft.com/services/event-hubs/ \"Event Hubs\") documentation.\n\n### Consumer groups\nEach Stream Analytics Event Hub input should be configured to have its own consumer group. When a job contains a self-join or multiple inputs, some input may be read by more than one reader downstream, which impacts the number of readers in a single consumer group.  To avoid exceeding Event Hub limit of 5 readers per consumer group per partition, it is a best practice to designate a consumer group for each Stream Analytics job. Note that there is also a limit of 20 consumer groups per Event Hub. For details, see the [Event Hubs Programming Guide](https://msdn.microsoft.com/library/azure/dn789972.aspx \"Event Hubs Programming Guide\").\n\n## Creating an Event Hub input data stream\n---\n### Adding an Event Hub as a data stream input  ###\n\n1. On the inputs tab of the Stream Analytics job, click **ADD INPUT** and then select the default option, **Data stream**, and click the right button.\n\n    ![image1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)  \n\n2. Next select **Event Hub**.\n\n    ![image6](./media/stream-analytics-connect-data-event-inputs/06-stream-analytics-create-inputs.png)  \n\n3. Type or select the following fields and click the right button:\n\n    - **Input Alias**: A friendly name that will be used in the job query to reference this input  \n    - **Service Bus Namespace**: A Service Bus namespace is a container for a set of messaging entities. When you created a new Event Hub, you also created a Service Bus namespace.  \n    - **Event Hub**: The name of your Event Hub input  \n    - **Event Hub Policy Name**: The shared access policy, which can be created on the Event Hub Configure tab. Each shared access policy will have a name, permissions that you set, and access keys.  \n    - **Event Hub Consumer Group** (Optional): The Consumer Group to ingest data from the Event Hub. If not specified, Stream Analytics jobs will use the Default Consumer Group to ingest data from the Event Hub. It is recommended to use a distinct consumer Group for each Stream Analytics job.  \n\n    ![image7](./media/stream-analytics-connect-data-event-inputs/07-stream-analytics-create-inputs.png)  \n\n4. Specify the following settings:\n\n    - **Event Serialization Format**: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.  \n    - **Encoding**: UTF-8 is the only supported encoding format at this time.  \n\n    ![image8](./media/stream-analytics-connect-data-event-inputs/08-stream-analytics-create-inputs.png)  \n\n5. Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Event Hub.\n\n## Creating a Blob storage data stream input\n---\nFor scenarios with large amounts of unstructured data to store in the cloud, Blob storage offers a cost-effective and scalable solution. Data in Blob storage is generally considered data “at rest” but it can be processed as a data stream by Stream Analytics. One common scenario for Blob storage inputs with Stream Analytics is log processing, where telemetry is captured from a system and needs to be parsed and processed to extract meaningful data. \nIt is important to note that the default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was created. To process the data as a stream using a timestamp in the event payload, the [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx) keyword must be used.\nFor further information on Blob storage see the [Blob storage](http://azure.microsoft.com/services/storage/blobs/) documentation.\n\n### Adding Blob storage as a data stream input  ###\n\n1. On the inputs tab of the Stream Analytics job, click **ADD INPUT** and then select the default option, **Data stream**, and click the right button.\n\n    ![image1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)  \n\n2. Select **Blob storage** and click the right button.\n\n    ![image2](./media/stream-analytics-connect-data-event-inputs/02-stream-analytics-create-inputs.png)  \n\n3. Type or select the following fields:\n\n    - **Input Alias**: A friendly name that  will be used in the job query to reference this input  \n    - **Storage Account**: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.  \n    - **Storage Container**: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service. When you upload a blob to the Blob service, you must specify a container for that blob.  \n\n    ![image3](./media/stream-analytics-connect-data-event-inputs/03-stream-analytics-create-inputs.png)  \n\n4. Click the **Configure Advanced Settings** box for the option to configure the Path Prefix Pattern for readings blobs in a customized path. If this field is not specified, Stream Analytics will read all blobs in the container.\n\n    ![image4](./media/stream-analytics-connect-data-event-inputs/04-stream-analytics-create-inputs.png)  \n\n5. Choose the following settings:\n\n    - **Event Serialization Format**: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.  \n    - **Encoding**: UTF-8 is the only supported encoding format at this time.  \n\n\n    ![image5](./media/stream-analytics-connect-data-event-inputs/05-stream-analytics-create-inputs.png)  \n\n6. Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.\n\n## Creating a Blob storage reference data\n---\nBlob storage can be used to define reference data for a Stream Analytics job.  This is static or slow-changing data that is used for performing lookups or correlating data.\nSupport for refreshing reference data can be enabled by specifying a path pattern in the input configuration using the {date} and {time} tokens. Stream Analytics will update reference data definitions based on this path pattern. For example, a pattern of `\"/sample/{date}/{time}/products.csv\"` with a date format of “YYYY-MM-DD” and a time format of “HH:mm” tells Stream Analytics to pick up the updated blob `\"/sample/2015-04-16/17:30/products.csv\"` at 5:30 PM on April 16th 2015 UTC time zone .\n\n> [AZURE.NOTE] Currently Stream Analytics jobs look for reference blob refresh data only when the time coincides with the time encoded in the blob name: e.g. jobs look for /sample/2015-04-16/17:30/products.csv between 5:30 PM and 5:30:59.999999999PM on April 16th 2015 UTC time zone. When the clock strikes 5:31PM it stops looking for /sample/2015-04-16/17:30/products.csv and starts looking for /sample/2015-04-16/17:31/products.csv.\n\nThe only time previous reference data blobs are considered is when the job starts. At that time the job is looking for the blob which has a latest date/time encoded in its name with a value before than the job start time (the newest reference data blob from before the job start time). This is done to ensure there is a non-empty reference data set at the start of the job. If one cannot be found, the job will fail and display a diagnostic notice to the user:\n\n### Adding Blob storage as reference data  ###\n\n1. On the inputs tab of the Stream Analytics job, click **ADD INPUT** and then select **Reference data** and click the right button.\n\n    ![image9](./media/stream-analytics-connect-data-event-inputs/09-stream-analytics-create-inputs.png)  \n\n2.  Type or select the following fields:\n\n    - **Input Alias**: A friendly name that  will be used in the job query to reference this input  \n    - **Storage Account**: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.  \n    - **Storage Container**: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service. When you upload a blob to the Blob service, you must specify a container for that blob.  \n    - **Path Pattern**: The file path used to locate your blobs within the specified container. Within the path, you may choose to specify one or more instances of the following 2 variables: {date}, {time}  \n\n    ![image10](./media/stream-analytics-connect-data-event-inputs/10-stream-analytics-create-inputs.png)  \n\n3. Choose the following settings:\n\n    - **Event Serialization Format**: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.  \n    - **Encoding**: UTF-8 is the only supported encoding format at this time.  \n\n    ![image12](./media/stream-analytics-connect-data-event-inputs/12-stream-analytics-create-inputs.png)  \n\n4.  Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.\n\n    ![image11](./media/stream-analytics-connect-data-event-inputs/11-stream-analytics-create-inputs.png)  \n\n\n## Get help\nFor further assistance, try our [Azure Stream Analytics forum](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)\n\n## Next steps\n\n- [Introduction to Azure Stream Analytics](stream-analytics-introduction.md)\n- [Get started using Azure Stream Analytics](stream-analytics-get-started.md)\n- [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n- [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx)\n- [Azure Stream Analytics Management REST API Reference](https://msdn.microsoft.com/library/azure/dn835031.aspx)\n"
}