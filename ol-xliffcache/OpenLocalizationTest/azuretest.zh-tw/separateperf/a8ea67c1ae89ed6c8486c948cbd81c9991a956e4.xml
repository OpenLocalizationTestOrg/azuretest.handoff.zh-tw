{
  "nodes": [
    {
      "content": "Pi estimator Hadoop sample in HDInsight | Microsoft Azure",
      "pos": [
        27,
        84
      ]
    },
    {
      "content": "Learn how to run a Hadoop MapReduce sample Pi estimator on HDInsight.",
      "pos": [
        103,
        172
      ]
    },
    {
      "content": "The pi estimator Hadoop sample in HDInsight",
      "pos": [
        496,
        539
      ]
    },
    {
      "content": "This topic shows how to run a Hadoop MapReduce program in Azure HDInsight that estimates the value of the mathematical constant pi by using Azure PowerShell.",
      "pos": [
        541,
        698
      ]
    },
    {
      "content": "It also provides the Java code used in the MapReduce program used to estimate the value of pi for inspection.",
      "pos": [
        699,
        808
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The steps in this document require a Windows-based HDInsight cluster.",
      "pos": [
        812,
        894
      ]
    },
    {
      "content": "For information on running this and other samples with Linux-based clusters, see <bpt id=\"p1\">[</bpt>Run the Hadoop samples in HDInsight<ept id=\"p1\">](hdinsight-hadoop-run-samples-linux.md)</ept>",
      "pos": [
        895,
        1052
      ]
    },
    {
      "content": "The program uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.",
      "pos": [
        1054,
        1140
      ]
    },
    {
      "content": "Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.",
      "pos": [
        1141,
        1301
      ]
    },
    {
      "content": "The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.",
      "pos": [
        1302,
        1486
      ]
    },
    {
      "content": "The larger the sample of points used, the better the estimate is.",
      "pos": [
        1487,
        1552
      ]
    },
    {
      "content": "The pi estimator Java code that contains the mapper and reducer functions is available for inspection below.",
      "pos": [
        1554,
        1662
      ]
    },
    {
      "content": "The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle.",
      "pos": [
        1663,
        1832
      ]
    },
    {
      "content": "The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.",
      "pos": [
        1833,
        2079
      ]
    },
    {
      "content": "The script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values.",
      "pos": [
        2081,
        2271
      ]
    },
    {
      "content": "These parameter values can be changed to improve the estimated value of pi.",
      "pos": [
        2272,
        2347
      ]
    },
    {
      "content": "For reference, the first 10 decimal places of pi are 3.1415926535.",
      "pos": [
        2348,
        2414
      ]
    },
    {
      "content": "The .jar file that contains the files needed by Hadoop on Azure to deploy the application is a .zip file and is available for download.",
      "pos": [
        2416,
        2551
      ]
    },
    {
      "content": "You can unzip it with various compression utilities and then explore the files at your convenience.",
      "pos": [
        2552,
        2651
      ]
    },
    {
      "pos": [
        2653,
        2877
      ],
      "content": "The other samples that are available to help you get up to speed in using HDInsight to run MapReduce jobs are listed on <bpt id=\"p1\">[</bpt>Running the HDInsight Samples<ept id=\"p1\">][hdinsight-samples]</ept>, along with links to instructions on how to run them."
    },
    {
      "content": "You will learn:",
      "pos": [
        2881,
        2896
      ]
    },
    {
      "content": "How to use Azure PowerShell to run the pi estimator MapReduce program on Azure HDInsight.",
      "pos": [
        2902,
        2991
      ]
    },
    {
      "content": "What a MapReduce program written in Java looks like.",
      "pos": [
        2994,
        3046
      ]
    },
    {
      "pos": [
        3048,
        3066
      ],
      "content": "<bpt id=\"p1\">**</bpt>Prerequisites<ept id=\"p1\">**</ept>:"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>An Azure subscription<ept id=\"p1\">**</ept>.",
      "pos": [
        3070,
        3096
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Get Azure free trial<ept id=\"p1\">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "pos": [
        3097,
        3227
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>an HDInsight cluster<ept id=\"p1\">**</ept>.",
      "pos": [
        3230,
        3255
      ]
    },
    {
      "content": "For instructions on the various ways in which such clusters can be created, see <bpt id=\"p1\">[</bpt>Provision HDInsight Clusters<ept id=\"p1\">](hdinsight-provision-clusters.md)</ept>.",
      "pos": [
        3256,
        3400
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>A workstation with Azure PowerShell<ept id=\"p1\">**</ept>.",
      "pos": [
        3403,
        3443
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Install and use Azure PowerShell<ept id=\"p1\">](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>.",
      "pos": [
        3444,
        3566
      ]
    },
    {
      "pos": [
        3573,
        3632
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"run-sample\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run the sample with Azure PowerShell"
    },
    {
      "pos": [
        3634,
        3666
      ],
      "content": "<bpt id=\"p1\">**</bpt>To submit the MapReduce job<ept id=\"p1\">**</ept>s"
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        3671,
        3693
      ]
    },
    {
      "content": "For instructions on how to use the Azure PowerShell console window, see <bpt id=\"p1\">[</bpt>Install and configure Azure PowerShell<ept id=\"p1\">][powershell-install-configure]</ept>.",
      "pos": [
        3694,
        3837
      ]
    },
    {
      "content": "Set the two variables in the following commands, and then run them:",
      "pos": [
        3841,
        3908
      ]
    },
    {
      "content": "Run the following command to create a MapReduce definition:",
      "pos": [
        4067,
        4126
      ]
    },
    {
      "content": "The first argument indicates how many maps to create (default is 16).",
      "pos": [
        4317,
        4386
      ]
    },
    {
      "content": "The second argument indicates how many samples are generated per map (10 million by default).",
      "pos": [
        4387,
        4480
      ]
    },
    {
      "content": "So this program uses 10*10 million = 160 million random points to make its estimate of pi.",
      "pos": [
        4481,
        4571
      ]
    },
    {
      "content": "The third argument indicates the location and name of the jar file used to run the sample on HDInsight 3.0 and 3.1 clusters.",
      "pos": [
        4572,
        4696
      ]
    },
    {
      "content": "(See below for the contents of this file.)",
      "pos": [
        4697,
        4739
      ]
    },
    {
      "content": "Run the following commands to submit the MapReduce job and wait for the job to finish:",
      "pos": [
        4744,
        4830
      ]
    },
    {
      "content": "Run the following command to retrieve the standard output of the MapReduce job:",
      "pos": [
        5164,
        5243
      ]
    },
    {
      "content": "For comparison, the first 10 decimal places of pi are 3.1415926535.",
      "pos": [
        5412,
        5479
      ]
    },
    {
      "pos": [
        5485,
        5559
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"java-code\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>The Java code for the pi estimator MapReduce program"
    },
    {
      "pos": [
        17062,
        17089
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"summary\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Summary"
    },
    {
      "content": "In this tutorial, you saw how to run a MapReduce job on HDInsight and how to use Monte Carlo methods that require and generate large data sets that can be managed by this service.",
      "pos": [
        17091,
        17270
      ]
    },
    {
      "content": "Here is the complete script used to run this sample on a default HDInsight 3.1 cluster or on a 3.0 cluster:",
      "pos": [
        17272,
        17379
      ]
    },
    {
      "pos": [
        18229,
        18262
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"next-steps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "For tutorials that describe running other samples and provide instructions on using Pig, Hive, and MapReduce jobs on Azure HDInsight with Azure PowerShell, see the following topics:",
      "pos": [
        18264,
        18445
      ]
    },
    {
      "content": "Get Started with Azure HDInsight",
      "pos": [
        18450,
        18482
      ]
    },
    {
      "content": "Sample: 10GB GraySort",
      "pos": [
        18510,
        18531
      ]
    },
    {
      "content": "Sample: Wordcount",
      "pos": [
        18568,
        18585
      ]
    },
    {
      "content": "Sample: C# Streaming",
      "pos": [
        18618,
        18638
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        18678,
        18700
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        18724,
        18747
      ]
    },
    {
      "content": "Azure HDInsight SDK documentation",
      "pos": [
        18772,
        18805
      ]
    },
    {
      "content": "test",
      "pos": [
        19459,
        19463
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Pi estimator Hadoop sample in HDInsight | Microsoft Azure\"\n    description=\"Learn how to run a Hadoop MapReduce sample Pi estimator on HDInsight.\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"07/09/2015\"\n    ms.author=\"jgao\"/>\n\n# The pi estimator Hadoop sample in HDInsight\n\nThis topic shows how to run a Hadoop MapReduce program in Azure HDInsight that estimates the value of the mathematical constant pi by using Azure PowerShell. It also provides the Java code used in the MapReduce program used to estimate the value of pi for inspection.\n\n> [AZURE.NOTE] The steps in this document require a Windows-based HDInsight cluster. For information on running this and other samples with Linux-based clusters, see [Run the Hadoop samples in HDInsight](hdinsight-hadoop-run-samples-linux.md)\n\nThe program uses a statistical (quasi-Monte Carlo) method to estimate the value of pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4. The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.\n\nThe pi estimator Java code that contains the mapper and reducer functions is available for inspection below. The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle. The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.\n\nThe script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values. These parameter values can be changed to improve the estimated value of pi. For reference, the first 10 decimal places of pi are 3.1415926535.\n\nThe .jar file that contains the files needed by Hadoop on Azure to deploy the application is a .zip file and is available for download. You can unzip it with various compression utilities and then explore the files at your convenience.\n\nThe other samples that are available to help you get up to speed in using HDInsight to run MapReduce jobs are listed on [Running the HDInsight Samples][hdinsight-samples], along with links to instructions on how to run them.\n\n**You will learn:**\n\n* How to use Azure PowerShell to run the pi estimator MapReduce program on Azure HDInsight.\n* What a MapReduce program written in Java looks like.\n\n**Prerequisites**:\n\n- **An Azure subscription**. See [Get Azure free trial](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- **an HDInsight cluster**. For instructions on the various ways in which such clusters can be created, see [Provision HDInsight Clusters](hdinsight-provision-clusters.md).\n- **A workstation with Azure PowerShell**. See [Install and use Azure PowerShell](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/).\n\n\n\n## <a id=\"run-sample\"></a>Run the sample with Azure PowerShell\n\n**To submit the MapReduce job**s\n\n1. Open Azure PowerShell. For instructions on how to use the Azure PowerShell console window, see [Install and configure Azure PowerShell][powershell-install-configure].\n2. Set the two variables in the following commands, and then run them:\n\n        $subscriptionName = \"<SubscriptionName>\"   # Azure subscription name\n        $clusterName = \"<ClusterName>\"             # HDInsight cluster name\n\n4. Run the following command to create a MapReduce definition:\n\n        $piEstimatorJobDefinition = New-AzureHDInsightMapReduceJobDefinition -JarFile \"wasb:///example/jars/hadoop-mapreduce-examples.jar\" -ClassName \"pi\" -Arguments \"16\", \"10000000\"\n\n\n    The first argument indicates how many maps to create (default is 16). The second argument indicates how many samples are generated per map (10 million by default). So this program uses 10*10 million = 160 million random points to make its estimate of pi. The third argument indicates the location and name of the jar file used to run the sample on HDInsight 3.0 and 3.1 clusters. (See below for the contents of this file.)\n\n5. Run the following commands to submit the MapReduce job and wait for the job to finish:\n\n        # Run the pi estimator MapReduce job\n        Select-AzureSubscription $subscriptionName\n        $piJob = $piEstimatorJobDefinition | Start-AzureHDInsightJob -Cluster $clusterName\n\n        # Wait for the job to finish  \n        $piJob | Wait-AzureHDInsightJob -Subscription $subscriptionName -WaitTimeoutInSeconds 3600  \n\n6. Run the following command to retrieve the standard output of the MapReduce job:\n\n        # Print output and standard error file of the MapReduce job\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $piJob.JobId -StandardOutput\n\n    For comparison, the first 10 decimal places of pi are 3.1415926535.\n\n\n## <a id=\"java-code\"></a>The Java code for the pi estimator MapReduce program\n\n\n\n    /**\n    * Licensed to the Apache Software Foundation (ASF) under one\n    * or more contributor license agreements. See the NOTICE file\n    * distributed with this work for additional information\n    * regarding copyright ownership. The ASF licenses this file\n    * to you under the Apache License, Version 2.0 (the\n    * \"License\"); you may not use this file except in compliance\n    * with the License. You may obtain a copy of the License at\n    *\n    * http://www.apache.org/licenses/LICENSE-2.0\n    *\n    * Unless required by applicable law or agreed to in writing, software\n    * distributed under the License is distributed on an \"AS IS\" BASIS,\n    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or   implied.\n    * See the License for the specific language governing permissions and\n    * limitations under the License.\n    */\n\n    package org.apache.hadoop.examples;\n\n    import java.io.IOException;\n    import java.math.BigDecimal;\n    import java.util.Iterator;\n\n    import org.apache.hadoop.conf.Configured;\n    import org.apache.hadoop.fs.FileSystem;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.BooleanWritable;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.SequenceFile;\n    import org.apache.hadoop.io.Writable;\n    import org.apache.hadoop.io.WritableComparable;\n    import org.apache.hadoop.io.SequenceFile.CompressionType;\n    import org.apache.hadoop.mapred.FileInputFormat;\n    import org.apache.hadoop.mapred.FileOutputFormat;\n    import org.apache.hadoop.mapred.JobClient;\n    import org.apache.hadoop.mapred.JobConf;\n    import org.apache.hadoop.mapred.MapReduceBase;\n    import org.apache.hadoop.mapred.Mapper;\n    import org.apache.hadoop.mapred.OutputCollector;\n    import org.apache.hadoop.mapred.Reducer;\n    import org.apache.hadoop.mapred.Reporter;\n    import org.apache.hadoop.mapred.SequenceFileInputFormat;\n    import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n    import org.apache.hadoop.util.Tool;\n    import org.apache.hadoop.util.ToolRunner;\n\n\n    //A Map-reduce program to estimate the value of Pi\n    //using quasi-Monte Carlo method.\n    //\n    //Mapper:\n    //Generate points in a unit square\n    //and then count points inside/outside of the inscribed circle of the square.\n    //\n    //Reducer:\n    //Accumulate points inside/outside results from the mappers.\n    //Let numTotal = numInside + numOutside.\n    //The fraction numInside/numTotal is a rational approximation of\n    //the value (Area of the circle)/(Area of the square),\n    //where the area of the inscribed circle is Pi/4\n    //and the area of unit square is 1.\n    //Then, Pi is estimated value to be 4(numInside/numTotal).\n    //\n\n    public class PiEstimator extends Configured implements Tool {\n    //tmp directory for input/output\n    static private final Path TMP_DIR = new Path(\n    PiEstimator.class.getSimpleName() + \"_TMP_3_141592654\");\n\n    //2-dimensional Halton sequence {H(i)},\n    //where H(i) is a 2-dimensional point and i >= 1 is the index.\n    //Halton sequence is used to generate sample points for Pi estimation.\n    private static class HaltonSequence {\n    // Bases\n    static final int[] P = {2, 3};\n    //Maximum number of digits allowed\n    static final int[] K = {63, 40};\n\n    private long index;\n    private double[] x;\n    private double[][] q;\n    private int[][] d;\n\n    //Initialize to H(startindex),\n    //so the sequence begins with H(startindex+1).\n    HaltonSequence(long startindex) {\n    index = startindex;\n    x = new double[K.length];\n    q = new double[K.length][];\n    d = new int[K.length][];\n    for(int i = 0; i < K.length; i++) {\n    q[i] = new double[K[i]];\n    d[i] = new int[K[i]];\n    }\n\n    for(int i = 0; i < K.length; i++) {\n    long k = index;\n    x[i] = 0;\n\n    for(int j = 0; j < K[i]; j++) {\n    q[i][j] = (j == 0? 1.0: q[i][j-1])/P[i];\n    d[i][j] = (int)(k % P[i]);\n    k = (k - d[i][j])/P[i];\n    x[i] += d[i][j] * q[i][j];\n    }\n    }\n    }\n\n    //Compute next point.\n    //Assume the current point is H(index).\n    //Compute H(index+1).\n    //@return a 2-dimensional point with coordinates in [0,1)^2\n    double[] nextPoint() {\n    index++;\n    for(int i = 0; i < K.length; i++) {\n    for(int j = 0; j < K[i]; j++) {\n    d[i][j]++;\n    x[i] += q[i][j];\n    if (d[i][j] < P[i]) {\n    break;\n    }\n    d[i][j] = 0;\n    x[i] -= (j == 0? 1.0: q[i][j-1]);\n    }\n    }\n    return x;\n    }\n    }\n\n    //Mapper class for Pi estimation.\n    //Generate points in a unit square and then\n    //count points inside/outside of the inscribed circle of the square.\n    public static class PiMapper extends MapReduceBase\n    implements Mapper<LongWritable, LongWritable, BooleanWritable, LongWritable> {\n\n    //Map method.\n    //@param offset samples starting from the (offset+1)th sample.\n    //@param size the number of samples for this map\n    //@param out output {ture->numInside, false->numOutside}\n    //@param reporter\n    public void map(LongWritable offset,\n    LongWritable size,\n    OutputCollector<BooleanWritable, LongWritable> out,\n    Reporter reporter) throws IOException {\n\n    final HaltonSequence haltonsequence = new HaltonSequence(offset.get());\n    long numInside = 0L;\n    long numOutside = 0L;\n\n    for(long i = 0; i < size.get(); ) {\n    //generate points in a unit square\n    final double[] point = haltonsequence.nextPoint();\n\n    //count points inside/outside of the inscribed circle of the square\n    final double x = point[0] - 0.5;\n    final double y = point[1] - 0.5;\n    if (x*x + y*y > 0.25) {\n    numOutside++;\n    } else {\n    numInside++;\n    }\n\n    //report status\n    i++;\n    if (i % 1000 == 0) {\n    reporter.setStatus(\"Generated \" + i + \" samples.\");\n    }\n    }\n\n    //output map results\n    out.collect(new BooleanWritable(true), new LongWritable(numInside));\n    out.collect(new BooleanWritable(false), new LongWritable(numOutside));\n    }\n    }\n\n\n    //Reducer class for Pi estimation.\n    //Accumulate points inside/outside results from the mappers.\n    public static class PiReducer extends MapReduceBase\n    implements Reducer<BooleanWritable, LongWritable, WritableComparable<?>, Writable> {\n\n    private long numInside = 0;\n    private long numOutside = 0;\n    private JobConf conf; //configuration for accessing the file system\n\n    //Store job configuration.\n    @Override\n    public void configure(JobConf job) {\n    conf = job;\n    }\n\n\n    // Accumulate number of points inside/outside results from the mappers.\n    // @param isInside Is the points inside?\n    // @param values An iterator to a list of point counts\n    // @param output dummy, not used here.\n    // @param reporter\n\n    public void reduce(BooleanWritable isInside,\n    Iterator<LongWritable> values,\n    OutputCollector<WritableComparable<?>, Writable> output,\n    Reporter reporter) throws IOException {\n    if (isInside.get()) {\n    for(; values.hasNext(); numInside += values.next().get());\n    } else {\n    for(; values.hasNext(); numOutside += values.next().get());\n    }\n    }\n\n    //Reduce task done, write output to a file.\n    @Override\n    public void close() throws IOException {\n    //write output to a file\n    Path outDir = new Path(TMP_DIR, \"out\");\n    Path outFile = new Path(outDir, \"reduce-out\");\n    FileSystem fileSys = FileSystem.get(conf);\n    SequenceFile.Writer writer = SequenceFile.createWriter(fileSys, conf,\n    outFile, LongWritable.class, LongWritable.class,\n    CompressionType.NONE);\n    writer.append(new LongWritable(numInside), new LongWritable(numOutside));\n    writer.close();\n    }\n    }\n\n    //Run a map/reduce job for estimating Pi.\n    //@return the estimated value of Pi.\n    public static BigDecimal estimate(int numMaps, long numPoints, JobConf jobConf\n    )\n    throws IOException {\n    //setup job conf\n    jobConf.setJobName(PiEstimator.class.getSimpleName());\n\n    jobConf.setInputFormat(SequenceFileInputFormat.class);\n\n    jobConf.setOutputKeyClass(BooleanWritable.class);\n    jobConf.setOutputValueClass(LongWritable.class);\n    jobConf.setOutputFormat(SequenceFileOutputFormat.class);\n\n    jobConf.setMapperClass(PiMapper.class);\n    jobConf.setNumMapTasks(numMaps);\n\n    jobConf.setReducerClass(PiReducer.class);\n    jobConf.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn't handle\n    // multiple writers to the same file.\n    jobConf.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir = new Path(TMP_DIR, \"in\");\n    final Path outDir = new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(jobConf, inDir);\n    FileOutputFormat.setOutputPath(jobConf, outDir);\n\n    final FileSystem fs = FileSystem.get(jobConf);\n    if (fs.exists(TMP_DIR)) {\n     throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n     + \" already exists. Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n     throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n\n     //generate an input file for each map task\n     try {\n     for(int i=0; i < numMaps; ++i) {\n     final Path file = new Path(inDir, \"part\"+i);\n     final LongWritable offset = new LongWritable(i * numPoints);\n     final LongWritable size = new LongWritable(numPoints);\n     final SequenceFile.Writer writer = SequenceFile.createWriter(\n     fs, jobConf, file,\n     LongWritable.class, LongWritable.class, CompressionType.NONE);\n     try {\n     writer.append(offset, size);\n     } finally {\n     writer.close();\n     }\n     System.out.println(\"Wrote input for Map #\"+i);\n     }\n\n     //start a map/reduce job\n     System.out.println(\"Starting Job\");\n     final long startTime = System.currentTimeMillis();\n     JobClient.runJob(jobConf);\n     final double duration = (System.currentTimeMillis() - startTime)/1000.0;\n     System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n     //read outputs\n     Path inFile = new Path(outDir, \"reduce-out\");\n     LongWritable numInside = new LongWritable();\n     LongWritable numOutside = new LongWritable();\n     SequenceFile.Reader reader = new SequenceFile.Reader(fs, inFile, jobConf);\n     try {\n     reader.next(numInside, numOutside);\n     } finally {\n     reader.close();\n     }\n\n     //compute estimated value\n     return BigDecimal.valueOf(4).setScale(20)\n     .multiply(BigDecimal.valueOf(numInside.get()))\n     .divide(BigDecimal.valueOf(numMaps))\n     .divide(BigDecimal.valueOf(numPoints));\n     } finally {\n     fs.delete(TMP_DIR, true);\n     }\n     }\n\n    //Parse arguments and then runs a map/reduce job.\n    //Print output in standard out.\n    //@return a non-zero if there is an error. Otherwise, return 0.\n     public int run(String[] args) throws Exception {\n     if (args.length != 2) {\n     System.err.println(\"Usage: \"+getClass().getName()+\" <nMaps> <nSamples>\");\n     ToolRunner.printGenericCommandUsage(System.err);\n     return -1;\n     }\n\n     final int nMaps = Integer.parseInt(args[0]);\n     final long nSamples = Long.parseLong(args[1]);\n\n     System.out.println(\"Number of Maps = \" + nMaps);\n     System.out.println(\"Samples per Map = \" + nSamples);\n\n     final JobConf jobConf = new JobConf(getConf(), getClass());\n     System.out.println(\"Estimated value of Pi is \"\n     + estimate(nMaps, nSamples, jobConf));\n     return 0;\n     }\n\n     //main method for running it as a stand alone command.\n     public static void main(String[] argv) throws Exception {\n     System.exit(ToolRunner.run(null, new PiEstimator(), argv));\n     }\n     }\n\n## <a id=\"summary\"></a>Summary\n\nIn this tutorial, you saw how to run a MapReduce job on HDInsight and how to use Monte Carlo methods that require and generate large data sets that can be managed by this service.\n\nHere is the complete script used to run this sample on a default HDInsight 3.1 cluster or on a 3.0 cluster:\n\n    ### Provide the Azure subscription name and the HDInsight cluster name\n    $subscriptionName = \"<SubscriptionName>\"\n    $clusterName = \"<ClusterName>\"  \n\n    ###Select the Azure subscription to use\n    Select-AzureSubscription $subscriptionName\n\n    ### Create a MapReduce job definition\n    $piEstimatorJobDefinition = New-AzureHDInsightMapReduceJobDefinition -ClassName \"pi\" –Arguments “32”, “1000000000” -JarFile \"wasb:///example/jars/hadoop-mapreduce-examples.jar\"\n\n    ### Run the MapReduce job\n    $piJob = $piEstimatorJobDefinition | Start-AzureHDInsightJob -Cluster $clusterName\n\n    ### Wait for the job to finish  \n    $piJob | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600\n\n    ### Print the standard error file of the MapReduce job\n    Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $piJob.JobId -StandardOutput\n\n\n\n## <a id=\"next-steps\"></a>Next steps\n\nFor tutorials that describe running other samples and provide instructions on using Pig, Hive, and MapReduce jobs on Azure HDInsight with Azure PowerShell, see the following topics:\n\n* [Get Started with Azure HDInsight][hdinsight-get-started]\n* [Sample: 10GB GraySort][hdinsight-sample-10gb-graysort]\n* [Sample: Wordcount][hdinsight-sample-wordcount]\n* [Sample: C# Streaming][hdinsight-sample-csharp-streaming]\n* [Use Pig with HDInsight][hdinsight-use-pig]\n* [Use Hive with HDInsight][hdinsight-use-hive]\n* [Azure HDInsight SDK documentation][hdinsight-sdk-documentation]\n\n[hdinsight-sdk-documentation]: http://msdnstage.redmond.corp.microsoft.com/library/dn479185.aspx\n\n[powershell-install-configure]: ../install-configure-powershell.md\n\n[hdinsight-get-started]: ../hdinsight-get-started.md\n\n[hdinsight-samples]: hdinsight-run-samples.md\n[hdinsight-sample-10gb-graysort]: hdinsight-sample-10gb-graysort.md\n[hdinsight-sample-csharp-streaming]: hdinsight-sample-csharp-streaming.md\n[hdinsight-sample-pi-estimator]: hdinsight-sample-pi-estimator.md\n[hdinsight-sample-wordcount]: hdinsight-sample-wordcount.md\n\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n\ntest\n"
}