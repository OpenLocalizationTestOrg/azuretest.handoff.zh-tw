{
  "nodes": [
    {
      "content": "Import data into Machine Learning Studio | Microsoft Azure",
      "pos": [
        27,
        85
      ]
    },
    {
      "content": "How to import your training data Azure Machine Learning Studio from various data sources.",
      "pos": [
        104,
        193
      ]
    },
    {
      "content": "Learn what data types and data formats are supported.",
      "pos": [
        194,
        247
      ]
    },
    {
      "content": "Import your training data into Azure Machine Learning Studio from various data sources",
      "pos": [
        651,
        737
      ]
    },
    {
      "content": "When you develop a predictive analytics solution in Azure Machine Learning Studio, you train your model using data representative of your problem space.",
      "pos": [
        739,
        891
      ]
    },
    {
      "content": "This tutorial shows you how to import data from various data sources for training your model in Machine Learning Studio.",
      "pos": [
        892,
        1012
      ]
    },
    {
      "content": "You'll also learn which data formats are supported.",
      "pos": [
        1013,
        1064
      ]
    },
    {
      "content": "There are a number of sample datasets available in Machine Learning Studio that you can use for this purpose",
      "pos": [
        1066,
        1174
      ]
    },
    {
      "content": "(see <bpt id=\"p1\">[</bpt>Use the sample datasets in Azure Machine Learning Studio<ept id=\"p1\">](machine-learning-use-sample-datasets.md)</ept>).",
      "pos": [
        1175,
        1281
      ]
    },
    {
      "content": "But you can also import your own data into Machine Learning Studio for use in your experiments.",
      "pos": [
        1282,
        1377
      ]
    },
    {
      "content": "To use your own data in Machine Learning Studio, you can upload a data file ahead of time from your local hard drive to create a dataset module in your workspace.",
      "pos": [
        1473,
        1635
      ]
    },
    {
      "content": "Or you can access data from one of several online data sources while your experiment is running using the [Reader][reader] module:",
      "pos": [
        1636,
        1766
      ]
    },
    {
      "content": "Azure Blob storage, table, or SQL database",
      "pos": [
        1770,
        1812
      ]
    },
    {
      "content": "Hadoop using HiveQL",
      "pos": [
        1815,
        1834
      ]
    },
    {
      "content": "A web URL using HTTP",
      "pos": [
        1837,
        1857
      ]
    },
    {
      "content": "A data feed provider",
      "pos": [
        1860,
        1880
      ]
    },
    {
      "content": "Machine Learning Studio is designed to work with rectangular or tabular data, such as text data that's delimited or structured data from a database, though in some circumstances non-rectangular data may be used.",
      "pos": [
        1882,
        2093
      ]
    },
    {
      "content": "It's best if your data is relatively clean.",
      "pos": [
        2095,
        2138
      ]
    },
    {
      "content": "That is, you'll want to take care of issues such as unquoted strings before you upload the data into your experiment.",
      "pos": [
        2139,
        2256
      ]
    },
    {
      "content": "However, there are modules available in Machine Learning Studio that will let you do some manipulation of data within your experiment.",
      "pos": [
        2258,
        2392
      ]
    },
    {
      "content": "Depending on the machine learning algorithms you'll be using, you may need to decide how you'll handle data structural issues such as missing values and sparse data, and there are modules that can help with that.",
      "pos": [
        2393,
        2605
      ]
    },
    {
      "content": "Look in the <bpt id=\"p1\">**</bpt>Data Transformation<ept id=\"p1\">**</ept> section of the module palette for modules that perform these functions.",
      "pos": [
        2606,
        2713
      ]
    },
    {
      "content": "At any point in your experiment you can view or download the data that's produced by a module by right-clicking the output port.",
      "pos": [
        2715,
        2843
      ]
    },
    {
      "content": "Depending on the module there may be different download options available, or you may be able to view the data within your web browser in Machine Learning Studio.",
      "pos": [
        2844,
        3006
      ]
    },
    {
      "content": "Data formats",
      "pos": [
        3012,
        3024
      ]
    },
    {
      "content": "You can import a number of data types into your experiment, depending on what mechanism you use to import data and where it's coming from:",
      "pos": [
        3026,
        3164
      ]
    },
    {
      "content": "Plain text (.txt)",
      "pos": [
        3168,
        3185
      ]
    },
    {
      "content": "Comma-separated values (CSV) with a header (.csv) or without (.nh.csv)",
      "pos": [
        3188,
        3258
      ]
    },
    {
      "content": "Tab-separated values (TSV) with a header (.tsv) or without (.nh.tsv)",
      "pos": [
        3261,
        3329
      ]
    },
    {
      "content": "Hive table",
      "pos": [
        3332,
        3342
      ]
    },
    {
      "content": "SQL database table",
      "pos": [
        3345,
        3363
      ]
    },
    {
      "content": "OData values",
      "pos": [
        3366,
        3378
      ]
    },
    {
      "pos": [
        3381,
        3492
      ],
      "content": "SVMLight data (.svmlight) (see the <bpt id=\"p1\">[</bpt>SVMLight definition<ept id=\"p1\">](http://svmlight.joachims.org/)</ept> for format information)"
    },
    {
      "pos": [
        3495,
        3629
      ],
      "content": "Attribute Relation File Format (ARFF) data (.arff) (see the <bpt id=\"p1\">[</bpt>ARFF definition<ept id=\"p1\">](http://weka.wikispaces.com/ARFF)</ept> for format information)"
    },
    {
      "content": "Zip file (.zip)",
      "pos": [
        3632,
        3647
      ]
    },
    {
      "content": "R object or workspace file (.RData)",
      "pos": [
        3650,
        3685
      ]
    },
    {
      "content": "If you import data in a format such as ARFF that includes metadata, Machine Learning Studio uses this metadata to define the heading and data type of each column.",
      "pos": [
        3687,
        3849
      ]
    },
    {
      "content": "If you import data such as TSV or CSV format that doesn't include this metadata, Machine Learning Studio infers the data type for each column by sampling the data.",
      "pos": [
        3850,
        4013
      ]
    },
    {
      "content": "If the data also doesn't have column headings, Machine Learning Studio provides default names.",
      "pos": [
        4014,
        4108
      ]
    },
    {
      "content": "You can explicitly specify or change the headings and data types for columns using the [Metadata Editor][metadata-editor].",
      "pos": [
        4109,
        4231
      ]
    },
    {
      "content": "The following data types are recognized by Machine Learning Studio:",
      "pos": [
        4233,
        4300
      ]
    },
    {
      "content": "String",
      "pos": [
        4304,
        4310
      ]
    },
    {
      "content": "Integer",
      "pos": [
        4313,
        4320
      ]
    },
    {
      "content": "Double",
      "pos": [
        4323,
        4329
      ]
    },
    {
      "content": "Boolean",
      "pos": [
        4332,
        4339
      ]
    },
    {
      "content": "DateTime",
      "pos": [
        4342,
        4350
      ]
    },
    {
      "content": "TimeSpan",
      "pos": [
        4353,
        4361
      ]
    },
    {
      "content": "Machine Learning Studio uses an internal data type called <bpt id=\"p1\">*</bpt>Data Table<ept id=\"p1\">*</ept> to pass data between modules.",
      "pos": [
        4363,
        4463
      ]
    },
    {
      "content": "You can explicitly convert your data into Data Table format using the [Convert to Dataset][convert-to-dataset] module.",
      "pos": [
        4464,
        4582
      ]
    },
    {
      "content": "Any module that accepts formats other than Data Table will convert the data to Data Table silently before passing it to the next module.",
      "pos": [
        4583,
        4719
      ]
    },
    {
      "content": "If necessary, you can convert Data Table format back into CSV, TSV, ARFF, or SVMLight format using other conversion modules.",
      "pos": [
        4720,
        4844
      ]
    },
    {
      "content": "Look in the <bpt id=\"p1\">**</bpt>Data Format Conversions<ept id=\"p1\">**</ept> section of the module palette for modules that perform these functions.",
      "pos": [
        4845,
        4956
      ]
    },
    {
      "content": "Import data from a local file",
      "pos": [
        4962,
        4991
      ]
    },
    {
      "content": "You can import data from a local hard drive by doing the following:",
      "pos": [
        4993,
        5060
      ]
    },
    {
      "pos": [
        5065,
        5132
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>+NEW<ept id=\"p1\">**</ept> at the bottom of the Machine Learning Studio window."
    },
    {
      "pos": [
        5136,
        5179
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>DATASET<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>FROM LOCAL FILE<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        5183,
        5260
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Upload a new dataset<ept id=\"p1\">**</ept> dialog, browse to the file you want to upload"
    },
    {
      "content": "Enter a name, identify the data type, and optionally enter a description.",
      "pos": [
        5264,
        5337
      ]
    },
    {
      "content": "A description is recommended - it allows you to record any characteristics about the data that you will want to remember when using the data in the future.",
      "pos": [
        5338,
        5493
      ]
    },
    {
      "content": "The checkbox <bpt id=\"p1\">**</bpt>This is the new version of an existing dataset<ept id=\"p1\">**</ept> allows you to update an existing dataset with new data.",
      "pos": [
        5497,
        5616
      ]
    },
    {
      "content": "Just click this checkbox and then enter the name of an existing dataset.",
      "pos": [
        5617,
        5689
      ]
    },
    {
      "content": "During upload, you will see a message that your file is being uploaded.",
      "pos": [
        5691,
        5762
      ]
    },
    {
      "content": "Upload time will depend on the size of your data and the speed of your connection to the service.",
      "pos": [
        5763,
        5860
      ]
    },
    {
      "content": "If you know the file will take a long time, you can do other things inside Machine Learning Studio while you wait.",
      "pos": [
        5861,
        5975
      ]
    },
    {
      "content": "However, closing the browser will cause the data upload to fail.",
      "pos": [
        5976,
        6040
      ]
    },
    {
      "content": "Once your data is uploaded, it's stored in a dataset module and is available to any experiment in your workspace.",
      "pos": [
        6042,
        6155
      ]
    },
    {
      "content": "You can find the dataset, along with all the pre-loaded sample datasets, in the <bpt id=\"p1\">**</bpt>Saved Datasets<ept id=\"p1\">**</ept> list in the module palette when you're editing an experiment.",
      "pos": [
        6156,
        6316
      ]
    },
    {
      "content": "Access online data with the Reader module",
      "pos": [
        6322,
        6363
      ]
    },
    {
      "content": "Using the [Reader][reader] module in your experiment, you can access data from several online sources while your experiment is running.",
      "pos": [
        6365,
        6500
      ]
    },
    {
      "content": "Because this training data is accessed while your experiment is running, it's only available in one experiment (as opposed to dataset modules which are available to any experiment in your workspace).",
      "pos": [
        6501,
        6700
      ]
    },
    {
      "content": "After adding the [Reader][reader] module to your experiment, you select the <bpt id=\"p1\">**</bpt>Data source<ept id=\"p1\">**</ept> and then provide access information using module parameters.",
      "pos": [
        6702,
        6854
      ]
    },
    {
      "content": "For example, if you select <bpt id=\"p1\">**</bpt>Web URL via HTTP<ept id=\"p1\">**</ept>, you provide the source URL and data format.",
      "pos": [
        6855,
        6947
      ]
    },
    {
      "content": "If you're accessing your training data from Azure storage or HDInsight (using a Hive query), you provide the appropriate account information and the location of the data.",
      "pos": [
        6948,
        7118
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> This article provides general information about the [Reader][reader] module.",
      "pos": [
        7122,
        7211
      ]
    },
    {
      "content": "For more detailed information about the types of data you can access, formats, parameters, and answers to common questions, see the module reference topic for the [Reader][reader] module.",
      "pos": [
        7212,
        7399
      ]
    },
    {
      "content": "Get data from Azure",
      "pos": [
        7406,
        7425
      ]
    },
    {
      "content": "You can import from three Azure data sources:",
      "pos": [
        7427,
        7472
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Blob Storage<ept id=\"p1\">**</ept> - If you use the ARFF format for storage, columns are mapped by using the header metadata.",
      "pos": [
        7476,
        7589
      ]
    },
    {
      "content": "If you use TSV or CSV formats, mappings are inferred by sampling column data.",
      "pos": [
        7590,
        7667
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Table Storage<ept id=\"p1\">**</ept> - The [Reader][reader] module scans your data to identify column data types.",
      "pos": [
        7670,
        7770
      ]
    },
    {
      "content": "If your data is fairly homogenous and predictable you can limit the number of rows that are scanned.",
      "pos": [
        7771,
        7871
      ]
    },
    {
      "pos": [
        7874,
        8026
      ],
      "content": "<bpt id=\"p1\">**</bpt>Azure SQL Database<ept id=\"p1\">**</ept> - The [Reader][reader] module leverages the SQL Azure Transact client API to import data using a database query that you provide."
    },
    {
      "content": "For Blob and table storage you supply a Shared Access Signature URI (SAS URI) or Azure storage account information to provide access to the data.",
      "pos": [
        8028,
        8173
      ]
    },
    {
      "content": "For an Azure SQL database you supply your database and account information, plus a database query that identifies the data you want to import.",
      "pos": [
        8174,
        8316
      ]
    },
    {
      "content": "Get data from the web",
      "pos": [
        8322,
        8343
      ]
    },
    {
      "content": "You can use the [Reader][reader] module to read training data from a web or FTP site.",
      "pos": [
        8345,
        8430
      ]
    },
    {
      "content": "You need to provide:",
      "pos": [
        8431,
        8451
      ]
    },
    {
      "content": "A complete HTTP URL address of a file",
      "pos": [
        8455,
        8492
      ]
    },
    {
      "content": "The data format of the file (CSV, TSV, ARFF, or SvmLight)",
      "pos": [
        8495,
        8552
      ]
    },
    {
      "content": "For CSV or TSV files, indicate if the first line in the file is a header",
      "pos": [
        8555,
        8627
      ]
    },
    {
      "content": "Get data from Hadoop",
      "pos": [
        8633,
        8653
      ]
    },
    {
      "content": "You can use the [Reader][reader] module to read training data from distributed storage using the HiveQL query language.",
      "pos": [
        8655,
        8774
      ]
    },
    {
      "content": "You'll need to specify the Hive database query and provide user access information on the HCatalog server.",
      "pos": [
        8775,
        8881
      ]
    },
    {
      "content": "You also need to specify whether the data is stored in a Hadoop distributed file system (HDFS) or in Azure, and, if in Azure, the Azure account information",
      "pos": [
        8882,
        9037
      ]
    },
    {
      "content": "Get data from a data feed provider",
      "pos": [
        9045,
        9079
      ]
    },
    {
      "content": "By specifying an OData URL, you can read directly from a data feed provider.",
      "pos": [
        9081,
        9157
      ]
    },
    {
      "content": "You'll need to provide the source URL and the data content type.",
      "pos": [
        9158,
        9222
      ]
    },
    {
      "content": "Save data from your experiment",
      "pos": [
        9230,
        9260
      ]
    },
    {
      "content": "There will be times when you'll want to take an intermediate result from an experiment and use it as part of another experiment.",
      "pos": [
        9263,
        9391
      ]
    },
    {
      "content": "To do this:",
      "pos": [
        9392,
        9403
      ]
    },
    {
      "content": "Right-click the output of the module that you want to save as a dataset.",
      "pos": [
        9408,
        9480
      ]
    },
    {
      "pos": [
        9485,
        9511
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Save as Dataset<ept id=\"p1\">**</ept>."
    },
    {
      "content": "When prompted, enter a name and a description that would allow you to identify the dataset easily.",
      "pos": [
        9516,
        9614
      ]
    },
    {
      "pos": [
        9619,
        9646
      ],
      "content": "Click the <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> checkmark."
    },
    {
      "content": "When the save finishes, the dataset will be available for use within any experiment in your workspace.",
      "pos": [
        9648,
        9750
      ]
    },
    {
      "content": "You can find it in the <bpt id=\"p1\">**</bpt>Saved Datasets<ept id=\"p1\">**</ept> list in the module palette.",
      "pos": [
        9751,
        9820
      ]
    },
    {
      "content": "test",
      "pos": [
        10139,
        10143
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Import data into Machine Learning Studio | Microsoft Azure\"\n    description=\"How to import your training data Azure Machine Learning Studio from various data sources. Learn what data types and data formats are supported.\"\n    keywords=\"import data,data format,data types,data sources,training data\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"garyericson\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"08/14/2015\"\n    ms.author=\"garye\" />\n\n\n# Import your training data into Azure Machine Learning Studio from various data sources\n\nWhen you develop a predictive analytics solution in Azure Machine Learning Studio, you train your model using data representative of your problem space. This tutorial shows you how to import data from various data sources for training your model in Machine Learning Studio. You'll also learn which data formats are supported.\n\nThere are a number of sample datasets available in Machine Learning Studio that you can use for this purpose\n(see [Use the sample datasets in Azure Machine Learning Studio](machine-learning-use-sample-datasets.md)). But you can also import your own data into Machine Learning Studio for use in your experiments.\n\n[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]\n\nTo use your own data in Machine Learning Studio, you can upload a data file ahead of time from your local hard drive to create a dataset module in your workspace. Or you can access data from one of several online data sources while your experiment is running using the [Reader][reader] module:\n\n- Azure Blob storage, table, or SQL database\n- Hadoop using HiveQL\n- A web URL using HTTP\n- A data feed provider\n\nMachine Learning Studio is designed to work with rectangular or tabular data, such as text data that's delimited or structured data from a database, though in some circumstances non-rectangular data may be used.\n\nIt's best if your data is relatively clean. That is, you'll want to take care of issues such as unquoted strings before you upload the data into your experiment.\n\nHowever, there are modules available in Machine Learning Studio that will let you do some manipulation of data within your experiment. Depending on the machine learning algorithms you'll be using, you may need to decide how you'll handle data structural issues such as missing values and sparse data, and there are modules that can help with that.\nLook in the **Data Transformation** section of the module palette for modules that perform these functions.\n\nAt any point in your experiment you can view or download the data that's produced by a module by right-clicking the output port. Depending on the module there may be different download options available, or you may be able to view the data within your web browser in Machine Learning Studio.\n\n\n## Data formats\n\nYou can import a number of data types into your experiment, depending on what mechanism you use to import data and where it's coming from:\n\n- Plain text (.txt)\n- Comma-separated values (CSV) with a header (.csv) or without (.nh.csv)\n- Tab-separated values (TSV) with a header (.tsv) or without (.nh.tsv)\n- Hive table\n- SQL database table\n- OData values\n- SVMLight data (.svmlight) (see the [SVMLight definition](http://svmlight.joachims.org/) for format information)\n- Attribute Relation File Format (ARFF) data (.arff) (see the [ARFF definition](http://weka.wikispaces.com/ARFF) for format information)\n- Zip file (.zip)\n- R object or workspace file (.RData)\n\nIf you import data in a format such as ARFF that includes metadata, Machine Learning Studio uses this metadata to define the heading and data type of each column.\nIf you import data such as TSV or CSV format that doesn't include this metadata, Machine Learning Studio infers the data type for each column by sampling the data. If the data also doesn't have column headings, Machine Learning Studio provides default names.\nYou can explicitly specify or change the headings and data types for columns using the [Metadata Editor][metadata-editor].\n\nThe following data types are recognized by Machine Learning Studio:\n\n- String\n- Integer\n- Double\n- Boolean\n- DateTime\n- TimeSpan\n\nMachine Learning Studio uses an internal data type called *Data Table* to pass data between modules. You can explicitly convert your data into Data Table format using the [Convert to Dataset][convert-to-dataset] module.\nAny module that accepts formats other than Data Table will convert the data to Data Table silently before passing it to the next module.\nIf necessary, you can convert Data Table format back into CSV, TSV, ARFF, or SVMLight format using other conversion modules.\nLook in the **Data Format Conversions** section of the module palette for modules that perform these functions.\n\n\n## Import data from a local file\n\nYou can import data from a local hard drive by doing the following:\n\n1. Click **+NEW** at the bottom of the Machine Learning Studio window.\n2. Select **DATASET** and **FROM LOCAL FILE**.\n3. In the **Upload a new dataset** dialog, browse to the file you want to upload\n4. Enter a name, identify the data type, and optionally enter a description. A description is recommended - it allows you to record any characteristics about the data that you will want to remember when using the data in the future.\n5. The checkbox **This is the new version of an existing dataset** allows you to update an existing dataset with new data. Just click this checkbox and then enter the name of an existing dataset.\n\nDuring upload, you will see a message that your file is being uploaded. Upload time will depend on the size of your data and the speed of your connection to the service.\nIf you know the file will take a long time, you can do other things inside Machine Learning Studio while you wait. However, closing the browser will cause the data upload to fail.\n\nOnce your data is uploaded, it's stored in a dataset module and is available to any experiment in your workspace.\nYou can find the dataset, along with all the pre-loaded sample datasets, in the **Saved Datasets** list in the module palette when you're editing an experiment.\n\n\n## Access online data with the Reader module\n\nUsing the [Reader][reader] module in your experiment, you can access data from several online sources while your experiment is running.\nBecause this training data is accessed while your experiment is running, it's only available in one experiment (as opposed to dataset modules which are available to any experiment in your workspace).\n\nAfter adding the [Reader][reader] module to your experiment, you select the **Data source** and then provide access information using module parameters.\nFor example, if you select **Web URL via HTTP**, you provide the source URL and data format.\nIf you're accessing your training data from Azure storage or HDInsight (using a Hive query), you provide the appropriate account information and the location of the data.\n\n> [AZURE.NOTE] This article provides general information about the [Reader][reader] module. For more detailed information about the types of data you can access, formats, parameters, and answers to common questions, see the module reference topic for the [Reader][reader] module.\n\n\n### Get data from Azure\n\nYou can import from three Azure data sources:\n\n- **Azure Blob Storage** - If you use the ARFF format for storage, columns are mapped by using the header metadata. If you use TSV or CSV formats, mappings are inferred by sampling column data.\n- **Azure Table Storage** - The [Reader][reader] module scans your data to identify column data types. If your data is fairly homogenous and predictable you can limit the number of rows that are scanned.\n- **Azure SQL Database** - The [Reader][reader] module leverages the SQL Azure Transact client API to import data using a database query that you provide.\n\nFor Blob and table storage you supply a Shared Access Signature URI (SAS URI) or Azure storage account information to provide access to the data. For an Azure SQL database you supply your database and account information, plus a database query that identifies the data you want to import.\n\n### Get data from the web\n\nYou can use the [Reader][reader] module to read training data from a web or FTP site. You need to provide:\n\n- A complete HTTP URL address of a file\n- The data format of the file (CSV, TSV, ARFF, or SvmLight)\n- For CSV or TSV files, indicate if the first line in the file is a header\n\n### Get data from Hadoop\n\nYou can use the [Reader][reader] module to read training data from distributed storage using the HiveQL query language. You'll need to specify the Hive database query and provide user access information on the HCatalog server.\nYou also need to specify whether the data is stored in a Hadoop distributed file system (HDFS) or in Azure, and, if in Azure, the Azure account information  \n\n### Get data from a data feed provider\n\nBy specifying an OData URL, you can read directly from a data feed provider. You'll need to provide the source URL and the data content type.  \n\n\n## Save data from your experiment\n\n\nThere will be times when you'll want to take an intermediate result from an experiment and use it as part of another experiment. To do this:\n\n1. Right-click the output of the module that you want to save as a dataset.\n\n2. Click **Save as Dataset**.\n\n3. When prompted, enter a name and a description that would allow you to identify the dataset easily.\n\n4. Click the **OK** checkmark.\n\nWhen the save finishes, the dataset will be available for use within any experiment in your workspace. You can find it in the **Saved Datasets** list in the module palette.\n\n\n<!-- Module References -->\n[convert-to-dataset]: https://msdn.microsoft.com/library/azure/72bf58e0-fc87-4bb1-9704-f1805003b975/\n[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/\n\ntest\n"
}