<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Blob storage | Microsoft Azure</source>
          <target state="new">Create and load data into Hive tables from Blob storage | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Create Hive tables and load data in blob to hive tables</source>
          <target state="new">Create Hive tables and load data in blob to hive tables</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Azure blob storage</source>
          <target state="new">Create and load data into Hive tables from Azure blob storage</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</source>
          <target state="new">In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>These Hive queries are shared in the GitHub repository.</source>
          <target state="new">These Hive queries are shared in the GitHub repository.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Github repository<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Github repository<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept>.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>If you create an Azure virtual machine by following the instructions in "Set Up an Azure Virtual Machine with IPython Notebook Server", this script file has been downloaded to the <ph id="ph1">`C:\Users\&lt;user name&gt;\Documents\Data Science Scripts`</ph> directory on the virtual machine.</source>
          <target state="new">If you create an Azure virtual machine by following the instructions in "Set Up an Azure Virtual Machine with IPython Notebook Server", this script file has been downloaded to the <ph id="ph1">`C:\Users\&lt;user name&gt;\Documents\Data Science Scripts`</ph> directory on the virtual machine.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission.</source>
          <target state="new">You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>We assume that the data for Hive tables is in <bpt id="p1">**</bpt>uncompressed<ept id="p1">**</ept> tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster.</source>
          <target state="new">We assume that the data for Hive tables is in <bpt id="p1">**</bpt>uncompressed<ept id="p1">**</ept> tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>If users want to practice on the <bpt id="p1">_</bpt>NYC Taxi Trip Data<ept id="p1">_</ept>, they need to first <bpt id="p2">[</bpt>download all 24 files<ept id="p2">](http://www.andresmh.com/nyctaxitrips/)</ept> (12 Trip files, and 12 Fair files), <bpt id="p3">**</bpt>unzip<ept id="p3">**</ept> all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the <bpt id="p4">[</bpt>Azure HDInsight Hadoop cluster is customized<ept id="p4">](machine-learning-data-science-customize-hadoop-cluster.html)</ept>.</source>
          <target state="new">If users want to practice on the <bpt id="p1">_</bpt>NYC Taxi Trip Data<ept id="p1">_</ept>, they need to first <bpt id="p2">[</bpt>download all 24 files<ept id="p2">](http://www.andresmh.com/nyctaxitrips/)</ept> (12 Trip files, and 12 Fair files), <bpt id="p3">**</bpt>unzip<ept id="p3">**</ept> all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the <bpt id="p4">[</bpt>Azure HDInsight Hadoop cluster is customized<ept id="p4">](machine-learning-data-science-customize-hadoop-cluster.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Hive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster.</source>
          <target state="new">Hive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>You need to:</source>
          <target state="new">You need to:</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Enable remote access to the head node of the Hadoop cluster, and log on to the head node<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Enable remote access to the head node of the Hadoop cluster, and log on to the head node<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Submit the Hive queries in the Hadoop Command Line on the head node<ept id="p1">](machine-learning-data-science-hive-queries.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Submit the Hive queries in the Hadoop Command Line on the head node<ept id="p1">](machine-learning-data-science-hive-queries.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Users can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://</source>
          <target state="new">Users can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>.azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can <bpt id="p1">[</bpt>submit Hive jobs using PowerShell<ept id="p1">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept>.</source>
          <target state="new">.azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can <bpt id="p1">[</bpt>submit Hive jobs using PowerShell<ept id="p1">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Step 1: Create Hive database and tables</source>
          <target state="new">Step 1: Create Hive database and tables</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Step 2: Load data to Hive tables</source>
          <target state="new">Step 2: Load data to Hive tables</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Advanced topics: partitioned table and store Hive data in ORC format</source>
          <target state="new">Advanced topics: partitioned table and store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="create-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables</source>
          <target state="new"><ph id="ph1">&lt;a name="create-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Here are the descriptions of the fields that users need to plug in and other configurations:</source>
          <target state="new">Here are the descriptions of the fields that users need to plug in and other configurations:</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;database name&gt;`</ph>: the name of the database users want to create.</source>
          <target state="new"><ph id="ph1">`&lt;database name&gt;`</ph>: the name of the database users want to create.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>If users just want to use the default database, the query <ph id="ph1">`create database...`</ph> can be omitted.</source>
          <target state="new">If users just want to use the default database, the query <ph id="ph1">`create database...`</ph> can be omitted.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;table name&gt;`</ph>: the name of the table users want to create within the specified database.</source>
          <target state="new"><ph id="ph1">`&lt;table name&gt;`</ph>: the name of the table users want to create within the specified database.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>If users want to use the default database, the table can be directly referred by <ph id="ph1">`&lt;table name&gt;`</ph> without <ph id="ph2">`&lt;database name&gt;.`</ph>.</source>
          <target state="new">If users want to use the default database, the table can be directly referred by <ph id="ph1">`&lt;table name&gt;`</ph> without <ph id="ph2">`&lt;database name&gt;.`</ph>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;field separator&gt;`</ph>: the separator that separates fields in the data file to be uploaded to the Hive table.</source>
          <target state="new"><ph id="ph1">`&lt;field separator&gt;`</ph>: the separator that separates fields in the data file to be uploaded to the Hive table.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;line separator&gt;`</ph>: the separator that separates lines in the data file.</source>
          <target state="new"><ph id="ph1">`&lt;line separator&gt;`</ph>: the separator that separates lines in the data file.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;storage location&gt;`</ph>: the Azure storage location to save the data of Hive tables.</source>
          <target state="new"><ph id="ph1">`&lt;storage location&gt;`</ph>: the Azure storage location to save the data of Hive tables.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>If users do not specify <ph id="ph1">`LOCATION '&lt;storage location&gt;'`</ph>, by default the database and the tables are stored in <ph id="ph2">`hive/warehouse/`</ph> directory in the default container of the Hive cluster.</source>
          <target state="new">If users do not specify <ph id="ph1">`LOCATION '&lt;storage location&gt;'`</ph>, by default the database and the tables are stored in <ph id="ph2">`hive/warehouse/`</ph> directory in the default container of the Hive cluster.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables.</source>
          <target state="new">If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>This location has to be referred as relative location to the default container of the cluster in the format of <ph id="ph1">`'wasb:///&lt;directory 1&gt;/'`</ph> or <ph id="ph2">`'wasb:///&lt;directory 1&gt;/&lt;directory 2&gt;/'`</ph>, etc. After the query is executed, the relative directories will be created within the default container.</source>
          <target state="new">This location has to be referred as relative location to the default container of the cluster in the format of <ph id="ph1">`'wasb:///&lt;directory 1&gt;/'`</ph> or <ph id="ph2">`'wasb:///&lt;directory 1&gt;/&lt;directory 2&gt;/'`</ph>, etc. After the query is executed, the relative directories will be created within the default container.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`TBLPROPERTIES("skip.header.line.count"="1")`</ph>: If the data file has a header line, users have to add this property at the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> of the <ph id="ph2">`create table`</ph> query.</source>
          <target state="new"><ph id="ph1">`TBLPROPERTIES("skip.header.line.count"="1")`</ph>: If the data file has a header line, users have to add this property at the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> of the <ph id="ph2">`create table`</ph> query.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Otherwise, the header line will be loaded as a record to the table.</source>
          <target state="new">Otherwise, the header line will be loaded as a record to the table.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If the data file does not have a header line, this configuration can be omitted in the query.</source>
          <target state="new">If the data file does not have a header line, this configuration can be omitted in the query.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load data to Hive tables</source>
          <target state="new"><ph id="ph1">&lt;a name="load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load data to Hive tables</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`&lt;path to blob data&gt;`</ph>: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the <ph id="ph2">`&lt;path to blob data&gt;`</ph> should be in the format <ph id="ph3">`'wasb:///&lt;directory in this container&gt;/&lt;blob file name&gt;'`</ph>.</source>
          <target state="new"><ph id="ph1">`&lt;path to blob data&gt;`</ph>: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the <ph id="ph2">`&lt;path to blob data&gt;`</ph> should be in the format <ph id="ph3">`'wasb:///&lt;directory in this container&gt;/&lt;blob file name&gt;'`</ph>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>The blob file can also be in the additional container of the HDInsight Hadoop cluster.</source>
          <target state="new">The blob file can also be in the additional container of the HDInsight Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>In this case, <ph id="ph1">`&lt;path to blob data&gt;`</ph> should be in the format <ph id="ph2">`'wasb://&lt;container name&gt;@&lt;storage account name&gt;.blob.windows.core.net/&lt;blob file name&gt;'`</ph>.</source>
          <target state="new">In this case, <ph id="ph1">`&lt;path to blob data&gt;`</ph> should be in the format <ph id="ph2">`'wasb://&lt;container name&gt;@&lt;storage account name&gt;.blob.windows.core.net/&lt;blob file name&gt;'`</ph>.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Otherwise, the <ph id="ph1">`LOAD DATA`</ph> query will fail complaining that it cannot access the data.</source>
          <target state="new">Otherwise, the <ph id="ph1">`LOAD DATA`</ph> query will fail complaining that it cannot access the data.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="partition-orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Advanced topics: partitioned table and store Hive data in ORC format</source>
          <target state="new"><ph id="ph1">&lt;a name="partition-orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Advanced topics: partitioned table and store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>If the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table.</source>
          <target state="new">If the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>For instance,  it is reasonable to partition the log data of a web site by dates.</source>
          <target state="new">For instance,  it is reasonable to partition the log data of a web site by dates.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>In addition to partition the table, it is also beneficial to store the Hive data in ORC format.</source>
          <target state="new">In addition to partition the table, it is also beneficial to store the Hive data in ORC format.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>ORC stands for "Optimized Row Columnar".</source>
          <target state="new">ORC stands for "Optimized Row Columnar".</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Please refer to <bpt id="p1">_</bpt>"<bpt id="p2">[</bpt>Using ORC files improves performance when Hive is reading, writing, and processing data<ept id="p2">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles)</ept>."<ept id="p1">_</ept> for more details.</source>
          <target state="new">Please refer to <bpt id="p1">_</bpt>"<bpt id="p2">[</bpt>Using ORC files improves performance when Hive is reading, writing, and processing data<ept id="p2">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles)</ept>."<ept id="p1">_</ept> for more details.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Partitioned table</source>
          <target state="new">Partitioned table</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>The queries of creating a partitioned table and loading data to it are as follows:</source>
          <target state="new">The queries of creating a partitioned table and loading data to it are as follows:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p1">**</bpt>beginning<ept id="p1">**</ept> of the <ph id="ph1">`where`</ph> clause so that the searching efficacy can be significantly improved.</source>
          <target state="new">When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p1">**</bpt>beginning<ept id="p1">**</ept> of the <ph id="ph1">`where`</ph> clause so that the searching efficacy can be significantly improved.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Store Hive data in ORC format</source>
          <target state="new"><ph id="ph1">&lt;a name="orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Users cannot directly load data in blob to Hive tables in ORC storage format.</source>
          <target state="new">Users cannot directly load data in blob to Hive tables in ORC storage format.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</source>
          <target state="new">Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Create an external table <bpt id="p1">**</bpt>STORED AS TEXTFILE<ept id="p1">**</ept> and load data from blob storage to the table.</source>
          <target state="new">Create an external table <bpt id="p1">**</bpt>STORED AS TEXTFILE<ept id="p1">**</ept> and load data from blob storage to the table.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Create an internal table with the same schema as the external table in step 1, and the same field delimiter.</source>
          <target state="new">Create an internal table with the same schema as the external table in step 1, and the same field delimiter.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>And store the Hive data in the ORC format</source>
          <target state="new">And store the Hive data in the ORC format</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Select data from the external table in step 1 and insert into the ORC table</source>
          <target state="new">Select data from the external table in step 1 and insert into the ORC table</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> If the TEXTFILE table <ph id="ph2">`&lt;database name&gt;.&lt;external textfile table name&gt;`</ph> has partitions, in STEP 3, <ph id="ph3">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph> will select the partition variable as a field in the returned data set.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> If the TEXTFILE table <ph id="ph2">`&lt;database name&gt;.&lt;external textfile table name&gt;`</ph> has partitions, in STEP 3, <ph id="ph3">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph> will select the partition variable as a field in the returned data set.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Inserting it to the <ph id="ph1">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> will fail since <ph id="ph2">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> does not have the partition variable as a field in the table schema.</source>
          <target state="new">Inserting it to the <ph id="ph1">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> will fail since <ph id="ph2">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> does not have the partition variable as a field in the table schema.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>In this case, users need to specifically select the fields to be inserted to <ph id="ph1">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> like follows:</source>
          <target state="new">In this case, users need to specifically select the fields to be inserted to <ph id="ph1">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> like follows:</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>It is safe to drop the <ph id="ph1">`&lt;external textfile table name&gt;`</ph> using the following query after all data has been inserted into <ph id="ph2">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph>:</source>
          <target state="new">It is safe to drop the <ph id="ph1">`&lt;external textfile table name&gt;`</ph> using the following query after all data has been inserted into <ph id="ph2">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph>:</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Now we have a table with data in the ORC format ready to use.</source>
          <target state="new">Now we have a table with data in the ORC format ready to use.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">88058462abba6282680111e7b6a78cdb8e342645</xliffext:olfilehash>
  </header>
</xliff>