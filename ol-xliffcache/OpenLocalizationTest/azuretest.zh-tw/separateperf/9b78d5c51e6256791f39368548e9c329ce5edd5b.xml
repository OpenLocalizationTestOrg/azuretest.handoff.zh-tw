{
  "nodes": [
    {
      "content": "Stream Analytics Release Notes | Microsoft Azure",
      "pos": [
        28,
        76
      ]
    },
    {
      "content": "Stream Analytics GA Release Notes",
      "pos": [
        96,
        129
      ]
    },
    {
      "content": "Microsoft Stream Analytic release notes",
      "pos": [
        469,
        508
      ]
    },
    {
      "content": "Notes for 08/20/2015 release of Stream Analytics",
      "pos": [
        513,
        561
      ]
    },
    {
      "content": "This release contains the following updates.",
      "pos": [
        566,
        610
      ]
    },
    {
      "content": "Title",
      "pos": [
        612,
        617
      ]
    },
    {
      "content": "Description",
      "pos": [
        618,
        629
      ]
    },
    {
      "content": "Added LAST function",
      "pos": [
        638,
        657
      ]
    },
    {
      "pos": [
        659,
        862
      ],
      "content": "The <bpt id=\"p1\">[</bpt>LAST<ept id=\"p1\">](http://msdn.microsoft.com/library/mt421186.aspx)</ept> function is now available in Stream Analytics jobs, enabling you to retrieve the most recent event in an event stream within a given timeframe."
    },
    {
      "content": "New Array functions",
      "pos": [
        863,
        882
      ]
    },
    {
      "pos": [
        883,
        1123
      ],
      "content": "Array functions <bpt id=\"p1\">[</bpt>GetArrayElement<ept id=\"p1\">](http://msdn.microsoft.com/library/mt270218.aspx)</ept>, <bpt id=\"p2\">[</bpt>GetArrayElements<ept id=\"p2\">](http://msdn.microsoft.com/library/mt298451.aspx)</ept> and <bpt id=\"p3\">[</bpt>GetArrayLength<ept id=\"p3\">](http://msdn.microsoft.com/library/mt270226.aspx)</ept> are now available."
    },
    {
      "content": "New Record functions",
      "pos": [
        1124,
        1144
      ]
    },
    {
      "pos": [
        1145,
        1329
      ],
      "content": "Record functions <bpt id=\"p1\">[</bpt>GetRecordProperties<ept id=\"p1\">](http://msdn.microsoft.com/library/mt270221.aspx)</ept> and <bpt id=\"p2\">[</bpt>GetRecordPropertyValue<ept id=\"p2\">](http://msdn.microsoft.com/library/mt270220.aspx)</ept> are now available."
    },
    {
      "content": "Notes for 07/30/2015 release of Stream Analytics",
      "pos": [
        1334,
        1382
      ]
    },
    {
      "content": "This release contains the following updates.",
      "pos": [
        1387,
        1431
      ]
    },
    {
      "content": "Title",
      "pos": [
        1433,
        1438
      ]
    },
    {
      "content": "Description",
      "pos": [
        1439,
        1450
      ]
    },
    {
      "content": "Power BI Org Id decoupled from Azure Id",
      "pos": [
        1459,
        1498
      ]
    },
    {
      "content": "This feature enables <bpt id=\"p1\">[</bpt>Power BI output<ept id=\"p1\">](stream-analytics-power-bi-dashboard.md)</ept> for ASA jobs under any Azure account type (Live Id or Org Id).",
      "pos": [
        1499,
        1640
      ]
    },
    {
      "content": "Additionally, you can have one Org Id for your Azure account and use a different one for authorizing Power BI output.",
      "pos": [
        1641,
        1758
      ]
    },
    {
      "content": "Support for Service Bus Queues output",
      "pos": [
        1759,
        1796
      ]
    },
    {
      "pos": [
        1797,
        1936
      ],
      "content": "<bpt id=\"p1\">[</bpt>Service Bus Queues<ept id=\"p1\">](stream-analytics-connect-data-event-outputs.md#service-bus-queues)</ept> outputs are now available in Stream Analytics jobs."
    },
    {
      "content": "Support for Service Bus Topics output",
      "pos": [
        1937,
        1974
      ]
    },
    {
      "pos": [
        1975,
        2114
      ],
      "content": "<bpt id=\"p1\">[</bpt>Service Bus Topics<ept id=\"p1\">](stream-analytics-connect-data-event-outputs.md#service-bus-topics)</ept> outputs are now available in Stream Analytics jobs."
    },
    {
      "content": "Notes for 07/09/2015 release of Stream Analytics",
      "pos": [
        2119,
        2167
      ]
    },
    {
      "content": "This release contains the following updates.",
      "pos": [
        2172,
        2216
      ]
    },
    {
      "content": "Title",
      "pos": [
        2219,
        2224
      ]
    },
    {
      "content": "Description",
      "pos": [
        2225,
        2236
      ]
    },
    {
      "content": "Custom Blob Output Partitioning",
      "pos": [
        2245,
        2276
      ]
    },
    {
      "content": "Blob storage outputs now allow an option to specify the frequency that output blobs are written and the structure and format of the output data path folder structure.",
      "pos": [
        2277,
        2443
      ]
    },
    {
      "content": "Notes for 05/03/2015 release of Stream Analytics",
      "pos": [
        2449,
        2497
      ]
    },
    {
      "content": "This release contains the following updates.",
      "pos": [
        2502,
        2546
      ]
    },
    {
      "content": "Title",
      "pos": [
        2549,
        2554
      ]
    },
    {
      "content": "Description",
      "pos": [
        2555,
        2566
      ]
    },
    {
      "content": "Increased maximum value for Out of Order Tolerance Window",
      "pos": [
        2575,
        2632
      ]
    },
    {
      "content": "The maximum size for the Out of Order Tolerance Window is now 59:59 (MM:SS)",
      "pos": [
        2633,
        2708
      ]
    },
    {
      "content": "JSON Output Format: Line Separated or Array",
      "pos": [
        2709,
        2752
      ]
    },
    {
      "content": "Now there is an option when outputting to Blob Storage or Event Hub to output as either an array of JSON objects or by separating JSON objects with a new line.",
      "pos": [
        2753,
        2912
      ]
    },
    {
      "content": "Notes for 04/16/2015 release of Stream Analytics",
      "pos": [
        2918,
        2966
      ]
    },
    {
      "content": "Title",
      "pos": [
        2972,
        2977
      ]
    },
    {
      "content": "Description",
      "pos": [
        2978,
        2989
      ]
    },
    {
      "content": "Delay in Azure Storage account configuration",
      "pos": [
        2998,
        3042
      ]
    },
    {
      "content": "When creating a Stream Analytics job in a region for the first time, you will be prompted to create a new Storage account or specify an existing account for monitoring Stream Analytics jobs in that region.",
      "pos": [
        3043,
        3248
      ]
    },
    {
      "content": "Due to latency in configuring monitoring, creating another Stream Analytics job in the same region within 30 minutes will prompt for the specifying of a second Storage account instead of showing the recently configured one in the Monitoring Storage Account drop-down.",
      "pos": [
        3249,
        3516
      ]
    },
    {
      "content": "To avoid creating an unnecessary Storage account, wait 30 minutes after creating a job in a region for the first time before provisioning additional jobs in that region.",
      "pos": [
        3517,
        3686
      ]
    },
    {
      "content": "Job Upgrade",
      "pos": [
        3687,
        3698
      ]
    },
    {
      "content": "At this time, Stream Analytics does not support live edits to the definition or configuration of a running job.",
      "pos": [
        3699,
        3810
      ]
    },
    {
      "content": "In order to change the input, output, query, scale or configuration of a running job, you must first stop the job.",
      "pos": [
        3811,
        3925
      ]
    },
    {
      "content": "Data types inferred from input source",
      "pos": [
        3926,
        3963
      ]
    },
    {
      "content": "If a CREATE TABLE statement is not used, the input type is derived from input format, for example all fields from CSV are string.",
      "pos": [
        3964,
        4093
      ]
    },
    {
      "content": "Fields need to be converted explicitly to the right type using the CAST function in order to avoid type mismatch errors.",
      "pos": [
        4094,
        4214
      ]
    },
    {
      "content": "Missing fields are outputted as null values",
      "pos": [
        4215,
        4258
      ]
    },
    {
      "content": "Referencing a field that is not present in the input source will result in null values in the output event.",
      "pos": [
        4259,
        4366
      ]
    },
    {
      "content": "WITH statements must precede SELECT statements",
      "pos": [
        4367,
        4413
      ]
    },
    {
      "content": "In your query, SELECT statements must follow subqueries defined in WITH statements.",
      "pos": [
        4414,
        4497
      ]
    },
    {
      "content": "Out-of-memory issue",
      "pos": [
        4498,
        4517
      ]
    },
    {
      "content": "Streaming Analytics jobs with a large tolerance for out-of-order events and/or complex queries maintaining a large amount of state may cause the job to run out of memory, resulting in a job restart.",
      "pos": [
        4518,
        4716
      ]
    },
    {
      "content": "The start and stop operations will be visible in the job’s operation logs.",
      "pos": [
        4717,
        4791
      ]
    },
    {
      "content": "To avoid this behavior, scale the query out across multiple partitions.",
      "pos": [
        4792,
        4863
      ]
    },
    {
      "content": "In a future release, this limitation will be addressed by degrading performance on impacted jobs instead of restarting them.",
      "pos": [
        4864,
        4988
      ]
    },
    {
      "content": "Large blob inputs without payload timestamp may cause Out-of-memory issue",
      "pos": [
        4989,
        5062
      ]
    },
    {
      "content": "Consuming large files from Blob storage may cause Stream Analytics jobs to crash if a timestamp field is not specified via TIMESTAMP BY.",
      "pos": [
        5063,
        5199
      ]
    },
    {
      "content": "To avoid this issue, keep each blob under 10MB in size.",
      "pos": [
        5200,
        5255
      ]
    },
    {
      "content": "SQL Database event volume limitation",
      "pos": [
        5256,
        5292
      ]
    },
    {
      "content": "When using SQL Database as an output target, very high volumes of output data may cause the Stream Analytics job to time out.",
      "pos": [
        5293,
        5418
      ]
    },
    {
      "content": "To resolve this issue, either reduce the output volume by using aggregates or filter operators, or choose Azure Blob storage or Event Hubs as an output target instead.",
      "pos": [
        5419,
        5586
      ]
    },
    {
      "content": "PowerBI datasets can only contain one table",
      "pos": [
        5587,
        5630
      ]
    },
    {
      "content": "PowerBI does not support more than one table in a given dataset.",
      "pos": [
        5631,
        5695
      ]
    },
    {
      "content": "Get help",
      "pos": [
        5700,
        5708
      ]
    },
    {
      "pos": [
        5709,
        5851
      ],
      "content": "For further assistance, try our <bpt id=\"p1\">[</bpt>Azure Stream Analytics forum<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept>"
    },
    {
      "content": "Next steps",
      "pos": [
        5856,
        5866
      ]
    },
    {
      "content": "Introduction to Azure Stream Analytics",
      "pos": [
        5871,
        5909
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics",
      "pos": [
        5948,
        5988
      ]
    },
    {
      "content": "Scale Azure Stream Analytics jobs",
      "pos": [
        6029,
        6062
      ]
    },
    {
      "content": "Azure Stream Analytics Query Language Reference",
      "pos": [
        6099,
        6146
      ]
    },
    {
      "content": "Azure Stream Analytics Management REST API Reference",
      "pos": [
        6207,
        6259
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Stream Analytics Release Notes | Microsoft Azure\" \n    description=\"Stream Analytics GA Release Notes\" \n    services=\"stream-analytics\" \n    documentationCenter=\"\" \n    authors=\"jeffstokes72\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"stream-analytics\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.tgt_pltfrm=\"na\" \n    ms.workload=\"data-services\" \n    ms.date=\"08/20/2015\" \n    ms.author=\"jeffstok\"/>\n\n#Microsoft Stream Analytic release notes\n\n## Notes for 08/20/2015 release of Stream Analytics ##\n\nThis release contains the following updates.\n\nTitle|Description\n---|---\nAdded LAST function |The [LAST](http://msdn.microsoft.com/library/mt421186.aspx) function is now available in Stream Analytics jobs, enabling you to retrieve the most recent event in an event stream within a given timeframe.\nNew Array functions|Array functions [GetArrayElement](http://msdn.microsoft.com/library/mt270218.aspx), [GetArrayElements](http://msdn.microsoft.com/library/mt298451.aspx) and [GetArrayLength](http://msdn.microsoft.com/library/mt270226.aspx) are now available.\nNew Record functions|Record functions [GetRecordProperties](http://msdn.microsoft.com/library/mt270221.aspx) and [GetRecordPropertyValue](http://msdn.microsoft.com/library/mt270220.aspx) are now available.\n\n## Notes for 07/30/2015 release of Stream Analytics ##\n\nThis release contains the following updates.\n\nTitle|Description\n---|---\nPower BI Org Id decoupled from Azure Id|This feature enables [Power BI output](stream-analytics-power-bi-dashboard.md) for ASA jobs under any Azure account type (Live Id or Org Id). Additionally, you can have one Org Id for your Azure account and use a different one for authorizing Power BI output.\nSupport for Service Bus Queues output|[Service Bus Queues](stream-analytics-connect-data-event-outputs.md#service-bus-queues) outputs are now available in Stream Analytics jobs.\nSupport for Service Bus Topics output|[Service Bus Topics](stream-analytics-connect-data-event-outputs.md#service-bus-topics) outputs are now available in Stream Analytics jobs.\n\n## Notes for 07/09/2015 release of Stream Analytics ##\n\nThis release contains the following updates.\n\n\nTitle|Description\n---|---\nCustom Blob Output Partitioning|Blob storage outputs now allow an option to specify the frequency that output blobs are written and the structure and format of the output data path folder structure. \n\n## Notes for 05/03/2015 release of Stream Analytics ##\n\nThis release contains the following updates.\n\n\nTitle|Description\n---|---\nIncreased maximum value for Out of Order Tolerance Window|The maximum size for the Out of Order Tolerance Window is now 59:59 (MM:SS)\nJSON Output Format: Line Separated or Array|Now there is an option when outputting to Blob Storage or Event Hub to output as either an array of JSON objects or by separating JSON objects with a new line. \n\n## Notes for 04/16/2015 release of Stream Analytics ##\n\n\nTitle|Description\n---|---\nDelay in Azure Storage account configuration|When creating a Stream Analytics job in a region for the first time, you will be prompted to create a new Storage account or specify an existing account for monitoring Stream Analytics jobs in that region. Due to latency in configuring monitoring, creating another Stream Analytics job in the same region within 30 minutes will prompt for the specifying of a second Storage account instead of showing the recently configured one in the Monitoring Storage Account drop-down. To avoid creating an unnecessary Storage account, wait 30 minutes after creating a job in a region for the first time before provisioning additional jobs in that region.\nJob Upgrade|At this time, Stream Analytics does not support live edits to the definition or configuration of a running job. In order to change the input, output, query, scale or configuration of a running job, you must first stop the job.\nData types inferred from input source|If a CREATE TABLE statement is not used, the input type is derived from input format, for example all fields from CSV are string. Fields need to be converted explicitly to the right type using the CAST function in order to avoid type mismatch errors.\nMissing fields are outputted as null values|Referencing a field that is not present in the input source will result in null values in the output event.\nWITH statements must precede SELECT statements|In your query, SELECT statements must follow subqueries defined in WITH statements.\nOut-of-memory issue|Streaming Analytics jobs with a large tolerance for out-of-order events and/or complex queries maintaining a large amount of state may cause the job to run out of memory, resulting in a job restart. The start and stop operations will be visible in the job’s operation logs. To avoid this behavior, scale the query out across multiple partitions. In a future release, this limitation will be addressed by degrading performance on impacted jobs instead of restarting them.\nLarge blob inputs without payload timestamp may cause Out-of-memory issue|Consuming large files from Blob storage may cause Stream Analytics jobs to crash if a timestamp field is not specified via TIMESTAMP BY. To avoid this issue, keep each blob under 10MB in size.\nSQL Database event volume limitation|When using SQL Database as an output target, very high volumes of output data may cause the Stream Analytics job to time out. To resolve this issue, either reduce the output volume by using aggregates or filter operators, or choose Azure Blob storage or Event Hubs as an output target instead.\nPowerBI datasets can only contain one table|PowerBI does not support more than one table in a given dataset.\n\n## Get help\nFor further assistance, try our [Azure Stream Analytics forum](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)\n\n## Next steps\n\n- [Introduction to Azure Stream Analytics](stream-analytics-introduction.md)\n- [Get started using Azure Stream Analytics](../stream.analytics.get.started.md)\n- [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n- [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx)\n- [Azure Stream Analytics Management REST API Reference](https://msdn.microsoft.com/library/azure/dn835031.aspx)\n \n"
}