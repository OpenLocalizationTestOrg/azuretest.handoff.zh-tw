<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Azure Media Services Fragmented MP4 Live Ingest Specification</source>
          <target state="new">Azure Media Services Fragmented MP4 Live Ingest Specification</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.</source>
          <target state="new">This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.</source>
          <target state="new">Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.</source>
          <target state="new">At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>This document also discusses best practices in building highly redundant and robust live ingest mechanisms.</source>
          <target state="new">This document also discusses best practices in building highly redundant and robust live ingest mechanisms.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Azure Media Services Fragmented MP4 Live Ingest Specification</source>
          <target state="new">Azure Media Services Fragmented MP4 Live Ingest Specification</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.</source>
          <target state="new">This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.</source>
          <target state="new">Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.</source>
          <target state="new">At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>This document also discusses best practices in building highly redundant and robust live ingest mechanisms.</source>
          <target state="new">This document also discusses best practices in building highly redundant and robust live ingest mechanisms.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Conformance Notation</source>
          <target state="new">Conformance Notation</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in RFC 2119.</source>
          <target state="new">The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in RFC 2119.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Service Diagram</source>
          <target state="new">Service Diagram</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>The diagram below shows the high level architecture of the live streaming service in Microsoft Azure Media Services:</source>
          <target state="new">The diagram below shows the high level architecture of the live streaming service in Microsoft Azure Media Services:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Live Encoder pushes live feeds into Channels which are created and provisioned via the Microsoft Azure Media Services SDK.</source>
          <target state="new">Live Encoder pushes live feeds into Channels which are created and provisioned via the Microsoft Azure Media Services SDK.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Channels, Programs and Streaming endpoint in Microsoft Azure Media Services handle all the live streaming functionalities including ingest, formatting, cloud DVR, security, scalability and redundancy.</source>
          <target state="new">Channels, Programs and Streaming endpoint in Microsoft Azure Media Services handle all the live streaming functionalities including ingest, formatting, cloud DVR, security, scalability and redundancy.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Optionally customers could choose to deploy a CDN layer between the Streaming endpoint and the client endpoints.</source>
          <target state="new">Optionally customers could choose to deploy a CDN layer between the Streaming endpoint and the client endpoints.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Client endpoints stream from the Streaming endpoint using HTTP Adaptive Streaming protocols (e.g. Smooth Streaming, DASH, HDS or HLS).</source>
          <target state="new">Client endpoints stream from the Streaming endpoint using HTTP Adaptive Streaming protocols (e.g. Smooth Streaming, DASH, HDS or HLS).</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>image1</source>
          <target state="new">image1</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Bit-stream Format – ISO 14496-12 Fragmented MP4</source>
          <target state="new">Bit-stream Format – ISO 14496-12 Fragmented MP4</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>The wire format for live streaming ingest that is discussed in this document is based on [ISO-14496-12].</source>
          <target state="new">The wire format for live streaming ingest that is discussed in this document is based on [ISO-14496-12].</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Please refer to <bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> for detailed explanation of Fragmented MP4 format and extensions for both video-on-demand files and live streaming ingestion.</source>
          <target state="new">Please refer to <bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> for detailed explanation of Fragmented MP4 format and extensions for both video-on-demand files and live streaming ingestion.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Below is a list of special format definitions that apply to live ingest into Microsoft Azure Media Services:</source>
          <target state="new">Below is a list of special format definitions that apply to live ingest into Microsoft Azure Media Services:</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>The ‘ftyp’, LiveServerManifestBox, and ‘moov’ box MUST be sent with each request (HTTP POST).</source>
          <target state="new">The ‘ftyp’, LiveServerManifestBox, and ‘moov’ box MUST be sent with each request (HTTP POST).</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>It MUST be sent at the beginning of the stream and anytime the encoder must reconnect to resume stream ingest.</source>
          <target state="new">It MUST be sent at the beginning of the stream and anytime the encoder must reconnect to resume stream ingest.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Please refer to Section 6 in [1] for more details.</source>
          <target state="new">Please refer to Section 6 in [1] for more details.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Section 3.3.2 in [1] defines an optional box called StreamManifestBox for live ingest.</source>
          <target state="new">Section 3.3.2 in [1] defines an optional box called StreamManifestBox for live ingest.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Due to the routing logic of Microsoft Azure’s load balancer, usage of this box is deprecated and SHOULD NOT be present when ingesting into Microsoft Azure Media Service.</source>
          <target state="new">Due to the routing logic of Microsoft Azure’s load balancer, usage of this box is deprecated and SHOULD NOT be present when ingesting into Microsoft Azure Media Service.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>If this box is present, Azure Media Services silently ignores it.</source>
          <target state="new">If this box is present, Azure Media Services silently ignores it.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>The TrackFragmentExtendedHeaderBox defined in 3.2.3.2 in [1] MUST be present for each fragment.</source>
          <target state="new">The TrackFragmentExtendedHeaderBox defined in 3.2.3.2 in [1] MUST be present for each fragment.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Version 2 of the TrackFragmentExtendedHeaderBox SHOULD be used in order to generate media segments with identical URLs in multiple datacenters.</source>
          <target state="new">Version 2 of the TrackFragmentExtendedHeaderBox SHOULD be used in order to generate media segments with identical URLs in multiple datacenters.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>The fragment index field is REQUIRED for cross-datacenter failover of index-based streaming formats such as Apple HTTP Live Streaming (HLS) and index-based MPEG-DASH.</source>
          <target state="new">The fragment index field is REQUIRED for cross-datacenter failover of index-based streaming formats such as Apple HTTP Live Streaming (HLS) and index-based MPEG-DASH.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>To enable cross-datacenter failover, the fragment index MUST be synchronized across multiple encoders, and increase by 1 for each successive media fragment, even across encoder restarts or failures.</source>
          <target state="new">To enable cross-datacenter failover, the fragment index MUST be synchronized across multiple encoders, and increase by 1 for each successive media fragment, even across encoder restarts or failures.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Section 3.3.6 in [1] defines box called MovieFragmentRandomAccessBox (‘mfra’) that MAY be sent at the end of live ingestion to indicate EOS (End-of-Stream) to the channel.</source>
          <target state="new">Section 3.3.6 in [1] defines box called MovieFragmentRandomAccessBox (‘mfra’) that MAY be sent at the end of live ingestion to indicate EOS (End-of-Stream) to the channel.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Due to the ingest logic of Azure Media Services, usage of EOS (End-of-Stream) is deprecated and the ‘mfra’ box for live ingestion SHOULD NOT be sent.</source>
          <target state="new">Due to the ingest logic of Azure Media Services, usage of EOS (End-of-Stream) is deprecated and the ‘mfra’ box for live ingestion SHOULD NOT be sent.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>If sent, Azure Media Services silently ignores it.</source>
          <target state="new">If sent, Azure Media Services silently ignores it.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>It is recommended to use <bpt id="p1">[</bpt>Channel Reset<ept id="p1">](https://msdn.microsoft.com/library/azure/dn783458.aspx#reset_channels)</ept> to reset the state of the ingest point and also it is recommended to use <bpt id="p2">[</bpt>Program Stop<ept id="p2">](https://msdn.microsoft.com/library/azure/dn783463.aspx#stop_programs)</ept> to end a presentation and stream.</source>
          <target state="new">It is recommended to use <bpt id="p1">[</bpt>Channel Reset<ept id="p1">](https://msdn.microsoft.com/library/azure/dn783458.aspx#reset_channels)</ept> to reset the state of the ingest point and also it is recommended to use <bpt id="p2">[</bpt>Program Stop<ept id="p2">](https://msdn.microsoft.com/library/azure/dn783463.aspx#stop_programs)</ept> to end a presentation and stream.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>The MP4 fragment duration SHOULD be constant, in order to reduce the size of the client manifests and improve client download heuristics through use of repeat tags.</source>
          <target state="new">The MP4 fragment duration SHOULD be constant, in order to reduce the size of the client manifests and improve client download heuristics through use of repeat tags.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The duration MAY fluctuate in order to compensate for non-integer frame rates.</source>
          <target state="new">The duration MAY fluctuate in order to compensate for non-integer frame rates.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>The MP4 fragment duration SHOULD be between approximately 2 and 6 seconds.</source>
          <target state="new">The MP4 fragment duration SHOULD be between approximately 2 and 6 seconds.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>MP4 fragment timestamps and indexes (TrackFragmentExtendedHeaderBox fragment_absolute_time and fragment_index) SHOULD arrive in increasing order.</source>
          <target state="new">MP4 fragment timestamps and indexes (TrackFragmentExtendedHeaderBox fragment_absolute_time and fragment_index) SHOULD arrive in increasing order.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Although Azure Media Services is resilient to duplicate fragments, it has very limited ability to reorder fragments according to the media timeline.</source>
          <target state="new">Although Azure Media Services is resilient to duplicate fragments, it has very limited ability to reorder fragments according to the media timeline.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Protocol Format – HTTP</source>
          <target state="new">Protocol Format – HTTP</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>ISO Fragmented MP4 based live ingest for Microsoft Azure Media Services uses a standard long running HTTP POST request to transmit encoded media data packaged in Fragmented MP4 format to the service.</source>
          <target state="new">ISO Fragmented MP4 based live ingest for Microsoft Azure Media Services uses a standard long running HTTP POST request to transmit encoded media data packaged in Fragmented MP4 format to the service.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Each HTTP POST sends a complete Fragmented MP4 bit-stream (“Stream”) starting from beginning with header boxes ( ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) and continuing with a sequence of fragments (‘moof’ and ‘mdat’ boxes).</source>
          <target state="new">Each HTTP POST sends a complete Fragmented MP4 bit-stream (“Stream”) starting from beginning with header boxes ( ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) and continuing with a sequence of fragments (‘moof’ and ‘mdat’ boxes).</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Please refer to section 9.2 in [1] for URL syntax for HTTP POST request.</source>
          <target state="new">Please refer to section 9.2 in [1] for URL syntax for HTTP POST request.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>An example of the POST URL is:</source>
          <target state="new">An example of the POST URL is:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Here are the detailed requirements:</source>
          <target state="new">Here are the detailed requirements:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Encoder SHOULD start the broadcast by sending an HTTP POST request with an empty “body” (zero content length) using the same ingestion URL.</source>
          <target state="new">Encoder SHOULD start the broadcast by sending an HTTP POST request with an empty “body” (zero content length) using the same ingestion URL.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>This can help quickly detect if the live ingestion endpoint is valid and if there is any authentication or other conditions required.</source>
          <target state="new">This can help quickly detect if the live ingestion endpoint is valid and if there is any authentication or other conditions required.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Per HTTP protocol, the server won’t be able to send back HTTP response until the entire request including POST body is received.</source>
          <target state="new">Per HTTP protocol, the server won’t be able to send back HTTP response until the entire request including POST body is received.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Given the long running nature of live event, without this step, the encoder may not be able to detect any error until it finishes sending all the data.</source>
          <target state="new">Given the long running nature of live event, without this step, the encoder may not be able to detect any error until it finishes sending all the data.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Encoder MUST handle any errors or authentication challenges as a result of (1).</source>
          <target state="new">Encoder MUST handle any errors or authentication challenges as a result of (1).</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>If (1) succeeds with a 200 response, continue.</source>
          <target state="new">If (1) succeeds with a 200 response, continue.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Encoder MUST start a new HTTP POST request with the fragmented MP4 stream.</source>
          <target state="new">Encoder MUST start a new HTTP POST request with the fragmented MP4 stream.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>The payload MUST start with the header boxes followed by fragments.</source>
          <target state="new">The payload MUST start with the header boxes followed by fragments.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Note the ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box (in this order) MUST be sent with each request, even if the encoder must reconnect because the previous request was terminated prior to the end of the stream.</source>
          <target state="new">Note the ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box (in this order) MUST be sent with each request, even if the encoder must reconnect because the previous request was terminated prior to the end of the stream.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Encoder MUST use Chunked Transfer Encoding for uploading since it’s impossible to predict the entire content length of the live event.</source>
          <target state="new">Encoder MUST use Chunked Transfer Encoding for uploading since it’s impossible to predict the entire content length of the live event.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>When the event is over, after sending the last fragment, the encoder MUST gracefully end the Chunked Transfer Encoding message sequence (most HTTP client stacks handle it automatically).</source>
          <target state="new">When the event is over, after sending the last fragment, the encoder MUST gracefully end the Chunked Transfer Encoding message sequence (most HTTP client stacks handle it automatically).</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Encoder MUST wait for the service to return the final response code and then terminate the connection.</source>
          <target state="new">Encoder MUST wait for the service to return the final response code and then terminate the connection.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Encoder MUST NOT use the Events() noun as described in 9.2 in [1] for live ingestion into Microsoft Azure Media Services.</source>
          <target state="new">Encoder MUST NOT use the Events() noun as described in 9.2 in [1] for live ingestion into Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>If the HTTP POST request terminates or times out prior to the end of the stream with a TCP error, the encoder MUST issue a new POST request using a new connection and follow the requirements above with the additional requirement that the encoder MUST resend the previous two MP4 fragments for each track in the stream, and resume without introducing discontinuities in the media timeline.</source>
          <target state="new">If the HTTP POST request terminates or times out prior to the end of the stream with a TCP error, the encoder MUST issue a new POST request using a new connection and follow the requirements above with the additional requirement that the encoder MUST resend the previous two MP4 fragments for each track in the stream, and resume without introducing discontinuities in the media timeline.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Resending the last two MP4 fragments for each track ensures that there is no data loss.</source>
          <target state="new">Resending the last two MP4 fragments for each track ensures that there is no data loss.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>In other words, if a stream contains both an audio and video track, and the current POST request fails, the encoder must reconnect and resend the last two fragments for the audio track, which were previously successfully sent, and the last two fragments for the video track, which were previously successfully sent, in order to ensure that there is no data loss.</source>
          <target state="new">In other words, if a stream contains both an audio and video track, and the current POST request fails, the encoder must reconnect and resend the last two fragments for the audio track, which were previously successfully sent, and the last two fragments for the video track, which were previously successfully sent, in order to ensure that there is no data loss.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The encoder MUST maintain a “forward” buffer of media fragments, which it resends when reconnecting.</source>
          <target state="new">The encoder MUST maintain a “forward” buffer of media fragments, which it resends when reconnecting.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Timescale</source>
          <target state="new">Timescale</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> describes the usage of “Timescale” for SmoothStreamingMedia (Section 2.2.2.1), StreamElement (Section 2.2.2.3), StreamFragmentElement(2.2.2.6) and LiveSMIL (Section 2.2.7.3.1).</source>
          <target state="new"><bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> describes the usage of “Timescale” for SmoothStreamingMedia (Section 2.2.2.1), StreamElement (Section 2.2.2.3), StreamFragmentElement(2.2.2.6) and LiveSMIL (Section 2.2.7.3.1).</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>If timescale value is not present, the default value used is 10,000,000 (10 MHz).</source>
          <target state="new">If timescale value is not present, the default value used is 10,000,000 (10 MHz).</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Although Smooth Streaming Format Specification doesn’t block usage of other timescale values, most of the encoder implementations uses this default value (10 MHz) to generate Smooth Streaming ingest data.</source>
          <target state="new">Although Smooth Streaming Format Specification doesn’t block usage of other timescale values, most of the encoder implementations uses this default value (10 MHz) to generate Smooth Streaming ingest data.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Due to <bpt id="p1">[</bpt>Azure Media Dynamic Packaging<ept id="p1">](media-services-dynamic-packaging-overview.md)</ept> feature, it is recommend to use 90 kHz timescale for video streams and 44.1 or 48.1 kHz for audio streams.</source>
          <target state="new">Due to <bpt id="p1">[</bpt>Azure Media Dynamic Packaging<ept id="p1">](media-services-dynamic-packaging-overview.md)</ept> feature, it is recommend to use 90 kHz timescale for video streams and 44.1 or 48.1 kHz for audio streams.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>If different timescale values are used for different streams, the stream level timescale MUST be sent.</source>
          <target state="new">If different timescale values are used for different streams, the stream level timescale MUST be sent.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Please refer to <bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept>.</source>
          <target state="new">Please refer to <bpt id="p1">[</bpt>[MS-SSTR]<ept id="p1">](https://msdn.microsoft.com/library/ff469518.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Definition of “Stream”</source>
          <target state="new">Definition of “Stream”</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>“Stream” is the basic unit of operation in live ingestion for composing live presentation, handling streaming failover and redundancy scenarios.</source>
          <target state="new">“Stream” is the basic unit of operation in live ingestion for composing live presentation, handling streaming failover and redundancy scenarios.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>“Stream” is defined as one unique Fragmented MP4 bit-stream which may contain a single track or multiple tracks.</source>
          <target state="new">“Stream” is defined as one unique Fragmented MP4 bit-stream which may contain a single track or multiple tracks.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>A full live presentation could contain one or more streams depending on the configuration of the live encoder(s).</source>
          <target state="new">A full live presentation could contain one or more streams depending on the configuration of the live encoder(s).</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>The examples below illustrate various options of using stream(s) to compose a full live presentation.</source>
          <target state="new">The examples below illustrate various options of using stream(s) to compose a full live presentation.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Example:</source>
          <target state="new">Example:</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Customer wants to create a live streaming presentation which includes the following audio/video bitrates:</source>
          <target state="new">Customer wants to create a live streaming presentation which includes the following audio/video bitrates:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Video – 3000kbps, 1500kbps, 750kbps</source>
          <target state="new">Video – 3000kbps, 1500kbps, 750kbps</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Audio – 128kbps</source>
          <target state="new">Audio – 128kbps</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Option 1: All tracks in one stream</source>
          <target state="new">Option 1: All tracks in one stream</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>In this option, a single encoder generates all audio/video tracks and bundle them into one Fragmented MP4 bit-stream which then gets sent via a single HTTP POST connection.</source>
          <target state="new">In this option, a single encoder generates all audio/video tracks and bundle them into one Fragmented MP4 bit-stream which then gets sent via a single HTTP POST connection.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>In this example, there is only one stream for this live presentation:</source>
          <target state="new">In this example, there is only one stream for this live presentation:</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>image2</source>
          <target state="new">image2</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Option 2: Each track in a separate stream</source>
          <target state="new">Option 2: Each track in a separate stream</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>In this option, the encoder(s) only put one track into each Fragment MP4 bit-stream and post all the streams over multiple separate HTTP connections.</source>
          <target state="new">In this option, the encoder(s) only put one track into each Fragment MP4 bit-stream and post all the streams over multiple separate HTTP connections.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>This could be done with one encoders or multiple encoders.</source>
          <target state="new">This could be done with one encoders or multiple encoders.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>From live ingestion’s point of view, this live presentation is composed of four streams.</source>
          <target state="new">From live ingestion’s point of view, this live presentation is composed of four streams.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>image3</source>
          <target state="new">image3</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Option 3: Bundle audio track with the lowest bitrate video track into one stream</source>
          <target state="new">Option 3: Bundle audio track with the lowest bitrate video track into one stream</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>In this option, the customer chooses to bundle the audio track with the lowest bitrate video track into one Fragment MP4 bit-stream and leave the other two video tracks each being its own stream.</source>
          <target state="new">In this option, the customer chooses to bundle the audio track with the lowest bitrate video track into one Fragment MP4 bit-stream and leave the other two video tracks each being its own stream.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>image4</source>
          <target state="new">image4</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Summary</source>
          <target state="new">Summary</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>What’s shown above is NOT an exhaustive list of all the possible ingestion options for this example.</source>
          <target state="new">What’s shown above is NOT an exhaustive list of all the possible ingestion options for this example.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>As a matter of fact, any grouping of tracks into streams is supported by the live ingestion.</source>
          <target state="new">As a matter of fact, any grouping of tracks into streams is supported by the live ingestion.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Customers and encoder vendors can choose their own implementations based on engineering complexity, encoder capacity, and redundancy and failover considerations.</source>
          <target state="new">Customers and encoder vendors can choose their own implementations based on engineering complexity, encoder capacity, and redundancy and failover considerations.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>However it should be noted that in most cases there is only one audio track for the entire live presentation so it’s important to ensure the healthiness of the ingest stream that contains the audio track.</source>
          <target state="new">However it should be noted that in most cases there is only one audio track for the entire live presentation so it’s important to ensure the healthiness of the ingest stream that contains the audio track.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>This consideration often results in putting audio track into its own stream (as in Option 2) or bundling it with the lowest bitrate video track (as in Option 3).</source>
          <target state="new">This consideration often results in putting audio track into its own stream (as in Option 2) or bundling it with the lowest bitrate video track (as in Option 3).</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Also for better redundancy and fault-tolerance, sending the same audio track in two different streams (Option 2 with redundant audio tracks) or bundling the audio track at least two of the lowest bitrate video tracks (Option 3 with audio bundled in at least two video streams)  is highly recommended for live ingest into Microsoft Azure Media Services.</source>
          <target state="new">Also for better redundancy and fault-tolerance, sending the same audio track in two different streams (Option 2 with redundant audio tracks) or bundling the audio track at least two of the lowest bitrate video tracks (Option 3 with audio bundled in at least two video streams)  is highly recommended for live ingest into Microsoft Azure Media Services.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Service Failover</source>
          <target state="new">Service Failover</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Given the nature of live streaming, good failover support is critical for ensuring the availability of the service.</source>
          <target state="new">Given the nature of live streaming, good failover support is critical for ensuring the availability of the service.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Microsoft Azure Media Services is designed to handle various types of failures including network errors, server errors, storage problems, etc. When used in conjunction with proper failover logic from the live encoder side, customer can achieve a highly reliable live streaming service from the cloud.</source>
          <target state="new">Microsoft Azure Media Services is designed to handle various types of failures including network errors, server errors, storage problems, etc. When used in conjunction with proper failover logic from the live encoder side, customer can achieve a highly reliable live streaming service from the cloud.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>In this section, we will discuss service failover scenarios.</source>
          <target state="new">In this section, we will discuss service failover scenarios.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>In this case, the failure happens somewhere within the service and manifests itself as a network error.</source>
          <target state="new">In this case, the failure happens somewhere within the service and manifests itself as a network error.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Here are some recommendations for the encoder implementation for handling service failover:</source>
          <target state="new">Here are some recommendations for the encoder implementation for handling service failover:</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Use a 10 second timeout for establishing the TCP connection.</source>
          <target state="new">Use a 10 second timeout for establishing the TCP connection.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>If an attempt to establish the connection takes longer than 10 seconds, abort the operation and try again.</source>
          <target state="new">If an attempt to establish the connection takes longer than 10 seconds, abort the operation and try again.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Use a short timeout for sending the HTTP request message chunks.</source>
          <target state="new">Use a short timeout for sending the HTTP request message chunks.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>If the target MP4 fragment duration is N seconds, use a send timeout between N and 2N seconds; for example, use a timeout of 6 to 12 seconds if the MP4 fragment duration is 6 seconds.</source>
          <target state="new">If the target MP4 fragment duration is N seconds, use a send timeout between N and 2N seconds; for example, use a timeout of 6 to 12 seconds if the MP4 fragment duration is 6 seconds.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>If a timeout occurs, reset the connection, open a new connection, and resume stream ingest on the new connection.</source>
          <target state="new">If a timeout occurs, reset the connection, open a new connection, and resume stream ingest on the new connection.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Maintain a rolling buffer containing the last two fragments, for each track, that were successfully and completely sent to the service.</source>
          <target state="new">Maintain a rolling buffer containing the last two fragments, for each track, that were successfully and completely sent to the service.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>If the HTTP POST request for a stream is terminated or times out prior to the end of the stream, open a new connection and begin another HTTP POST request, resend the stream headers, resend the last two fragments for each track, and resume the stream without introducing a discontinuity in the media timeline.</source>
          <target state="new">If the HTTP POST request for a stream is terminated or times out prior to the end of the stream, open a new connection and begin another HTTP POST request, resend the stream headers, resend the last two fragments for each track, and resume the stream without introducing a discontinuity in the media timeline.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>This will reduce the chance of data loss.</source>
          <target state="new">This will reduce the chance of data loss.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>It is recommended that the encoder does NOT limit the number of retries to establish a connection or resume streaming after a TCP error occurs.</source>
          <target state="new">It is recommended that the encoder does NOT limit the number of retries to establish a connection or resume streaming after a TCP error occurs.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>After a TCP error:</source>
          <target state="new">After a TCP error:</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>The current connection MUST be closed, and a new connection MUST be created for a new HTTP POST request.</source>
          <target state="new">The current connection MUST be closed, and a new connection MUST be created for a new HTTP POST request.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>The new HTTP POST URL MUST be the same as the initial POST URL.</source>
          <target state="new">The new HTTP POST URL MUST be the same as the initial POST URL.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>The new HTTP POST MUST include stream headers (‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) identical to the stream headers in the initial POST.</source>
          <target state="new">The new HTTP POST MUST include stream headers (‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) identical to the stream headers in the initial POST.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>The last two fragments sent for each track MUST be resent, and streaming resumed without introducing a discontinuity in the media timeline.</source>
          <target state="new">The last two fragments sent for each track MUST be resent, and streaming resumed without introducing a discontinuity in the media timeline.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>The MP4 fragment timestamps must increase continuously, even across HTTP POST requests.</source>
          <target state="new">The MP4 fragment timestamps must increase continuously, even across HTTP POST requests.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>The encoder SHOULD terminate the HTTP POST request if data is not being sent at a rate commensurate with the MP4 fragment duration.</source>
          <target state="new">The encoder SHOULD terminate the HTTP POST request if data is not being sent at a rate commensurate with the MP4 fragment duration.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>An HTTP POST request that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update.</source>
          <target state="new">An HTTP POST request that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>For this reason, the HTTP POST for sparse (ad signal) tracks SHOULD be short lived, terminating as soon as the sparse fragment is sent.</source>
          <target state="new">For this reason, the HTTP POST for sparse (ad signal) tracks SHOULD be short lived, terminating as soon as the sparse fragment is sent.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>Encoder Failover</source>
          <target state="new">Encoder Failover</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>Encoder failover is the second type of failover scenario that needs to be addressed for end-to-end live streaming delivery.</source>
          <target state="new">Encoder failover is the second type of failover scenario that needs to be addressed for end-to-end live streaming delivery.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>In this scenario, the error condition happened on the encoder side.</source>
          <target state="new">In this scenario, the error condition happened on the encoder side.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>image5</source>
          <target state="new">image5</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>Below are the expectations from the live ingestion endpoint when encoder failover happens:</source>
          <target state="new">Below are the expectations from the live ingestion endpoint when encoder failover happens:</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>A new encoder instance SHOULD be created in order to continue streaming, as illustrated in the diagram above (Stream for 3000k video with dashed line).</source>
          <target state="new">A new encoder instance SHOULD be created in order to continue streaming, as illustrated in the diagram above (Stream for 3000k video with dashed line).</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>The new encoder MUST use the same URL for HTTP POST requests as the failed instance.</source>
          <target state="new">The new encoder MUST use the same URL for HTTP POST requests as the failed instance.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>The new encoder’s POST request MUST include the same fragmented MP4 header boxes as the failed instance.</source>
          <target state="new">The new encoder’s POST request MUST include the same fragmented MP4 header boxes as the failed instance.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>The new encoder MUST be properly synchronized with all other running encoders for the same live presentation to generate synchronized audio/video samples with aligned fragment boundaries.</source>
          <target state="new">The new encoder MUST be properly synchronized with all other running encoders for the same live presentation to generate synchronized audio/video samples with aligned fragment boundaries.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>The new stream MUST be semantically equivalent with the previous stream and interchangeable at header and fragment level.</source>
          <target state="new">The new stream MUST be semantically equivalent with the previous stream and interchangeable at header and fragment level.</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>The new encoder SHOULD try to minimize data loss.</source>
          <target state="new">The new encoder SHOULD try to minimize data loss.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>The fragment_absolute_time and fragment_index of media fragments SHOULD increase from the point where the encoder last stopped.</source>
          <target state="new">The fragment_absolute_time and fragment_index of media fragments SHOULD increase from the point where the encoder last stopped.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>The fragment_absolute_time and fragment_index SHOULD increase in a continuous fashion, but it is permissible to introduce a discontinuity if necessary.</source>
          <target state="new">The fragment_absolute_time and fragment_index SHOULD increase in a continuous fashion, but it is permissible to introduce a discontinuity if necessary.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>Azure Media Services will ignore fragments that it has already received and processed, so it is better to err on the side of resending fragments than to introduce discontinuities in the media timeline.</source>
          <target state="new">Azure Media Services will ignore fragments that it has already received and processed, so it is better to err on the side of resending fragments than to introduce discontinuities in the media timeline.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>Encoder Redundancy</source>
          <target state="new">Encoder Redundancy</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>For certain critical live events that demand even higher availability and quality of experience, it is recommended to employ active-active redundant encoders to achieve seamless failover with no data loss.</source>
          <target state="new">For certain critical live events that demand even higher availability and quality of experience, it is recommended to employ active-active redundant encoders to achieve seamless failover with no data loss.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>image6</source>
          <target state="new">image6</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>As illustrated in the diagram above, there are two group of encoders pushing two copies of each stream simultaneously into the live service.</source>
          <target state="new">As illustrated in the diagram above, there are two group of encoders pushing two copies of each stream simultaneously into the live service.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>This setup is supported because Microsoft Azure Media Services has the ability to filter out duplicate fragments based on stream ID and fragment timestamp.</source>
          <target state="new">This setup is supported because Microsoft Azure Media Services has the ability to filter out duplicate fragments based on stream ID and fragment timestamp.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>The resulting live stream and archive will be a single of copy of all the streams that is the best possible aggregation from the two sources.</source>
          <target state="new">The resulting live stream and archive will be a single of copy of all the streams that is the best possible aggregation from the two sources.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>For example, in a hypothetical extreme case, as long as there is one encoder (doesn’t have to be the same one) running at any given point in time for each stream, the resulting live stream from the service will be continuous without data loss.</source>
          <target state="new">For example, in a hypothetical extreme case, as long as there is one encoder (doesn’t have to be the same one) running at any given point in time for each stream, the resulting live stream from the service will be continuous without data loss.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>The requirement for this scenario is almost the same as the requirements in Encoder Failover case with the exception that the second set of encoders are running at the same time as the primary encoders.</source>
          <target state="new">The requirement for this scenario is almost the same as the requirements in Encoder Failover case with the exception that the second set of encoders are running at the same time as the primary encoders.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>Service Redundancy</source>
          <target state="new">Service Redundancy</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>For highly redundant global distribution, it is sometimes required to have cross-region backup to handle regional disasters.</source>
          <target state="new">For highly redundant global distribution, it is sometimes required to have cross-region backup to handle regional disasters.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>Expanding on the “Encoder Redundancy” topology, customers can choose to have a redundant service deployment in a different region which is connected with the 2nd set of encoders.</source>
          <target state="new">Expanding on the “Encoder Redundancy” topology, customers can choose to have a redundant service deployment in a different region which is connected with the 2nd set of encoders.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>Customers could also work with a CDN provider to deploy a GTM (Global Traffic Manager) in front of the two service deployments to seamlessly route client traffic.</source>
          <target state="new">Customers could also work with a CDN provider to deploy a GTM (Global Traffic Manager) in front of the two service deployments to seamlessly route client traffic.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>The requirements for the encoders are the same as “Encoder Redundancy” case with the only exception that the second set of encoders need to be pointed to a different live ingest end point.</source>
          <target state="new">The requirements for the encoders are the same as “Encoder Redundancy” case with the only exception that the second set of encoders need to be pointed to a different live ingest end point.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>The diagram below shows this setup:</source>
          <target state="new">The diagram below shows this setup:</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>image7</source>
          <target state="new">image7</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>Special Types of Ingestion Formats</source>
          <target state="new">Special Types of Ingestion Formats</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>This section discusses some special type of live ingestion formats that are designed to handle some specific scenarios.</source>
          <target state="new">This section discusses some special type of live ingestion formats that are designed to handle some specific scenarios.</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>Sparse Track</source>
          <target state="new">Sparse Track</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>When delivering a live streaming presentation with rich client experience, it is often necessary to transmit time-synchronized events or signals in-band with the main media data.</source>
          <target state="new">When delivering a live streaming presentation with rich client experience, it is often necessary to transmit time-synchronized events or signals in-band with the main media data.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>One example of this is dynamic live Ads insertion.</source>
          <target state="new">One example of this is dynamic live Ads insertion.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>This type of event signaling is different from regular audio/video streaming because of its sparse nature.</source>
          <target state="new">This type of event signaling is different from regular audio/video streaming because of its sparse nature.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>In another words, the signaling data usually does not happen continuously and the interval can be hard to predict.</source>
          <target state="new">In another words, the signaling data usually does not happen continuously and the interval can be hard to predict.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>The concept of Sparse Track was specifically designed to ingest and broadcast in-band signaling data.</source>
          <target state="new">The concept of Sparse Track was specifically designed to ingest and broadcast in-band signaling data.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>Below is a recommended implementation for ingesting sparse track:</source>
          <target state="new">Below is a recommended implementation for ingesting sparse track:</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Create a separate Fragmented MP4 bit-stream which just contains sparse track(s) without audio/video tracks.</source>
          <target state="new">Create a separate Fragmented MP4 bit-stream which just contains sparse track(s) without audio/video tracks.</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>In the “Live Server Manifest Box” as defined in Section 6 in [1], use “parentTrackName” parameter to specify the name of the parent track.</source>
          <target state="new">In the “Live Server Manifest Box” as defined in Section 6 in [1], use “parentTrackName” parameter to specify the name of the parent track.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>Please refer to section 4.2.1.2.1.2 in [1] for more details.</source>
          <target state="new">Please refer to section 4.2.1.2.1.2 in [1] for more details.</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>In the “Live Server Manifest Box”, manifestOutput MUST be set to “true”.</source>
          <target state="new">In the “Live Server Manifest Box”, manifestOutput MUST be set to “true”.</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>Given the sparse nature of the signaling event, it is recommended that:</source>
          <target state="new">Given the sparse nature of the signaling event, it is recommended that:</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>At the beginning of the live event, encoder sends the initial header boxes to the service which would allow the service to register the sparse track in the client manifest.</source>
          <target state="new">At the beginning of the live event, encoder sends the initial header boxes to the service which would allow the service to register the sparse track in the client manifest.</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>The encoder SHOULD terminate the HTTP POST request when data is not being sent.</source>
          <target state="new">The encoder SHOULD terminate the HTTP POST request when data is not being sent.</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>A long running HTTP POST that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update or server reboot, as the media server will be temporarily blocked in a receive operation on the socket.</source>
          <target state="new">A long running HTTP POST that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update or server reboot, as the media server will be temporarily blocked in a receive operation on the socket.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>During the time when signaling data is not available, the encoder SHOULD close the HTTP POST request.</source>
          <target state="new">During the time when signaling data is not available, the encoder SHOULD close the HTTP POST request.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>While the POST request is active, the encoder SHOULD send data</source>
          <target state="new">While the POST request is active, the encoder SHOULD send data</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>When sending sparse fragments, encoder can set explicit Content-Length header if it’s available.</source>
          <target state="new">When sending sparse fragments, encoder can set explicit Content-Length header if it’s available.</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source>When sending sparse fragment with a new connection, encoder SHOULD start sending from the header boxes followed by the new fragments.</source>
          <target state="new">When sending sparse fragment with a new connection, encoder SHOULD start sending from the header boxes followed by the new fragments.</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>This is to handle the case where failover happened in between and the new sparse connection is being established to a new server which has not seen the sparse track before.</source>
          <target state="new">This is to handle the case where failover happened in between and the new sparse connection is being established to a new server which has not seen the sparse track before.</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>The sparse track fragment will be made available to the client when the corresponding parent track fragment that has equal or bigger timestamp value is made available to the client.</source>
          <target state="new">The sparse track fragment will be made available to the client when the corresponding parent track fragment that has equal or bigger timestamp value is made available to the client.</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>For example, if the sparse fragment has a timestamp of t=1000, it is expected after the client sees video (assuming the parent track name is video) fragment timestamp 1000 or beyond, it can download the sparse fragment t=1000.</source>
          <target state="new">For example, if the sparse fragment has a timestamp of t=1000, it is expected after the client sees video (assuming the parent track name is video) fragment timestamp 1000 or beyond, it can download the sparse fragment t=1000.</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>Please note that the actual signal could very well be used for a different position in the presentation timeline for its designated purpose.</source>
          <target state="new">Please note that the actual signal could very well be used for a different position in the presentation timeline for its designated purpose.</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>In the example above, it’s possible that the sparse fragment of t=1000 has a XML payload which is for inserting an Ad in a position that’s a few seconds later.</source>
          <target state="new">In the example above, it’s possible that the sparse fragment of t=1000 has a XML payload which is for inserting an Ad in a position that’s a few seconds later.</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>The payload of sparse track fragment can be in various different formats (e.g. XML or text or binary, etc.) depending on different scenarios.</source>
          <target state="new">The payload of sparse track fragment can be in various different formats (e.g. XML or text or binary, etc.) depending on different scenarios.</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source>Redundant Audio Track</source>
          <target state="new">Redundant Audio Track</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>In a typical HTTP Adaptive Streaming scenario (e.g. Smooth Streaming or DASH), there is often only one audio track in the entire presentation.</source>
          <target state="new">In a typical HTTP Adaptive Streaming scenario (e.g. Smooth Streaming or DASH), there is often only one audio track in the entire presentation.</target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>Unlike video tracks which have multiple quality levels for the client to choose from in error conditions, the audio track can be a single point of failure if the ingestion of the stream that contains the audio track is broken.</source>
          <target state="new">Unlike video tracks which have multiple quality levels for the client to choose from in error conditions, the audio track can be a single point of failure if the ingestion of the stream that contains the audio track is broken.</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source>To solve this problem, Microsoft Azure Media Services supports live ingestion of redundant audio tracks.</source>
          <target state="new">To solve this problem, Microsoft Azure Media Services supports live ingestion of redundant audio tracks.</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source>The idea is that the same audio track can be sent multiple times in different streams.</source>
          <target state="new">The idea is that the same audio track can be sent multiple times in different streams.</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>While the service will only register the audio track once in the client manifest, it is able to use redundant audio tracks as backups for retrieving audio fragments if the primary audio track is having issues.</source>
          <target state="new">While the service will only register the audio track once in the client manifest, it is able to use redundant audio tracks as backups for retrieving audio fragments if the primary audio track is having issues.</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source>In order to ingest redundant audio tracks, the encoder needs to:</source>
          <target state="new">In order to ingest redundant audio tracks, the encoder needs to:</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>Create the same audio track in multiple Fragment MP4 bit-streams.</source>
          <target state="new">Create the same audio track in multiple Fragment MP4 bit-streams.</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source>The redundant audio tracks MUST be semantically equivalent with exactly the same fragment timestamps and interchangeable at header and fragment level.</source>
          <target state="new">The redundant audio tracks MUST be semantically equivalent with exactly the same fragment timestamps and interchangeable at header and fragment level.</target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>Ensure that the “audio” entry in the Live Server Manifest (Section 6 in [1]) be the same for all redundant audio tracks.</source>
          <target state="new">Ensure that the “audio” entry in the Live Server Manifest (Section 6 in [1]) be the same for all redundant audio tracks.</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source>Below is a recommended implementation for redundant audio tracks:</source>
          <target state="new">Below is a recommended implementation for redundant audio tracks:</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source>Send each unique audio track in a stream by itself.</source>
          <target state="new">Send each unique audio track in a stream by itself.</target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source>Also send a redundant stream for each of these audio track streams, where the 2nd stream differs from the 1st only by the identifier in the HTTP POST URL:  {protocol}://{server address}/{publishing point path}/Streams({identifier}).</source>
          <target state="new">Also send a redundant stream for each of these audio track streams, where the 2nd stream differs from the 1st only by the identifier in the HTTP POST URL:  {protocol}://{server address}/{publishing point path}/Streams({identifier}).</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source>Use separate streams to send the two lowest video bitrates.</source>
          <target state="new">Use separate streams to send the two lowest video bitrates.</target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source>Each of these streams SHOULD also contain a copy of each unique audio track.</source>
          <target state="new">Each of these streams SHOULD also contain a copy of each unique audio track.</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source>For example, when multiple languages are supported, these streams SHOULD contain audio tracks for each language.</source>
          <target state="new">For example, when multiple languages are supported, these streams SHOULD contain audio tracks for each language.</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>Use separate server (encoder) instances to encode and send the redundant streams mentioned in (1) and (2).</source>
          <target state="new">Use separate server (encoder) instances to encode and send the redundant streams mentioned in (1) and (2).</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">7b8c28f3cb0f924d46d0eea651a415457ea180c8</xliffext:olfilehash>
  </header>
</xliff>