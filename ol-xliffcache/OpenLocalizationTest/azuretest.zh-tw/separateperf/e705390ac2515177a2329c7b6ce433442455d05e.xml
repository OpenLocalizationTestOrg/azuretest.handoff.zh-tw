{
  "nodes": [
    {
      "content": "Tutorial: Create a pipeline with Copy Activity using Data Factory Editor",
      "pos": [
        28,
        100
      ]
    },
    {
      "content": "In this tutorial, you will create an Azure Data Factory pipeline with a Copy Activity by using the Data Factory Editor in the Azure Portal.",
      "pos": [
        120,
        259
      ]
    },
    {
      "content": "Tutorial: Create a pipeline with Copy Activity using Data Factory Editor",
      "pos": [
        587,
        659
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        662,
        678
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        682,
        699
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        733,
        758
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        805,
        821
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        875,
        894
      ]
    },
    {
      "content": "In This Tutorial",
      "pos": [
        939,
        955
      ]
    },
    {
      "content": "This tutorial contains the following steps:",
      "pos": [
        956,
        999
      ]
    },
    {
      "content": "Step",
      "pos": [
        1001,
        1005
      ]
    },
    {
      "content": "Description",
      "pos": [
        1008,
        1019
      ]
    },
    {
      "content": "Step 1: Create an Azure Data Factory",
      "pos": [
        1040,
        1076
      ]
    },
    {
      "pos": [
        1100,
        1185
      ],
      "content": "In this step, you will create an Azure data factory named <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Step 2: Create linked services",
      "pos": [
        1189,
        1219
      ]
    },
    {
      "content": "In this step, you will create two linked services: <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        1246,
        1352
      ]
    },
    {
      "content": "The StorageLinkedService links the Azure storage and AzureSqlLinkedService links the Azure SQL database to the ADFTutorialDataFactory.",
      "pos": [
        1353,
        1487
      ]
    },
    {
      "content": "The input data for the pipeline resides in a blob container in the Azure blob storage and output data will be stored in a table in the Azure SQL database.",
      "pos": [
        1488,
        1642
      ]
    },
    {
      "content": "Therefore, you add these two data stores as linked services to the data factory.",
      "pos": [
        1643,
        1723
      ]
    },
    {
      "content": "Step 3: Create input and output tables",
      "pos": [
        1731,
        1769
      ]
    },
    {
      "content": "In the previous step, you created linked services that refer to data stores that contain input/output data.",
      "pos": [
        1804,
        1911
      ]
    },
    {
      "content": "In this step, you will define two data factory tables -- <bpt id=\"p1\">**</bpt>EmpTableFromBlob<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept> -- that represent the input/output data that is stored in the data stores.",
      "pos": [
        1912,
        2084
      ]
    },
    {
      "content": "For the EmpTableFromBlob, you will specify the blob container that contains a blob with the source data and for the EmpSQLTable, you will specify the SQL table that will store the output data.",
      "pos": [
        2085,
        2277
      ]
    },
    {
      "content": "You will also specify other properties such as structure of the data, availability of the data, etc...",
      "pos": [
        2278,
        2380
      ]
    },
    {
      "content": "Step 4: Create and run a pipeline",
      "pos": [
        2383,
        2416
      ]
    },
    {
      "content": "In this step, you will create a pipeline named <bpt id=\"p1\">**</bpt>ADFTutorialPipeline<ept id=\"p1\">**</ept> in the ADFTutorialDataFactory.",
      "pos": [
        2444,
        2545
      ]
    },
    {
      "content": "The pipeline will have a <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> that copies input data from the Azure blob to the output Azure SQL table.",
      "pos": [
        2546,
        2662
      ]
    },
    {
      "content": "Step 5: Monitor slices and pipeline",
      "pos": [
        2664,
        2699
      ]
    },
    {
      "content": "In this step, you will monitor slices of input and output tables by using Azure Preview Portal.",
      "pos": [
        2732,
        2827
      ]
    },
    {
      "pos": [
        2834,
        2902
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateDataFactory\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 1: Create an Azure Data Factory"
    },
    {
      "pos": [
        2903,
        3015
      ],
      "content": "In this step, you use the Azure Preview Portal to create an Azure data factory named <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        3021,
        3243
      ],
      "content": "After logging into the [Azure Preview Portal][azure-preview-portal], click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept> from the bottom-left corner, select <bpt id=\"p2\">**</bpt>Data analytics<ept id=\"p2\">**</ept> in the <bpt id=\"p3\">**</bpt>Create<ept id=\"p3\">**</ept> blade, and click <bpt id=\"p4\">**</bpt>Data Factory<ept id=\"p4\">**</ept> in the <bpt id=\"p5\">**</bpt>Data analytics<ept id=\"p5\">**</ept> blade."
    },
    {
      "content": "New-&gt;DataFactory",
      "pos": [
        3252,
        3268
      ]
    },
    {
      "pos": [
        3319,
        3353
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade:"
    },
    {
      "pos": [
        3361,
        3411
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>name<ept id=\"p2\">**</ept>."
    },
    {
      "content": "New data factory blade",
      "pos": [
        3428,
        3450
      ]
    },
    {
      "pos": [
        3513,
        3564
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>RESOURCE GROUP NAME<ept id=\"p1\">**</ept> and do the following:"
    },
    {
      "pos": [
        3576,
        3614
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Create a new resource group<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        3626,
        3758
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Create resource group<ept id=\"p1\">**</ept> blade, enter <bpt id=\"p2\">**</bpt>ADFTutorialResourceGroup<ept id=\"p2\">**</ept> for the <bpt id=\"p3\">**</bpt>name<ept id=\"p3\">**</ept> of the resource group, and click <bpt id=\"p4\">**</bpt>OK<ept id=\"p4\">**</ept>."
    },
    {
      "content": "Create Resource Group",
      "pos": [
        3775,
        3796
      ]
    },
    {
      "content": "Some of the steps in this tutorial assume that you use the name: <bpt id=\"p1\">**</bpt>ADFTutorialResourceGroup<ept id=\"p1\">**</ept> for the resource group.",
      "pos": [
        3849,
        3966
      ]
    },
    {
      "content": "To learn about resource groups, see <bpt id=\"p1\">[</bpt>Using resource groups to manage your Azure resources<ept id=\"p1\">](resource-group-overview.md)</ept>.",
      "pos": [
        3967,
        4086
      ]
    },
    {
      "pos": [
        4092,
        4173
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade, notice that <bpt id=\"p2\">**</bpt>Add to Startboard<ept id=\"p2\">**</ept> is selected."
    },
    {
      "pos": [
        4177,
        4228
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Create<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>New data factory<ept id=\"p2\">**</ept> blade."
    },
    {
      "content": "The name of the Azure data factory must be globally unique.",
      "pos": [
        4234,
        4293
      ]
    },
    {
      "content": "If you receive the error: <bpt id=\"p1\">**</bpt>Data factory name “ADFTutorialDataFactory” is not available<ept id=\"p1\">**</ept>, change the name of the data factory (for example, yournameADFTutorialDataFactory) and try creating again.",
      "pos": [
        4294,
        4490
      ]
    },
    {
      "content": "Use this name in place of ADFTutorialFactory while performing remaining steps in this tutorial.",
      "pos": [
        4491,
        4586
      ]
    },
    {
      "content": "See [Data Factory - Naming Rules][data-factory-naming-rules] topic for naming rules for Data Factory artifacts.",
      "pos": [
        4587,
        4698
      ]
    },
    {
      "content": "Data Factory name not available",
      "pos": [
        4713,
        4744
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>NOTIFICATIONS<ept id=\"p1\">**</ept> hub on the left and look for notifications from the creation process.",
      "pos": [
        4789,
        4882
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> to close the <bpt id=\"p2\">**</bpt>NOTIFICATIONS<ept id=\"p2\">**</ept> blade if it is open.",
      "pos": [
        4883,
        4946
      ]
    },
    {
      "pos": [
        4952,
        5039
      ],
      "content": "After the creation is complete, you will see the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade as shown below."
    },
    {
      "content": "Data factory home page",
      "pos": [
        5047,
        5069
      ]
    },
    {
      "pos": [
        5124,
        5189
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateLinkedServices\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 2: Create linked services"
    },
    {
      "content": "Linked services link data stores or compute services to an Azure data factory.",
      "pos": [
        5190,
        5268
      ]
    },
    {
      "content": "A data store can be an Azure Storage, Azure SQL Database or an on-premises SQL Server database.",
      "pos": [
        5269,
        5364
      ]
    },
    {
      "content": "In this step, you will create two linked services: <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        5366,
        5472
      ]
    },
    {
      "content": "StorageLinkedService linked service links an Azure Storage Account and AzureSqlLinkedService links an Azure SQL database to the <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept>.",
      "pos": [
        5473,
        5628
      ]
    },
    {
      "content": "You will create a pipeline later in this tutorial that copies data from a blob container in StorageLinkedService to a SQL table in AzureSqlLinkedService.",
      "pos": [
        5629,
        5782
      ]
    },
    {
      "content": "Create a linked service for the Azure storage account",
      "pos": [
        5788,
        5841
      ]
    },
    {
      "pos": [
        5846,
        5956
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Author and deploy<ept id=\"p2\">**</ept> tile to launch the <bpt id=\"p3\">**</bpt>Editor<ept id=\"p3\">**</ept> for the data factory."
    },
    {
      "content": "Author and Deploy Tile",
      "pos": [
        5964,
        5986
      ]
    },
    {
      "content": "See [Data Factory Editor][data-factory-editor] topic for detailed overview of the Data Factory editor.",
      "pos": [
        6020,
        6122
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New data store<ept id=\"p2\">**</ept> button on the toolbar and select <bpt id=\"p3\">**</bpt>Azure storage<ept id=\"p3\">**</ept> from the drop down menu.",
      "pos": [
        6133,
        6252
      ]
    },
    {
      "content": "You should see the JSON template for creating an Azure storage linked service in the right pane.",
      "pos": [
        6253,
        6349
      ]
    },
    {
      "content": "Editor New data store button",
      "pos": [
        6358,
        6386
      ]
    },
    {
      "pos": [
        6430,
        6549
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>accountname<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>accountkey<ept id=\"p2\">**</ept> with the account name and account key values for your Azure storage account."
    },
    {
      "content": "Editor Blob Storage JSON",
      "pos": [
        6558,
        6582
      ]
    },
    {
      "pos": [
        6629,
        6741
      ],
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=516971)</ept> for details about JSON properties."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to deploy the StorageLinkedService.",
      "pos": [
        6746,
        6813
      ]
    },
    {
      "content": "Confirm that you see the message <bpt id=\"p1\">**</bpt>LINKED SERVICE CREATED SUCCESSFULLY<ept id=\"p1\">**</ept> on the title bar.",
      "pos": [
        6814,
        6904
      ]
    },
    {
      "content": "Editor Blob Storage Deploy",
      "pos": [
        6912,
        6938
      ]
    },
    {
      "content": "Create a linked service for the Azure SQL Database",
      "pos": [
        6979,
        7029
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept> , click <bpt id=\"p2\">**</bpt>New data store<ept id=\"p2\">**</ept> button on the toolbar and select <bpt id=\"p3\">**</bpt>Azure SQL database<ept id=\"p3\">**</ept> from the drop down menu.",
      "pos": [
        7033,
        7171
      ]
    },
    {
      "content": "You should see the JSON template for creating the Azure SQL linked service in the right pane.",
      "pos": [
        7172,
        7265
      ]
    },
    {
      "content": "Editr Azure SQL Settings",
      "pos": [
        7273,
        7297
      ]
    },
    {
      "pos": [
        7336,
        7495
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>servername<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>databasename<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>username@servername<ept id=\"p3\">**</ept>, and <bpt id=\"p4\">**</bpt>password<ept id=\"p4\">**</ept> with names of your Azure SQL server, database, user account, and  password."
    },
    {
      "pos": [
        7500,
        7579
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to create and deploy the AzureSqlLinkedService."
    },
    {
      "pos": [
        7589,
        7670
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateInputAndOutputDataSets\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 3: Create input and output tables"
    },
    {
      "content": "In the previous step, you created linked services <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept> to link an Azure Storage account and Azure SQL database to the data factory: <bpt id=\"p3\">**</bpt>ADFTutorialDataFactory<ept id=\"p3\">**</ept>.",
      "pos": [
        7671,
        7880
      ]
    },
    {
      "content": "In this step, you will define two data factory tables -- <bpt id=\"p1\">**</bpt>EmpTableFromBlob<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept> -- that represent the input/output data that is stored in the data stores referred by StorageLinkedService and AzureSqlLinkedService respectively.",
      "pos": [
        7881,
        8125
      ]
    },
    {
      "content": "For  EmpTableFromBlob, you will specify the blob container that contains a blob with the source data and for EmpSQLTable, you will specify the SQL table that will store the output data.",
      "pos": [
        8126,
        8311
      ]
    },
    {
      "content": "Create input table",
      "pos": [
        8318,
        8336
      ]
    },
    {
      "content": "A table is a rectangular dataset and has a schema.",
      "pos": [
        8338,
        8388
      ]
    },
    {
      "content": "In this step, you will create a table named <bpt id=\"p1\">**</bpt>EmpBlobTable<ept id=\"p1\">**</ept> that points to a blob container in the Azure Storage represented by the <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> linked service.",
      "pos": [
        8389,
        8562
      ]
    },
    {
      "pos": [
        8567,
        8700
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept> for the Data Factory, click <bpt id=\"p2\">**</bpt>New dataset<ept id=\"p2\">**</ept> button on the toolbar and click <bpt id=\"p3\">**</bpt>Blob table<ept id=\"p3\">**</ept> from the drop down menu."
    },
    {
      "content": "Replace JSON in the right pane with the following JSON snippet:",
      "pos": [
        8705,
        8768
      ]
    },
    {
      "content": "Note the following:",
      "pos": [
        9602,
        9621
      ]
    },
    {
      "pos": [
        9634,
        9675
      ],
      "content": "dataset <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureBlob<ept id=\"p2\">**</ept>."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        9682,
        9739
      ]
    },
    {
      "content": "You had created this linked service in Step 2.",
      "pos": [
        9740,
        9786
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> is set to the <bpt id=\"p2\">**</bpt>adftutorial<ept id=\"p2\">**</ept> container.",
      "pos": [
        9793,
        9848
      ]
    },
    {
      "content": "You can also specify the name of a blob within the folder.",
      "pos": [
        9849,
        9907
      ]
    },
    {
      "content": "Since you are not specifying the name of the blob, data from all blobs in the container is considered as an input data.",
      "pos": [
        9908,
        10027
      ]
    },
    {
      "pos": [
        10036,
        10076
      ],
      "content": "format <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>TextFormat<ept id=\"p2\">**</ept>"
    },
    {
      "pos": [
        10083,
        10208
      ],
      "content": "There are two fields in the text file – <bpt id=\"p1\">**</bpt>FirstName<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>LastName<ept id=\"p2\">**</ept> – separated by a comma character (<bpt id=\"p3\">**</bpt>columnDelimiter<ept id=\"p3\">**</ept>)"
    },
    {
      "pos": [
        10216,
        10465
      ],
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>hourly<ept id=\"p2\">**</ept> (<bpt id=\"p3\">**</bpt>frequency<ept id=\"p3\">**</ept> is set to <bpt id=\"p4\">**</bpt>hour<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>interval<ept id=\"p5\">**</ept> is set to <bpt id=\"p6\">**</bpt>1<ept id=\"p6\">**</ept> ), so the Data Factory service will look for input data every hour in the root folder in the blob container (<bpt id=\"p7\">**</bpt>adftutorial<ept id=\"p7\">**</ept>) you specified."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to create and deploy the <bpt id=\"p2\">**</bpt>EmpTableFromBlob<ept id=\"p2\">**</ept> table.",
      "pos": [
        12182,
        12266
      ]
    },
    {
      "content": "Confirm that you see the <bpt id=\"p1\">**</bpt>TABLE CREATED SUCCESSFULLY<ept id=\"p1\">**</ept> message on the title bar of the Editor.",
      "pos": [
        12267,
        12362
      ]
    },
    {
      "content": "Create output table",
      "pos": [
        12368,
        12387
      ]
    },
    {
      "pos": [
        12388,
        12589
      ],
      "content": "In this part of the step, you will create an output table named <bpt id=\"p1\">**</bpt>EmpSQLTable<ept id=\"p1\">**</ept> that points to a SQL table in the Azure SQL database that is represented by the <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept> linked service."
    },
    {
      "pos": [
        12595,
        12733
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept> for the Data Factory, click <bpt id=\"p2\">**</bpt>New dataset<ept id=\"p2\">**</ept> button on the toolbar and click <bpt id=\"p3\">**</bpt>Azure SQL table<ept id=\"p3\">**</ept> from the drop down menu."
    },
    {
      "content": "Replace JSON in the right pane with the following JSON snippet:",
      "pos": [
        12738,
        12801
      ]
    },
    {
      "content": "Note the following:",
      "pos": [
        13437,
        13456
      ]
    },
    {
      "pos": [
        13469,
        13514
      ],
      "content": "dataset <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureSQLTable<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13521,
        13627
      ],
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureSqlLinkedService<ept id=\"p2\">**</ept> (you had created this linked service in Step 2)."
    },
    {
      "pos": [
        13634,
        13666
      ],
      "content": "<bpt id=\"p1\">**</bpt>tablename<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13673,
        13869
      ],
      "content": "There are three columns – <bpt id=\"p1\">**</bpt>ID<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>FirstName<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>LastName<ept id=\"p3\">**</ept> – in the emp table in the database, but ID is an identity column, so you need to specify only <bpt id=\"p4\">**</bpt>FirstName<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>LastName<ept id=\"p5\">**</ept> here."
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>hourly<ept id=\"p2\">**</ept> (<bpt id=\"p3\">**</bpt>frequency<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>hour<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>interval<ept id=\"p5\">**</ept> set to <bpt id=\"p6\">**</bpt>1<ept id=\"p6\">**</ept>).",
      "pos": [
        13876,
        13980
      ]
    },
    {
      "content": "The Data Factory service will generate an output data slice every hour in the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table in the Azure SQL database.",
      "pos": [
        13982,
        14100
      ]
    },
    {
      "pos": [
        14106,
        14185
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to create and deploy the <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept> table."
    },
    {
      "pos": [
        14191,
        14260
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateAndRunAPipeline\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 4: Create and run a pipeline"
    },
    {
      "pos": [
        14261,
        14392
      ],
      "content": "In this step, you create a pipeline with a <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> that uses <bpt id=\"p2\">**</bpt>EmpTableFromBlob<ept id=\"p2\">**</ept> as input and <bpt id=\"p3\">**</bpt>EmpSQLTable<ept id=\"p3\">**</ept> as output."
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept> for the Data Factory, click <bpt id=\"p2\">**</bpt>New pipeline<ept id=\"p2\">**</ept> button on the toolbar.",
      "pos": [
        14397,
        14482
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>... (Ellipsis)<ept id=\"p1\">**</ept> on the toolbar if you do not see the button.",
      "pos": [
        14483,
        14552
      ]
    },
    {
      "content": "Alternatively, you can right-click <bpt id=\"p1\">**</bpt>Pipelines<ept id=\"p1\">**</ept> in the tree view and click <bpt id=\"p2\">**</bpt>New pipeline<ept id=\"p2\">**</ept>.",
      "pos": [
        14553,
        14646
      ]
    },
    {
      "content": "Editor New Pipeline Button",
      "pos": [
        14654,
        14680
      ]
    },
    {
      "content": "Replace JSON in the right pane with the following JSON snippet:",
      "pos": [
        14720,
        14783
      ]
    },
    {
      "content": "Note the following:",
      "pos": [
        16113,
        16132
      ]
    },
    {
      "pos": [
        16140,
        16236
      ],
      "content": "In the activities section, there is only one activity whose <bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>CopyActivity<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        16243,
        16351
      ],
      "content": "Input for the activity is set to <bpt id=\"p1\">**</bpt>EmpTableFromBlob<ept id=\"p1\">**</ept> and output for the activity is set to <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        16358,
        16486
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>transformation<ept id=\"p1\">**</ept> section, <bpt id=\"p2\">**</bpt>BlobSource<ept id=\"p2\">**</ept> is specified as the source type and <bpt id=\"p3\">**</bpt>SqlSink<ept id=\"p3\">**</ept> is specified as the sink type."
    },
    {
      "content": "Replace the value of the <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> property with the current day and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> value with the next day.",
      "pos": [
        16492,
        16593
      ]
    },
    {
      "content": "You can specify only the date part and skip the time part of the date time.",
      "pos": [
        16594,
        16669
      ]
    },
    {
      "content": "For example, \"2015-02-03\", which is equivalent to \"2015-02-03T00:00:00Z\"",
      "pos": [
        16670,
        16742
      ]
    },
    {
      "content": "Both start and end datetimes must be in <bpt id=\"p1\">[</bpt>ISO format<ept id=\"p1\">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.",
      "pos": [
        16752,
        16844
      ]
    },
    {
      "content": "For example: 2014-10-14T16:32:41Z.",
      "pos": [
        16845,
        16879
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> time is optional, but we will use it in this tutorial.",
      "pos": [
        16880,
        16946
      ]
    },
    {
      "content": "If you do not specify value for the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> property, it is calculated as \"<bpt id=\"p2\">**</bpt>start + 48 hours<ept id=\"p2\">**</ept>\".",
      "pos": [
        16957,
        17054
      ]
    },
    {
      "content": "To run the pipeline indefinitely, specify <bpt id=\"p1\">**</bpt>9999-09-09<ept id=\"p1\">**</ept> as the value for the <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> property.",
      "pos": [
        17055,
        17150
      ]
    },
    {
      "content": "In the example above, there will be 24 data slices as each data slice is produced hourly.",
      "pos": [
        17160,
        17249
      ]
    },
    {
      "pos": [
        17259,
        17371
      ],
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=516971)</ept> for details about JSON properties."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to create and deploy the <bpt id=\"p2\">**</bpt>ADFTutorialPipeline<ept id=\"p2\">**</ept>.",
      "pos": [
        17376,
        17457
      ]
    },
    {
      "content": "Confirm that you see the <bpt id=\"p1\">**</bpt>PIPELINE CREATED SUCCESSFULLY<ept id=\"p1\">**</ept> message.",
      "pos": [
        17458,
        17525
      ]
    },
    {
      "content": "Now, close the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept> blade by clicking <bpt id=\"p2\">**</bpt>X<ept id=\"p2\">**</ept>.",
      "pos": [
        17529,
        17579
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> again to close the ADFTutorialDataFactory blade with the toolbar and tree view.",
      "pos": [
        17580,
        17671
      ]
    },
    {
      "content": "If you see <bpt id=\"p1\">**</bpt>your unsaved edits will be discarded<ept id=\"p1\">**</ept> message, click <bpt id=\"p2\">**</bpt>OK<ept id=\"p2\">**</ept>.",
      "pos": [
        17672,
        17746
      ]
    },
    {
      "pos": [
        17750,
        17834
      ],
      "content": "You should be back to the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade for the <bpt id=\"p2\">**</bpt>ADFTutorialDataFactory<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        17836,
        17977
      ],
      "content": "<bpt id=\"p1\">**</bpt>Congratulations!<ept id=\"p1\">**</ept> You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline."
    },
    {
      "content": "View the data factory in a Diagram View",
      "pos": [
        17987,
        18026
      ]
    },
    {
      "pos": [
        18031,
        18080
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Diagram<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Data Factory Blade - Diagram Tile",
      "pos": [
        18088,
        18121
      ]
    },
    {
      "content": "You should see the diagram similar to the following:",
      "pos": [
        18163,
        18215
      ]
    },
    {
      "content": "Diagram view",
      "pos": [
        18224,
        18236
      ]
    },
    {
      "content": "You can zoom in, zoom out, zoom to 100%, zoom to fit, automatically position pipelines and tables, and show lineage information (highlights upstream and downstream items of selected items).",
      "pos": [
        18289,
        18478
      ]
    },
    {
      "content": "You can double-blick on an object (input/output table or pipeline) to see properties for it.",
      "pos": [
        18480,
        18572
      ]
    },
    {
      "content": "Right-click on <bpt id=\"p1\">**</bpt>ADFTutorialPipeline<ept id=\"p1\">**</ept> in the Diagram View and click <bpt id=\"p2\">**</bpt>Open pipeline<ept id=\"p2\">**</ept>.",
      "pos": [
        18577,
        18664
      ]
    },
    {
      "content": "You should see the activities in the pipeline along with input and output datasets for the activities.",
      "pos": [
        18665,
        18767
      ]
    },
    {
      "content": "In this tutorial, you have only one activity in the pipeline (Copy Activity) with EmpTableBlob as input dataset and EmpSQLTable as output dataset.",
      "pos": [
        18768,
        18914
      ]
    },
    {
      "content": "Open Pipeline",
      "pos": [
        18925,
        18938
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Data factory<ept id=\"p1\">**</ept> in the breadcrumb in the top-left corner to get back to the diagram view.",
      "pos": [
        19020,
        19116
      ]
    },
    {
      "content": "The diagram view displays all the pipelines.",
      "pos": [
        19117,
        19161
      ]
    },
    {
      "content": "In this example, you have only created one pipeline.",
      "pos": [
        19162,
        19214
      ]
    },
    {
      "pos": [
        19224,
        19306
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"MonitorDataSetsAndPipeline\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 5: Monitor the datasets and pipeline"
    },
    {
      "content": "In this step, you will use the Azure Portal to monitor what’s going on in an Azure data factory.",
      "pos": [
        19307,
        19403
      ]
    },
    {
      "content": "You can also use PowerShell cmdlets to monitor datasets and pipelines.",
      "pos": [
        19404,
        19474
      ]
    },
    {
      "content": "For details about using cmdlets for monitoring, see [Monitor and Manage Data Factory using PowerShell Cmdlets][monitor-manage-using-powershell].",
      "pos": [
        19475,
        19619
      ]
    },
    {
      "content": "Navigate to [Azure Portal (Preview)][azure-preview-portal] if you don't have it open.",
      "pos": [
        19624,
        19709
      ]
    },
    {
      "pos": [
        19714,
        19840
      ],
      "content": "If the blade for <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept> is not open, open it by clicking <bpt id=\"p2\">**</bpt>ADFTutorialDataFactory<ept id=\"p2\">**</ept> on the <bpt id=\"p3\">**</bpt>Startboard<ept id=\"p3\">**</ept>."
    },
    {
      "content": "You should see the count and names of tables and pipeline you created on this blade.",
      "pos": [
        19845,
        19929
      ]
    },
    {
      "content": "home page with names",
      "pos": [
        19937,
        19957
      ]
    },
    {
      "pos": [
        20021,
        20050
      ],
      "content": "Now, click <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> tile."
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>EmpTableFromBlob<ept id=\"p2\">**</ept>.",
      "pos": [
        20054,
        20108
      ]
    },
    {
      "content": "This is the input table for the <bpt id=\"p1\">**</bpt>ADFTutorialPipeline<ept id=\"p1\">**</ept>.",
      "pos": [
        20109,
        20165
      ]
    },
    {
      "content": "Datasets with EmpTableFromBlob selected",
      "pos": [
        20173,
        20212
      ]
    },
    {
      "content": "Notice that the data slices up to the current time have already been produced and they are <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> because the <bpt id=\"p2\">**</bpt>emp.txt<ept id=\"p2\">**</ept> file exists all the time in the blob container: <bpt id=\"p3\">**</bpt>adftutorial\\input<ept id=\"p3\">**</ept>.",
      "pos": [
        20279,
        20474
      ]
    },
    {
      "content": "Confirm that no slices show up in the <bpt id=\"p1\">**</bpt>Recently failed slices<ept id=\"p1\">**</ept> section at the bottom.",
      "pos": [
        20475,
        20562
      ]
    },
    {
      "content": "Both <bpt id=\"p1\">**</bpt>Recently updated slices<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Recently failed slices<ept id=\"p2\">**</ept> lists are sorted by the <bpt id=\"p3\">**</bpt>LAST UPDATE TIME<ept id=\"p3\">**</ept>.",
      "pos": [
        20568,
        20677
      ]
    },
    {
      "content": "The update time of a slice is changed in the following situations.",
      "pos": [
        20678,
        20744
      ]
    },
    {
      "content": "Now, in the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> blade, click the <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>.",
      "pos": [
        21398,
        21456
      ]
    },
    {
      "content": "This is the output table for the <bpt id=\"p1\">**</bpt>ADFTutorialPipeline<ept id=\"p1\">**</ept>.",
      "pos": [
        21457,
        21514
      ]
    },
    {
      "content": "data sets blade",
      "pos": [
        21522,
        21537
      ]
    },
    {
      "pos": [
        21598,
        21654
      ],
      "content": "You should see the <bpt id=\"p1\">**</bpt>EmpSQLTable<ept id=\"p1\">**</ept> blade as shown below:"
    },
    {
      "content": "table blade",
      "pos": [
        21662,
        21673
      ]
    },
    {
      "content": "Notice that the data slices up to the current time have already been produced and they are <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept>.",
      "pos": [
        21724,
        21825
      ]
    },
    {
      "content": "No slices show up in the <bpt id=\"p1\">**</bpt>Problem slices<ept id=\"p1\">**</ept> section at the bottom.",
      "pos": [
        21826,
        21892
      ]
    },
    {
      "pos": [
        21896,
        21941
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>… (Ellipsis)<ept id=\"p1\">**</ept> to see all the slices."
    },
    {
      "content": "data slices blade",
      "pos": [
        21949,
        21966
      ]
    },
    {
      "pos": [
        22021,
        22103
      ],
      "content": "Click on any data slice from the list and you should see the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade."
    },
    {
      "content": "data slice blade",
      "pos": [
        22111,
        22127
      ]
    },
    {
      "pos": [
        22184,
        22380
      ],
      "content": "If the slice is not in the <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state, you can see the upstream slices that are not Ready and are blocking the current slice from executing in the <bpt id=\"p2\">**</bpt>Upstream slices that are not ready<ept id=\"p2\">**</ept> list."
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade, you should see all activity runs in the list at the bottom.",
      "pos": [
        22387,
        22475
      ]
    },
    {
      "content": "Click on an <bpt id=\"p1\">**</bpt>activity run<ept id=\"p1\">**</ept> to see the <bpt id=\"p2\">**</bpt>ACTIVITY RUN DETAILS<ept id=\"p2\">**</ept> blade.",
      "pos": [
        22476,
        22547
      ]
    },
    {
      "content": "Activity Run Details",
      "pos": [
        22556,
        22576
      ]
    },
    {
      "pos": [
        22641,
        22749
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> to close all the blades until you get back to the home blade for the <bpt id=\"p2\">**</bpt>ADFTutorialDataFactory<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        22754,
        22970
      ],
      "content": "(optional) Click <bpt id=\"p1\">**</bpt>Pipelines<ept id=\"p1\">**</ept> on the home page for <bpt id=\"p2\">**</bpt>ADFTutorialDataFactory<ept id=\"p2\">**</ept>, click <bpt id=\"p3\">**</bpt>ADFTutorialPipeline<ept id=\"p3\">**</ept> in the <bpt id=\"p4\">**</bpt>Pipelines<ept id=\"p4\">**</ept> blade, and drill through input tables (<bpt id=\"p5\">**</bpt>Consumed<ept id=\"p5\">**</ept>) or output tables (<bpt id=\"p6\">**</bpt>Produced<ept id=\"p6\">**</ept>)."
    },
    {
      "pos": [
        22975,
        23128
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>SQL Server Management Studio<ept id=\"p1\">**</ept>, connect to the Azure SQL Database, and verify that the rows are inserted into the <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept> table in the database."
    },
    {
      "content": "sql query results",
      "pos": [
        23136,
        23153
      ]
    },
    {
      "content": "Summary",
      "pos": [
        23210,
        23217
      ]
    },
    {
      "content": "In this tutorial, you created an Azure data factory to copy data from an Azure blob to an Azure SQL database.",
      "pos": [
        23219,
        23328
      ]
    },
    {
      "content": "You used the Azure Preview Portal to create the data factory, linked services, tables, and a pipeline.",
      "pos": [
        23329,
        23431
      ]
    },
    {
      "content": "Here are the high level steps you performed in this tutorial:",
      "pos": [
        23432,
        23493
      ]
    },
    {
      "pos": [
        23501,
        23534
      ],
      "content": "Create an Azure <bpt id=\"p1\">**</bpt>data factory<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        23539,
        23655
      ],
      "content": "Create <bpt id=\"p1\">**</bpt>linked services<ept id=\"p1\">**</ept> that link data stores and computes (referred as <bpt id=\"p2\">**</bpt>Linked Services<ept id=\"p2\">**</ept>) to the data factory."
    },
    {
      "pos": [
        23660,
        23734
      ],
      "content": "Create <bpt id=\"p1\">**</bpt>tables<ept id=\"p1\">**</ept> which describe input data and output data for pipelines."
    },
    {
      "content": "Create <bpt id=\"p1\">**</bpt>pipelines<ept id=\"p1\">**</ept>.",
      "pos": [
        23739,
        23760
      ]
    },
    {
      "content": "A pipeline consists of one or more activities and processes the inputs and produces outputs.",
      "pos": [
        23761,
        23853
      ]
    },
    {
      "content": "Set the active period for the pipeline by specifying <bpt id=\"p1\">**</bpt>Start<ept id=\"p1\">**</ept> time and <bpt id=\"p2\">**</bpt>End<ept id=\"p2\">**</ept> time for the pipeline.",
      "pos": [
        23854,
        23956
      ]
    },
    {
      "content": "The active period defines the time duration in which data slices will be produced.",
      "pos": [
        23957,
        24039
      ]
    },
    {
      "content": "For a list of supported activities, see [Pipelines and Activities][msdn-activities]",
      "pos": [
        24043,
        24126
      ]
    },
    {
      "content": "topic and for a list of supported linked services, see [Linked Services][msdn-linkedservices]",
      "pos": [
        24128,
        24221
      ]
    },
    {
      "content": "topic on MSDN Library.",
      "pos": [
        24223,
        24245
      ]
    },
    {
      "content": "To do this tutorial using Azure PowerShell, see [Create and monitor a data factory using Azure PowerShell][monitor-manage-using-powershell].",
      "pos": [
        24248,
        24388
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        24395,
        24408
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        24409,
        24466
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-get-started-using-editor.md)</ept>.",
      "pos": [
        24467,
        24615
      ]
    },
    {
      "content": "test",
      "pos": [
        31979,
        31983
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Tutorial: Create a pipeline with Copy Activity using Data Factory Editor\" \n    description=\"In this tutorial, you will create an Azure Data Factory pipeline with a Copy Activity by using the Data Factory Editor in the Azure Portal.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/25/2015\" \n    ms.author=\"spelluru\"/>\n\n# Tutorial: Create a pipeline with Copy Activity using Data Factory Editor\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-get-started.md)\n- [Using Data Factory Editor](data-factory-get-started-using-editor.md)\n- [Using PowerShell](data-factory-monitor-manage-using-powershell.md)\n- [Using Visual Studio](data-factory-get-started-using-vs.md)\n\n\n\n##In This Tutorial\nThis tutorial contains the following steps:\n\nStep | Description\n-----| -----------\n[Step 1: Create an Azure Data Factory](#CreateDataFactory) | In this step, you will create an Azure data factory named **ADFTutorialDataFactory**.  \n[Step 2: Create linked services](#CreateLinkedServices) | In this step, you will create two linked services: **StorageLinkedService** and **AzureSqlLinkedService**. The StorageLinkedService links the Azure storage and AzureSqlLinkedService links the Azure SQL database to the ADFTutorialDataFactory. The input data for the pipeline resides in a blob container in the Azure blob storage and output data will be stored in a table in the Azure SQL database. Therefore, you add these two data stores as linked services to the data factory.      \n[Step 3: Create input and output tables](#CreateInputAndOutputDataSets) | In the previous step, you created linked services that refer to data stores that contain input/output data. In this step, you will define two data factory tables -- **EmpTableFromBlob** and **EmpSQLTable** -- that represent the input/output data that is stored in the data stores. For the EmpTableFromBlob, you will specify the blob container that contains a blob with the source data and for the EmpSQLTable, you will specify the SQL table that will store the output data. You will also specify other properties such as structure of the data, availability of the data, etc... \n[Step 4: Create and run a pipeline](#CreateAndRunAPipeline) | In this step, you will create a pipeline named **ADFTutorialPipeline** in the ADFTutorialDataFactory. The pipeline will have a **Copy Activity** that copies input data from the Azure blob to the output Azure SQL table.\n[Step 5: Monitor slices and pipeline](#MonitorDataSetsAndPipeline) | In this step, you will monitor slices of input and output tables by using Azure Preview Portal.\n \n\n## <a name=\"CreateDataFactory\"></a>Step 1: Create an Azure Data Factory\nIn this step, you use the Azure Preview Portal to create an Azure data factory named **ADFTutorialDataFactory**.\n\n1.  After logging into the [Azure Preview Portal][azure-preview-portal], click **NEW** from the bottom-left corner, select **Data analytics** in the **Create** blade, and click **Data Factory** in the **Data analytics** blade. \n\n    ![New->DataFactory][image-data-factory-new-datafactory-menu]    \n\n6. In the **New data factory** blade:\n    1. Enter **ADFTutorialDataFactory** for the **name**. \n    \n        ![New data factory blade][image-data-factory-getstarted-new-data-factory-blade]\n    2. Click **RESOURCE GROUP NAME** and do the following:\n        1. Click **Create a new resource group**.\n        2. In the **Create resource group** blade, enter **ADFTutorialResourceGroup** for the **name** of the resource group, and click **OK**. \n\n            ![Create Resource Group][image-data-factory-create-resource-group]\n\n        Some of the steps in this tutorial assume that you use the name: **ADFTutorialResourceGroup** for the resource group. To learn about resource groups, see [Using resource groups to manage your Azure resources](resource-group-overview.md).  \n7. In the **New data factory** blade, notice that **Add to Startboard** is selected.\n8. Click **Create** in the **New data factory** blade.\n\n    The name of the Azure data factory must be globally unique. If you receive the error: **Data factory name “ADFTutorialDataFactory” is not available**, change the name of the data factory (for example, yournameADFTutorialDataFactory) and try creating again. Use this name in place of ADFTutorialFactory while performing remaining steps in this tutorial. See [Data Factory - Naming Rules][data-factory-naming-rules] topic for naming rules for Data Factory artifacts.  \n     \n    ![Data Factory name not available][image-data-factory-name-not-available]\n\n9. Click **NOTIFICATIONS** hub on the left and look for notifications from the creation process. Click **X** to close the **NOTIFICATIONS** blade if it is open. \n10. After the creation is complete, you will see the **DATA FACTORY** blade as shown below.\n\n    ![Data factory home page][image-data-factory-get-stated-factory-home-page]\n\n## <a name=\"CreateLinkedServices\"></a>Step 2: Create linked services\nLinked services link data stores or compute services to an Azure data factory. A data store can be an Azure Storage, Azure SQL Database or an on-premises SQL Server database.\n\nIn this step, you will create two linked services: **StorageLinkedService** and **AzureSqlLinkedService**. StorageLinkedService linked service links an Azure Storage Account and AzureSqlLinkedService links an Azure SQL database to the **ADFTutorialDataFactory**. You will create a pipeline later in this tutorial that copies data from a blob container in StorageLinkedService to a SQL table in AzureSqlLinkedService.\n\n### Create a linked service for the Azure storage account\n1.  In the **DATA FACTORY** blade, click **Author and deploy** tile to launch the **Editor** for the data factory.\n\n    ![Author and Deploy Tile][image-author-deploy-tile] \n\n    See [Data Factory Editor][data-factory-editor] topic for detailed overview of the Data Factory editor. \n     \n5. In the **Editor**, click **New data store** button on the toolbar and select **Azure storage** from the drop down menu. You should see the JSON template for creating an Azure storage linked service in the right pane. \n\n    ![Editor New data store button][image-editor-newdatastore-button]\n    \n6. Replace **accountname** and **accountkey** with the account name and account key values for your Azure storage account. \n\n    ![Editor Blob Storage JSON][image-editor-blob-storage-json]    \n    \n    See [JSON Scripting Reference](http://go.microsoft.com/fwlink/?LinkId=516971) for details about JSON properties.\n\n6. Click **Deploy** on the toolbar to deploy the StorageLinkedService. Confirm that you see the message **LINKED SERVICE CREATED SUCCESSFULLY** on the title bar.\n\n    ![Editor Blob Storage Deploy][image-editor-blob-storage-deploy]\n\n### Create a linked service for the Azure SQL Database\n1. In the **Data Factory Editor** , click **New data store** button on the toolbar and select **Azure SQL database** from the drop down menu. You should see the JSON template for creating the Azure SQL linked service in the right pane.\n\n    ![Editr Azure SQL Settings][image-editor-azure-sql-settings]\n\n2. Replace **servername**, **databasename**, **username@servername**, and **password** with names of your Azure SQL server, database, user account, and  password. \n3. Click **Deploy** on the toolbar to create and deploy the AzureSqlLinkedService. \n   \n\n## <a name=\"CreateInputAndOutputDataSets\"></a>Step 3: Create input and output tables\nIn the previous step, you created linked services **StorageLinkedService** and **AzureSqlLinkedService** to link an Azure Storage account and Azure SQL database to the data factory: **ADFTutorialDataFactory**. In this step, you will define two data factory tables -- **EmpTableFromBlob** and **EmpSQLTable** -- that represent the input/output data that is stored in the data stores referred by StorageLinkedService and AzureSqlLinkedService respectively. For  EmpTableFromBlob, you will specify the blob container that contains a blob with the source data and for EmpSQLTable, you will specify the SQL table that will store the output data. \n\n### Create input table \nA table is a rectangular dataset and has a schema. In this step, you will create a table named **EmpBlobTable** that points to a blob container in the Azure Storage represented by the **StorageLinkedService** linked service.\n\n1. In the **Editor** for the Data Factory, click **New dataset** button on the toolbar and click **Blob table** from the drop down menu. \n2. Replace JSON in the right pane with the following JSON snippet: \n\n        {\n          \"name\": \"EmpTableFromBlob\",\n          \"properties\": {\n            \"structure\": [\n              {\n                \"name\": \"FirstName\",\n                \"type\": \"String\"\n              },\n              {\n                \"name\": \"LastName\",\n                \"type\": \"String\"\n              }\n            ],\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"adftutorial/\",\n              \"fileName\": \"emp.txt\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"external\": true,\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            }\n          }\n        }\n\n        \n     Note the following: \n    \n    - dataset **type** is set to **AzureBlob**.\n    - **linkedServiceName** is set to **StorageLinkedService**. You had created this linked service in Step 2.\n    - **folderPath** is set to the **adftutorial** container. You can also specify the name of a blob within the folder. Since you are not specifying the name of the blob, data from all blobs in the container is considered as an input data.  \n    - format **type** is set to **TextFormat**\n    - There are two fields in the text file – **FirstName** and **LastName** – separated by a comma character (**columnDelimiter**) \n    - The **availability** is set to **hourly** (**frequency** is set to **hour** and **interval** is set to **1** ), so the Data Factory service will look for input data every hour in the root folder in the blob container (**adftutorial**) you specified. \n    \n\n    if you don't specify a **fileName** for an **input** **table**, all files/blobs from the input folder (**folderPath**) are considered as inputs. If you specify a fileName in the JSON, only the specified file/blob is considered asn input. See the sample files in the [tutorial][adf-tutorial] for examples.\n \n    If you do not specify a **fileName** for an **output table**, the generated files in the **folderPath** are named in the following format: Data.&lt;Guid\\&gt;.txt (example: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).\n\n    To set **folderPath** and **fileName** dynamically based on the **SliceStart** time, use the **partitionedBy** property. In the following example, folderPath uses Year, Month, and Day from from the SliceStart (start time of the slice being processed) and fileName uses Hour from the SliceStart. For example, if a slice is being produced for 2014-10-20T08:00:00, the folderName is set to wikidatagateway/wikisampledataout/2014/10/20 and the fileName is set to 08.csv. \n\n        \"folderPath\": \"wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}\",\n        \"fileName\": \"{Hour}.csv\",\n        \"partitionedBy\": \n        [\n            { \"name\": \"Year\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyy\" } },\n            { \"name\": \"Month\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"MM\" } }, \n            { \"name\": \"Day\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"dd\" } }, \n            { \"name\": \"Hour\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"hh\" } } \n        ],\n\n    See [JSON Scripting Reference](http://go.microsoft.com/fwlink/?LinkId=516971) for details about JSON properties.\n\n2. Click **Deploy** on the toolbar to create and deploy the **EmpTableFromBlob** table. Confirm that you see the **TABLE CREATED SUCCESSFULLY** message on the title bar of the Editor.\n\n### Create output table\nIn this part of the step, you will create an output table named **EmpSQLTable** that points to a SQL table in the Azure SQL database that is represented by the **AzureSqlLinkedService** linked service. \n\n1. In the **Editor** for the Data Factory, click **New dataset** button on the toolbar and click **Azure SQL table** from the drop down menu. \n2. Replace JSON in the right pane with the following JSON snippet:\n\n        {\n          \"name\": \"EmpSQLTable\",\n          \"properties\": {\n            \"structure\": [\n              {\n                \"name\": \"FirstName\",\n                \"type\": \"String\"\n              },\n              {\n                \"name\": \"LastName\",\n                \"type\": \"String\"\n              }\n            ],\n            \"type\": \"AzureSqlTable\",\n            \"linkedServiceName\": \"AzureSqlLinkedService\",\n            \"typeProperties\": {\n              \"tableName\": \"emp\"\n            },\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            }\n          }\n        }\n\n        \n     Note the following: \n    \n    * dataset **type** is set to **AzureSQLTable**.\n    * **linkedServiceName** is set to **AzureSqlLinkedService** (you had created this linked service in Step 2).\n    * **tablename** is set to **emp**.\n    * There are three columns – **ID**, **FirstName**, and **LastName** – in the emp table in the database, but ID is an identity column, so you need to specify only **FirstName** and **LastName** here.\n    * The **availability** is set to **hourly** (**frequency** set to **hour** and **interval** set to **1**).  The Data Factory service will generate an output data slice every hour in the **emp** table in the Azure SQL database.\n\n\n3. Click **Deploy** on the toolbar to create and deploy the **EmpSQLTable** table.\n\n\n## <a name=\"CreateAndRunAPipeline\"></a>Step 4: Create and run a pipeline\nIn this step, you create a pipeline with a **Copy Activity** that uses **EmpTableFromBlob** as input and **EmpSQLTable** as output.\n\n1. In the **Editor** for the Data Factory, click **New pipeline** button on the toolbar. Click **... (Ellipsis)** on the toolbar if you do not see the button. Alternatively, you can right-click **Pipelines** in the tree view and click **New pipeline**.\n\n    ![Editor New Pipeline Button][image-editor-newpipeline-button]\n \n2. Replace JSON in the right pane with the following JSON snippet: \n        \n        {\n          \"name\": \"ADFTutorialPipeline\",\n          \"properties\": {\n            \"description\": \"Copy data from a blob to Azure SQL table\",\n            \"activities\": [\n              {\n                \"name\": \"CopyFromBlobToSQL\",\n                \"description\": \"Push Regional Effectiveness Campaign data to Azure SQL database\",\n                \"type\": \"Copy\",\n                \"inputs\": [\n                  {\n                    \"name\": \"EmpTableFromBlob\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"EmpSQLTable\"\n                  }\n                ],\n                \"typeProperties\": {\n                  \"source\": {\n                    \"type\": \"BlobSource\"\n                  },\n                  \"sink\": {\n                    \"type\": \"SqlSink\",\n                    \"writeBatchSize\": 10000,\n                    \"writeBatchTimeout\": \"60:00:00\"\n                  }\n                },\n                \"Policy\": {\n                  \"concurrency\": 1,\n                  \"executionPriorityOrder\": \"NewestFirst\",\n                  \"retry\": 0,\n                  \"timeout\": \"01:00:00\"\n                }\n              }\n            ],\n            \"start\": \"2015-07-12T00:00:00Z\",\n            \"end\": \"2015-07-13T00:00:00Z\"\n          }\n        } \n\n    Note the following:\n\n    - In the activities section, there is only one activity whose **type** is set to **CopyActivity**.\n    - Input for the activity is set to **EmpTableFromBlob** and output for the activity is set to **EmpSQLTable**.\n    - In the **transformation** section, **BlobSource** is specified as the source type and **SqlSink** is specified as the sink type.\n\n    Replace the value of the **start** property with the current day and **end** value with the next day. You can specify only the date part and skip the time part of the date time. For example, \"2015-02-03\", which is equivalent to \"2015-02-03T00:00:00Z\"\n    \n    Both start and end datetimes must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-10-14T16:32:41Z. The **end** time is optional, but we will use it in this tutorial. \n    \n    If you do not specify value for the **end** property, it is calculated as \"**start + 48 hours**\". To run the pipeline indefinitely, specify **9999-09-09** as the value for the **end** property.\n    \n    In the example above, there will be 24 data slices as each data slice is produced hourly.\n    \n    See [JSON Scripting Reference](http://go.microsoft.com/fwlink/?LinkId=516971) for details about JSON properties.\n\n4. Click **Deploy** on the toolbar to create and deploy the **ADFTutorialPipeline**. Confirm that you see the **PIPELINE CREATED SUCCESSFULLY** message.\n5. Now, close the **Editor** blade by clicking **X**. Click **X** again to close the ADFTutorialDataFactory blade with the toolbar and tree view. If you see **your unsaved edits will be discarded** message, click **OK**.\n6. You should be back to the **DATA FACTORY** blade for the **ADFTutorialDataFactory**.\n\n**Congratulations!** You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline.   \n \n### View the data factory in a Diagram View \n1. In the **DATA FACTORY** blade, click **Diagram**.\n\n    ![Data Factory Blade - Diagram Tile][image-datafactoryblade-diagramtile]\n\n2. You should see the diagram similar to the following: \n\n    ![Diagram view][image-data-factory-get-started-diagram-blade]\n\n    You can zoom in, zoom out, zoom to 100%, zoom to fit, automatically position pipelines and tables, and show lineage information (highlights upstream and downstream items of selected items).  You can double-blick on an object (input/output table or pipeline) to see properties for it. \n3. Right-click on **ADFTutorialPipeline** in the Diagram View and click **Open pipeline**. You should see the activities in the pipeline along with input and output datasets for the activities. In this tutorial, you have only one activity in the pipeline (Copy Activity) with EmpTableBlob as input dataset and EmpSQLTable as output dataset.   \n\n    ![Open Pipeline](./media/data-factory-get-started-using-editor/DiagramView-OpenPipeline.png)\n\n4. Click **Data factory** in the breadcrumb in the top-left corner to get back to the diagram view. The diagram view displays all the pipelines. In this example, you have only created one pipeline.   \n \n\n## <a name=\"MonitorDataSetsAndPipeline\"></a>Step 5: Monitor the datasets and pipeline\nIn this step, you will use the Azure Portal to monitor what’s going on in an Azure data factory. You can also use PowerShell cmdlets to monitor datasets and pipelines. For details about using cmdlets for monitoring, see [Monitor and Manage Data Factory using PowerShell Cmdlets][monitor-manage-using-powershell].\n\n1. Navigate to [Azure Portal (Preview)][azure-preview-portal] if you don't have it open. \n2. If the blade for **ADFTutorialDataFactory** is not open, open it by clicking **ADFTutorialDataFactory** on the **Startboard**. \n3. You should see the count and names of tables and pipeline you created on this blade.\n\n    ![home page with names][image-data-factory-get-started-home-page-pipeline-tables]\n\n4. Now, click **Datasets** tile.\n5. In the **Datasets** blade, click **EmpTableFromBlob**. This is the input table for the **ADFTutorialPipeline**.\n\n    ![Datasets with EmpTableFromBlob selected][image-data-factory-get-started-datasets-emptable-selected]   \n5. Notice that the data slices up to the current time have already been produced and they are **Ready** because the **emp.txt** file exists all the time in the blob container: **adftutorial\\input**. Confirm that no slices show up in the **Recently failed slices** section at the bottom.\n\n    Both **Recently updated slices** and **Recently failed slices** lists are sorted by the **LAST UPDATE TIME**. The update time of a slice is changed in the following situations. \n    \n\n    -  You update the status of the slice manually, for example, by using the **Set-AzureDataFactorySliceStatus** (or) by clicking **RUN** on the **SLICE** blade for the slice.\n    -  The slice changes status due to an execution (e.g. a run started, a run ended and failed, a run ended and succeeded, etc).\n \n    Click on the title of the lists or **... (ellipses)** to see the larger list of slices. Click **Filter** on the toolbar to filter the slices.  \n    \n    To view the data slices sorted by the slice start/end times instead, click **Data slices (by slice time)** tile.   \n\n    ![Data Slices by Slice Time][DataSlicesBySliceTime]   \n\n6. Now, in the **Datasets** blade, click the **EmpSQLTable**. This is the output table for the **ADFTutorialPipeline**.\n\n    ![data sets blade][image-data-factory-get-started-datasets-blade]\n\n\n\n     \n6. You should see the **EmpSQLTable** blade as shown below:\n\n    ![table blade][image-data-factory-get-started-table-blade]\n \n7. Notice that the data slices up to the current time have already been produced and they are **Ready**. No slices show up in the **Problem slices** section at the bottom.\n8. Click **… (Ellipsis)** to see all the slices.\n\n    ![data slices blade][image-data-factory-get-started-dataslices-blade]\n\n9. Click on any data slice from the list and you should see the **DATA SLICE** blade.\n\n    ![data slice blade][image-data-factory-get-started-dataslice-blade]\n  \n    If the slice is not in the **Ready** state, you can see the upstream slices that are not Ready and are blocking the current slice from executing in the **Upstream slices that are not ready** list. \n\n11. In the **DATA SLICE** blade, you should see all activity runs in the list at the bottom. Click on an **activity run** to see the **ACTIVITY RUN DETAILS** blade. \n\n    ![Activity Run Details][image-data-factory-get-started-activity-run-details]\n\n    \n12. Click **X** to close all the blades until you get back to the home blade for the **ADFTutorialDataFactory**.\n14. (optional) Click **Pipelines** on the home page for **ADFTutorialDataFactory**, click **ADFTutorialPipeline** in the **Pipelines** blade, and drill through input tables (**Consumed**) or output tables (**Produced**).\n15. Launch **SQL Server Management Studio**, connect to the Azure SQL Database, and verify that the rows are inserted into the **emp** table in the database.\n\n    ![sql query results][image-data-factory-get-started-sql-query-results]\n\n\n## Summary \nIn this tutorial, you created an Azure data factory to copy data from an Azure blob to an Azure SQL database. You used the Azure Preview Portal to create the data factory, linked services, tables, and a pipeline. Here are the high level steps you performed in this tutorial:  \n\n1.  Create an Azure **data factory**.\n2.  Create **linked services** that link data stores and computes (referred as **Linked Services**) to the data factory.\n3.  Create **tables** which describe input data and output data for pipelines.\n4.  Create **pipelines**. A pipeline consists of one or more activities and processes the inputs and produces outputs. Set the active period for the pipeline by specifying **Start** time and **End** time for the pipeline. The active period defines the time duration in which data slices will be produced. \n\n\nFor a list of supported activities, see [Pipelines and Activities][msdn-activities] \ntopic and for a list of supported linked services, see [Linked Services][msdn-linkedservices] \ntopic on MSDN Library.\n \nTo do this tutorial using Azure PowerShell, see [Create and monitor a data factory using Azure PowerShell][monitor-manage-using-powershell].  \n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-get-started-using-editor.md). \n\n<!--Link references-->\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[msdn-activities]: https://msdn.microsoft.com/library/dn834988.aspx\n[msdn-linkedservices]: https://msdn.microsoft.com/library/dn834986.aspx\n[data-factory-naming-rules]: https://msdn.microsoft.com/library/azure/dn835027.aspx\n\n[azure-preview-portal]: https://portal.azure.com/\n[download-azure-powershell]: http://azure.microsoft.com/documentation/articles/install-configure-powershell\n[sql-management-studio]: http://azure.microsoft.com/documentation/articles/sql-database-manage-azure-ssms/#Step2\n[sql-cmd-exe]: https://msdn.microsoft.com/library/azure/ee336280.aspx\n\n[data-factory-editor]: data-factory-editor.md\n[monitor-manage-using-powershell]: data-factory-monitor-manage-using-powershell.md\n[adf-tutorial]: data-factory-tutorial.md\n[use-custom-activities]: data-factory-use-custom-activities.md\n[troubleshoot]: data-factory-troubleshoot.md\n[data-factory-introduction]: data-factory-introduction.md\n[data-factory-create-storage]: http://azure.microsoft.com/documentation/articles/storage-create-storage-account/#create-a-storage-account\n[data-factory-create-sql-database]: ../sql-database-create-configure.md\n\n\n[developer-reference]: http://go.microsoft.com/fwlink/?LinkId=516908\n[cmdlet-reference]: http://go.microsoft.com/fwlink/?LinkId=517456\n\n<!--Image references-->\n\n[DataSlicesBySliceTime]: ./media/data-factory-get-started-using-editor/DataSlicesBySliceTime.png\n\n[image-data-factory-getstarted-new-everything]: ./media/data-factory-get-started-using-editor/GetStarted-New-Everything.png\n\n[image-data-factory-gallery-storagecachebackup]: ./media/data-factory-get-started-using-editor/getstarted-gallery-datastoragecachebackup.png\n\n[image-data-factory-gallery-storagecachebackup-seeall]: ./media/data-factory-get-started-using-editor/getstarted-gallery-datastoragecachebackup-seeall.png\n\n[image-data-factory-getstarted-data-services-data-factory-selected]: ./media/data-factory-get-started-using-editor/getstarted-data-services-data-factory-selected.png\n\n[image-data-factory-getstarted-data-factory-create-button]: ./media/data-factory-get-started-using-editor/getstarted-data-factory-create-button.png\n\n[image-data-factory-getstarted-new-data-factory-blade]: ./media/data-factory-get-started-using-editor/getstarted-new-data-factory.png\n\n[image-data-factory-get-stated-factory-home-page]: ./media/data-factory-get-started-using-editor/getstarted-data-factory-home-page.png\n\n[image-author-deploy-tile]: ./media/data-factory-get-started-using-editor/getstarted-author-deploy-tile.png\n\n[image-editor-newdatastore-button]: ./media/data-factory-get-started-using-editor/getstarted-editor-newdatastore-button.png\n\n[image-editor-blob-storage-json]: ./media/data-factory-get-started-using-editor/getstarted-editor-blob-storage-json.png\n\n[image-editor-blob-storage-deploy]: ./media/data-factory-get-started-using-editor/getstarted-editor-blob-storage-deploy.png\n\n[image-editor-azure-sql-settings]: ./media/data-factory-get-started-using-editor/getstarted-editor-azure-sql-settings.png\n\n[image-editor-newpipeline-button]: ./media/data-factory-get-started-using-editor/getstarted-editor-newpipeline-button.png\n\n[image-datafactoryblade-diagramtile]: ./media/data-factory-get-started-using-editor/getstarted-datafactoryblade-diagramtile.png\n\n\n[image-data-factory-get-started-startboard]: ./media/data-factory-get-started-using-editor/getstarted-data-factory-startboard.png\n\n[image-data-factory-get-started-linked-services-link]: ./media/data-factory-get-started-using-editor/getstarted-data-factory-linked-services-link.png\n\n[image-data-factory-get-started-linked-services-add-store-button]: ./media/data-factory-get-started-using-editor/getstarted-linked-services-add-store-button.png\n\n[image-data-factory-linked-services-get-started-new-data-store]: ./media/data-factory-get-started-using-editor/getstarted-linked-services-new-data-store.png\n\n[image-data-factory-get-started-new-data-store-with-storage]: ./media/data-factory-get-started-using-editor/getstarted-linked-services-new-data-store-with-storage.png\n\n[image-data-factory-get-started-storage-account-name-key]: ./media/data-factory-get-started-using-editor/getstarted-storage-account-name-key.png\n\n[image-data-factory-get-started-linked-services-list-with-myblobstore]: ./media/data-factory-get-started-using-editor/getstarted-linked-services-list-with-myblobstore.png\n\n[image-data-factory-get-started-linked-azure-sql-properties]: ./media/data-factory-get-started-using-editor/getstarted-linked-azure-sql-properties.png\n\n[image-data-factory-get-started-azure-sql-connection-string]: ./media/data-factory-get-started-using-editor/getstarted-azure-sql-connection-string.png\n\n[image-data-factory-get-started-linked-services-list-two-stores]: ./media/data-factory-get-started-using-editor/getstarted-linked-services-list-two-stores.png\n\n[image-data-factory-get-started-storage-explorer]: ./media/data-factory-get-started-using-editor/getstarted-storage-explorer.png\n\n[image-data-factory-get-started-diagram-link]: ./media/data-factory-get-started-using-editor/getstarted-diagram-link.png\n\n[image-data-factory-get-started-diagram-blade]: ./media/data-factory-get-started-using-editor/getstarted-diagram-blade.png\n\n[image-data-factory-get-started-home-page-pipeline-tables]: ./media/data-factory-get-started-using-editor/getstarted-datafactory-home-page-pipeline-tables.png\n\n[image-data-factory-get-started-datasets-blade]: ./media/data-factory-get-started-using-editor/getstarted-datasets-blade.png\n\n[image-data-factory-get-started-table-blade]: ./media/data-factory-get-started-using-editor/getstarted-table-blade.png\n\n[image-data-factory-get-started-dataslices-blade]: ./media/data-factory-get-started-using-editor/getstarted-dataslices-blade.png\n\n[image-data-factory-get-started-dataslice-blade]: ./media/data-factory-get-started-using-editor/getstarted-dataslice-blade.png\n\n[image-data-factory-get-started-sql-query-results]: ./media/data-factory-get-started-using-editor/getstarted-sql-query-results.png\n\n[image-data-factory-get-started-datasets-emptable-selected]: ./media/data-factory-get-started-using-editor/DataSetsWithEmpTableFromBlobSelected.png\n\n[image-data-factory-get-started-activity-run-details]: ./media/data-factory-get-started-using-editor/ActivityRunDetails.png\n\n[image-data-factory-create-resource-group]: ./media/data-factory-get-started-using-editor/CreateNewResourceGroup.png\n\n[image-data-factory-preview-storage-key]: ./media/data-factory-get-started-using-editor/PreviewPortalStorageKey.png\n\n[image-data-factory-database-connection-string]: ./media/data-factory-get-started-using-editor/DatabaseConnectionString.png\n\n[image-data-factory-new-datafactory-menu]: ./media/data-factory-get-started-using-editor/NewDataFactoryMenu.png\n\n[image-data-factory-sql-management-console]: ./media/data-factory-get-started-using-editor/getstarted-azure-sql-management-console.png\n\n[image-data-factory-sql-management-console-2]: ./media/data-factory-get-started-using-editor/getstarted-azure-sql-management-console-2.png\n\n[image-data-factory-name-not-available]: ./media/data-factory-get-started-using-editor/getstarted-data-factory-not-available.png\n \ntest\n"
}