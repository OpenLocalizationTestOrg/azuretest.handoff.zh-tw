{
  "nodes": [
    {
      "content": "Process Azure blob data with advanced analytics | Microsoft Azure",
      "pos": [
        28,
        93
      ]
    },
    {
      "content": "Process Data in Azure Blob storage.",
      "pos": [
        113,
        148
      ]
    },
    {
      "pos": [
        543,
        612
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"heading\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Process Azure blob data with advanced analytics"
    },
    {
      "content": "This document covers exploring data and generating features from data stored in Azure Blob storage.",
      "pos": [
        614,
        713
      ]
    },
    {
      "content": "Load the data into a Pandas data frame",
      "pos": [
        719,
        757
      ]
    },
    {
      "content": "In order to do explore and manipulate a dataset, it must be downloaded from the blob source to a local file which can then be loaded in a Pandas data frame.",
      "pos": [
        758,
        914
      ]
    },
    {
      "content": "Here are the steps to follow for this procedure:",
      "pos": [
        915,
        963
      ]
    },
    {
      "content": "Download the data from Azure blob with the following sample Python code using blob service.",
      "pos": [
        968,
        1059
      ]
    },
    {
      "content": "Replace the variable in the code below with your specific values:",
      "pos": [
        1060,
        1125
      ]
    },
    {
      "content": "Read the data into a Pandas data-frame from the downloaded file.",
      "pos": [
        1749,
        1813
      ]
    },
    {
      "content": "Now you are ready to explore the data and generate features on this dataset.",
      "pos": [
        1905,
        1981
      ]
    },
    {
      "pos": [
        1985,
        2036
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"blob-dataexploration\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Data Exploration"
    },
    {
      "content": "Here are a few examples of ways to explore data using Pandas:",
      "pos": [
        2038,
        2099
      ]
    },
    {
      "content": "Inspect the number of rows and columns",
      "pos": [
        2104,
        2142
      ]
    },
    {
      "content": "Inspect the first or last few rows in the dataset as below:",
      "pos": [
        2241,
        2300
      ]
    },
    {
      "content": "Check the data type each column was imported as using the following sample code",
      "pos": [
        2387,
        2466
      ]
    },
    {
      "content": "Check the basic stats for the columns in the data set as follows",
      "pos": [
        2608,
        2672
      ]
    },
    {
      "content": "Look at the number of entries for each column value as follows",
      "pos": [
        2721,
        2783
      ]
    },
    {
      "content": "Count missing values versus the actual number of entries in each column using the following sample code",
      "pos": [
        2848,
        2951
      ]
    },
    {
      "content": "If you have missing values for a specific column in the data, you can drop them as follows:",
      "pos": [
        3062,
        3153
      ]
    },
    {
      "content": "Another way to replace missing values is with the mode function:",
      "pos": [
        3260,
        3324
      ]
    },
    {
      "content": "Create a histogram plot using variable number of bins to plot the distribution of a variable",
      "pos": [
        3467,
        3559
      ]
    },
    {
      "content": "Look at correlations between variables using a scatterplot or using the built-in correlation function",
      "pos": [
        3727,
        3828
      ]
    },
    {
      "pos": [
        4125,
        4173
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"blob-featuregen\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Feature Generation"
    },
    {
      "content": "We can generate features using Python as follows:",
      "pos": [
        4179,
        4228
      ]
    },
    {
      "pos": [
        4233,
        4305
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"blob-countfeature\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Indicator value based Feature Generation"
    },
    {
      "content": "Categorical features can be created as follows:",
      "pos": [
        4307,
        4354
      ]
    },
    {
      "content": "Inspect the distribution of the categorical column:",
      "pos": [
        4359,
        4410
      ]
    },
    {
      "content": "Generate indicator values for each of the column values",
      "pos": [
        4486,
        4541
      ]
    },
    {
      "content": "Join the indicator column with the original data frame",
      "pos": [
        4723,
        4777
      ]
    },
    {
      "content": "Remove the original variable itself:",
      "pos": [
        4955,
        4991
      ]
    },
    {
      "pos": [
        5157,
        5217
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"blob-binningfeature\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Binning Feature Generation"
    },
    {
      "content": "For generating binned features, we proceed as follows:",
      "pos": [
        5219,
        5273
      ]
    },
    {
      "content": "Add a sequence of columns to bin a numeric column",
      "pos": [
        5278,
        5327
      ]
    },
    {
      "content": "Convert binning to a sequence of boolean variables",
      "pos": [
        5467,
        5517
      ]
    },
    {
      "content": "Finally, Join the dummy variables back to the original data frame",
      "pos": [
        5634,
        5699
      ]
    },
    {
      "pos": [
        5801,
        5901
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"sql-featuregen\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Writing data back to Azure blob and consuming in Azure Machine Learning"
    },
    {
      "content": "After you have explored the data and created the necessary features, you can upload the data (sampled or featurized) to an Azure blob and consume it in Azure Machine Learning using the following steps:",
      "pos": [
        5903,
        6104
      ]
    },
    {
      "content": "Note that additional features can be created in the Azure Machine Learning Studio as well.",
      "pos": [
        6105,
        6195
      ]
    },
    {
      "content": "Write the data frame to local file",
      "pos": [
        6200,
        6234
      ]
    },
    {
      "content": "Upload the data to Azure blob as follows:",
      "pos": [
        6347,
        6388
      ]
    },
    {
      "content": "Now the data can be read from the blob using the Azure Machine Learning [Reader][reader] module as shown in the screen below:",
      "pos": [
        7155,
        7280
      ]
    },
    {
      "content": "reader blob",
      "pos": [
        7285,
        7296
      ]
    },
    {
      "content": "test",
      "pos": [
        7499,
        7503
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Process Azure blob data with advanced analytics | Microsoft Azure\" \n    description=\"Process Data in Azure Blob storage.\" \n    services=\"machine-learning,storage\" \n    solutions=\"\" \n    documentationCenter=\"\" \n    authors=\"msolhab\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"09/01/2015\" \n    ms.author=\"sunliangms;fashah;msolhab;garye;bradsev\" /> \n\n#<a name=\"heading\"></a>Process Azure blob data with advanced analytics\n\nThis document covers exploring data and generating features from data stored in Azure Blob storage. \n\n## Load the data into a Pandas data frame\nIn order to do explore and manipulate a dataset, it must be downloaded from the blob source to a local file which can then be loaded in a Pandas data frame. Here are the steps to follow for this procedure:\n\n1. Download the data from Azure blob with the following sample Python code using blob service. Replace the variable in the code below with your specific values: \n\n        from azure.storage import BlobService\n        import tables\n        \n        STORAGEACCOUNTNAME= <storage_account_name>\n        STORAGEACCOUNTKEY= <storage_account_key>\n        LOCALFILENAME= <local_file_name>        \n        CONTAINERNAME= <container_name>\n        BLOBNAME= <blob_name>\n\n        #download from blob\n        t1=time.time()\n        blob_service=BlobService(account_name=STORAGEACCOUNTNAME,account_key=STORAGEACCOUNTKEY)\n        blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\n        t2=time.time()\n        print((\"It takes %s seconds to download \"+blobname) % (t2 - t1))\n\n\n2. Read the data into a Pandas data-frame from the downloaded file.\n\n        #LOCALFILE is the file path \n        dataframe_blobdata = pd.read_csv(LOCALFILE)\n\nNow you are ready to explore the data and generate features on this dataset.\n\n##<a name=\"blob-dataexploration\"></a>Data Exploration\n\nHere are a few examples of ways to explore data using Pandas:\n\n1. Inspect the number of rows and columns \n\n        print 'the size of the data is: %d rows and  %d columns' % dataframe_blobdata.shape\n\n2. Inspect the first or last few rows in the dataset as below:\n\n        dataframe_blobdata.head(10)\n        \n        dataframe_blobdata.tail(10)\n\n3. Check the data type each column was imported as using the following sample code\n    \n        for col in dataframe_blobdata.columns:\n            print dataframe_blobdata[col].name, ':\\t', dataframe_blobdata[col].dtype\n\n4. Check the basic stats for the columns in the data set as follows\n \n        dataframe_blobdata.describe()\n    \n5. Look at the number of entries for each column value as follows\n\n        dataframe_blobdata['<column_name>'].value_counts()\n\n6. Count missing values versus the actual number of entries in each column using the following sample code\n\n        miss_num = dataframe_blobdata.shape[0] - dataframe_blobdata.count()\n        print miss_num\n     \n7.  If you have missing values for a specific column in the data, you can drop them as follows:\n\n        dataframe_blobdata_noNA = dataframe_blobdata.dropna()\n        dataframe_blobdata_noNA.shape\n\n    Another way to replace missing values is with the mode function:\n    \n        dataframe_blobdata_mode = dataframe_blobdata.fillna({'<column_name>':dataframe_blobdata['<column_name>'].mode()[0]})        \n\n8. Create a histogram plot using variable number of bins to plot the distribution of a variable \n    \n        dataframe_blobdata['<column_name>'].value_counts().plot(kind='bar')\n        \n        np.log(dataframe_blobdata['<column_name>']+1).hist(bins=50)\n    \n9. Look at correlations between variables using a scatterplot or using the built-in correlation function\n\n        #relationship between column_a and column_b using scatter plot\n        plt.scatter(dataframe_blobdata['<column_a>'], dataframe_blobdata['<column_b>'])\n        \n        #correlation between column_a and column_b\n        dataframe_blobdata[['<column_a>', '<column_b>']].corr()\n    \n    \n##<a name=\"blob-featuregen\"></a>Feature Generation\n    \nWe can generate features using Python as follows:\n\n###<a name=\"blob-countfeature\"></a>Indicator value based Feature Generation\n\nCategorical features can be created as follows:\n\n1. Inspect the distribution of the categorical column:\n    \n        dataframe_blobdata['<categorical_column>'].value_counts()\n\n2. Generate indicator values for each of the column values\n\n        #generate the indicator column\n        dataframe_blobdata_identity = pd.get_dummies(dataframe_blobdata['<categorical_column>'], prefix='<categorical_column>_identity')\n\n3. Join the indicator column with the original data frame \n \n            #Join the dummy variables back to the original data frame\n            dataframe_blobdata_with_identity = dataframe_blobdata.join(dataframe_blobdata_identity)\n\n4. Remove the original variable itself:\n\n        #Remove the original column rate_code in df1_with_dummy\n        dataframe_blobdata_with_identity.drop('<categorical_column>', axis=1, inplace=True)\n    \n###<a name=\"blob-binningfeature\"></a>Binning Feature Generation\n\nFor generating binned features, we proceed as follows:\n\n1. Add a sequence of columns to bin a numeric column\n \n        bins = [0, 1, 2, 4, 10, 40]\n        dataframe_blobdata_bin_id = pd.cut(dataframe_blobdata['<numeric_column>'], bins)\n        \n2. Convert binning to a sequence of boolean variables\n\n        dataframe_blobdata_bin_bool = pd.get_dummies(dataframe_blobdata_bin_id, prefix='<numeric_column>')\n    \n3. Finally, Join the dummy variables back to the original data frame\n\n        dataframe_blobdata_with_bin_bool = dataframe_blobdata.join(dataframe_blobdata_bin_bool) \n\n##<a name=\"sql-featuregen\"></a>Writing data back to Azure blob and consuming in Azure Machine Learning\n\nAfter you have explored the data and created the necessary features, you can upload the data (sampled or featurized) to an Azure blob and consume it in Azure Machine Learning using the following steps:\nNote that additional features can be created in the Azure Machine Learning Studio as well. \n1. Write the data frame to local file\n\n        dataframe.to_csv(os.path.join(os.getcwd(),LOCALFILENAME), sep='\\t', encoding='utf-8', index=False)\n\n2. Upload the data to Azure blob as follows:\n\n        from azure.storage import BlobService\n        import tables\n\n        STORAGEACCOUNTNAME= <storage_account_name>\n        LOCALFILENAME= <local_file_name>\n        STORAGEACCOUNTKEY= <storage_account_key>\n        CONTAINERNAME= <container_name>\n        BLOBNAME= <blob_name>\n\n        output_blob_service=BlobService(account_name=STORAGEACCOUNTNAME,account_key=STORAGEACCOUNTKEY)    \n        localfileprocessed = os.path.join(os.getcwd(),LOCALFILENAME) #assuming file is in current working directory\n        \n        try:\n       \n        #perform upload\n        output_blob_service.put_block_blob_from_path(CONTAINERNAME,BLOBNAME,localfileprocessed)\n        \n        except:         \n            print (\"Something went wrong with uploading blob:\"+BLOBNAME)\n\n3. Now the data can be read from the blob using the Azure Machine Learning [Reader][reader] module as shown in the screen below:\n \n![reader blob][1]\n\n[1]: ./media/machine-learning-data-science-process-data-blob/reader_blob.png\n\n\n<!-- Module References -->\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/\n \ntest\n"
}