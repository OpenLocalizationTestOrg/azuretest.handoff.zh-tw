{
  "nodes": [
    {
      "content": "Use Script Action to install Spark on Hadoop cluster | Microsoft Azure",
      "pos": [
        28,
        98
      ]
    },
    {
      "content": "Learn how to customize an HDInsight cluster with Spark.",
      "pos": [
        118,
        173
      ]
    },
    {
      "content": "You'll use a Script Action configuration option to use a script to install Spark.",
      "pos": [
        174,
        255
      ]
    },
    {
      "content": "Install and use Spark on HDInsight Hadoop clusters",
      "pos": [
        571,
        621
      ]
    },
    {
      "content": "You can install Spark on any type of cluster in Hadoop on Azure HDInsight by using <bpt id=\"p1\">**</bpt>Script Action<ept id=\"p1\">**</ept> cluster customization.",
      "pos": [
        623,
        746
      ]
    },
    {
      "content": "Script Action lets you run scripts to customize a cluster, only when the cluster is being created.",
      "pos": [
        747,
        845
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p1\">][hdinsight-cluster-customize]</ept>.",
      "pos": [
        846,
        951
      ]
    },
    {
      "content": "In this topic, you will learn how to install Spark by using Script Action.",
      "pos": [
        953,
        1027
      ]
    },
    {
      "content": "Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.",
      "pos": [
        1028,
        1124
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> HDInsight now provides Spark as a first-class cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster.",
      "pos": [
        1128,
        1295
      ]
    },
    {
      "content": "Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.",
      "pos": [
        1296,
        1392
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p1\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.",
      "pos": [
        1393,
        1527
      ]
    },
    {
      "pos": [
        1533,
        1568
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"whatis\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>What is Spark?"
    },
    {
      "content": "<ph id=\"ph1\">&lt;a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\"&gt;</ph>Apache Spark<ph id=\"ph2\">&lt;/a&gt;</ph> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.",
      "pos": [
        1570,
        1802
      ]
    },
    {
      "content": "Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
      "pos": [
        1803,
        1936
      ]
    },
    {
      "content": "Spark can also be used to perform conventional disk-based data processing.",
      "pos": [
        1938,
        2012
      ]
    },
    {
      "content": "Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.",
      "pos": [
        2013,
        2118
      ]
    },
    {
      "content": "Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
      "pos": [
        2119,
        2270
      ]
    },
    {
      "content": "This topic provides instructions on how to customize an HDInsight cluster to install Spark.",
      "pos": [
        2273,
        2364
      ]
    },
    {
      "pos": [
        2369,
        2427
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"whatis\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Which version of Spark can I install?"
    },
    {
      "content": "In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster.",
      "pos": [
        2429,
        2522
      ]
    },
    {
      "content": "This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you provision.",
      "pos": [
        2523,
        2638
      ]
    },
    {
      "pos": [
        2642,
        2741
      ],
      "content": "If you use the script while provisioning an <bpt id=\"p1\">**</bpt>HDInsight 3.2<ept id=\"p1\">**</ept> cluster, it installs <bpt id=\"p2\">**</bpt>Spark 1.2.0<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        2744,
        2843
      ],
      "content": "If you use the script while provisioning an <bpt id=\"p1\">**</bpt>HDInsight 3.1<ept id=\"p1\">**</ept> cluster, it installs <bpt id=\"p2\">**</bpt>Spark 1.0.2<ept id=\"p2\">**</ept>."
    },
    {
      "content": "You can modify this script or create your own script to install other versions of Spark.",
      "pos": [
        2846,
        2934
      ]
    },
    {
      "pos": [
        2940,
        2985
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>How do I install Spark?"
    },
    {
      "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p1\">[</bpt>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ept id=\"p1\">](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1)</ept>.",
      "pos": [
        2987,
        3283
      ]
    },
    {
      "content": "This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal.",
      "pos": [
        3284,
        3408
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The sample script works only with HDInsight 3.1 and 3.2 clusters.",
      "pos": [
        3413,
        3491
      ]
    },
    {
      "content": "For more information on HDInsight cluster versions, see <bpt id=\"p1\">[</bpt>HDInsight cluster versions<ept id=\"p1\">](hdinsight-component-versioning.md)</ept>.",
      "pos": [
        3492,
        3612
      ]
    },
    {
      "content": "Start provisioning a cluster by using the <bpt id=\"p1\">**</bpt>CUSTOM CREATE<ept id=\"p1\">**</ept> option, as described at <bpt id=\"p2\">[</bpt>Provisioning a cluster using custom options<ept id=\"p2\">](hdinsight-provision-clusters.md#portal)</ept>.",
      "pos": [
        3617,
        3787
      ]
    },
    {
      "content": "Pick the cluster version depending on the following:",
      "pos": [
        3788,
        3840
      ]
    },
    {
      "pos": [
        3848,
        3923
      ],
      "content": "If you want to install <bpt id=\"p1\">**</bpt>Spark 1.2.0<ept id=\"p1\">**</ept>, provision an HDInsight 3.2 cluster."
    },
    {
      "pos": [
        3930,
        4005
      ],
      "content": "If you want to install <bpt id=\"p1\">**</bpt>Spark 1.0.2<ept id=\"p1\">**</ept>, provision an HDInsight 3.1 cluster."
    },
    {
      "pos": [
        4011,
        4144
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Script Actions<ept id=\"p1\">**</ept> page of the wizard, click <bpt id=\"p2\">**</bpt>add script action<ept id=\"p2\">**</ept> to provide details about the script action, as shown below:"
    },
    {
      "content": "Use Script Action to customize a cluster",
      "pos": [
        4152,
        4192
      ]
    },
    {
      "pos": [
        4316,
        5184
      ],
      "content": "<table border='1'>\n     <tr><th>Property</th><th>Value</th></tr>\n     <tr><td>Name</td>\n         <td>Specify a name for the script action. For example, <b>Install Spark</b>.</td></tr>\n     <tr><td>Script URI</td>\n         <td>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster. For example, <i>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1</i></td></tr>\n     <tr><td>Node Type</td>\n         <td>Specify the nodes on which the customization script is run. You can choose <b>All nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes only</b>.\n     <tr><td>Parameters</td>\n         <td>Specify the parameters, if required by the script. The script to install Spark does not require any parameters so you can leave this blank.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            32,
            40
          ]
        },
        {
          "content": "Value",
          "pos": [
            49,
            54
          ]
        },
        {
          "content": "Name",
          "pos": [
            78,
            82
          ]
        },
        {
          "content": "Specify a name for the script action. For example, <b>Install Spark</b>.",
          "pos": [
            101,
            173
          ],
          "nodes": [
            {
              "content": "Specify a name for the script action.",
              "pos": [
                0,
                37
              ]
            },
            {
              "content": "For example, <ph id=\"ph1\">&lt;b&gt;</ph>Install Spark<ph id=\"ph2\">&lt;/b&gt;</ph>.",
              "pos": [
                38,
                72
              ]
            }
          ]
        },
        {
          "content": "Script URI",
          "pos": [
            197,
            207
          ]
        },
        {
          "content": "Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster. For example, <i>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1</i>",
          "pos": [
            226,
            439
          ],
          "nodes": [
            {
              "content": "Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster.",
              "pos": [
                0,
                101
              ]
            },
            {
              "content": "For example, <ph id=\"ph1\">&lt;i&gt;</ph>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ph id=\"ph2\">&lt;/i&gt;</ph>",
              "pos": [
                102,
                213
              ]
            }
          ]
        },
        {
          "content": "Node Type",
          "pos": [
            463,
            472
          ]
        },
        {
          "content": "Specify the nodes on which the customization script is run. You can choose <b>All nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes only</b>.",
          "pos": [
            491,
            636
          ],
          "nodes": [
            {
              "content": "Specify the nodes on which the customization script is run.",
              "pos": [
                0,
                59
              ]
            },
            {
              "content": "You can choose <ph id=\"ph1\">&lt;b&gt;</ph>All nodes<ph id=\"ph2\">&lt;/b&gt;</ph>, <ph id=\"ph3\">&lt;b&gt;</ph>Head nodes only<ph id=\"ph4\">&lt;/b&gt;</ph>, or <ph id=\"ph5\">&lt;b&gt;</ph>Worker nodes only<ph id=\"ph6\">&lt;/b&gt;</ph>.",
              "pos": [
                60,
                145
              ]
            }
          ]
        },
        {
          "content": "Parameters",
          "pos": [
            650,
            660
          ]
        },
        {
          "content": "Specify the parameters, if required by the script. The script to install Spark does not require any parameters so you can leave this blank.",
          "pos": [
            679,
            818
          ],
          "nodes": [
            {
              "content": "Specify the parameters, if required by the script.",
              "pos": [
                0,
                50
              ]
            },
            {
              "content": "The script to install Spark does not require any parameters so you can leave this blank.",
              "pos": [
                51,
                139
              ]
            }
          ]
        }
      ]
    },
    {
      "content": "You can add more than one script action to install multiple components on the cluster.",
      "pos": [
        5194,
        5280
      ]
    },
    {
      "content": "After you have added the scripts, click the checkmark to start provisioning the cluster.",
      "pos": [
        5281,
        5369
      ]
    },
    {
      "content": "You can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK.",
      "pos": [
        5371,
        5481
      ]
    },
    {
      "content": "Instructions for these procedures are provided later in this topic.",
      "pos": [
        5482,
        5549
      ]
    },
    {
      "pos": [
        5554,
        5609
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"usespark\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>How do I use Spark in HDInsight?"
    },
    {
      "content": "Spark provides APIs in Scala, Python, and Java.",
      "pos": [
        5610,
        5657
      ]
    },
    {
      "content": "You can also use the interactive Spark shell to run Spark queries.",
      "pos": [
        5658,
        5724
      ]
    },
    {
      "content": "This section provides instructions on how to use the different approaches to work with Spark:",
      "pos": [
        5725,
        5818
      ]
    },
    {
      "content": "Using the Spark shell to run interactive queries",
      "pos": [
        5823,
        5871
      ]
    },
    {
      "content": "Using the Spark shell to run Spark SQL queries",
      "pos": [
        5889,
        5935
      ]
    },
    {
      "content": "Using a standalone Scala program",
      "pos": [
        5952,
        5984
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"sparkshell\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using the Spark shell to run interactive queries",
      "pos": [
        6003,
        6076
      ]
    },
    {
      "content": "Perform the following steps to run Spark queries from an interactive Spark shell.",
      "pos": [
        6077,
        6158
      ]
    },
    {
      "content": "In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.",
      "pos": [
        6159,
        6308
      ]
    },
    {
      "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.",
      "pos": [
        6313,
        6441
      ]
    },
    {
      "content": "For instructions, see <ph id=\"ph1\">&lt;a href=\"http://azure.microsoft.com/documentation/articles/hdinsight-administer-use-management-portal/#rdp\" target=\"_blank\"&gt;</ph>Connect to HDInsight clusters using RDP<ph id=\"ph2\">&lt;/a&gt;</ph>.",
      "pos": [
        6442,
        6632
      ]
    },
    {
      "pos": [
        6637,
        6854
      ],
      "content": "In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id=\"p1\">**</bpt>C:\\apps\\dist\\spark-1.2.0<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to start the Spark shell:",
      "pos": [
        6860,
        6911
      ]
    },
    {
      "content": "After the command finishes running, you should get a Scala prompt:",
      "pos": [
        6959,
        7025
      ]
    },
    {
      "content": "On the Scala prompt, enter the Spark query shown below.",
      "pos": [
        7047,
        7102
      ]
    },
    {
      "content": "This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
      "pos": [
        7103,
        7286
      ]
    },
    {
      "content": "The output should resemble the following:",
      "pos": [
        7505,
        7546
      ]
    },
    {
      "content": "Output from running Scala interactive shell in an HDInsight cluster",
      "pos": [
        7554,
        7621
      ]
    },
    {
      "content": "Enter :q to exit the Scala prompt.",
      "pos": [
        7702,
        7736
      ]
    },
    {
      "pos": [
        7753,
        7822
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"sparksql\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using the Spark shell to run Spark SQL queries"
    },
    {
      "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.",
      "pos": [
        7824,
        7947
      ]
    },
    {
      "content": "In this section, we look at using Spark to run a Hive query on a sample Hive table.",
      "pos": [
        7948,
        8031
      ]
    },
    {
      "content": "The Hive table used in this section (called <bpt id=\"p1\">**</bpt>hivesampletable<ept id=\"p1\">**</ept>) is available by default when you provision a cluster.",
      "pos": [
        8032,
        8150
      ]
    },
    {
      "pos": [
        8153,
        8309
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The sample below was created against <bpt id=\"p1\">**</bpt>Spark 1.2.0<ept id=\"p1\">**</ept>, which is installed if you run the script action while provisioning HDInsight 3.2 cluster."
    },
    {
      "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.",
      "pos": [
        8314,
        8442
      ]
    },
    {
      "content": "For instructions, see <ph id=\"ph1\">&lt;a href=\"http://azure.microsoft.com/documentation/articles/hdinsight-administer-use-management-portal/#rdp\" target=\"_blank\"&gt;</ph>Connect to HDInsight clusters using RDP<ph id=\"ph2\">&lt;/a&gt;</ph>.",
      "pos": [
        8443,
        8633
      ]
    },
    {
      "pos": [
        8638,
        8829
      ],
      "content": "In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id=\"p1\">**</bpt>C:\\apps\\dist\\spark-1.2.0<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to start the Spark shell:",
      "pos": [
        8835,
        8886
      ]
    },
    {
      "content": "After the command finishes running, you should get a Scala prompt:",
      "pos": [
        8934,
        9000
      ]
    },
    {
      "content": "On the Scala prompt, set the Hive context.",
      "pos": [
        9022,
        9064
      ]
    },
    {
      "content": "This is required to work with Hive queries by using Spark.",
      "pos": [
        9065,
        9123
      ]
    },
    {
      "pos": [
        9202,
        9287
      ],
      "content": "Note that <bpt id=\"p1\">**</bpt>sc<ept id=\"p1\">**</ept> is default Spark context that is set when you start the Spark shell."
    },
    {
      "content": "Run a Hive query by using the Hive context and print the output to the console.",
      "pos": [
        9292,
        9371
      ]
    },
    {
      "content": "The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
      "pos": [
        9372,
        9476
      ]
    },
    {
      "content": "You should see an output like the following:",
      "pos": [
        9608,
        9652
      ]
    },
    {
      "content": "Output from running Spark SQL on an HDInsight cluster",
      "pos": [
        9660,
        9713
      ]
    },
    {
      "content": "Enter :q to exit the Scala prompt.",
      "pos": [
        9777,
        9811
      ]
    },
    {
      "pos": [
        9829,
        9886
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"standalone\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using a standalone Scala program"
    },
    {
      "content": "In this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.",
      "pos": [
        9888,
        10112
      ]
    },
    {
      "content": "To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:",
      "pos": [
        10113,
        10245
      ]
    },
    {
      "content": "Write a Scala program",
      "pos": [
        10249,
        10270
      ]
    },
    {
      "content": "Build the Scala program to get the .jar file",
      "pos": [
        10273,
        10317
      ]
    },
    {
      "content": "Run the job on the cluster",
      "pos": [
        10320,
        10346
      ]
    },
    {
      "content": "Write a Scala program",
      "pos": [
        10353,
        10374
      ]
    },
    {
      "content": "In this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file.",
      "pos": [
        10375,
        10497
      ]
    },
    {
      "content": "Open a text editor and paste the following code:",
      "pos": [
        10503,
        10551
      ]
    },
    {
      "pos": [
        11353,
        11401
      ],
      "content": "Save the file with the name <bpt id=\"p1\">**</bpt>SimpleApp.scala<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Build the Scala program",
      "pos": [
        11408,
        11431
      ]
    },
    {
      "content": "In this section, you use the <ph id=\"ph1\">&lt;a href=\"http://www.scala-sbt.org/0.13/docs/index.html\" target=\"_blank\"&gt;</ph>Simple Build Tool<ph id=\"ph2\">&lt;/a&gt;</ph> (or sbt) to build the Scala program.",
      "pos": [
        11432,
        11591
      ]
    },
    {
      "content": "sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.",
      "pos": [
        11592,
        11718
      ]
    },
    {
      "content": "Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html.",
      "pos": [
        11723,
        11810
      ]
    },
    {
      "content": "Create a folder called <bpt id=\"p1\">**</bpt>SimpleScalaApp<ept id=\"p1\">**</ept>, and within this folder create a file called <bpt id=\"p2\">**</bpt>simple.sbt<ept id=\"p2\">**</ept>.",
      "pos": [
        11814,
        11916
      ]
    },
    {
      "content": "This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:",
      "pos": [
        11917,
        12081
      ]
    },
    {
      "pos": [
        12341,
        12530
      ],
      "content": "Under the <bpt id=\"p1\">**</bpt>SimpleScalaApp<ept id=\"p1\">**</ept> folder, create a directory structure <bpt id=\"p2\">**</bpt>\\src\\main\\scala<ept id=\"p2\">**</ept> and paste the Scala program (<bpt id=\"p3\">**</bpt>SimpleApp.scala<ept id=\"p3\">**</ept>) you created earlier under the \\src\\main\\scala folder."
    },
    {
      "content": "Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:",
      "pos": [
        12534,
        12631
      ]
    },
    {
      "content": "Run the job on the cluster",
      "pos": [
        12838,
        12864
      ]
    },
    {
      "content": "In this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder.",
      "pos": [
        12865,
        12992
      ]
    },
    {
      "content": "You then use the <bpt id=\"p1\">**</bpt>spark-submit<ept id=\"p1\">**</ept> command to submit the job on the cluster.",
      "pos": [
        12993,
        13068
      ]
    },
    {
      "content": "Remote into the cluster that has Spark installed.",
      "pos": [
        13073,
        13122
      ]
    },
    {
      "content": "From the computer where you wrote and built the SimpleApp.scala program, copy the <bpt id=\"p1\">**</bpt>SimpleScalaApp\\target<ept id=\"p1\">**</ept> folder and paste it to a location on the cluster.",
      "pos": [
        13123,
        13280
      ]
    },
    {
      "pos": [
        13284,
        13420
      ],
      "content": "In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the <bpt id=\"p1\">**</bpt>target<ept id=\"p1\">**</ept> folder."
    },
    {
      "content": "Enter the following command to run the SimpleApp.scala program:",
      "pos": [
        13424,
        13487
      ]
    },
    {
      "content": "When the program finishes running, the output is displayed on the console.",
      "pos": [
        13620,
        13694
      ]
    },
    {
      "pos": [
        13750,
        13840
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"usingPS\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Install Spark on HDInsight Hadoop clusters by using Azure PowerShell"
    },
    {
      "content": "In this section, we use the <bpt id=\"p1\">**</bpt><ph id=\"ph1\">&lt;a href = \"http://msdn.microsoft.com/library/dn858088.aspx\" target=\"_blank\"&gt;</ph>Add-AzureHDInsightScriptAction<ph id=\"ph2\">&lt;/a&gt;</ph><ept id=\"p1\">**</ept> cmdlet to invoke scripts by using Script Action to customize a cluster.",
      "pos": [
        13842,
        14056
      ]
    },
    {
      "content": "Before proceeding, make sure you have installed and configured Azure PowerShell.",
      "pos": [
        14057,
        14137
      ]
    },
    {
      "content": "For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see <bpt id=\"p1\">[</bpt>Install and configure Azure PowerShell<ept id=\"p1\">][powershell-install-configure]</ept>.",
      "pos": [
        14138,
        14305
      ]
    },
    {
      "content": "Perform the following steps:",
      "pos": [
        14307,
        14335
      ]
    },
    {
      "content": "Open an Azure PowerShell window and declare the following variables:",
      "pos": [
        14340,
        14408
      ]
    },
    {
      "content": "Specify the configuration values such as nodes in the cluster and the default storage to be used.",
      "pos": [
        15198,
        15295
      ]
    },
    {
      "content": "Use the <bpt id=\"p1\">**</bpt>Add-AzureHDInsightScriptAction<ept id=\"p1\">**</ept> cmdlet to add a script action to cluster configuration.",
      "pos": [
        15734,
        15832
      ]
    },
    {
      "content": "Later, when the cluster is being created, the script action gets executed.",
      "pos": [
        15833,
        15907
      ]
    },
    {
      "pos": [
        16190,
        16263
      ],
      "content": "<bpt id=\"p1\">**</bpt>Add-AzureHDInsightScriptAction<ept id=\"p1\">**</ept> cmdlet takes the following parameters:"
    },
    {
      "pos": [
        16269,
        18695
      ],
      "content": "<table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\">\n <tr>\n <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Parameter</th>\n <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;\">Definition</th></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Config</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\">The configuration object to which script action information is added.</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name of the script action.</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">ClusterRoleCollection</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the nodes on which the customization script is run. The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes). You can use either or both values.</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Uri</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the URI to the script that is executed.</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters required by the script. The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.\n </td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Parameter",
          "pos": [
            264,
            273
          ]
        },
        {
          "content": "Definition",
          "pos": [
            432,
            442
          ]
        },
        {
          "content": "Config",
          "pos": [
            580,
            586
          ]
        },
        {
          "content": "The configuration object to which script action information is added.",
          "pos": [
            732,
            801
          ]
        },
        {
          "content": "Name",
          "pos": [
            939,
            943
          ]
        },
        {
          "content": "Name of the script action.",
          "pos": [
            1070,
            1096
          ]
        },
        {
          "content": "ClusterRoleCollection",
          "pos": [
            1234,
            1255
          ]
        },
        {
          "content": "Specifies the nodes on which the customization script is run. The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes). You can use either or both values.",
          "pos": [
            1382,
            1586
          ],
          "nodes": [
            {
              "content": "Specifies the nodes on which the customization script is run.",
              "pos": [
                0,
                61
              ]
            },
            {
              "content": "The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes).",
              "pos": [
                62,
                169
              ]
            },
            {
              "content": "You can use either or both values.",
              "pos": [
                170,
                204
              ]
            }
          ]
        },
        {
          "content": "Uri",
          "pos": [
            1724,
            1727
          ]
        },
        {
          "content": "Specifies the URI to the script that is executed.",
          "pos": [
            1854,
            1903
          ]
        },
        {
          "content": "Parameters",
          "pos": [
            2041,
            2051
          ]
        },
        {
          "content": "Parameters required by the script. The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.",
          "pos": [
            2178,
            2344
          ],
          "nodes": [
            {
              "content": "Parameters required by the script.",
              "pos": [
                0,
                34
              ]
            },
            {
              "content": "The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.",
              "pos": [
                35,
                166
              ]
            }
          ]
        }
      ]
    },
    {
      "content": "Finally, start provisioning a customized cluster with Spark installed.",
      "pos": [
        18704,
        18774
      ]
    },
    {
      "content": "When prompted, enter the credentials for the cluster.",
      "pos": [
        18951,
        19004
      ]
    },
    {
      "content": "It can take several minutes before the cluster is created.",
      "pos": [
        19005,
        19063
      ]
    },
    {
      "pos": [
        19069,
        19156
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"usingSDK\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Install Spark on HDInsight Hadoop clusters by using the .NET SDK"
    },
    {
      "content": "The HDInsight .NET SDK provides .NET client libraries that make it easier to work with HDInsight from a .NET Framework application.",
      "pos": [
        19158,
        19289
      ]
    },
    {
      "content": "This section provides instructions on how to use Script Action from the SDK to provision a cluster that has Spark installed.",
      "pos": [
        19290,
        19414
      ]
    },
    {
      "content": "The following procedures must be performed:",
      "pos": [
        19415,
        19458
      ]
    },
    {
      "content": "Install the HDInsight .NET SDK",
      "pos": [
        19462,
        19492
      ]
    },
    {
      "content": "Create a self-signed certificate",
      "pos": [
        19495,
        19527
      ]
    },
    {
      "content": "Create a console application",
      "pos": [
        19530,
        19558
      ]
    },
    {
      "content": "Run the application",
      "pos": [
        19561,
        19580
      ]
    },
    {
      "content": "To install the HDInsight .NET SDK",
      "pos": [
        19585,
        19618
      ]
    },
    {
      "content": "You can install latest published build of the SDK from <bpt id=\"p1\">[</bpt>NuGet<ept id=\"p1\">](http://nuget.codeplex.com/wikipage?title=Getting%20Started)</ept>.",
      "pos": [
        19622,
        19745
      ]
    },
    {
      "content": "The instructions will be shown in the next procedure.",
      "pos": [
        19746,
        19799
      ]
    },
    {
      "content": "To create a self-signed certificate",
      "pos": [
        19803,
        19838
      ]
    },
    {
      "content": "Create a self-signed certificate, install it on your workstation, and upload it to your Azure subscription.",
      "pos": [
        19842,
        19949
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Create a self-signed certificate<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=511138)</ept>.",
      "pos": [
        19950,
        20054
      ]
    },
    {
      "content": "To create a Visual Studio application",
      "pos": [
        20060,
        20097
      ]
    },
    {
      "content": "Open Visual Studio 2013.",
      "pos": [
        20104,
        20128
      ]
    },
    {
      "pos": [
        20133,
        20199
      ],
      "content": "From the <bpt id=\"p1\">**</bpt>File<ept id=\"p1\">**</ept> menu, click <bpt id=\"p2\">**</bpt>New<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Project<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        20204,
        20262
      ],
      "content": "From <bpt id=\"p1\">**</bpt>New Project<ept id=\"p1\">**</ept>, type or select the following values:"
    },
    {
      "pos": [
        20272,
        21664
      ],
      "content": "<table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\">\n <tr>\n <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Property</th>\n <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Value</th></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Category</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\">Templates/Visual C#/Windows</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Template</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Console Application</td></tr>\n <tr>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name</td>\n <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">CreateSparkCluster</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            264,
            272
          ]
        },
        {
          "content": "Value",
          "pos": [
            430,
            435
          ]
        },
        {
          "content": "Category",
          "pos": [
            573,
            581
          ]
        },
        {
          "content": "Templates/Visual C#/Windows",
          "pos": [
            727,
            754
          ]
        },
        {
          "content": "Template",
          "pos": [
            892,
            900
          ]
        },
        {
          "content": "Console Application",
          "pos": [
            1027,
            1046
          ]
        },
        {
          "content": "Name",
          "pos": [
            1184,
            1188
          ]
        },
        {
          "content": "CreateSparkCluster",
          "pos": [
            1315,
            1333
          ]
        }
      ]
    },
    {
      "pos": [
        21669,
        21704
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> to create the project."
    },
    {
      "pos": [
        21709,
        21810
      ],
      "content": "From the <bpt id=\"p1\">**</bpt>Tools<ept id=\"p1\">**</ept> menu, click <bpt id=\"p2\">**</bpt>Nuget Package Manager<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Package Manager Console<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Run the following command in the console to install the package:",
      "pos": [
        21815,
        21879
      ]
    },
    {
      "content": "This command adds the .NET libraries and references to them from the current Visual Studio project.",
      "pos": [
        21954,
        22053
      ]
    },
    {
      "pos": [
        22058,
        22121
      ],
      "content": "From Solution Explorer, double-click <bpt id=\"p1\">**</bpt>Program.cs<ept id=\"p1\">**</ept> to open it."
    },
    {
      "content": "Add the following using statements to the top of the file:",
      "pos": [
        22126,
        22184
      ]
    },
    {
      "content": "In the Main() function, copy and paste the following code, and provide values for the variables:",
      "pos": [
        22470,
        22566
      ]
    },
    {
      "pos": [
        24213,
        24459
      ],
      "content": "Append the following code to the Main() function to use the <bpt id=\"p1\">[</bpt>ScriptAction<ept id=\"p1\">](http://msdn.microsoft.com/library/microsoft.windowsazure.management.hdinsight.clusterprovisioning.data.scriptaction.aspx)</ept> class to invoke a custom script to install Spark."
    },
    {
      "content": "Finally, create the cluster.",
      "pos": [
        24970,
        24998
      ]
    },
    {
      "content": "Save changes to the application and build the solution.",
      "pos": [
        25048,
        25103
      ]
    },
    {
      "content": "To run the application",
      "pos": [
        25108,
        25130
      ]
    },
    {
      "content": "Open an Azure PowerShell console, navigate to the location where you saved the Visual Studio project, navigate to the \\bin\\debug directory within the project, and then run the following command:",
      "pos": [
        25134,
        25328
      ]
    },
    {
      "content": "Provide a cluster name and press ENTER to provision a cluster with Spark installed.",
      "pos": [
        25371,
        25454
      ]
    },
    {
      "content": "See also",
      "pos": [
        25460,
        25468
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install R on HDInsight clusters<ept id=\"p1\">][hdinsight-install-r]</ept> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.",
      "pos": [
        25473,
        25636
      ]
    },
    {
      "content": "R is an open-source language and environment for statistical computing.",
      "pos": [
        25637,
        25708
      ]
    },
    {
      "content": "It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.",
      "pos": [
        25709,
        25865
      ]
    },
    {
      "content": "It also provides extensive graphical capabilities.",
      "pos": [
        25866,
        25916
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-giraph-install.md)</ept>.",
      "pos": [
        25919,
        25994
      ]
    },
    {
      "content": "Use cluster customization to install Giraph on HDInsight Hadoop clusters.",
      "pos": [
        25995,
        26068
      ]
    },
    {
      "content": "Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
      "pos": [
        26069,
        26169
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-solr-install.md)</ept>.",
      "pos": [
        26172,
        26243
      ]
    },
    {
      "content": "Use cluster customization to install Solr on HDInsight Hadoop clusters.",
      "pos": [
        26244,
        26315
      ]
    },
    {
      "content": "Solr allows you to perform powerful search operations on data stored.",
      "pos": [
        26316,
        26385
      ]
    },
    {
      "content": "test",
      "pos": [
        26637,
        26641
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Use Script Action to install Spark on Hadoop cluster | Microsoft Azure\" \n    description=\"Learn how to customize an HDInsight cluster with Spark. You'll use a Script Action configuration option to use a script to install Spark.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/11/2015\" \n    ms.author=\"nitinme\"/>\n\n# Install and use Spark on HDInsight Hadoop clusters\n\nYou can install Spark on any type of cluster in Hadoop on Azure HDInsight by using **Script Action** cluster customization. Script Action lets you run scripts to customize a cluster, only when the cluster is being created. For more information, see [Customize HDInsight cluster using Script Action][hdinsight-cluster-customize].\n\nIn this topic, you will learn how to install Spark by using Script Action. Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.\n\n> [AZURE.NOTE] HDInsight now provides Spark as a first-class cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster. Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1. For more information, see [Get Started with Apache Spark on HDInsight](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md).\n\n\n## <a name=\"whatis\"></a>What is Spark?\n\n<a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.\n\nSpark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark. \n\nThis topic provides instructions on how to customize an HDInsight cluster to install Spark.\n\n## <a name=\"whatis\"></a>Which version of Spark can I install?\n\nIn this topic, we use a Script Action custom script to install Spark on an HDInsight cluster. This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you provision.\n\n- If you use the script while provisioning an **HDInsight 3.2** cluster, it installs **Spark 1.2.0**.\n- If you use the script while provisioning an **HDInsight 3.1** cluster, it installs **Spark 1.0.2**. \n\nYou can modify this script or create your own script to install other versions of Spark.\n\n\n## <a name=\"install\"></a>How do I install Spark?\n\nA sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at [https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1). This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal. \n\n> [AZURE.NOTE] The sample script works only with HDInsight 3.1 and 3.2 clusters. For more information on HDInsight cluster versions, see [HDInsight cluster versions](hdinsight-component-versioning.md).\n\n1. Start provisioning a cluster by using the **CUSTOM CREATE** option, as described at [Provisioning a cluster using custom options](hdinsight-provision-clusters.md#portal). Pick the cluster version depending on the following:\n\n    - If you want to install **Spark 1.2.0**, provision an HDInsight 3.2 cluster.\n    - If you want to install **Spark 1.0.2**, provision an HDInsight 3.1 cluster.\n\n\n2. On the **Script Actions** page of the wizard, click **add script action** to provide details about the script action, as shown below:\n\n    ![Use Script Action to customize a cluster](./media/hdinsight-hadoop-spark-install/HDI.CustomProvision.Page6.png \"Use Script Action to customize a cluster\")\n    \n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>Name</td>\n            <td>Specify a name for the script action. For example, <b>Install Spark</b>.</td></tr>\n        <tr><td>Script URI</td>\n            <td>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster. For example, <i>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1</i></td></tr>\n        <tr><td>Node Type</td>\n            <td>Specify the nodes on which the customization script is run. You can choose <b>All nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes only</b>.\n        <tr><td>Parameters</td>\n            <td>Specify the parameters, if required by the script. The script to install Spark does not require any parameters so you can leave this blank.</td></tr>\n    </table>    \n\n    You can add more than one script action to install multiple components on the cluster. After you have added the scripts, click the checkmark to start provisioning the cluster.\n\nYou can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK. Instructions for these procedures are provided later in this topic.\n\n## <a name=\"usespark\"></a>How do I use Spark in HDInsight?\nSpark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. This section provides instructions on how to use the different approaches to work with Spark:\n\n- [Using the Spark shell to run interactive queries](#sparkshell)\n- [Using the Spark shell to run Spark SQL queries](#sparksql) \n- [Using a standalone Scala program](#standalone)\n\n###<a name=\"sparkshell\"></a>Using the Spark shell to run interactive queries\nPerform the following steps to run Spark queries from an interactive Spark shell. In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.\n\n1. From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see <a href=\"http://azure.microsoft.com/documentation/articles/hdinsight-administer-use-management-portal/#rdp\" target=\"_blank\">Connect to HDInsight clusters using RDP</a>.\n\n2. In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, **C:\\apps\\dist\\spark-1.2.0**.\n\n\n3. Run the following command to start the Spark shell:\n\n         .\\bin\\spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n5. On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.\n\n        val file = sc.textFile(\"/example/data/gutenberg/davinci.txt\")\n        val counts = file.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n        counts.toArray().foreach(println)\n\n6. The output should resemble the following:\n\n    ![Output from running Scala interactive shell in an HDInsight cluster](./media/hdinsight-hadoop-spark-install/hdi-scala-interactive.png)\n        \n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n###<a name=\"sparksql\"></a>Using the Spark shell to run Spark SQL queries\n\nSpark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called **hivesampletable**) is available by default when you provision a cluster.\n\n>[AZURE.NOTE] The sample below was created against **Spark 1.2.0**, which is installed if you run the script action while provisioning HDInsight 3.2 cluster.\n\n1. From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see <a href=\"http://azure.microsoft.com/documentation/articles/hdinsight-administer-use-management-portal/#rdp\" target=\"_blank\">Connect to HDInsight clusters using RDP</a>.\n\n2. In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, **C:\\apps\\dist\\spark-1.2.0**.\n\n\n3. Run the following command to start the Spark shell:\n\n         .\\bin\\spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n4. On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.\n\n        val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n\n    Note that **sc** is default Spark context that is set when you start the Spark shell.\n\n5. Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.\n\n        hiveContext.sql(\"\"\"SELECT * FROM hivesampletable WHERE devicemake LIKE \"HTC%\" LIMIT 20\"\"\").collect().foreach(println)\n\n6. You should see an output like the following:\n\n    ![Output from running Spark SQL on an HDInsight cluster](./media/hdinsight-hadoop-spark-install/hdi-spark-sql.png)\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n### <a name=\"standalone\"></a>Using a standalone Scala program\n\nIn this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default. To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:\n\n- Write a Scala program\n- Build the Scala program to get the .jar file\n- Run the job on the cluster\n\n#### Write a Scala program\nIn this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file. \n\n1. Open a text editor and paste the following code:\n\n\n        /* SimpleApp.scala */\n        import org.apache.spark.SparkContext\n        import org.apache.spark.SparkContext._\n        import org.apache.spark.SparkConf\n        \n        object SimpleApp {\n          def main(args: Array[String]) {\n            val logFile = \"/example/data/gutenberg/davinci.txt\"         //Location of the sample data file on Azure Blob storage\n            val conf = new SparkConf().setAppName(\"SimpleApplication\")\n            val sc = new SparkContext(conf)\n            val logData = sc.textFile(logFile, 2).cache()\n            val numAs = logData.filter(line => line.contains(\"a\")).count()\n            val numBs = logData.filter(line => line.contains(\"b\")).count()\n            println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n          }\n        }\n\n2. Save the file with the name **SimpleApp.scala**.\n\n#### Build the Scala program\nIn this section, you use the <a href=\"http://www.scala-sbt.org/0.13/docs/index.html\" target=\"_blank\">Simple Build Tool</a> (or sbt) to build the Scala program. sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.\n\n1. Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html.\n2. Create a folder called **SimpleScalaApp**, and within this folder create a file called **simple.sbt**. This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:\n\n\n        name := \"SimpleApp\"\n    \n        version := \"1.0\"\n    \n        scalaVersion := \"2.10.4\"\n    \n        libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n\n\n\n    >[AZURE.NOTE] Make sure you retain the empty lines in the file.\n\n    \n3. Under the **SimpleScalaApp** folder, create a directory structure **\\src\\main\\scala** and paste the Scala program (**SimpleApp.scala**) you created earlier under the \\src\\main\\scala folder.\n4. Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:\n\n\n        sbt package\n\n\n    Once the application is compiled, you will see a **simpleapp_2.10-1.0.jar** file created under the **\\target\\scala-2.10** directory within the root SimpleScalaApp folder.\n\n\n#### Run the job on the cluster\nIn this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder. You then use the **spark-submit** command to submit the job on the cluster.\n\n1. Remote into the cluster that has Spark installed. From the computer where you wrote and built the SimpleApp.scala program, copy the **SimpleScalaApp\\target** folder and paste it to a location on the cluster.\n2. In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the **target** folder.\n3. Enter the following command to run the SimpleApp.scala program:\n\n\n        C:\\apps\\dist\\spark-1.2.0\\bin\\spark-submit --class \"SimpleApp\" --master local target/scala-2.10/simpleapp_2.10-1.0.jar\n\n4. When the program finishes running, the output is displayed on the console.\n\n\n        Lines with a: 21374, Lines with b: 11430\n\n## <a name=\"usingPS\"></a>Install Spark on HDInsight Hadoop clusters by using Azure PowerShell\n\nIn this section, we use the **<a href = \"http://msdn.microsoft.com/library/dn858088.aspx\" target=\"_blank\">Add-AzureHDInsightScriptAction</a>** cmdlet to invoke scripts by using Script Action to customize a cluster. Before proceeding, make sure you have installed and configured Azure PowerShell. For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see [Install and configure Azure PowerShell][powershell-install-configure].\n\nPerform the following steps:\n\n1. Open an Azure PowerShell window and declare the following variables:\n\n        # Provide values for these variables\n        $subscriptionName = \"<SubscriptionName>\"        # Name of the Azure subscription\n        $clusterName = \"<HDInsightClusterName>\"         # HDInsight cluster name\n        $storageAccountName = \"<StorageAccountName>\"    # Azure Storage account that hosts the default container\n        $storageAccountKey = \"<StorageAccountKey>\"      # Key for the Storage account\n        $containerName = $clusterName\n        $location = \"<MicrosoftDataCenter>\"             # Location of the HDInsight cluster. It must be in the same data center as the Storage account.\n        $clusterNodes = <ClusterSizeInNumbers>          # Number of nodes in the HDInsight cluster\n        $version = \"<HDInsightClusterVersion>\"          # For example, \"3.2\"\n    \n2. Specify the configuration values such as nodes in the cluster and the default storage to be used.\n\n        # Specify the configuration options\n        Select-AzureSubscription $subscriptionName\n        $config = New-AzureHDInsightClusterConfig -ClusterSizeInNodes $clusterNodes\n        $config.DefaultStorageAccount.StorageAccountName=\"$storageAccountName.blob.core.windows.net\"\n        $config.DefaultStorageAccount.StorageAccountKey=$storageAccountKey\n        $config.DefaultStorageAccount.StorageContainerName=$containerName\n    \n3. Use the **Add-AzureHDInsightScriptAction** cmdlet to add a script action to cluster configuration. Later, when the cluster is being created, the script action gets executed. \n\n        # Add a script action to the cluster configuration\n        $config = Add-AzureHDInsightScriptAction -Config $config -Name \"Install Spark\" -ClusterRoleCollection HeadNode -Uri https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1\n\n    **Add-AzureHDInsightScriptAction** cmdlet takes the following parameters:\n\n    <table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\">\n    <tr>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Parameter</th>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;\">Definition</th></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Config</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\">The configuration object to which script action information is added.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name of the script action.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">ClusterRoleCollection</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the nodes on which the customization script is run. The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes). You can use either or both values.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Uri</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the URI to the script that is executed.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters required by the script. The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.\n    </td></tr>\n    </table>\n    \n4. Finally, start provisioning a customized cluster with Spark installed.  \n    \n        # Start provisioning a cluster with Spark installed\n        New-AzureHDInsightCluster -Config $config -Name $clusterName -Location $location -Version $version \n\nWhen prompted, enter the credentials for the cluster. It can take several minutes before the cluster is created.\n\n\n## <a name=\"usingSDK\"></a>Install Spark on HDInsight Hadoop clusters by using the .NET SDK\n\nThe HDInsight .NET SDK provides .NET client libraries that make it easier to work with HDInsight from a .NET Framework application. This section provides instructions on how to use Script Action from the SDK to provision a cluster that has Spark installed. The following procedures must be performed:\n\n- Install the HDInsight .NET SDK\n- Create a self-signed certificate\n- Create a console application\n- Run the application\n\n\n**To install the HDInsight .NET SDK**\n\nYou can install latest published build of the SDK from [NuGet](http://nuget.codeplex.com/wikipage?title=Getting%20Started). The instructions will be shown in the next procedure.\n\n**To create a self-signed certificate**\n\nCreate a self-signed certificate, install it on your workstation, and upload it to your Azure subscription. For instructions, see [Create a self-signed certificate](http://go.microsoft.com/fwlink/?LinkId=511138). \n\n\n**To create a Visual Studio application**\n\n1. Open Visual Studio 2013.\n\n2. From the **File** menu, click **New**, and then click **Project**.\n\n3. From **New Project**, type or select the following values:\n    \n    <table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\">\n    <tr>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Property</th>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Value</th></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Category</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\">Templates/Visual C#/Windows</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Template</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Console Application</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">CreateSparkCluster</td></tr>\n    </table>\n\n4. Click **OK** to create the project.\n\n5. From the **Tools** menu, click **Nuget Package Manager**, and then click **Package Manager Console**.\n\n6. Run the following command in the console to install the package:\n\n        Install-Package Microsoft.WindowsAzure.Management.HDInsight\n\n    This command adds the .NET libraries and references to them from the current Visual Studio project.\n\n7. From Solution Explorer, double-click **Program.cs** to open it.\n\n8. Add the following using statements to the top of the file:\n\n        using System.Security.Cryptography.X509Certificates;\n        using Microsoft.WindowsAzure.Management.HDInsight;\n        using Microsoft.WindowsAzure.Management.HDInsight.ClusterProvisioning;\n        using Microsoft.WindowsAzure.Management.HDInsight.Framework.Logging;\n    \n9. In the Main() function, copy and paste the following code, and provide values for the variables:\n        \n        var clusterName = args[0];\n\n        // Provide values for the variables\n        string thumbprint = \"<CertificateThumbprint>\";  \n        string subscriptionId = \"<AzureSubscriptionID>\";\n        string location = \"<MicrosoftDataCenterLocation>\";\n        string storageaccountname = \"<AzureStorageAccountName>.blob.core.windows.net\";\n        string storageaccountkey = \"<AzureStorageAccountKey>\";\n        string username = \"<HDInsightUsername>\";\n        string password = \"<HDInsightUserPassword>\";\n        int clustersize = <NumberOfNodesInTheCluster>;\n\n        // Provide the certificate thumbprint to retrieve the certificate from the certificate store \n        X509Store store = new X509Store();\n        store.Open(OpenFlags.ReadOnly);\n        X509Certificate2 cert = store.Certificates.Cast<X509Certificate2>().First(item => item.Thumbprint == thumbprint);\n\n        // Create an HDInsight client object\n        HDInsightCertificateCredential creds = new HDInsightCertificateCredential(new Guid(subscriptionId), cert);\n        var client = HDInsightClient.Connect(creds);\n        client.IgnoreSslErrors = true;\n        \n        // Provide the cluster information\n        var clusterInfo = new ClusterCreateParameters()\n        {\n            Name = clusterName,\n            Location = location,\n            DefaultStorageAccountName = storageaccountname,\n            DefaultStorageAccountKey = storageaccountkey,\n            DefaultStorageContainer = clusterName,\n            UserName = username,\n            Password = password,\n            ClusterSizeInNodes = clustersize,\n            Version = \"3.2\"\n        };        \n\n10. Append the following code to the Main() function to use the [ScriptAction](http://msdn.microsoft.com/library/microsoft.windowsazure.management.hdinsight.clusterprovisioning.data.scriptaction.aspx) class to invoke a custom script to install Spark.\n\n        // Add the script action to install Spark\n        clusterInfo.ConfigActions.Add(new ScriptAction(\n          \"Install Spark\", // Name of the config action\n          new ClusterNodeType[] { ClusterNodeType.HeadNode }, // List of nodes to install Spark on\n          new Uri(\"https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1\"), // Location of the script to install Spark.\n          null //Because the script used does not require any parameters\n        ));\n\n11. Finally, create the cluster.\n\n        client.CreateCluster(clusterInfo);\n\n11. Save changes to the application and build the solution. \n\n**To run the application**\n\nOpen an Azure PowerShell console, navigate to the location where you saved the Visual Studio project, navigate to the \\bin\\debug directory within the project, and then run the following command:\n\n    .\\CreateSparkCluster <cluster-name>\n\nProvide a cluster name and press ENTER to provision a cluster with Spark installed.\n\n\n## See also##\n- [Install R on HDInsight clusters][hdinsight-install-r] provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.\n- [Install Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install.md). Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.\n- [Install Solr on HDInsight clusters](hdinsight-hadoop-solr-install.md). Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.\n\n\n\n\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-install-r]: hdinsight-hadoop-r-scripts.md\n[hdinsight-cluster-customize]: hdinsight-hadoop-customize-cluster.md\n[powershell-install-configure]: ../install-configure-powershell.md\n \ntest\n"
}