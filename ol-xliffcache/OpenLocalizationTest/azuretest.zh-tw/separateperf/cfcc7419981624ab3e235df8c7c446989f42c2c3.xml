{
  "nodes": [
    {
      "content": "Working with Channels that Receive Multi-bitrate Live Stream from On-premises Encoders",
      "pos": [
        28,
        114
      ]
    },
    {
      "content": "This topic describes how to set up a Channel that receives a multi-bitrate live stream from an on-premises encoder.",
      "pos": [
        134,
        249
      ]
    },
    {
      "content": "The stream can then be delivered to client playback applications through one or more Streaming Endpoints, using one of the following adaptive streaming protocols: HLS, Smooth Stream, MPEG DASH, HDS.",
      "pos": [
        250,
        448
      ]
    },
    {
      "content": "Working with Channels that Receive Multi-bitrate Live Stream from On-premises Encoders",
      "pos": [
        759,
        845
      ]
    },
    {
      "content": "Overview",
      "pos": [
        849,
        857
      ]
    },
    {
      "content": "In Azure Media Services, a <bpt id=\"p1\">**</bpt>Channel<ept id=\"p1\">**</ept> represents a pipeline for processing live streaming content.",
      "pos": [
        859,
        958
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>Channel<ept id=\"p1\">**</ept> receives live input streams in the following way:",
      "pos": [
        959,
        1022
      ]
    },
    {
      "content": "An on-premises live encoder sends multi-bitrate <bpt id=\"p1\">**</bpt>RTMP<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>Smooth Streaming<ept id=\"p2\">**</ept> (Fragmented MP4) to the Channel.",
      "pos": [
        1026,
        1139
      ]
    },
    {
      "content": "You can use the following live encoders that output multi-bitrate Smooth Streaming: Elemental, Envivio, Cisco.",
      "pos": [
        1140,
        1250
      ]
    },
    {
      "content": "The following live encoders output RTMP: Adobe Flash Live, Telestream Wirecast, and Tricaster transcoders.",
      "pos": [
        1252,
        1358
      ]
    },
    {
      "content": "The ingested streams pass through <bpt id=\"p1\">**</bpt>Channel<ept id=\"p1\">**</ept> without any further processing.",
      "pos": [
        1359,
        1436
      ]
    },
    {
      "content": "Your live encoder can lso send a single bitrate stream, but that is not recommended.",
      "pos": [
        1437,
        1521
      ]
    },
    {
      "content": "When requested, Media Services delivers the stream to customers.",
      "pos": [
        1522,
        1586
      ]
    },
    {
      "content": "The following diagram represents a live streaming workflow that uses an on-premises live encoder to output multi-bitrate RTMP or Fragmented MP4 (Smooth Streaming) streams.",
      "pos": [
        1589,
        1760
      ]
    },
    {
      "content": "Live workflow",
      "pos": [
        1765,
        1778
      ]
    },
    {
      "content": "This topic covers the following:",
      "pos": [
        1796,
        1828
      ]
    },
    {
      "content": "Common live streaming scenario",
      "pos": [
        1833,
        1863
      ]
    },
    {
      "content": "Description of a Channel and its related components",
      "pos": [
        1921,
        1972
      ]
    },
    {
      "content": "Considerations",
      "pos": [
        2029,
        2043
      ]
    },
    {
      "content": "Tasks related to Live Streaming",
      "pos": [
        2107,
        2138
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"scenario\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Common live streaming scenario",
      "pos": [
        2193,
        2244
      ]
    },
    {
      "content": "The following steps describe tasks involved in creating common live streaming applications.",
      "pos": [
        2245,
        2336
      ]
    },
    {
      "content": "Connect a video camera to a computer.",
      "pos": [
        2341,
        2378
      ]
    },
    {
      "content": "Launch and configure an on-premises live encoder that outputs a multi-bitrate RTMP or Fragmented MP4 (Smooth Streaming) stream.",
      "pos": [
        2379,
        2506
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Azure Media Services RTMP Support and Live Encoders<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=532824)</ept>.",
      "pos": [
        2507,
        2634
      ]
    },
    {
      "content": "This step could also be performed after you create your Channel.",
      "pos": [
        2644,
        2708
      ]
    },
    {
      "content": "Create and start a Channel.",
      "pos": [
        2713,
        2740
      ]
    },
    {
      "content": "Retrieve the Channel ingest URL.",
      "pos": [
        2744,
        2776
      ]
    },
    {
      "content": "The ingest URL is used by the live encoder to send the stream to the Channel.",
      "pos": [
        2783,
        2860
      ]
    },
    {
      "content": "Retrieve the Channel preview URL.",
      "pos": [
        2864,
        2897
      ]
    },
    {
      "content": "Use this URL to verify that your channel is properly receiving the live stream.",
      "pos": [
        2904,
        2983
      ]
    },
    {
      "content": "Create a program.",
      "pos": [
        2988,
        3005
      ]
    },
    {
      "content": "When using the Azure Management Portal, creating a program also creates an asset.",
      "pos": [
        3012,
        3093
      ]
    },
    {
      "content": "When using .NET SDK or REST you need to create an asset and specify to use this asset when creating a Program.",
      "pos": [
        3100,
        3210
      ]
    },
    {
      "content": "Publish the asset associated with the program.",
      "pos": [
        3215,
        3261
      ]
    },
    {
      "content": "Make sure to have at least one streaming reserved unit on the streaming endpoint from which you want to stream content.",
      "pos": [
        3270,
        3389
      ]
    },
    {
      "content": "Start the program when you are ready to start streaming and archiving.",
      "pos": [
        3393,
        3463
      ]
    },
    {
      "content": "Optionally, the live encoder can be signaled to start an advertisement.",
      "pos": [
        3467,
        3538
      ]
    },
    {
      "content": "The advertisement is inserted in the output stream.",
      "pos": [
        3539,
        3590
      ]
    },
    {
      "content": "Stop the program whenever you want to stop streaming and archiving the event.",
      "pos": [
        3594,
        3671
      ]
    },
    {
      "content": "Delete the Program (and optionally delete the asset).",
      "pos": [
        3675,
        3728
      ]
    },
    {
      "pos": [
        3735,
        3890
      ],
      "content": "The <bpt id=\"p1\">[</bpt>live streaming tasks<ept id=\"p1\">](media-services-manage-channels-overview.md#tasks)</ept> section links to topics that demonstrate how to achieve tasks described above."
    },
    {
      "pos": [
        3894,
        3965
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"channel\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Description of a Channel and its related components"
    },
    {
      "pos": [
        3970,
        4033
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"channel_input\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Channel input (ingest) configurations"
    },
    {
      "pos": [
        4039,
        4093
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"ingest_protocols\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Ingest streaming protocol"
    },
    {
      "content": "Media Services supports ingesting live feeds using the following streaming protocol:",
      "pos": [
        4095,
        4179
      ]
    },
    {
      "content": "Multi-bitrate Fragmented MP4",
      "pos": [
        4186,
        4214
      ]
    },
    {
      "content": "Multi-bitrate RTMP",
      "pos": [
        4223,
        4241
      ]
    },
    {
      "pos": [
        4250,
        4363
      ],
      "content": "When the <bpt id=\"p1\">**</bpt>RTMP<ept id=\"p1\">**</ept> ingest streaming protocol is selected, two ingest(input) endpoints are created for the channel:"
    },
    {
      "pos": [
        4374,
        4471
      ],
      "content": "<bpt id=\"p1\">**</bpt>Primary URL<ept id=\"p1\">**</ept>: Specifies the fully qualified URL of the channel's primary RTMP ingest endpoint."
    },
    {
      "pos": [
        4477,
        4589
      ],
      "content": "<bpt id=\"p1\">**</bpt>Secondary URL<ept id=\"p1\">**</ept> (optional): Specifies the fully qualified URL of the channel's secondary RTMP ingest endpoint."
    },
    {
      "pos": [
        6734,
        6881
      ],
      "content": "For information about RTMP live encoders, see <bpt id=\"p1\">[</bpt>Azure Media Services RTMP Support and Live Encoders<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=532824)</ept>."
    },
    {
      "content": "The following considerations apply:",
      "pos": [
        6883,
        6918
      ]
    },
    {
      "content": "Make sure you have sufficient free Internet connectivity to send data to the ingest points.",
      "pos": [
        6922,
        7013
      ]
    },
    {
      "content": "Using secondary ingest URL requires additional bandwidth.",
      "pos": [
        7017,
        7074
      ]
    },
    {
      "content": "The incoming multi-bitrate stream can have a maximum of 10 video quality levels (aka layers), and a maximum of 5 audio tracks.",
      "pos": [
        7078,
        7204
      ]
    },
    {
      "content": "The highest average bitrate for any of the video quality levels or layers should be below 10 Mbps.",
      "pos": [
        7207,
        7305
      ]
    },
    {
      "content": "The aggregate of the average bitrates for all the video and audio streams should be below 25 Mbps.",
      "pos": [
        7308,
        7406
      ]
    },
    {
      "content": "You cannot change the input protocol while the Channel or its associated programs are running.",
      "pos": [
        7409,
        7503
      ]
    },
    {
      "content": "If you require different protocols, you should create separate channels for each input protocol.",
      "pos": [
        7504,
        7600
      ]
    },
    {
      "content": "You can ingest a single bitrate into your channel, but since the stream is not processed by the channel, the client applications will also receive a single bitrate stream (this option is not recommended).",
      "pos": [
        7604,
        7808
      ]
    },
    {
      "content": "Ingest URLs (endpoints)",
      "pos": [
        7814,
        7837
      ]
    },
    {
      "content": "A Channel provides an input endpoint (ingest URL) that you specify in the live encoder, so the encoder can push streams to your channels.",
      "pos": [
        7840,
        7977
      ]
    },
    {
      "content": "You can get the ingest URLs when you create the channel.",
      "pos": [
        7982,
        8038
      ]
    },
    {
      "content": "To get these URLs, the channel does not have to be in the <bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept> state.",
      "pos": [
        8039,
        8115
      ]
    },
    {
      "content": "When you are ready to start pushing data into the channel, the channel must be in the <bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept> state.",
      "pos": [
        8116,
        8220
      ]
    },
    {
      "content": "Once the channel starts ingesting data, you can preview your stream through the preview URL.",
      "pos": [
        8221,
        8313
      ]
    },
    {
      "content": "You have an option of ingesting Fragmented MP4 (Smooth Streaming) live stream over an SSL connection.",
      "pos": [
        8315,
        8416
      ]
    },
    {
      "content": "To ingest over SSL, make sure to update the ingest URL to HTTPS.",
      "pos": [
        8417,
        8481
      ]
    },
    {
      "content": "Currently, you cannot ingest RTMP over SSL.",
      "pos": [
        8482,
        8525
      ]
    },
    {
      "pos": [
        8532,
        8579
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"keyframe_interval\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Keyframe interval"
    },
    {
      "content": "When using an on-premises live encoder to generate multi-bitrate stream, the keyframe interval specifies GOP duration (as used by that external encoder).",
      "pos": [
        8581,
        8734
      ]
    },
    {
      "content": "Once this incoming stream is received by the Channel, you can then deliver your live stream to client playback applications in any of the following formats: Smooth Streaming, DASH and HLS.",
      "pos": [
        8735,
        8923
      ]
    },
    {
      "content": "When doing live streaming, HLS is always packaged dynamically.",
      "pos": [
        8924,
        8986
      ]
    },
    {
      "content": "By default, Media Services automatically calculates HLS segment packaging ratio (fragments per segment) based on the keyframe interval, also referred to as Group of Pictures – GOP, that is received from the live encoder.",
      "pos": [
        8987,
        9207
      ]
    },
    {
      "content": "The following table shows how the segment duration is being calculated:",
      "pos": [
        9210,
        9281
      ]
    },
    {
      "content": "Keyframe Interval",
      "pos": [
        9283,
        9300
      ]
    },
    {
      "content": "HLS segment packaging ratio (FragmentsPerSegment)",
      "pos": [
        9301,
        9350
      ]
    },
    {
      "content": "Example",
      "pos": [
        9351,
        9358
      ]
    },
    {
      "content": "less than or equal to 3 seconds",
      "pos": [
        9371,
        9402
      ]
    },
    {
      "content": "3:1",
      "pos": [
        9403,
        9406
      ]
    },
    {
      "content": "If the KeyFrameInterval (or GOP) is 2 seconds long, the default HLS segment packaging ratio will be 3 to 1, which will create a 6 seconds HLS segment.",
      "pos": [
        9407,
        9557
      ]
    },
    {
      "content": "3 to 5  seconds",
      "pos": [
        9558,
        9573
      ]
    },
    {
      "content": "2:1",
      "pos": [
        9574,
        9577
      ]
    },
    {
      "content": "If the KeyFrameInterval (or GOP) is 4 seconds long, the default HLS segment packaging ratio will be 2 to 1, which will create a 8 seconds HLS segment.",
      "pos": [
        9578,
        9728
      ]
    },
    {
      "content": "greater than 5 seconds",
      "pos": [
        9729,
        9751
      ]
    },
    {
      "content": "1:1",
      "pos": [
        9752,
        9755
      ]
    },
    {
      "content": "If the KeyFrameInterval (or GOP) is 6 seconds long, the default HLS segment packaging ratio will be 1 to 1, which will create a 6 second long HLS segment.",
      "pos": [
        9756,
        9910
      ]
    },
    {
      "content": "You can change the fragments per segment ratio by configuring channel’s output and setting FragmentsPerSegment on ChannelOutputHls.",
      "pos": [
        9913,
        10044
      ]
    },
    {
      "content": "You can also change the keyframe interval value, by setting the KeyFrameInterval property on ChanneInput.",
      "pos": [
        10047,
        10152
      ]
    },
    {
      "content": "If you explicitly set the KeyFrameInterval, the HLS segment packaging ratio FragmentsPerSegment is calculated using the rules described above.",
      "pos": [
        10155,
        10297
      ]
    },
    {
      "content": "If you explicitly set both KeyFrameInterval and FragmentsPerSegment, Media Services will use the values set by you.",
      "pos": [
        10301,
        10416
      ]
    },
    {
      "content": "Allowed IP addresses",
      "pos": [
        10424,
        10444
      ]
    },
    {
      "content": "You can define the IP addresses that are allowed to publish video to this channel.",
      "pos": [
        10446,
        10528
      ]
    },
    {
      "content": "Allowed IP addresses can be specified as either a single IP address (e.g. ‘10.0.0.1’), an IP range using an IP address and a CIDR subnet mask (e.g. ‘10.0.0.1/22’), or an IP range using an IP address and a dotted decimal subnet mask (e.g. ‘10.0.0.1(255.255.252.0)’).",
      "pos": [
        10529,
        10794
      ]
    },
    {
      "content": "If no IP addresses are specified and there is no rule definition, then no IP address will be allowed.",
      "pos": [
        10797,
        10898
      ]
    },
    {
      "content": "To allow any IP address, create a rule and set 0.0.0.0/0.",
      "pos": [
        10899,
        10956
      ]
    },
    {
      "content": "Channel preview",
      "pos": [
        10961,
        10976
      ]
    },
    {
      "content": "Preview URLs",
      "pos": [
        10983,
        10995
      ]
    },
    {
      "content": "Channels provide a preview endpoint (preview URL) that you use to preview and validate your stream before further processing and delivery.",
      "pos": [
        10997,
        11135
      ]
    },
    {
      "content": "You can get the preview URL when you create the channel.",
      "pos": [
        11137,
        11193
      ]
    },
    {
      "content": "To get the URL, the channel does not have to be in the <bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept> state.",
      "pos": [
        11194,
        11267
      ]
    },
    {
      "content": "Once the Channel starts ingesting data, you can preview your stream.",
      "pos": [
        11270,
        11338
      ]
    },
    {
      "content": "Note that currently the preview stream can only be delivered in Fragmented MP4 (Smooth Streaming) format regardless of the specified input type.",
      "pos": [
        11340,
        11484
      ]
    },
    {
      "content": "You can use the <bpt id=\"p1\">[</bpt>http://smf.cloudapp.net/healthmonitor<ept id=\"p1\">](http://smf.cloudapp.net/healthmonitor)</ept> player to test the Smooth Stream.",
      "pos": [
        11485,
        11613
      ]
    },
    {
      "content": "You can also use a player hosted in the Azure Management Portal to view your stream.",
      "pos": [
        11614,
        11698
      ]
    },
    {
      "content": "Allowed IP Addresses",
      "pos": [
        11705,
        11725
      ]
    },
    {
      "content": "You can define the IP addresses that are allowed to connect to the preview endpoint.",
      "pos": [
        11727,
        11811
      ]
    },
    {
      "content": "If no IP addresses are specified any IP address will be allowed.",
      "pos": [
        11812,
        11876
      ]
    },
    {
      "content": "Allowed IP addresses can be specified as either a single IP address (e.g. ‘10.0.0.1’), an IP range using an IP address and a CIDR subnet mask (e.g. ‘10.0.0.1/22’), or an IP range using an IP address and a dotted decimal subnet mask (e.g. ‘10.0.0.1(255.255.252.0)’).",
      "pos": [
        11877,
        12142
      ]
    },
    {
      "content": "Channel output",
      "pos": [
        12147,
        12161
      ]
    },
    {
      "pos": [
        12163,
        12248
      ],
      "content": "For more information see the <bpt id=\"p1\">[</bpt>setting keyframe interval<ept id=\"p1\">](#keyframe_interval)</ept> section."
    },
    {
      "content": "Channel's programs",
      "pos": [
        12254,
        12272
      ]
    },
    {
      "content": "A channel is associated with programs that enable you to control the publishing and storage of segments in a live stream.",
      "pos": [
        12274,
        12395
      ]
    },
    {
      "content": "Channels manage Programs.",
      "pos": [
        12396,
        12421
      ]
    },
    {
      "content": "The Channel and Program relationship is very similar to traditional media where a channel has a constant stream of content and a program is scoped to some timed event on that channel.",
      "pos": [
        12422,
        12605
      ]
    },
    {
      "content": "You can specify the number of hours you want to retain the recorded content for the program by setting the <bpt id=\"p1\">**</bpt>Archive Window<ept id=\"p1\">**</ept> length.",
      "pos": [
        12607,
        12740
      ]
    },
    {
      "content": "This value can be set from a minimum of 5 minutes to a maximum of 25 hours.",
      "pos": [
        12741,
        12816
      ]
    },
    {
      "content": "Archive window length also dictates the maximum amount of time clients can seek back in time from the current live position.",
      "pos": [
        12817,
        12941
      ]
    },
    {
      "content": "Programs can run over the specified amount of time, but content that falls behind the window length is continuously discarded.",
      "pos": [
        12942,
        13068
      ]
    },
    {
      "content": "This value of this property also determines how long the client manifests can grow.",
      "pos": [
        13069,
        13152
      ]
    },
    {
      "content": "Each program is associated with an Asset which stores the streamed content.",
      "pos": [
        13154,
        13229
      ]
    },
    {
      "content": "An asset is mapped to a blob container in the Azure Storage account and the files in the asset are stored as blobs in that container.",
      "pos": [
        13230,
        13363
      ]
    },
    {
      "content": "To publish the program so your customers can view the stream you must create an OnDemand locator for the associated asset.",
      "pos": [
        13364,
        13486
      ]
    },
    {
      "content": "Having this locator will enable you to build a streaming URL that you can provide to your clients.",
      "pos": [
        13487,
        13585
      ]
    },
    {
      "content": "A channel supports up to three concurrently running programs so you can create multiple archives of the same incoming stream.",
      "pos": [
        13587,
        13712
      ]
    },
    {
      "content": "This allows you to publish and archive different parts of an event as needed.",
      "pos": [
        13713,
        13790
      ]
    },
    {
      "content": "For example, your business requirement is to archive 6 hours of a program, but to broadcast only last 10 minutes.",
      "pos": [
        13791,
        13904
      ]
    },
    {
      "content": "To accomplish this, you need to create two concurrently running programs.",
      "pos": [
        13905,
        13978
      ]
    },
    {
      "content": "One program is set to archive 6 hours of the event but the program is not published.",
      "pos": [
        13979,
        14063
      ]
    },
    {
      "content": "The other program is set to archive for 10 minutes and this program is published.",
      "pos": [
        14064,
        14145
      ]
    },
    {
      "content": "You should not reuse existing programs for new events.",
      "pos": [
        14147,
        14201
      ]
    },
    {
      "content": "Instead, create and start a new program for each event as described in the Programming Live Streaming Applications section.",
      "pos": [
        14202,
        14325
      ]
    },
    {
      "content": "Start the program when you are ready to start streaming and archiving.",
      "pos": [
        14327,
        14397
      ]
    },
    {
      "content": "Stop the program whenever you want to stop streaming and archiving the event.",
      "pos": [
        14398,
        14475
      ]
    },
    {
      "content": "To delete archived content, stop and delete the program and then delete the associated asset.",
      "pos": [
        14478,
        14571
      ]
    },
    {
      "content": "An asset cannot be deleted if it is used by a program; the program must be deleted first.",
      "pos": [
        14572,
        14661
      ]
    },
    {
      "content": "Even after you stop and delete the program, the users would be able to stream your archived content as a video on demand, for as long as you do not delete the asset.",
      "pos": [
        14664,
        14829
      ]
    },
    {
      "content": "If you do want to retain the archived content, but not have it available for streaming, delete the streaming locator.",
      "pos": [
        14831,
        14948
      ]
    },
    {
      "pos": [
        14952,
        15024
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"states\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Channel states and how states map to the billing mode"
    },
    {
      "content": "The current state of a Channel.",
      "pos": [
        15027,
        15058
      ]
    },
    {
      "content": "Possible values include:",
      "pos": [
        15059,
        15083
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Stopped<ept id=\"p1\">**</ept>.",
      "pos": [
        15087,
        15099
      ]
    },
    {
      "content": "This is the initial state of the Channel after its creation.",
      "pos": [
        15100,
        15160
      ]
    },
    {
      "content": "In this state, the Channel properties can be updated but streaming is not allowed.",
      "pos": [
        15161,
        15243
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Starting<ept id=\"p1\">**</ept>.",
      "pos": [
        15246,
        15259
      ]
    },
    {
      "content": "The Channel is being started.",
      "pos": [
        15260,
        15289
      ]
    },
    {
      "content": "No updates or streaming is allowed during this state.",
      "pos": [
        15290,
        15343
      ]
    },
    {
      "content": "If an error occurs, the Channel returns to the Stopped state.",
      "pos": [
        15344,
        15405
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept>.",
      "pos": [
        15408,
        15420
      ]
    },
    {
      "content": "The Channel is capable of processing live streams.",
      "pos": [
        15421,
        15471
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Stopping<ept id=\"p1\">**</ept>.",
      "pos": [
        15474,
        15487
      ]
    },
    {
      "content": "The Channel is being stopped.",
      "pos": [
        15488,
        15517
      ]
    },
    {
      "content": "No updates or streaming is allowed during this state.",
      "pos": [
        15518,
        15571
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Deleting<ept id=\"p1\">**</ept>.",
      "pos": [
        15574,
        15587
      ]
    },
    {
      "content": "The Channel is being deleted.",
      "pos": [
        15588,
        15617
      ]
    },
    {
      "content": "No updates or streaming is allowed during this state.",
      "pos": [
        15618,
        15671
      ]
    },
    {
      "content": "The following table shows how Channel states map to the billing mode.",
      "pos": [
        15673,
        15742
      ]
    },
    {
      "content": "Channel state|Portal UI Indicators|Billed?",
      "pos": [
        15746,
        15788
      ]
    },
    {
      "content": "---|---|---|---",
      "pos": [
        15789,
        15804
      ]
    },
    {
      "content": "Starting|Starting|No (transient state)",
      "pos": [
        15805,
        15843
      ]
    },
    {
      "content": "Running|Ready (no running programs)",
      "pos": [
        15844,
        15879
      ]
    },
    {
      "content": "or",
      "pos": [
        15882,
        15884
      ]
    },
    {
      "content": "Streaming (at least one running program)|Yes",
      "pos": [
        15887,
        15931
      ]
    },
    {
      "content": "Stopping|Stopping|No (transient state)",
      "pos": [
        15932,
        15970
      ]
    },
    {
      "content": "Stopped|Stopped|No",
      "pos": [
        15971,
        15989
      ]
    },
    {
      "content": "Closed Captioning and Ad Insertion",
      "pos": [
        15994,
        16028
      ]
    },
    {
      "content": "The following table demonstrates supported closed captioning and ad insertion standards.",
      "pos": [
        16031,
        16119
      ]
    },
    {
      "content": "Standard",
      "pos": [
        16121,
        16129
      ]
    },
    {
      "content": "Notes",
      "pos": [
        16130,
        16135
      ]
    },
    {
      "content": "CEA-708 and EIA-608 (708/608)",
      "pos": [
        16144,
        16173
      ]
    },
    {
      "content": "CEA-708 and EIA-608 are closed captioning standards for the United States and Canada.",
      "pos": [
        16174,
        16259
      ]
    },
    {
      "content": "Currently, captioning is only supported if carried in the encoded input stream.",
      "pos": [
        16265,
        16344
      ]
    },
    {
      "content": "You need to use a live media encoder that can insert 608 or 708 captions into the encoded stream which is sent to Media Services.",
      "pos": [
        16345,
        16474
      ]
    },
    {
      "content": "Media Services will deliver the content with inserted captions to your viewers.",
      "pos": [
        16475,
        16554
      ]
    },
    {
      "content": "TTML inside ismt (Smooth Streaming Text Tracks)",
      "pos": [
        16555,
        16602
      ]
    },
    {
      "content": "Media Services dynamic packaging enables your clients to stream content in any of the following formats: MPEG DASH, HLS or Smooth Streaming.",
      "pos": [
        16603,
        16743
      ]
    },
    {
      "content": "However, if you ingest fragmented MP4 (Smooth Streaming) with captions inside .ismt (Smooth Streaming text tracks), you would only be able to deliver the stream to Smooth Streaming clients.",
      "pos": [
        16744,
        16933
      ]
    },
    {
      "content": "SCTE-35",
      "pos": [
        16934,
        16941
      ]
    },
    {
      "content": "Digital signaling system used to cue advertising insertion.",
      "pos": [
        16942,
        17001
      ]
    },
    {
      "content": "Downstream receivers use the signal to splice advertising into the stream for the allotted time.",
      "pos": [
        17002,
        17098
      ]
    },
    {
      "content": "SCTE-35 must be sent as a sparse track in the input stream.",
      "pos": [
        17099,
        17158
      ]
    },
    {
      "content": "Note that currently, the only supported input stream format that carries ad signals is fragmented MP4 (Smooth Streaming).",
      "pos": [
        17164,
        17285
      ]
    },
    {
      "content": "The only supported output format is also Smooth Streaming.",
      "pos": [
        17286,
        17344
      ]
    },
    {
      "pos": [
        17349,
        17390
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"Considerations\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Considerations"
    },
    {
      "content": "When using an on-premises live encoder to send a multi-bitrate stream into a Channel, the following constraints apply:",
      "pos": [
        17392,
        17510
      ]
    },
    {
      "content": "Make sure you have sufficient free internet connectivity to send data to the ingest points.",
      "pos": [
        17514,
        17605
      ]
    },
    {
      "content": "The incoming multi-bitrate stream can have a maximum of 10 video quality levels (10 layers), and maximum of 5 audio tracks.",
      "pos": [
        17609,
        17732
      ]
    },
    {
      "content": "The highest average bitrate for any of the video quality levels or layers should be below 10 Mbps",
      "pos": [
        17735,
        17832
      ]
    },
    {
      "content": "The aggregate of the average bitrates for all the video and audio streams should be below 25 Mbps",
      "pos": [
        17835,
        17932
      ]
    },
    {
      "content": "You cannot change the input protocol while the Channel or its associated programs are running.",
      "pos": [
        17935,
        18029
      ]
    },
    {
      "content": "If you require different protocols, you should create separate channels for each input protocol.",
      "pos": [
        18030,
        18126
      ]
    },
    {
      "content": "Other considerations related to working with channels and related components:",
      "pos": [
        18130,
        18207
      ]
    },
    {
      "content": "Every time you reconfigure the live encoder, call the <bpt id=\"p1\">**</bpt>Reset<ept id=\"p1\">**</ept> method on the channel.",
      "pos": [
        18211,
        18297
      ]
    },
    {
      "content": "Before you reset the channel, you have to stop the program.",
      "pos": [
        18298,
        18357
      ]
    },
    {
      "content": "After you reset the channel, restart the program.",
      "pos": [
        18358,
        18407
      ]
    },
    {
      "content": "A channel can be stopped only when it is in the Running state, and all programs on the channel have been stopped.",
      "pos": [
        18411,
        18524
      ]
    },
    {
      "content": "By default you can only add 5 channels to your Media Services account.",
      "pos": [
        18527,
        18597
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Quotas and Limitations<ept id=\"p1\">](media-services-quotas-and-limitations.md)</ept>.",
      "pos": [
        18598,
        18691
      ]
    },
    {
      "content": "You cannot change the input protocol while the Channel or its associated programs are running.",
      "pos": [
        18694,
        18788
      ]
    },
    {
      "content": "If you require different protocols, you should create separate channels for each input protocol.",
      "pos": [
        18789,
        18885
      ]
    },
    {
      "content": "You are only billed when your Channel is in the <bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept> state.",
      "pos": [
        18889,
        18955
      ]
    },
    {
      "content": "For more information, refer to <bpt id=\"p1\">[</bpt>this<ept id=\"p1\">](media-services-manage-channels-overview.md#states)</ept> section.",
      "pos": [
        18956,
        19053
      ]
    },
    {
      "content": "How to create channels that receive multi-bitrate live stream from on-premises encoders",
      "pos": [
        19057,
        19144
      ]
    },
    {
      "pos": [
        19146,
        19318
      ],
      "content": "For more information about on-premises live encoders, see <bpt id=\"p1\">[</bpt>Using 3rd Party Live Encoders with Azure Media Services<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn783464.aspx)</ept>."
    },
    {
      "pos": [
        19320,
        19416
      ],
      "content": "Choose <bpt id=\"p1\">**</bpt>Portal<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>.NET<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>REST API<ept id=\"p3\">**</ept> to see how to create and manage channels and programs."
    },
    {
      "content": "Related topics",
      "pos": [
        19538,
        19552
      ]
    },
    {
      "content": "Azure Media Services Fragmented MP4 Live Ingest Specification",
      "pos": [
        19555,
        19616
      ]
    },
    {
      "content": "Delivering Live Streaming Events with Azure Media Services",
      "pos": [
        19665,
        19723
      ]
    },
    {
      "content": "Media Services Concepts",
      "pos": [
        19770,
        19793
      ]
    },
    {
      "content": "test",
      "pos": [
        19934,
        19938
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Working with Channels that Receive Multi-bitrate Live Stream from On-premises Encoders\" \n    description=\"This topic describes how to set up a Channel that receives a multi-bitrate live stream from an on-premises encoder. The stream can then be delivered to client playback applications through one or more Streaming Endpoints, using one of the following adaptive streaming protocols: HLS, Smooth Stream, MPEG DASH, HDS.\" \n    services=\"media-services\" \n    documentationCenter=\"\" \n    authors=\"Juliako\" \n    manager=\"dwrede\" \n    editor=\"\"/>\n\n<tags \n    ms.service=\"media-services\" \n    ms.workload=\"media\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"ne\" \n    ms.topic=\"article\" \n    ms.date=\"08/20/2015\"\n    ms.author=\"juliako\"/>\n\n#Working with Channels that Receive Multi-bitrate Live Stream from On-premises Encoders\n\n##Overview\n\nIn Azure Media Services, a **Channel** represents a pipeline for processing live streaming content. A **Channel** receives live input streams in the following way:\n\n- An on-premises live encoder sends multi-bitrate **RTMP** or **Smooth Streaming** (Fragmented MP4) to the Channel. You can use the following live encoders that output multi-bitrate Smooth Streaming: Elemental, Envivio, Cisco.  The following live encoders output RTMP: Adobe Flash Live, Telestream Wirecast, and Tricaster transcoders. The ingested streams pass through **Channel** without any further processing. Your live encoder can lso send a single bitrate stream, but that is not recommended. When requested, Media Services delivers the stream to customers.\n\n\nThe following diagram represents a live streaming workflow that uses an on-premises live encoder to output multi-bitrate RTMP or Fragmented MP4 (Smooth Streaming) streams. \n\n![Live workflow][live-overview]\n\nThis topic covers the following:\n\n- [Common live streaming scenario](media-services-manage-channels-overview.md#scenario)\n- [Description of a Channel and its related components](media-services-manage-channels-overview.md#channel)\n- [Considerations](media-services-manage-channels-overview.md#considerations)\n- [Tasks related to Live Streaming](media-services-manage-channels-overview.md#tasks)\n\n##<a id=\"scenario\"></a>Common live streaming scenario\nThe following steps describe tasks involved in creating common live streaming applications.\n\n1. Connect a video camera to a computer. Launch and configure an on-premises live encoder that outputs a multi-bitrate RTMP or Fragmented MP4 (Smooth Streaming) stream. For more information, see [Azure Media Services RTMP Support and Live Encoders](http://go.microsoft.com/fwlink/?LinkId=532824).\n    \n    This step could also be performed after you create your Channel.\n\n1. Create and start a Channel.\n1. Retrieve the Channel ingest URL. \n\n    The ingest URL is used by the live encoder to send the stream to the Channel.\n1. Retrieve the Channel preview URL. \n\n    Use this URL to verify that your channel is properly receiving the live stream.\n\n3. Create a program. \n\n    When using the Azure Management Portal, creating a program also creates an asset. \n\n    When using .NET SDK or REST you need to create an asset and specify to use this asset when creating a Program. \n1. Publish the asset associated with the program.   \n\n    Make sure to have at least one streaming reserved unit on the streaming endpoint from which you want to stream content.\n1. Start the program when you are ready to start streaming and archiving.\n2. Optionally, the live encoder can be signaled to start an advertisement. The advertisement is inserted in the output stream.\n1. Stop the program whenever you want to stop streaming and archiving the event.\n1. Delete the Program (and optionally delete the asset).     \n\nThe [live streaming tasks](media-services-manage-channels-overview.md#tasks) section links to topics that demonstrate how to achieve tasks described above.\n\n##<a id=\"channel\"></a>Description of a Channel and its related components\n\n###<a id=\"channel_input\"></a>Channel input (ingest) configurations\n\n####<a id=\"ingest_protocols\"></a>Ingest streaming protocol\n\nMedia Services supports ingesting live feeds using the following streaming protocol: \n\n- **Multi-bitrate Fragmented MP4**\n \n- **Multi-bitrate RTMP** \n\n    When the **RTMP** ingest streaming protocol is selected, two ingest(input) endpoints are created for the channel: \n    \n    **Primary URL**: Specifies the fully qualified URL of the channel's primary RTMP ingest endpoint.\n\n    **Secondary URL** (optional): Specifies the fully qualified URL of the channel's secondary RTMP ingest endpoint. \n\n\n    Use the secondary URL if you want to improve the durability and fault tolerance of your ingest stream as well as encoder failover and fault-tolerance, especially for the following scenarios.\n\n    - Single encoder double pushing to both Primary and Secondary URLs:\n    \n        The main purpose of this is to provide more resiliency to network fluctuations and jitters. Some RTMP encoders do not handle network disconnects well. When a network disconnect happens, an encoder may stop encoding and will not send the buffered data when reconnect happens, this causes discontinuities and data lost. Network disconnects can happen because of a bad network or a maintenance on Azure side. Primary/secondary URLs reduce the network issues and also provide a controlled upgrade process. Each time a scheduled network disconnect happens, Media Services manages the primary and secondary disconnect and provides a delayed disconnect between the two which gives time for encoders to keep sending data and reconnect again. The order of the disconnects can be random, but there will be always a delay between primary/secondary or secondary/primary. In this scenario encoder is still the single point of failure.\n     \n    - Multiple encoders each encoder pushing to dedicated point:\n        \n        This scenario provides both encoder and ingest redundancy. In this scenario encoder1 pushes to the primary URL and encoder2 pushes to secondary URL. When there is an encoder failure other encoder can still keep sending data. Data redundancy can still be maintained because Media Services does not disconnect primary and secondary at the same time. This scenario assumes encoders are time sync and provides exactly same data.  \n \n    - Multiple encoder double pushing to both primary and secondary URLs:\n    \n        In this scenario both encoders push data to both primary and secondary URLs. This provides the best reliability and fault tolerance as well as data redundancy. It can tolerate both encoder failures and also disconnects even if one encoder stops working. This scenario assumes encoders are time sync and provides exactly same data.  \n\nFor information about RTMP live encoders, see [Azure Media Services RTMP Support and Live Encoders](http://go.microsoft.com/fwlink/?LinkId=532824).\n\nThe following considerations apply:\n\n- Make sure you have sufficient free Internet connectivity to send data to the ingest points. \n- Using secondary ingest URL requires additional bandwidth. \n- The incoming multi-bitrate stream can have a maximum of 10 video quality levels (aka layers), and a maximum of 5 audio tracks.\n- The highest average bitrate for any of the video quality levels or layers should be below 10 Mbps.\n- The aggregate of the average bitrates for all the video and audio streams should be below 25 Mbps.\n- You cannot change the input protocol while the Channel or its associated programs are running. If you require different protocols, you should create separate channels for each input protocol. \n- You can ingest a single bitrate into your channel, but since the stream is not processed by the channel, the client applications will also receive a single bitrate stream (this option is not recommended).\n\n####Ingest URLs (endpoints) \n\nA Channel provides an input endpoint (ingest URL) that you specify in the live encoder, so the encoder can push streams to your channels.   \n\nYou can get the ingest URLs when you create the channel. To get these URLs, the channel does not have to be in the **Running** state. When you are ready to start pushing data into the channel, the channel must be in the **Running** state. Once the channel starts ingesting data, you can preview your stream through the preview URL.\n\nYou have an option of ingesting Fragmented MP4 (Smooth Streaming) live stream over an SSL connection. To ingest over SSL, make sure to update the ingest URL to HTTPS. Currently, you cannot ingest RTMP over SSL. \n\n####<a id=\"keyframe_interval\"></a>Keyframe interval\n\nWhen using an on-premises live encoder to generate multi-bitrate stream, the keyframe interval specifies GOP duration (as used by that external encoder). Once this incoming stream is received by the Channel, you can then deliver your live stream to client playback applications in any of the following formats: Smooth Streaming, DASH and HLS. When doing live streaming, HLS is always packaged dynamically. By default, Media Services automatically calculates HLS segment packaging ratio (fragments per segment) based on the keyframe interval, also referred to as Group of Pictures – GOP, that is received from the live encoder. \n\nThe following table shows how the segment duration is being calculated:\n\nKeyframe Interval|HLS segment packaging ratio (FragmentsPerSegment)|Example\n---|---|---\nless than or equal to 3 seconds|3:1|If the KeyFrameInterval (or GOP) is 2 seconds long, the default HLS segment packaging ratio will be 3 to 1, which will create a 6 seconds HLS segment.\n3 to 5  seconds|2:1|If the KeyFrameInterval (or GOP) is 4 seconds long, the default HLS segment packaging ratio will be 2 to 1, which will create a 8 seconds HLS segment.\ngreater than 5 seconds|1:1|If the KeyFrameInterval (or GOP) is 6 seconds long, the default HLS segment packaging ratio will be 1 to 1, which will create a 6 second long HLS segment.\n\n\nYou can change the fragments per segment ratio by configuring channel’s output and setting FragmentsPerSegment on ChannelOutputHls. \n\nYou can also change the keyframe interval value, by setting the KeyFrameInterval property on ChanneInput. \n\nIf you explicitly set the KeyFrameInterval, the HLS segment packaging ratio FragmentsPerSegment is calculated using the rules described above.  \n\nIf you explicitly set both KeyFrameInterval and FragmentsPerSegment, Media Services will use the values set by you. \n\n\n####Allowed IP addresses\n\nYou can define the IP addresses that are allowed to publish video to this channel. Allowed IP addresses can be specified as either a single IP address (e.g. ‘10.0.0.1’), an IP range using an IP address and a CIDR subnet mask (e.g. ‘10.0.0.1/22’), or an IP range using an IP address and a dotted decimal subnet mask (e.g. ‘10.0.0.1(255.255.252.0)’). \n\nIf no IP addresses are specified and there is no rule definition, then no IP address will be allowed. To allow any IP address, create a rule and set 0.0.0.0/0.\n\n###Channel preview \n\n####Preview URLs\n\nChannels provide a preview endpoint (preview URL) that you use to preview and validate your stream before further processing and delivery.\n\nYou can get the preview URL when you create the channel. To get the URL, the channel does not have to be in the **Running** state. \n\nOnce the Channel starts ingesting data, you can preview your stream.\n\nNote that currently the preview stream can only be delivered in Fragmented MP4 (Smooth Streaming) format regardless of the specified input type. You can use the [http://smf.cloudapp.net/healthmonitor](http://smf.cloudapp.net/healthmonitor) player to test the Smooth Stream. You can also use a player hosted in the Azure Management Portal to view your stream.\n\n\n####Allowed IP Addresses\n\nYou can define the IP addresses that are allowed to connect to the preview endpoint. If no IP addresses are specified any IP address will be allowed. Allowed IP addresses can be specified as either a single IP address (e.g. ‘10.0.0.1’), an IP range using an IP address and a CIDR subnet mask (e.g. ‘10.0.0.1/22’), or an IP range using an IP address and a dotted decimal subnet mask (e.g. ‘10.0.0.1(255.255.252.0)’).\n\n###Channel output\n\nFor more information see the [setting keyframe interval](#keyframe_interval) section.\n\n\n###Channel's programs\n\nA channel is associated with programs that enable you to control the publishing and storage of segments in a live stream. Channels manage Programs. The Channel and Program relationship is very similar to traditional media where a channel has a constant stream of content and a program is scoped to some timed event on that channel.\n\nYou can specify the number of hours you want to retain the recorded content for the program by setting the **Archive Window** length. This value can be set from a minimum of 5 minutes to a maximum of 25 hours. Archive window length also dictates the maximum amount of time clients can seek back in time from the current live position. Programs can run over the specified amount of time, but content that falls behind the window length is continuously discarded. This value of this property also determines how long the client manifests can grow.\n\nEach program is associated with an Asset which stores the streamed content. An asset is mapped to a blob container in the Azure Storage account and the files in the asset are stored as blobs in that container. To publish the program so your customers can view the stream you must create an OnDemand locator for the associated asset. Having this locator will enable you to build a streaming URL that you can provide to your clients.\n\nA channel supports up to three concurrently running programs so you can create multiple archives of the same incoming stream. This allows you to publish and archive different parts of an event as needed. For example, your business requirement is to archive 6 hours of a program, but to broadcast only last 10 minutes. To accomplish this, you need to create two concurrently running programs. One program is set to archive 6 hours of the event but the program is not published. The other program is set to archive for 10 minutes and this program is published.\n\nYou should not reuse existing programs for new events. Instead, create and start a new program for each event as described in the Programming Live Streaming Applications section.\n\nStart the program when you are ready to start streaming and archiving. Stop the program whenever you want to stop streaming and archiving the event. \n\nTo delete archived content, stop and delete the program and then delete the associated asset. An asset cannot be deleted if it is used by a program; the program must be deleted first. \n\nEven after you stop and delete the program, the users would be able to stream your archived content as a video on demand, for as long as you do not delete the asset.\n\nIf you do want to retain the archived content, but not have it available for streaming, delete the streaming locator.\n\n##<a id=\"states\"></a>Channel states and how states map to the billing mode \n\nThe current state of a Channel. Possible values include:\n\n- **Stopped**. This is the initial state of the Channel after its creation. In this state, the Channel properties can be updated but streaming is not allowed.\n- **Starting**. The Channel is being started. No updates or streaming is allowed during this state. If an error occurs, the Channel returns to the Stopped state.\n- **Running**. The Channel is capable of processing live streams.\n- **Stopping**. The Channel is being stopped. No updates or streaming is allowed during this state.\n- **Deleting**. The Channel is being deleted. No updates or streaming is allowed during this state.\n\nThe following table shows how Channel states map to the billing mode. \n \nChannel state|Portal UI Indicators|Billed?\n---|---|---|---\nStarting|Starting|No (transient state)\nRunning|Ready (no running programs)<p>or<p>Streaming (at least one running program)|Yes\nStopping|Stopping|No (transient state)\nStopped|Stopped|No\n\n###Closed Captioning and Ad Insertion \n\nThe following table demonstrates supported closed captioning and ad insertion standards.\n\nStandard|Notes\n---|---\nCEA-708 and EIA-608 (708/608)|CEA-708 and EIA-608 are closed captioning standards for the United States and Canada.<p><p>Currently, captioning is only supported if carried in the encoded input stream. You need to use a live media encoder that can insert 608 or 708 captions into the encoded stream which is sent to Media Services. Media Services will deliver the content with inserted captions to your viewers.\nTTML inside ismt (Smooth Streaming Text Tracks)|Media Services dynamic packaging enables your clients to stream content in any of the following formats: MPEG DASH, HLS or Smooth Streaming. However, if you ingest fragmented MP4 (Smooth Streaming) with captions inside .ismt (Smooth Streaming text tracks), you would only be able to deliver the stream to Smooth Streaming clients.\nSCTE-35|Digital signaling system used to cue advertising insertion. Downstream receivers use the signal to splice advertising into the stream for the allotted time. SCTE-35 must be sent as a sparse track in the input stream.<p><p>Note that currently, the only supported input stream format that carries ad signals is fragmented MP4 (Smooth Streaming). The only supported output format is also Smooth Streaming.\n\n\n##<a id=\"Considerations\"></a>Considerations\n\nWhen using an on-premises live encoder to send a multi-bitrate stream into a Channel, the following constraints apply:\n\n- Make sure you have sufficient free internet connectivity to send data to the ingest points. \n- The incoming multi-bitrate stream can have a maximum of 10 video quality levels (10 layers), and maximum of 5 audio tracks.\n- The highest average bitrate for any of the video quality levels or layers should be below 10 Mbps\n- The aggregate of the average bitrates for all the video and audio streams should be below 25 Mbps\n- You cannot change the input protocol while the Channel or its associated programs are running. If you require different protocols, you should create separate channels for each input protocol. \n\n\nOther considerations related to working with channels and related components:\n\n- Every time you reconfigure the live encoder, call the **Reset** method on the channel. Before you reset the channel, you have to stop the program. After you reset the channel, restart the program. \n- A channel can be stopped only when it is in the Running state, and all programs on the channel have been stopped.\n- By default you can only add 5 channels to your Media Services account. For more information, see [Quotas and Limitations](media-services-quotas-and-limitations.md).\n- You cannot change the input protocol while the Channel or its associated programs are running. If you require different protocols, you should create separate channels for each input protocol. \n- You are only billed when your Channel is in the **Running** state. For more information, refer to [this](media-services-manage-channels-overview.md#states) section.\n\n##How to create channels that receive multi-bitrate live stream from on-premises encoders\n\nFor more information about on-premises live encoders, see [Using 3rd Party Live Encoders with Azure Media Services](https://msdn.microsoft.com/library/azure/dn783464.aspx).\n\nChoose **Portal**, **.NET**, **REST API** to see how to create and manage channels and programs.\n\n[AZURE.INCLUDE [media-services-selector-manage-channels](../../includes/media-services-selector-manage-channels.md)]\n\n##Related topics\n\n[Azure Media Services Fragmented MP4 Live Ingest Specification](media-services-fmp4-live-ingest-overview.md)\n\n[Delivering Live Streaming Events with Azure Media Services](media-services-live-streaming-workflow.md)\n\n[Media Services Concepts](media-services-concepts.md)\n\n[live-overview]: ./media/media-services-manage-channels-overview/media-services-live-streaming-current.png\n \n\ntest\n"
}