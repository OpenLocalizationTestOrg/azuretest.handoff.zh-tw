{
  "nodes": [
    {
      "content": "Get started with Stream Analytics: Real-time fraud detection | Microsoft Azure",
      "pos": [
        27,
        105
      ]
    },
    {
      "content": "Learn how to create a real-time fraud detection solution with Stream Analytics.",
      "pos": [
        124,
        203
      ]
    },
    {
      "content": "Use an event hub for real-time event processing.",
      "pos": [
        204,
        252
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics: Real-time fraud detection",
      "pos": [
        662,
        729
      ]
    },
    {
      "content": "Learn how to create an end-to-end solution for real-time fraud detection with Azure Stream Analytics.",
      "pos": [
        731,
        832
      ]
    },
    {
      "content": "Bring events into an Azure event hub, write Stream Analytics queries for aggregation or alerting, and send the results to an output sink to gain insight over data with real-time processing.",
      "pos": [
        833,
        1022
      ]
    },
    {
      "content": "Stream Analytics is a fully managed service providing low-latency, highly available, scalable complex event processing over streaming data in the cloud.",
      "pos": [
        1024,
        1176
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Introduction to Azure Stream Analytics<ept id=\"p1\">](stream-analytics-introduction.md)</ept>.",
      "pos": [
        1177,
        1278
      ]
    },
    {
      "content": "Scenario: Telecommunications and SIM fraud detection in real-time",
      "pos": [
        1284,
        1349
      ]
    },
    {
      "content": "A telecommunications company has a large volume of data for incoming calls.",
      "pos": [
        1351,
        1426
      ]
    },
    {
      "content": "The company needs the following from its data:",
      "pos": [
        1427,
        1473
      ]
    },
    {
      "content": "Pare this data down to a manageable amount and obtain insights about customer usage over time and geographical regions.",
      "pos": [
        1476,
        1595
      ]
    },
    {
      "content": "Detect SIM fraud (multiple calls coming from the same identity around the same time but in geographically different locations) in real-time so that they can easily respond by notifying customers or shutting down service.",
      "pos": [
        1598,
        1818
      ]
    },
    {
      "content": "In canonical Internet of Things (IoT) scenarios there is a ton of telemetry or sensor data being generated â€“ and customers want to aggregate them or alert over anomalies in real-time.",
      "pos": [
        1820,
        2003
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        2008,
        2021
      ]
    },
    {
      "content": "This scenario leverages an event generator located on GitHub.",
      "pos": [
        2023,
        2084
      ]
    },
    {
      "content": "Download it <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator)</ept> and follow the steps in this tutorial to set up your solution.",
      "pos": [
        2085,
        2257
      ]
    },
    {
      "content": "Create an Azure Event Hubs input and Consumer Group",
      "pos": [
        2262,
        2313
      ]
    },
    {
      "content": "The sample application will generate events and push them to an Event Hub instance for real-time processing.",
      "pos": [
        2315,
        2423
      ]
    },
    {
      "content": "Service Bus Event Hubs are the preferred method of event ingestion for Stream Analytics and you can learn more about Event Hubs in <bpt id=\"p1\">[</bpt>Azure Service Bus documentation<ept id=\"p1\">](/documentation/services/service-bus/)</ept>.",
      "pos": [
        2424,
        2627
      ]
    },
    {
      "content": "To create an Event Hub:",
      "pos": [
        2629,
        2652
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">[</bpt>Azure portal<ept id=\"p1\">](https://manage.windowsazure.com/)</ept> click <bpt id=\"p2\">**</bpt>New<ept id=\"p2\">**</ept> &gt; <bpt id=\"p3\">**</bpt>App Services<ept id=\"p3\">**</ept> &gt; <bpt id=\"p4\">**</bpt>Service Bus<ept id=\"p4\">**</ept> &gt; <bpt id=\"p5\">**</bpt>Event Hub<ept id=\"p5\">**</ept> &gt; <bpt id=\"p6\">**</bpt>Quick Create<ept id=\"p6\">**</ept>.",
      "pos": [
        2658,
        2800
      ]
    },
    {
      "content": "Provide a name, region, and new or existing namespace to create a new Event Hub.",
      "pos": [
        2801,
        2881
      ]
    },
    {
      "content": "As a best practice, each Stream Analytics job should read from a single Event Hub Consumer Group.",
      "pos": [
        2888,
        2985
      ]
    },
    {
      "content": "We will walk you through the process of creating a Consumer Group below, and you can <bpt id=\"p1\">[</bpt>learn more about Consumer Groups<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn836025.aspx)</ept>.",
      "pos": [
        2986,
        3162
      ]
    },
    {
      "content": "To create a Consumer Group, navigate to the newly created Event Hub and click the <bpt id=\"p1\">**</bpt>Consumer Groups<ept id=\"p1\">**</ept> tab, then click <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept> on the bottom of the page and provide a name for your Consumer Group.",
      "pos": [
        3163,
        3361
      ]
    },
    {
      "content": "To grant access to the Event Hub, we will need to create a shared access policy.",
      "pos": [
        3366,
        3446
      ]
    },
    {
      "content": "Click the <bpt id=\"p1\">**</bpt>Configure<ept id=\"p1\">**</ept> tab of your Event Hub.",
      "pos": [
        3448,
        3494
      ]
    },
    {
      "pos": [
        3499,
        3581
      ],
      "content": "Under <bpt id=\"p1\">**</bpt>Shared Access Policies<ept id=\"p1\">**</ept>, create a new policy with <bpt id=\"p2\">**</bpt>Manage<ept id=\"p2\">**</ept> permissions."
    },
    {
      "content": "Shared Access Policies where you can create a policy with Manage permissions.",
      "pos": [
        3589,
        3666
      ]
    },
    {
      "pos": [
        3756,
        3797
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Save<ept id=\"p1\">**</ept> at the bottom of the page."
    },
    {
      "pos": [
        3802,
        3946
      ],
      "content": "Navigate to the <bpt id=\"p1\">**</bpt>Dashboard<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Connection Information<ept id=\"p2\">**</ept> at the bottom of the page, and then copy and save the connection information."
    },
    {
      "content": "Configure and start event generator application",
      "pos": [
        3951,
        3998
      ]
    },
    {
      "content": "We have provided a client application that will generate sample incoming call metadata and push it to Event Hub.",
      "pos": [
        4000,
        4112
      ]
    },
    {
      "content": "Follow the steps below to set up this application.",
      "pos": [
        4113,
        4163
      ]
    },
    {
      "pos": [
        4171,
        4396
      ],
      "content": "Download the TelcoGenerator solution from <bpt id=\"p1\">[</bpt>https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator<ept id=\"p1\">](https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator)</ept>."
    },
    {
      "content": "Replace the Microsoft.ServiceBus.ConnectionString and EventHubName values in App.Config with your Event Hub connection string and name.",
      "pos": [
        4401,
        4536
      ]
    },
    {
      "content": "Build the solution to trigger the download of required nuget packages.",
      "pos": [
        4541,
        4611
      ]
    },
    {
      "content": "Start the application.",
      "pos": [
        4616,
        4638
      ]
    },
    {
      "content": "The usage is as follows:",
      "pos": [
        4639,
        4663
      ]
    },
    {
      "content": "The following example will generate 1000 events with a 20 percent probability of fraud over the course of 2 hours.",
      "pos": [
        4751,
        4865
      ]
    },
    {
      "content": "You will see records being sent to your Event Hub.",
      "pos": [
        4899,
        4949
      ]
    },
    {
      "content": "Some key fields that we will be using in this real-time fraud detection application are defined here:",
      "pos": [
        4950,
        5051
      ]
    },
    {
      "content": "Record",
      "pos": [
        5055,
        5061
      ]
    },
    {
      "content": "Definition",
      "pos": [
        5064,
        5074
      ]
    },
    {
      "content": "CallrecTime",
      "pos": [
        5113,
        5124
      ]
    },
    {
      "content": "Timestamp for the call start time.",
      "pos": [
        5127,
        5161
      ]
    },
    {
      "content": "SwitchNum",
      "pos": [
        5166,
        5175
      ]
    },
    {
      "content": "Telephone switch used to connect the call.",
      "pos": [
        5178,
        5220
      ]
    },
    {
      "content": "CallingNum",
      "pos": [
        5225,
        5235
      ]
    },
    {
      "content": "Phone number of the caller.",
      "pos": [
        5238,
        5265
      ]
    },
    {
      "content": "CallingIMSI",
      "pos": [
        5270,
        5281
      ]
    },
    {
      "content": "International Mobile Subscriber Identity (IMSI).",
      "pos": [
        5284,
        5332
      ]
    },
    {
      "content": "Unique identifier of the caller.",
      "pos": [
        5334,
        5366
      ]
    },
    {
      "content": "CalledNum",
      "pos": [
        5371,
        5380
      ]
    },
    {
      "content": "Phone number of the call recipient.",
      "pos": [
        5383,
        5418
      ]
    },
    {
      "content": "CalledIMSI",
      "pos": [
        5423,
        5433
      ]
    },
    {
      "content": "International Mobile Subscriber Identity (IMSI).",
      "pos": [
        5436,
        5484
      ]
    },
    {
      "content": "Unique identifier of the call recipient.",
      "pos": [
        5486,
        5526
      ]
    },
    {
      "content": "Create Stream Analytics job",
      "pos": [
        5534,
        5561
      ]
    },
    {
      "content": "Now that we have a stream of telecommunications events, we can set up a Stream Analytics job to analyze these events in real-time.",
      "pos": [
        5562,
        5692
      ]
    },
    {
      "content": "Provision a Stream Analytics job",
      "pos": [
        5698,
        5730
      ]
    },
    {
      "pos": [
        5736,
        5821
      ],
      "content": "In the Azure portal, click <bpt id=\"p1\">**</bpt>New &gt; Data Services &gt; Stream Analytics &gt; Quick Create<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        5826,
        5903
      ],
      "content": "Specify the following values, and then click <bpt id=\"p1\">**</bpt>Create Stream Analytics Job<ept id=\"p1\">**</ept>:"
    },
    {
      "pos": [
        5911,
        5942
      ],
      "content": "<bpt id=\"p1\">**</bpt>Job Name<ept id=\"p1\">**</ept>: Enter a job name."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Region<ept id=\"p1\">**</ept>: Select the region where you want to run the job.",
      "pos": [
        5950,
        6010
      ]
    },
    {
      "content": "Consider placing the job and the event hub in the same region to ensure better performance and to ensure that you will not be paying to transfer data between regions.",
      "pos": [
        6011,
        6177
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Storage Account<ept id=\"p1\">**</ept>: Choose the Azure storage account that you would like to use to store monitoring data for all Stream Analytics jobs running within this region.",
      "pos": [
        6185,
        6348
      ]
    },
    {
      "content": "You have the option to choose an existing storage account or to create a new one.",
      "pos": [
        6349,
        6430
      ]
    },
    {
      "pos": [
        6436,
        6514
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Stream Analytics<ept id=\"p1\">**</ept> in the left pane to list the Stream Analytics jobs."
    },
    {
      "content": "Stream Analytics service icon",
      "pos": [
        6522,
        6551
      ]
    },
    {
      "content": "The new job will be shown with a status of <bpt id=\"p1\">**</bpt>Created<ept id=\"p1\">**</ept>.",
      "pos": [
        6630,
        6685
      ]
    },
    {
      "content": "Notice that the <bpt id=\"p1\">**</bpt>Start<ept id=\"p1\">**</ept> button on the bottom of the page is disabled.",
      "pos": [
        6686,
        6757
      ]
    },
    {
      "content": "You must configure the job input, output, and query before you can start the job.",
      "pos": [
        6758,
        6839
      ]
    },
    {
      "content": "Specify job input",
      "pos": [
        6845,
        6862
      ]
    },
    {
      "content": "In your Stream Analytics job click <bpt id=\"p1\">**</bpt>Inputs<ept id=\"p1\">**</ept> from the top of the page, and then click <bpt id=\"p2\">**</bpt>Add Input<ept id=\"p2\">**</ept>.",
      "pos": [
        6867,
        6968
      ]
    },
    {
      "content": "The dialog box that opens will walk you through a number of steps to set up your input.",
      "pos": [
        6969,
        7056
      ]
    },
    {
      "pos": [
        7061,
        7117
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Data Stream<ept id=\"p1\">**</ept>, and then click the right button."
    },
    {
      "pos": [
        7122,
        7176
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Event Hub<ept id=\"p1\">**</ept>, and then click the right button."
    },
    {
      "content": "Type or select the following values on the third page:",
      "pos": [
        7181,
        7235
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Input Alias<ept id=\"p1\">**</ept>: Enter a friendly name for this job input such as <bpt id=\"p2\">*</bpt>CallStream<ept id=\"p2\">*</ept>.",
      "pos": [
        7243,
        7322
      ]
    },
    {
      "content": "Note that you will be using this name in the query later.",
      "pos": [
        7323,
        7380
      ]
    },
    {
      "pos": [
        7387,
        7534
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Hub<ept id=\"p1\">**</ept>: If the Event Hub you created is in the same subscription as the Stream Analytics job, select the namespace that the event hub is in."
    },
    {
      "pos": [
        7540,
        7810
      ],
      "content": "If your event hub is in a different subscription, select <bpt id=\"p1\">**</bpt>Use Event Hub from Another Subscription<ept id=\"p1\">**</ept> and manually enter information for <bpt id=\"p2\">**</bpt>Service Bus Namespace<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>Event Hub Name<ept id=\"p3\">**</ept>, <bpt id=\"p4\">**</bpt>Event Hub Policy Name<ept id=\"p4\">**</ept>, <bpt id=\"p5\">**</bpt>Event Hub Policy Key<ept id=\"p5\">**</ept>, and <bpt id=\"p6\">**</bpt>Event Hub Partition Count<ept id=\"p6\">**</ept>."
    },
    {
      "pos": [
        7818,
        7871
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Hub Name<ept id=\"p1\">**</ept>: Select the name of the Event Hub."
    },
    {
      "pos": [
        7879,
        7967
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Hub Policy Name<ept id=\"p1\">**</ept>: Select the event-hub policy created earlier in this tutorial."
    },
    {
      "pos": [
        7975,
        8062
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Hub Consumer Group<ept id=\"p1\">**</ept>: Type the Consumer Group created earlier in this tutorial."
    },
    {
      "content": "Click the right button.",
      "pos": [
        8067,
        8090
      ]
    },
    {
      "content": "Specify the following values:",
      "pos": [
        8095,
        8124
      ]
    },
    {
      "pos": [
        8132,
        8165
      ],
      "content": "<bpt id=\"p1\">**</bpt>Event Serializer Format<ept id=\"p1\">**</ept>: JSON"
    },
    {
      "pos": [
        8172,
        8190
      ],
      "content": "<bpt id=\"p1\">**</bpt>Encoding<ept id=\"p1\">**</ept>: UTF8"
    },
    {
      "content": "Click the check button to add this source and to verify that Stream Analytics can successfully connect to the event hub.",
      "pos": [
        8195,
        8315
      ]
    },
    {
      "content": "Specify job query",
      "pos": [
        8321,
        8338
      ]
    },
    {
      "content": "Stream Analytics supports a simple, declarative query model for describing transformations for real-time processing.",
      "pos": [
        8340,
        8456
      ]
    },
    {
      "content": "To learn more about the language, see the <bpt id=\"p1\">[</bpt>Azure Stream Analytics Query Language Reference<ept id=\"p1\">](https://msdn.microsoft.com/library/dn834998.aspx)</ept>.",
      "pos": [
        8457,
        8599
      ]
    },
    {
      "content": "This tutorial will help you author and test several queries over your real-time stream of call data.",
      "pos": [
        8600,
        8700
      ]
    },
    {
      "content": "Optional: Sample input data",
      "pos": [
        8707,
        8734
      ]
    },
    {
      "content": "To validate your query against actual job data, you can use the <bpt id=\"p1\">**</bpt>Sample Data<ept id=\"p1\">**</ept> feature to extract events from your stream and create a .JSON file of the events for testing.",
      "pos": [
        8735,
        8908
      ]
    },
    {
      "content": "The following steps show how to do this and we have also provided a sample <bpt id=\"p1\">[</bpt>Telco.json<ept id=\"p1\">](https://github.com/Azure/azure-stream-analytics/blob/master/Sample%20Data/telco.json)</ept> file for testing purposes.",
      "pos": [
        8910,
        9110
      ]
    },
    {
      "pos": [
        9116,
        9196
      ],
      "content": "Select your Event Hub input and click <bpt id=\"p1\">**</bpt>Sample Data<ept id=\"p1\">**</ept> at the bottom of the page."
    },
    {
      "pos": [
        9201,
        9347
      ],
      "content": "In the dialog box that appears, specify a <bpt id=\"p1\">**</bpt>Start Time<ept id=\"p1\">**</ept> to start collecting data from and a <bpt id=\"p2\">**</bpt>Duration<ept id=\"p2\">**</ept> for how much additional data to consume."
    },
    {
      "content": "Click the check button to start sampling data from the input.",
      "pos": [
        9352,
        9413
      ]
    },
    {
      "content": "It can take a minute or two for the data file to be produced.",
      "pos": [
        9415,
        9476
      ]
    },
    {
      "content": "When the process is completed, click <bpt id=\"p1\">**</bpt>Details<ept id=\"p1\">**</ept> and download and save the .JSON file that is generated.",
      "pos": [
        9478,
        9582
      ]
    },
    {
      "content": "Download and save processed data in a JSON file",
      "pos": [
        9590,
        9637
      ]
    },
    {
      "content": "Passthrough query",
      "pos": [
        9728,
        9745
      ]
    },
    {
      "content": "If you want to archive every event, you can use a passthrough query to read all the fields in the payload of the event or message.",
      "pos": [
        9747,
        9877
      ]
    },
    {
      "content": "To start with, do a simple passthrough query that projects all the fields in an event.",
      "pos": [
        9878,
        9964
      ]
    },
    {
      "pos": [
        9970,
        10032
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Query<ept id=\"p1\">**</ept> from the top of the Stream Analytics job page."
    },
    {
      "content": "Add the following to the code editor:",
      "pos": [
        10037,
        10074
      ]
    },
    {
      "content": "Make sure that the name of the input source matches the name of the input you specified earlier.",
      "pos": [
        10116,
        10212
      ]
    },
    {
      "pos": [
        10218,
        10256
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Test<ept id=\"p1\">**</ept> under the query editor."
    },
    {
      "pos": [
        10261,
        10440
      ],
      "content": "Supply a test file, either one that you created using the previous steps or use <bpt id=\"p1\">[</bpt>Telco.json<ept id=\"p1\">](https://github.com/Azure/azure-stream-analytics/blob/master/Sample%20Data/telco.json)</ept>."
    },
    {
      "content": "Click the check button and see the results displayed below the query definition.",
      "pos": [
        10445,
        10525
      ]
    },
    {
      "content": "Query definition results",
      "pos": [
        10533,
        10557
      ]
    },
    {
      "content": "Column projection",
      "pos": [
        10641,
        10658
      ]
    },
    {
      "content": "We'll now pare down the returned fields to a smaller set.",
      "pos": [
        10660,
        10717
      ]
    },
    {
      "content": "Change the query in the code editor to:",
      "pos": [
        10723,
        10762
      ]
    },
    {
      "pos": [
        10867,
        10938
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Rerun<ept id=\"p1\">**</ept> under the query editor to see the results of the query."
    },
    {
      "content": "Output in query editor.",
      "pos": [
        10946,
        10969
      ]
    },
    {
      "content": "Count of incoming calls by region: Tumbling window with aggregation",
      "pos": [
        11055,
        11122
      ]
    },
    {
      "pos": [
        11124,
        11339
      ],
      "content": "To compare the amount that incoming calls per region we'll leverage a <bpt id=\"p1\">[</bpt>TumblingWindow<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn835055.aspx)</ept> to get the count of incoming calls grouped by SwitchNum every 5 seconds."
    },
    {
      "content": "Change the query in the code editor to:",
      "pos": [
        11345,
        11384
      ]
    },
    {
      "content": "This query uses the <bpt id=\"p1\">**</bpt>Timestamp By<ept id=\"p1\">**</ept> keyword to specify a timestamp field in the payload to be used in the temporal computation.",
      "pos": [
        11568,
        11696
      ]
    },
    {
      "content": "If this field wasn't specified, the windowing operation would be performed using the time each event arrived at Event Hub.",
      "pos": [
        11697,
        11819
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>\"Arrival Time Vs Application Time\" in the Stream Analytics Query Language Reference<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn834998.aspx)</ept>.",
      "pos": [
        11820,
        11966
      ]
    },
    {
      "pos": [
        11972,
        12079
      ],
      "content": "Note that you can access a timestamp for the end of each window by using the <bpt id=\"p1\">**</bpt>System.Timestamp<ept id=\"p1\">**</ept> property."
    },
    {
      "pos": [
        12085,
        12156
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Rerun<ept id=\"p1\">**</ept> under the query editor to see the results of the query."
    },
    {
      "content": "Query results for Timestand By",
      "pos": [
        12164,
        12194
      ]
    },
    {
      "content": "SIM fraud detection with a Self-Join",
      "pos": [
        12280,
        12316
      ]
    },
    {
      "content": "To identify potentially fraudulent usage we'll look for calls originating from the same user but in different locations in less than 5 seconds.",
      "pos": [
        12318,
        12461
      ]
    },
    {
      "content": "We <bpt id=\"p1\">[</bpt>join<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn835026.aspx)</ept> the stream of call events with itself to check for these cases.",
      "pos": [
        12463,
        12592
      ]
    },
    {
      "content": "Change the query in the code editor to:",
      "pos": [
        12598,
        12637
      ]
    },
    {
      "pos": [
        13070,
        13141
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Rerun<ept id=\"p1\">**</ept> under the query editor to see the results of the query."
    },
    {
      "content": "Query results of a join",
      "pos": [
        13149,
        13172
      ]
    },
    {
      "content": "Create output sink",
      "pos": [
        13257,
        13275
      ]
    },
    {
      "content": "Now that we have defined an event stream, an Event Hub input to ingest events, and a query to perform a transformation over the stream, the last step is to define an output sink for the job.",
      "pos": [
        13277,
        13467
      ]
    },
    {
      "content": "We'll write events for fraudulent behavior to Blob storage.",
      "pos": [
        13469,
        13528
      ]
    },
    {
      "content": "Follow the steps below to create a container for Blob storage if you don't already have one.",
      "pos": [
        13530,
        13622
      ]
    },
    {
      "pos": [
        13628,
        13784
      ],
      "content": "Use an existing storage account or create a new storage account by clicking <bpt id=\"p1\">**</bpt>NEW &gt; DATA SERVICES &gt; STORAGE &gt; QUICK CREATE<ept id=\"p1\">**</ept> and following the instructions."
    },
    {
      "pos": [
        13789,
        13885
      ],
      "content": "Select the storage account, click <bpt id=\"p1\">**</bpt>CONTAINERS<ept id=\"p1\">**</ept> at the top of the page, and then click <bpt id=\"p2\">**</bpt>ADD<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13890,
        13966
      ],
      "content": "Specify a <bpt id=\"p1\">**</bpt>NAME<ept id=\"p1\">**</ept> for your container and set its <bpt id=\"p2\">**</bpt>ACCESS<ept id=\"p2\">**</ept> to Public Blob."
    },
    {
      "content": "Specify job output",
      "pos": [
        13971,
        13989
      ]
    },
    {
      "content": "In your Stream Analytics job click <bpt id=\"p1\">**</bpt>OUTPUT<ept id=\"p1\">**</ept> from the top of the page, and then click <bpt id=\"p2\">**</bpt>ADD OUTPUT<ept id=\"p2\">**</ept>.",
      "pos": [
        13995,
        14097
      ]
    },
    {
      "content": "The dialog box that opens will walk you through a number of steps to set up your output.",
      "pos": [
        14098,
        14186
      ]
    },
    {
      "pos": [
        14191,
        14248
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>BLOB STORAGE<ept id=\"p1\">**</ept>, and then click the right button."
    },
    {
      "content": "Type or select the following values on the third page:",
      "pos": [
        14253,
        14307
      ]
    },
    {
      "pos": [
        14315,
        14375
      ],
      "content": "<bpt id=\"p1\">**</bpt>OUTPUT ALIAS<ept id=\"p1\">**</ept>: Enter a friendly name for this job output."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>SUBSCRIPTION<ept id=\"p1\">**</ept>: If the Blob storage you created is in the same subscription as the Stream Analytics job, select <bpt id=\"p2\">**</bpt>Use Storage Account from Current Subscription<ept id=\"p2\">**</ept>.",
      "pos": [
        14382,
        14546
      ]
    },
    {
      "content": "If your storage is in a different subscription, select <bpt id=\"p1\">**</bpt>Use Storage Account from Another Subscription<ept id=\"p1\">**</ept> and manually enter information for <bpt id=\"p2\">**</bpt>STORAGE ACCOUNT<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>STORAGE ACCOUNT KEY<ept id=\"p3\">**</ept>, <bpt id=\"p4\">**</bpt>CONTAINER<ept id=\"p4\">**</ept>.",
      "pos": [
        14547,
        14747
      ]
    },
    {
      "pos": [
        14754,
        14814
      ],
      "content": "<bpt id=\"p1\">**</bpt>STORAGE ACCOUNT<ept id=\"p1\">**</ept>: Select the name of the storage account."
    },
    {
      "pos": [
        14821,
        14869
      ],
      "content": "<bpt id=\"p1\">**</bpt>CONTAINER<ept id=\"p1\">**</ept>: Select the name of the container."
    },
    {
      "pos": [
        14876,
        14951
      ],
      "content": "<bpt id=\"p1\">**</bpt>FILENAME PREFIX<ept id=\"p1\">**</ept>: Type in a file prefix to use when writing blob output."
    },
    {
      "content": "Click the right button.",
      "pos": [
        14957,
        14980
      ]
    },
    {
      "content": "Specify the following values:",
      "pos": [
        14985,
        15014
      ]
    },
    {
      "pos": [
        15022,
        15055
      ],
      "content": "<bpt id=\"p1\">**</bpt>EVENT SERIALIZER FORMAT<ept id=\"p1\">**</ept>: JSON"
    },
    {
      "pos": [
        15062,
        15080
      ],
      "content": "<bpt id=\"p1\">**</bpt>ENCODING<ept id=\"p1\">**</ept>: UTF8"
    },
    {
      "content": "Click the check button to add this source and to verify that Stream Analytics can successfully connect to the storage account.",
      "pos": [
        15086,
        15212
      ]
    },
    {
      "content": "Start job for real time processing",
      "pos": [
        15217,
        15251
      ]
    },
    {
      "content": "Since a job input, query, and output have all been specified, we are ready to start the Stream Analytics job for real-time fraud detection.",
      "pos": [
        15253,
        15392
      ]
    },
    {
      "pos": [
        15398,
        15468
      ],
      "content": "From the job <bpt id=\"p1\">**</bpt>DASHBOARD<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>START<ept id=\"p2\">**</ept> at the bottom of the page."
    },
    {
      "content": "In the dialog box that appears, select <bpt id=\"p1\">**</bpt>JOB START TIME<ept id=\"p1\">**</ept> and then click the check button on the bottom of the dialog box.",
      "pos": [
        15473,
        15595
      ]
    },
    {
      "content": "The job status will change to <bpt id=\"p1\">**</bpt>Starting<ept id=\"p1\">**</ept> and will shortly move to <bpt id=\"p2\">**</bpt>Running<ept id=\"p2\">**</ept>.",
      "pos": [
        15596,
        15676
      ]
    },
    {
      "content": "View fraud detection output",
      "pos": [
        15681,
        15708
      ]
    },
    {
      "pos": [
        15710,
        15952
      ],
      "content": "Use a tool like <bpt id=\"p1\">[</bpt>Azure Storage Explorer<ept id=\"p1\">](https://azurestorageexplorer.codeplex.com/)</ept> or <bpt id=\"p2\">[</bpt>Azure Explorer<ept id=\"p2\">](http://www.cerebrata.com/products/azure-explorer/introduction)</ept> to view fraudulent events as they are written to your output in real-time."
    },
    {
      "content": "Fraud detection: Fraudulent events viewed in real-time",
      "pos": [
        15958,
        16012
      ]
    },
    {
      "content": "Get support",
      "pos": [
        16109,
        16120
      ]
    },
    {
      "pos": [
        16121,
        16264
      ],
      "content": "For further assistance, try our <bpt id=\"p1\">[</bpt>Azure Stream Analytics forum<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept>."
    },
    {
      "content": "Next steps",
      "pos": [
        16270,
        16280
      ]
    },
    {
      "content": "Introduction to Azure Stream Analytics",
      "pos": [
        16285,
        16323
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics",
      "pos": [
        16362,
        16402
      ]
    },
    {
      "content": "Scale Azure Stream Analytics jobs",
      "pos": [
        16440,
        16473
      ]
    },
    {
      "content": "Azure Stream Analytics Query Language Reference",
      "pos": [
        16510,
        16557
      ]
    },
    {
      "content": "Azure Stream Analytics Management REST API Reference",
      "pos": [
        16618,
        16670
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Get started with Stream Analytics: Real-time fraud detection | Microsoft Azure\"\n    description=\"Learn how to create a real-time fraud detection solution with Stream Analytics. Use an event hub for real-time event processing.\"\n    keywords=\"event hub,fraud detection,real-time,real-time processing\"\n    services=\"stream-analytics\"\n    documentationCenter=\"\"\n    authors=\"jeffstokes72\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\" />\n\n<tags\n    ms.service=\"stream-analytics\"\n    ms.devlang=\"na\"\n    ms.topic=\"hero-article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"data-services\"\n    ms.date=\"08/19/2015\"\n    ms.author=\"jeffstok\" />\n\n\n\n# Get started using Azure Stream Analytics: Real-time fraud detection\n\nLearn how to create an end-to-end solution for real-time fraud detection with Azure Stream Analytics. Bring events into an Azure event hub, write Stream Analytics queries for aggregation or alerting, and send the results to an output sink to gain insight over data with real-time processing.\n\nStream Analytics is a fully managed service providing low-latency, highly available, scalable complex event processing over streaming data in the cloud. For more information, see [Introduction to Azure Stream Analytics](stream-analytics-introduction.md).\n\n\n## Scenario: Telecommunications and SIM fraud detection in real-time\n\nA telecommunications company has a large volume of data for incoming calls. The company needs the following from its data:\n* Pare this data down to a manageable amount and obtain insights about customer usage over time and geographical regions.\n* Detect SIM fraud (multiple calls coming from the same identity around the same time but in geographically different locations) in real-time so that they can easily respond by notifying customers or shutting down service.\n\nIn canonical Internet of Things (IoT) scenarios there is a ton of telemetry or sensor data being generated â€“ and customers want to aggregate them or alert over anomalies in real-time.\n\n## Prerequisites\n\nThis scenario leverages an event generator located on GitHub. Download it [here](https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator) and follow the steps in this tutorial to set up your solution.\n\n## Create an Azure Event Hubs input and Consumer Group\n\nThe sample application will generate events and push them to an Event Hub instance for real-time processing. Service Bus Event Hubs are the preferred method of event ingestion for Stream Analytics and you can learn more about Event Hubs in [Azure Service Bus documentation](/documentation/services/service-bus/).\n\nTo create an Event Hub:\n\n1.  In the [Azure portal](https://manage.windowsazure.com/) click **New** > **App Services** > **Service Bus** > **Event Hub** > **Quick Create**. Provide a name, region, and new or existing namespace to create a new Event Hub.  \n2.  As a best practice, each Stream Analytics job should read from a single Event Hub Consumer Group. We will walk you through the process of creating a Consumer Group below, and you can [learn more about Consumer Groups](https://msdn.microsoft.com/library/azure/dn836025.aspx). To create a Consumer Group, navigate to the newly created Event Hub and click the **Consumer Groups** tab, then click **Create** on the bottom of the page and provide a name for your Consumer Group.\n3.  To grant access to the Event Hub, we will need to create a shared access policy.  Click the **Configure** tab of your Event Hub.\n4.  Under **Shared Access Policies**, create a new policy with **Manage** permissions.\n\n    ![Shared Access Policies where you can create a policy with Manage permissions.](./media/stream-analytics-get-started/stream-ananlytics-shared-access-policies.png)\n\n5.  Click **Save** at the bottom of the page.\n6.  Navigate to the **Dashboard** and click **Connection Information** at the bottom of the page, and then copy and save the connection information.\n\n## Configure and start event generator application\n\nWe have provided a client application that will generate sample incoming call metadata and push it to Event Hub. Follow the steps below to set up this application.  \n\n1.  Download the TelcoGenerator solution from [https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator](https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator).\n2.  Replace the Microsoft.ServiceBus.ConnectionString and EventHubName values in App.Config with your Event Hub connection string and name.\n3.  Build the solution to trigger the download of required nuget packages.\n4.  Start the application. The usage is as follows:\n\n        telcodatagen [#NumCDRsPerHour] [SIM Card Fraud Probability] [#DurationHours]\n\nThe following example will generate 1000 events with a 20 percent probability of fraud over the course of 2 hours.\n\n    TelcoDataGen.exe 1000 .2 2\n\nYou will see records being sent to your Event Hub. Some key fields that we will be using in this real-time fraud detection application are defined here:\n\n| Record | Definition |\n| ------------- | ------------- |\n| CallrecTime | Timestamp for the call start time. |\n| SwitchNum | Telephone switch used to connect the call. |\n| CallingNum | Phone number of the caller. |\n| CallingIMSI | International Mobile Subscriber Identity (IMSI).  Unique identifier of the caller. |\n| CalledNum | Phone number of the call recipient. |\n| CalledIMSI | International Mobile Subscriber Identity (IMSI).  Unique identifier of the call recipient. |\n\n\n## Create Stream Analytics job\nNow that we have a stream of telecommunications events, we can set up a Stream Analytics job to analyze these events in real-time.\n\n### Provision a Stream Analytics job\n\n1.  In the Azure portal, click **New > Data Services > Stream Analytics > Quick Create**.\n2.  Specify the following values, and then click **Create Stream Analytics Job**:\n\n    * **Job Name**: Enter a job name.\n\n    * **Region**: Select the region where you want to run the job. Consider placing the job and the event hub in the same region to ensure better performance and to ensure that you will not be paying to transfer data between regions.\n\n    * **Storage Account**: Choose the Azure storage account that you would like to use to store monitoring data for all Stream Analytics jobs running within this region. You have the option to choose an existing storage account or to create a new one.\n\n3.  Click **Stream Analytics** in the left pane to list the Stream Analytics jobs.\n\n    ![Stream Analytics service icon](./media/stream-analytics-get-started/stream-analytics-service-icon.png)\n\n4.  The new job will be shown with a status of **Created**. Notice that the **Start** button on the bottom of the page is disabled. You must configure the job input, output, and query before you can start the job.\n\n### Specify job input\n1.  In your Stream Analytics job click **Inputs** from the top of the page, and then click **Add Input**. The dialog box that opens will walk you through a number of steps to set up your input.\n2.  Select **Data Stream**, and then click the right button.\n3.  Select **Event Hub**, and then click the right button.\n4.  Type or select the following values on the third page:\n\n    * **Input Alias**: Enter a friendly name for this job input such as *CallStream*. Note that you will be using this name in the query later.\n    * **Event Hub**: If the Event Hub you created is in the same subscription as the Stream Analytics job, select the namespace that the event hub is in.\n\n    If your event hub is in a different subscription, select **Use Event Hub from Another Subscription** and manually enter information for **Service Bus Namespace**, **Event Hub Name**, **Event Hub Policy Name**, **Event Hub Policy Key**, and **Event Hub Partition Count**.\n\n    * **Event Hub Name**: Select the name of the Event Hub.\n\n    * **Event Hub Policy Name**: Select the event-hub policy created earlier in this tutorial.\n\n    * **Event Hub Consumer Group**: Type the Consumer Group created earlier in this tutorial.\n5.  Click the right button.\n6.  Specify the following values:\n\n    * **Event Serializer Format**: JSON\n    * **Encoding**: UTF8\n7.  Click the check button to add this source and to verify that Stream Analytics can successfully connect to the event hub.\n\n### Specify job query\n\nStream Analytics supports a simple, declarative query model for describing transformations for real-time processing. To learn more about the language, see the [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/dn834998.aspx). This tutorial will help you author and test several queries over your real-time stream of call data.\n\n#### Optional: Sample input data\nTo validate your query against actual job data, you can use the **Sample Data** feature to extract events from your stream and create a .JSON file of the events for testing.  The following steps show how to do this and we have also provided a sample [Telco.json](https://github.com/Azure/azure-stream-analytics/blob/master/Sample%20Data/telco.json) file for testing purposes.\n\n1.  Select your Event Hub input and click **Sample Data** at the bottom of the page.\n2.  In the dialog box that appears, specify a **Start Time** to start collecting data from and a **Duration** for how much additional data to consume.\n3.  Click the check button to start sampling data from the input.  It can take a minute or two for the data file to be produced.  When the process is completed, click **Details** and download and save the .JSON file that is generated.\n\n    ![Download and save processed data in a JSON file](./media/stream-analytics-get-started/stream-analytics-download-save-json-file.png)\n\n#### Passthrough query\n\nIf you want to archive every event, you can use a passthrough query to read all the fields in the payload of the event or message. To start with, do a simple passthrough query that projects all the fields in an event.\n\n1.  Click **Query** from the top of the Stream Analytics job page.\n2.  Add the following to the code editor:\n\n        SELECT * FROM CallStream\n\n    > Make sure that the name of the input source matches the name of the input you specified earlier.\n\n3.  Click **Test** under the query editor.\n4.  Supply a test file, either one that you created using the previous steps or use [Telco.json](https://github.com/Azure/azure-stream-analytics/blob/master/Sample%20Data/telco.json).\n5.  Click the check button and see the results displayed below the query definition.\n\n    ![Query definition results](./media/stream-analytics-get-started/stream-analytics-sim-fraud-output.png)\n\n\n### Column projection\n\nWe'll now pare down the returned fields to a smaller set.\n\n1.  Change the query in the code editor to:\n\n        SELECT CallRecTime, SwitchNum, CallingIMSI, CallingNum, CalledNum\n        FROM CallStream\n\n2.  Click **Rerun** under the query editor to see the results of the query.\n\n    ![Output in query editor.](./media/stream-analytics-get-started/stream-analytics-query-editor-output.png)\n\n### Count of incoming calls by region: Tumbling window with aggregation\n\nTo compare the amount that incoming calls per region we'll leverage a [TumblingWindow](https://msdn.microsoft.com/library/azure/dn835055.aspx) to get the count of incoming calls grouped by SwitchNum every 5 seconds.\n\n1.  Change the query in the code editor to:\n\n        SELECT System.Timestamp as WindowEnd, SwitchNum, COUNT(*) as CallCount\n        FROM CallStream TIMESTAMP BY CallRecTime\n        GROUP BY TUMBLINGWINDOW(s, 5), SwitchNum\n\n    This query uses the **Timestamp By** keyword to specify a timestamp field in the payload to be used in the temporal computation. If this field wasn't specified, the windowing operation would be performed using the time each event arrived at Event Hub. See [\"Arrival Time Vs Application Time\" in the Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx).\n\n    Note that you can access a timestamp for the end of each window by using the **System.Timestamp** property.\n\n2.  Click **Rerun** under the query editor to see the results of the query.\n\n    ![Query results for Timestand By](./media/stream-analytics-get-started/stream-ananlytics-query-editor-rerun.png)\n\n### SIM fraud detection with a Self-Join\n\nTo identify potentially fraudulent usage we'll look for calls originating from the same user but in different locations in less than 5 seconds.  We [join](https://msdn.microsoft.com/library/azure/dn835026.aspx) the stream of call events with itself to check for these cases.\n\n1.  Change the query in the code editor to:\n\n        SELECT System.Timestamp as Time, CS1.CallingIMSI, CS1.CallingNum as CallingNum1,\n        CS2.CallingNum as CallingNum2, CS1.SwitchNum as Switch1, CS2.SwitchNum as Switch2\n        FROM CallStream CS1 TIMESTAMP BY CallRecTime\n        JOIN CallStream CS2 TIMESTAMP BY CallRecTime\n        ON CS1.CallingIMSI = CS2.CallingIMSI\n        AND DATEDIFF(ss, CS1, CS2) BETWEEN 1 AND 5\n        WHERE CS1.SwitchNum != CS2.SwitchNum\n\n2.  Click **Rerun** under the query editor to see the results of the query.\n\n    ![Query results of a join](./media/stream-analytics-get-started/stream-ananlytics-query-editor-join.png)\n\n### Create output sink\n\nNow that we have defined an event stream, an Event Hub input to ingest events, and a query to perform a transformation over the stream, the last step is to define an output sink for the job.  We'll write events for fraudulent behavior to Blob storage.\n\nFollow the steps below to create a container for Blob storage if you don't already have one.\n\n1.  Use an existing storage account or create a new storage account by clicking **NEW > DATA SERVICES > STORAGE > QUICK CREATE** and following the instructions.\n2.  Select the storage account, click **CONTAINERS** at the top of the page, and then click **ADD**.\n3.  Specify a **NAME** for your container and set its **ACCESS** to Public Blob.\n\n## Specify job output\n\n1.  In your Stream Analytics job click **OUTPUT** from the top of the page, and then click **ADD OUTPUT**. The dialog box that opens will walk you through a number of steps to set up your output.\n2.  Select **BLOB STORAGE**, and then click the right button.\n3.  Type or select the following values on the third page:\n\n    * **OUTPUT ALIAS**: Enter a friendly name for this job output.\n    * **SUBSCRIPTION**: If the Blob storage you created is in the same subscription as the Stream Analytics job, select **Use Storage Account from Current Subscription**. If your storage is in a different subscription, select **Use Storage Account from Another Subscription** and manually enter information for **STORAGE ACCOUNT**, **STORAGE ACCOUNT KEY**, **CONTAINER**.\n    * **STORAGE ACCOUNT**: Select the name of the storage account.\n    * **CONTAINER**: Select the name of the container.\n    * **FILENAME PREFIX**: Type in a file prefix to use when writing blob output.\n\n4.  Click the right button.\n5.  Specify the following values:\n\n    * **EVENT SERIALIZER FORMAT**: JSON\n    * **ENCODING**: UTF8\n\n6.  Click the check button to add this source and to verify that Stream Analytics can successfully connect to the storage account.\n\n## Start job for real time processing\n\nSince a job input, query, and output have all been specified, we are ready to start the Stream Analytics job for real-time fraud detection.\n\n1.  From the job **DASHBOARD**, click **START** at the bottom of the page.\n2.  In the dialog box that appears, select **JOB START TIME** and then click the check button on the bottom of the dialog box. The job status will change to **Starting** and will shortly move to **Running**.\n\n## View fraud detection output\n\nUse a tool like [Azure Storage Explorer](https://azurestorageexplorer.codeplex.com/) or [Azure Explorer](http://www.cerebrata.com/products/azure-explorer/introduction) to view fraudulent events as they are written to your output in real-time.  \n\n![Fraud detection: Fraudulent events viewed in real-time](./media/stream-analytics-get-started/stream-ananlytics-view-real-time-fraudent-events.png)\n\n## Get support\nFor further assistance, try our [Azure Stream Analytics forum](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics).\n\n\n## Next steps\n\n- [Introduction to Azure Stream Analytics](stream-analytics-introduction.md)\n- [Get started using Azure Stream Analytics](stream-analytics-get-started.md)\n- [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n- [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx)\n- [Azure Stream Analytics Management REST API Reference](https://msdn.microsoft.com/library/azure/dn835031.aspx)\n"
}