<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Common Scenarios for using Azure Data Factory</source>
          <target state="new">Common Scenarios for using Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn about a few common scenarios for using the Azure Data Factory service</source>
          <target state="new">Learn about a few common scenarios for using the Azure Data Factory service</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Common scenarios for using Azure Data Factory</source>
          <target state="new">Common scenarios for using Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>This section describes a few example scenarios that the Azure Data Factory can support today, and will continue to grow as Hub scenarios.</source>
          <target state="new">This section describes a few example scenarios that the Azure Data Factory can support today, and will continue to grow as Hub scenarios.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Read through the <bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">][datafactory-introduction]</ept> article before reading through this one.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Read through the <bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">][datafactory-introduction]</ept> article before reading through this one.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Scenario #1: Data Sources for the Data Hub</source>
          <target state="new">Scenario #1: Data Sources for the Data Hub</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Source the Data Hub</source>
          <target state="new">Source the Data Hub</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Enterprises have data of disparate types located in disparate sources.</source>
          <target state="new">Enterprises have data of disparate types located in disparate sources.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed for subsequent processing.</source>
          <target state="new">The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed for subsequent processing.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Without Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.</source>
          <target state="new">Without Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>This is expensive, and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.</source>
          <target state="new">This is expensive, and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>With Azure Data Factory data storage and processing services are collected into a Hub container which facilitates and optimizes computation and storage activities, enables unified resource consumption management, and provides services for data movement as-needed.</source>
          <target state="new">With Azure Data Factory data storage and processing services are collected into a Hub container which facilitates and optimizes computation and storage activities, enables unified resource consumption management, and provides services for data movement as-needed.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Scenario #2: Operationalize Information Production</source>
          <target state="new">Scenario #2: Operationalize Information Production</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Operationalization scenarios are the next logical step after data sourcing scenarios.</source>
          <target state="new">Operationalization scenarios are the next logical step after data sourcing scenarios.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Once data is present in a Hub, you want to author and operationalize data pipelines to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.</source>
          <target state="new">Once data is present in a Hub, you want to author and operationalize data pipelines to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Data transformation in Azure Data Factory is through Hive, Pig and custom C# processing running on Hadoop (Azure HDInsight).</source>
          <target state="new">Data transformation in Azure Data Factory is through Hive, Pig and custom C# processing running on Hadoop (Azure HDInsight).</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>These transformations can be used to clean data, mask critical data fields, and perform other operations on the data in a wide variety of complex ways.</source>
          <target state="new">These transformations can be used to clean data, mask critical data fields, and perform other operations on the data in a wide variety of complex ways.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Ordinarily, operationalization is achieved with complex and hard to maintain infrastructure and custom services, and poses a number of challenges for implementation, management, scaling, troubleshooting, and versioning such a solution.</source>
          <target state="new">Ordinarily, operationalization is achieved with complex and hard to maintain infrastructure and custom services, and poses a number of challenges for implementation, management, scaling, troubleshooting, and versioning such a solution.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>With Data Factory as a fully managed service, users can operationalize these pipelines by defining them with one-time or complex recurring schedules, and orchestration is handled directly by the Data Factory service.</source>
          <target state="new">With Data Factory as a fully managed service, users can operationalize these pipelines by defining them with one-time or complex recurring schedules, and orchestration is handled directly by the Data Factory service.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>With Hubs, cluster management for all of the data and processing within a Hub is handled on behalf of the user, so users can focus on transformative analytics instead on infrastructure management.</source>
          <target state="new">With Hubs, cluster management for all of the data and processing within a Hub is handled on behalf of the user, so users can focus on transformative analytics instead on infrastructure management.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Azure Data Factory removes the challenges of working with brittle custom services, and enables enterprises to produce trusted information reliably and reproducibly.</source>
          <target state="new">Azure Data Factory removes the challenges of working with brittle custom services, and enables enterprises to produce trusted information reliably and reproducibly.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Scenario #3:  Integrate Information Production with data discovery</source>
          <target state="new">Scenario #3:  Integrate Information Production with data discovery</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Traditional BI approaches and technologies, while providing an “authoritative source of the truth”, almost always have a serious side effect: a constant backlog of requests due to a carefully controlled change request process.</source>
          <target state="new">Traditional BI approaches and technologies, while providing an “authoritative source of the truth”, almost always have a serious side effect: a constant backlog of requests due to a carefully controlled change request process.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>To adapt to quickly changing business questions, there is a need for greater flexibility for enterprises to connect their information production systems with their information consumption systems.</source>
          <target state="new">To adapt to quickly changing business questions, there is a need for greater flexibility for enterprises to connect their information production systems with their information consumption systems.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Azure Data Factory helps address the challenge of connecting these systems with streamlined data pipelines for information production, and the information consumption challenge by making up-to-date trusted data available in easily consumable forms.</source>
          <target state="new">Azure Data Factory helps address the challenge of connecting these systems with streamlined data pipelines for information production, and the information consumption challenge by making up-to-date trusted data available in easily consumable forms.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Azure Data Factory supports the following capabilities to enable simple consumption of the data produced:</source>
          <target state="new">Azure Data Factory supports the following capabilities to enable simple consumption of the data produced:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Easily move (one time or scheduled) the produced data assets to relational data marts for consumption using existing BI tools (Excel, Tableau, etc…).</source>
          <target state="new">Easily move (one time or scheduled) the produced data assets to relational data marts for consumption using existing BI tools (Excel, Tableau, etc…).</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Consume data assets produced by a data factory directly using Power Query in Excel.</source>
          <target state="new">Consume data assets produced by a data factory directly using Power Query in Excel.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>See the following topics for consuming data using Power Query:</source>
          <target state="new">See the following topics for consuming data using Power Query:</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Power Query: Connect to Microsoft Azure Table Storage</source>
          <target state="new">Power Query: Connect to Microsoft Azure Table Storage</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Power Query: Connect to Microsoft Azure Blob Storage</source>
          <target state="new">Power Query: Connect to Microsoft Azure Blob Storage</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Power Query: Connect to Microsoft Azure SQL Database</source>
          <target state="new">Power Query: Connect to Microsoft Azure SQL Database</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Power Query: Connect to Microsoft On-premises SQL Server</source>
          <target state="new">Power Query: Connect to Microsoft On-premises SQL Server</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1ec37f1fb14b8b4f0ab1975d3976653a2b4ba718</xliffext:olfilehash>
  </header>
</xliff>