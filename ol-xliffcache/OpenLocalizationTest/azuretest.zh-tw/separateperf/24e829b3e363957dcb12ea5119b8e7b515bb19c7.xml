{
  "nodes": [
    {
      "content": "Troubleshoot Azure Data Factory issues",
      "pos": [
        28,
        66
      ]
    },
    {
      "content": "Learn how to troubleshoot issues with using Azure Data Factory.",
      "pos": [
        86,
        149
      ]
    },
    {
      "content": "Troubleshoot Data Factory issues",
      "pos": [
        477,
        509
      ]
    },
    {
      "content": "You can troubleshoot Azure Data Factory issues using Azure Portal (or) Azure PowerShell cmdlets.",
      "pos": [
        510,
        606
      ]
    },
    {
      "content": "This topic has walkthroughs that show you how to use the Azure Portal to quickly troubleshoot errors that you encounter with Data Factory.",
      "pos": [
        607,
        745
      ]
    },
    {
      "content": "Problem: Not able to run Data Factory cmdlets",
      "pos": [
        751,
        796
      ]
    },
    {
      "pos": [
        797,
        866
      ],
      "content": "To resolve this issue, switch Azure mode to <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept>:"
    },
    {
      "pos": [
        869,
        1060
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept> and execute the following command to switch to the <bpt id=\"p2\">**</bpt>AzureResourceManager<ept id=\"p2\">**</ept> mode.The Azure Data Factory cmdlets are available in the <bpt id=\"p3\">**</bpt>AzureResourceManager<ept id=\"p3\">**</ept> mode."
    },
    {
      "content": "Problem: Unauthorized error when running a Data Factory cmdlet",
      "pos": [
        1113,
        1175
      ]
    },
    {
      "content": "You are probably not using the right Azure account or subscription with the Azure PowerShell.",
      "pos": [
        1176,
        1269
      ]
    },
    {
      "content": "Use the following cmdlets to select the right Azure account and subscription to use with the Azure PowerShell.",
      "pos": [
        1270,
        1380
      ]
    },
    {
      "content": "Add-AzureAccount - Use the right user ID and password",
      "pos": [
        1386,
        1439
      ]
    },
    {
      "content": "Get-AzureSubscription - View all the subscriptions for the account.",
      "pos": [
        1443,
        1510
      ]
    },
    {
      "content": "Select-AzureSubscription",
      "pos": [
        1515,
        1539
      ]
    },
    {
      "content": "- Select the right subscription.",
      "pos": [
        1560,
        1592
      ]
    },
    {
      "content": "Use the same one you use to create a data factory on the Azure Preview Portal.",
      "pos": [
        1593,
        1671
      ]
    },
    {
      "content": "Problem: Fail to launch Data Gateway Express Setup from Azure Portal",
      "pos": [
        1676,
        1744
      ]
    },
    {
      "content": "The Express Setup for the Data Gateway requires Internet Explorer or a Microsoft ClickOnce compatible web browser.",
      "pos": [
        1745,
        1859
      ]
    },
    {
      "content": "If you fails to start the Express Setup, you can",
      "pos": [
        1860,
        1908
      ]
    },
    {
      "content": "Switch to Internet Explorer if you fails with other browsers.",
      "pos": [
        1913,
        1974
      ]
    },
    {
      "content": "Or",
      "pos": [
        1975,
        1977
      ]
    },
    {
      "content": "Use the \"Manual Setup\" links shown on the same blade in the portal to do the installation, and then copy the Key that is provided on the screen, and paste when the Data Management Gateway configuration is ready.",
      "pos": [
        1981,
        2192
      ]
    },
    {
      "content": "If it doesn't launch, check your start menu for \"Microsoft Data Management Gateway\" and paste in the key when it launches.",
      "pos": [
        2193,
        2315
      ]
    },
    {
      "content": "Problem: Fail to launch Credentials Manager from Azure Portal",
      "pos": [
        2322,
        2383
      ]
    },
    {
      "content": "When set up or update a SQL Server Linked Service via Azure Portal, the Credentials Manager application will be launched to guarantee security.",
      "pos": [
        2384,
        2527
      ]
    },
    {
      "content": "It requires Internet Explorer or a Microsoft ClickOnce compatible web browser.",
      "pos": [
        2528,
        2606
      ]
    },
    {
      "content": "You can switch to Internet Explorer if you fails with other browsers.",
      "pos": [
        2607,
        2676
      ]
    },
    {
      "content": "Problem: Fail to connect to on-premises SQL Server",
      "pos": [
        2681,
        2731
      ]
    },
    {
      "content": "Verify that the SQL Server is reachable from the machine where the gateway is installed.",
      "pos": [
        2733,
        2821
      ]
    },
    {
      "content": "On the machine on which the gateway is installed, you can",
      "pos": [
        2822,
        2879
      ]
    },
    {
      "content": "Ping the machine where the SQL Server is installed.",
      "pos": [
        2884,
        2935
      ]
    },
    {
      "content": "Or",
      "pos": [
        2936,
        2938
      ]
    },
    {
      "content": "Try connecting to the SQL Server instance using the credentials you specified on the Azure Portal using SQL Server Management Studio (SSMS).",
      "pos": [
        2942,
        3082
      ]
    },
    {
      "content": "Problem: Input slices are in PendingExecution or PendingValidation state for ever",
      "pos": [
        3088,
        3169
      ]
    },
    {
      "content": "The slices could be in <bpt id=\"p1\">**</bpt>PendingExecution<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>PendingValidation<ept id=\"p2\">**</ept> state due to a number of reasons and one of the common reasons is that the <bpt id=\"p3\">**</bpt>external<ept id=\"p3\">**</ept> property is not set to <bpt id=\"p4\">**</bpt>true<ept id=\"p4\">**</ept>.",
      "pos": [
        3171,
        3360
      ]
    },
    {
      "content": "Any dataset that is produced outside the scope of Azure Data Factory should be marked with <bpt id=\"p1\">**</bpt>external<ept id=\"p1\">**</ept> property .",
      "pos": [
        3361,
        3475
      ]
    },
    {
      "content": "This indicates that the data is external and not backed by any pipelines within the data factory.",
      "pos": [
        3476,
        3573
      ]
    },
    {
      "content": "The data slices are marked as <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> once the data is available in the respective store.",
      "pos": [
        3574,
        3665
      ]
    },
    {
      "content": "See the following example for the usage of the <bpt id=\"p1\">**</bpt>external<ept id=\"p1\">**</ept> property.",
      "pos": [
        3668,
        3737
      ]
    },
    {
      "content": "You can optionally specify <bpt id=\"p1\">**</bpt>externalData<ept id=\"p1\">**</ept>* when you set external to true..",
      "pos": [
        3738,
        3814
      ]
    },
    {
      "pos": [
        3817,
        3927
      ],
      "content": "See Tables topic in <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">][json-scripting-reference]</ept> for more details about this property."
    },
    {
      "pos": [
        4648,
        4807
      ],
      "content": "To resolve the error, add the <bpt id=\"p1\">**</bpt>external<ept id=\"p1\">**</ept> property and the optional <bpt id=\"p2\">**</bpt>externalData<ept id=\"p2\">**</ept> section to the JSON definition of the input table and recreate the table."
    },
    {
      "content": "Problem: Hybrid copy operation fails",
      "pos": [
        4813,
        4849
      ]
    },
    {
      "content": "To learn more details:",
      "pos": [
        4850,
        4872
      ]
    },
    {
      "content": "Launch Data Management Gateway Configuration Manager on the machine on which gateway was installed.",
      "pos": [
        4877,
        4976
      ]
    },
    {
      "content": "Verify that the <bpt id=\"p1\">**</bpt>Gateway name<ept id=\"p1\">**</ept> is set to the logical gateway name on the <bpt id=\"p2\">**</bpt>Azure Portal<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>Gateway key status<ept id=\"p3\">**</ept> is <bpt id=\"p4\">**</bpt>registered<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>Service status<ept id=\"p5\">**</ept> is <bpt id=\"p6\">**</bpt>Started<ept id=\"p6\">**</ept>.",
      "pos": [
        4977,
        5149
      ]
    },
    {
      "content": "Launch <bpt id=\"p1\">**</bpt>Event Viewer<ept id=\"p1\">**</ept>.",
      "pos": [
        5154,
        5178
      ]
    },
    {
      "content": "Expand <bpt id=\"p1\">**</bpt>Applications and Services Logs<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Data Management Gateway<ept id=\"p2\">**</ept>.",
      "pos": [
        5179,
        5259
      ]
    },
    {
      "content": "See if there are any errors related to Data Management Gateway.",
      "pos": [
        5260,
        5323
      ]
    },
    {
      "content": "Problem: On Demand HDInsight Provisioning Fails with Error",
      "pos": [
        5329,
        5387
      ]
    },
    {
      "content": "When using a linked service of type HDInsightOnDemandLinkedService, you should specify a linkedServiceName that points to  Azure Blob Storage.",
      "pos": [
        5389,
        5531
      ]
    },
    {
      "content": "This storage account will be used to copy all the logs and supporting files for your on-demand HDInsight cluster.",
      "pos": [
        5532,
        5645
      ]
    },
    {
      "content": "Sometimes the activity that does the on-demand provisioning on HDInsight may fail with the following error:",
      "pos": [
        5647,
        5754
      ]
    },
    {
      "content": "This error usually indicates that the Location of the storage account specified in the linkedServiceName is not in the same data center location as where the HDInsight provisioning is happening.",
      "pos": [
        5960,
        6154
      ]
    },
    {
      "content": "For example, if your Azure Data Factory location is West US, and the on-demand HDInsight provisioning happens in West US, but the Azure blob storage account location  is set to East US, the on-demand provisioning will fail.",
      "pos": [
        6155,
        6378
      ]
    },
    {
      "content": "Additionally, there is a second JSON property additionalLinkedServiceNames where additional storage accounts may be specified in on-demand HDInsight.",
      "pos": [
        6380,
        6529
      ]
    },
    {
      "content": "Those additional linked storage accounts should be in the same location as the HDInsight cluster, or it will fail with the same error.",
      "pos": [
        6530,
        6664
      ]
    },
    {
      "content": "Problem: Custom Activity Fails",
      "pos": [
        6671,
        6701
      ]
    },
    {
      "content": "When using a Custom Activity in Azure Data Factory (pipeline activity type CustomActivity), the custom application runs in the specified linked service to HDInsight as a Map only streaming MapReduce job.",
      "pos": [
        6702,
        6905
      ]
    },
    {
      "content": "When the custom activity runs, Azure Data Factory will be able to capture that output from the HDInsight cluster, and save it in the <bpt id=\"p1\">*</bpt>adfjobs<ept id=\"p1\">*</ept> storage container in your Azure Blob Storage account.",
      "pos": [
        6908,
        7104
      ]
    },
    {
      "content": "In case of an error, you can read the text from <bpt id=\"p1\">**</bpt>stderr<ept id=\"p1\">**</ept> output text file after a failure has occurred.",
      "pos": [
        7105,
        7210
      ]
    },
    {
      "content": "The files are accessible and readable from the Azure portal itself in the web browser, or by using storage explorer tools to access the files kept in the storage container in Azure Blob Storage directly.",
      "pos": [
        7211,
        7414
      ]
    },
    {
      "content": "To enumerate and read the logs for a particular Custom Activity, you may follow one of the illustrated walkthroughs later on this page.",
      "pos": [
        7417,
        7552
      ]
    },
    {
      "content": "In summary:",
      "pos": [
        7553,
        7564
      ]
    },
    {
      "pos": [
        7570,
        7629
      ],
      "content": "In the Azure portal <bpt id=\"p1\">**</bpt>Browse<ept id=\"p1\">**</ept> to locate your Data Factory."
    },
    {
      "pos": [
        7634,
        7799
      ],
      "content": "Use the <bpt id=\"p1\">**</bpt>Diagram<ept id=\"p1\">**</ept> button to view the data factory diagram, and click on the <bpt id=\"p2\">**</bpt>Dataset<ept id=\"p2\">**</ept> Table that follows the specific <bpt id=\"p3\">**</bpt>Pipeline<ept id=\"p3\">**</ept> which has the Custom Activity."
    },
    {
      "pos": [
        7805,
        7924
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Table<ept id=\"p1\">**</ept> blade, Click on the slice of interest in the <bpt id=\"p2\">**</bpt>Problem slices<ept id=\"p2\">**</ept> for the time frame to be investigated."
    },
    {
      "content": "The detailed <bpt id=\"p1\">**</bpt>Data Slice<ept id=\"p1\">**</ept> blade will appear and it can list multiple <bpt id=\"p2\">**</bpt>Activity runs<ept id=\"p2\">**</ept> for the slice.",
      "pos": [
        7929,
        8032
      ]
    },
    {
      "content": "Click on an <bpt id=\"p1\">**</bpt>Activity<ept id=\"p1\">**</ept> from the list.",
      "pos": [
        8033,
        8072
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Activity Run Details<ept id=\"p1\">**</ept> blade will appear.",
      "pos": [
        8078,
        8125
      ]
    },
    {
      "content": "It will list the <bpt id=\"p1\">**</bpt>Error Message<ept id=\"p1\">**</ept> in the middle of the blade, and several <bpt id=\"p2\">**</bpt>Log files<ept id=\"p2\">**</ept> listed at the bottom of the blade affiliated with that activity run.",
      "pos": [
        8126,
        8283
      ]
    },
    {
      "content": "Logs/system-0.log",
      "pos": [
        8290,
        8307
      ]
    },
    {
      "content": "Status",
      "pos": [
        8314,
        8320
      ]
    },
    {
      "content": "Status/exit",
      "pos": [
        8327,
        8338
      ]
    },
    {
      "content": "Status/stderr",
      "pos": [
        8345,
        8358
      ]
    },
    {
      "content": "Status/stdout",
      "pos": [
        8365,
        8378
      ]
    },
    {
      "content": "Click on the first <bpt id=\"p1\">**</bpt>Log file<ept id=\"p1\">**</ept> item in the list, and the log will open in a new blade with the full text displayed for you to read.",
      "pos": [
        8383,
        8515
      ]
    },
    {
      "content": "Review the text of each log by clicking on each one.",
      "pos": [
        8516,
        8568
      ]
    },
    {
      "content": "The text viewer blade will open.",
      "pos": [
        8569,
        8601
      ]
    },
    {
      "content": "You can click the <bpt id=\"p1\">**</bpt>Download<ept id=\"p1\">**</ept> button to download the text file for optional offline viewing.",
      "pos": [
        8602,
        8695
      ]
    },
    {
      "content": "One <bpt id=\"p1\">**</bpt>common error<ept id=\"p1\">**</ept> from a custom activity is",
      "pos": [
        8699,
        8745
      ]
    },
    {
      "content": "Package execution failed with exit code '1'.",
      "pos": [
        8755,
        8799
      ]
    },
    {
      "content": "See 'wasb://adfjobs@storageaccount.blob.core.windows.net/PackageJobs/",
      "pos": [
        8800,
        8869
      ]
    },
    {
      "content": "/",
      "pos": [
        8875,
        8876
      ]
    },
    {
      "content": "/Status/stderr' for more details.",
      "pos": [
        8883,
        8916
      ]
    },
    {
      "content": "To see more details for this kind of error, open the <bpt id=\"p1\">**</bpt>stderr<ept id=\"p1\">**</ept> file.",
      "pos": [
        8918,
        8987
      ]
    },
    {
      "content": "One common error seen there is a timeout condition such as this:",
      "pos": [
        8988,
        9052
      ]
    },
    {
      "content": "INFO mapreduce.Job: Task Id : attempt_1424212573646_0168_m_000000_0, Status : FAILED",
      "pos": [
        9061,
        9145
      ]
    },
    {
      "content": "AttemptID:attempt_1424212573646_0168_m_000000_0 Timed out after 600 secs",
      "pos": [
        9155,
        9227
      ]
    },
    {
      "content": "This same error may appear multiple times, if the job has retried 3 times for example, over the span of 30 or more minutes.",
      "pos": [
        9229,
        9352
      ]
    },
    {
      "content": "This time out error indicates a 600 second (10 minute) timeout has happened.",
      "pos": [
        9355,
        9431
      ]
    },
    {
      "content": "Typically this means the custom .Net application has not issued any status update for 10 minutes.",
      "pos": [
        9432,
        9529
      ]
    },
    {
      "content": "If the application is hanging or stalled waiting on something for too long, the 10 minute timeout is a safety mechanism to prevent it from waiting forever and delaying your Azure Data Factory pipeline.",
      "pos": [
        9530,
        9731
      ]
    },
    {
      "content": "This time out originates in the configuration of HDInsight cluster that is linked in the custom activity.",
      "pos": [
        9734,
        9839
      ]
    },
    {
      "content": "The setting is <bpt id=\"p1\">**</bpt>mapred.task.timeout<ept id=\"p1\">**</ept>, which defaults to 600000 milliseconds, as documented in the Apache default settings here: http://hadoop.apache.org/docs/r2.4.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml",
      "pos": [
        9840,
        10078
      ]
    },
    {
      "content": "You can overide this default by changing the defaults at the time of provisioning your HDInsight provisioning cluster.",
      "pos": [
        10080,
        10198
      ]
    },
    {
      "content": "When using Azure Data Factory and <bpt id=\"p1\">**</bpt>HDInsight On-demand<ept id=\"p1\">**</ept> linked service, the JSON property can be added near your HDInsightOnDemandLinkedService JSON properties.",
      "pos": [
        10199,
        10361
      ]
    },
    {
      "content": "For example, you can increase the value to 20 minutes using this JSON property.",
      "pos": [
        10362,
        10441
      ]
    },
    {
      "content": "For more context and a full example of the JSON to edit these map reduce Configuration properties see Example #3 in the MSDN documentation here https://msdn.microsoft.com/library/azure/dn893526.aspx",
      "pos": [
        10563,
        10761
      ]
    },
    {
      "content": "Problem: PowerShell request fails with error error 400 Bad Request \"No registered resource provider found...\"",
      "pos": [
        10766,
        10875
      ]
    },
    {
      "content": "As of March 10, 2015, the Azure Data Factory PowerShell early private preview versions 2014-05-01-preview, 2014-07-01-preview, and 2014-08-01-preview will be discontinued.",
      "pos": [
        10877,
        11048
      ]
    },
    {
      "content": "We recommend that you use the latest version of the ADF cmdlets, which are now part of the Azure PowerShell Download, such as the download from this URL http://go.microsoft.com/?linkid=9811175&amp;clcid=0x409",
      "pos": [
        11049,
        11253
      ]
    },
    {
      "content": "If you use the discontinued versions of the Azure PowerShell SDK you may receive the following errors:",
      "pos": [
        11256,
        11358
      ]
    },
    {
      "pos": [
        12068,
        12154
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"copywalkthrough\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Walkthrough: Troubleshooting an error with copying data"
    },
    {
      "content": "In this walkthrough, you will introduce an error in the tutorial from Get started with Data Factory article and learn how you can use Azure Portal to troubleshoot the error.",
      "pos": [
        12155,
        12328
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        12334,
        12347
      ]
    },
    {
      "pos": [
        12351,
        12441
      ],
      "content": "Complete the Tutorial in the <bpt id=\"p1\">[</bpt>Get started with Azure Data Factory<ept id=\"p1\">][adfgetstarted]</ept> article."
    },
    {
      "pos": [
        12445,
        12550
      ],
      "content": "Confirm that the <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept> produces data in the <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept> table in the Azure SQL Database."
    },
    {
      "content": "Now, delete the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table (<bpt id=\"p2\">**</bpt>drop table emp<ept id=\"p2\">**</ept>) from the Azure SQL Database.",
      "pos": [
        12556,
        12635
      ]
    },
    {
      "content": "This will introduce an error.",
      "pos": [
        12636,
        12665
      ]
    },
    {
      "pos": [
        12669,
        12849
      ],
      "content": "Run the following command in the <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept> to update the active period for the pipeline so that it tries to write data to the <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept> table, which doesn’t exist anymore."
    },
    {
      "content": "Use Azure Preview Portal to troubleshoot the error",
      "pos": [
        13184,
        13234
      ]
    },
    {
      "pos": [
        13240,
        13294
      ],
      "content": "Login to <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">][azure-preview-portal]</ept>."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>ADFTutorialDataFactory<ept id=\"p1\">**</ept> from the <bpt id=\"p2\">**</bpt>Startboard<ept id=\"p2\">**</ept>.",
      "pos": [
        13300,
        13357
      ]
    },
    {
      "content": "If you don’t see the data factory link on the <bpt id=\"p1\">**</bpt>Startboard<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>BROWSE<ept id=\"p2\">**</ept> hub and click <bpt id=\"p3\">**</bpt>Everything<ept id=\"p3\">**</ept>.",
      "pos": [
        13358,
        13466
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Data factories…<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>Browse<ept id=\"p2\">**</ept> blade, and click <bpt id=\"p3\">**</bpt>ADFTutorialDataFactory<ept id=\"p3\">**</ept>.",
      "pos": [
        13467,
        13555
      ]
    },
    {
      "content": "Notice that you see <bpt id=\"p1\">**</bpt>With errors<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>Datasets<ept id=\"p2\">**</ept> tile.",
      "pos": [
        13560,
        13621
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>With errors<ept id=\"p1\">**</ept>.",
      "pos": [
        13622,
        13644
      ]
    },
    {
      "content": "You should see <bpt id=\"p1\">**</bpt>Datasets with errors<ept id=\"p1\">**</ept> blade.",
      "pos": [
        13645,
        13691
      ]
    },
    {
      "content": "Data Factory with Errors link",
      "pos": [
        13699,
        13728
      ]
    },
    {
      "pos": [
        13783,
        13871
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> with errors blade, click <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept> to see the <bpt id=\"p3\">**</bpt>TABLE<ept id=\"p3\">**</ept> blade."
    },
    {
      "content": "Datasets with errors blade",
      "pos": [
        13880,
        13906
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>TABLE<ept id=\"p1\">**</ept> blade, you should see the problem slices, i.e., slices with an error in the <bpt id=\"p2\">**</bpt>Problem slices<ept id=\"p2\">**</ept> list at the bottom.",
      "pos": [
        13972,
        14103
      ]
    },
    {
      "content": "You can also see any recent slices with errors in the <bpt id=\"p1\">**</bpt>Recent slices<ept id=\"p1\">**</ept> list.",
      "pos": [
        14104,
        14181
      ]
    },
    {
      "content": "Click on a slice in the <bpt id=\"p1\">**</bpt>Problem slices<ept id=\"p1\">**</ept> list.",
      "pos": [
        14182,
        14230
      ]
    },
    {
      "content": "Table blade with problem slices",
      "pos": [
        14239,
        14270
      ]
    },
    {
      "pos": [
        14342,
        14546
      ],
      "content": "If you click <bpt id=\"p1\">**</bpt>Problem slices<ept id=\"p1\">**</ept> (not on a specific problem), you will see the <bpt id=\"p2\">**</bpt>DATA SLICES<ept id=\"p2\">**</ept> blade and then click a <bpt id=\"p3\">**</bpt>specific problem slice<ept id=\"p3\">**</ept> to see the <bpt id=\"p4\">**</bpt>DATA SLICE<ept id=\"p4\">**</ept> slide for the selected data slice."
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade for <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>, you see all <bpt id=\"p3\">**</bpt>activity runs<ept id=\"p3\">**</ept> for the slice in the list at the bottom.",
      "pos": [
        14551,
        14670
      ]
    },
    {
      "content": "Click on an <bpt id=\"p1\">**</bpt>activity run<ept id=\"p1\">**</ept> from the list that failed.",
      "pos": [
        14671,
        14726
      ]
    },
    {
      "content": "Data Slice blade with active runs",
      "pos": [
        14734,
        14767
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Activity Run Details<ept id=\"p1\">**</ept> blade for the activity run you selected, you should see details about the error.",
      "pos": [
        14840,
        14952
      ]
    },
    {
      "content": "In this scenario, you see: <bpt id=\"p1\">**</bpt>Invalid object name ‘emp’<ept id=\"p1\">**</ept>.",
      "pos": [
        14953,
        15010
      ]
    },
    {
      "content": "Activity run details with an error",
      "pos": [
        15018,
        15052
      ]
    },
    {
      "pos": [
        15112,
        15241
      ],
      "content": "To resolve this issue, create the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table using the SQL script from <bpt id=\"p2\">[</bpt>Get started with Data Factory<ept id=\"p2\">][adfgetstarted]</ept> article."
    },
    {
      "content": "Use Azure PowerShell cmdlets to troubleshoot the error",
      "pos": [
        15248,
        15302
      ]
    },
    {
      "pos": [
        15307,
        15335
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        15341,
        15441
      ],
      "content": "Switch to <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode as the Data Factory cmdlets are available only in this mode."
    },
    {
      "content": "Run Get-AzureDataFactorySlice command to see the slices and their statuses.",
      "pos": [
        15503,
        15578
      ]
    },
    {
      "content": "You should see a slice with the status: Failed.",
      "pos": [
        15579,
        15626
      ]
    },
    {
      "pos": [
        16474,
        16570
      ],
      "content": "Now, run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryRun<ept id=\"p1\">**</ept> cmdlet to get details about activity run for the slice."
    },
    {
      "content": "The value of <bpt id=\"p1\">**</bpt>StartDateTime<ept id=\"p1\">**</ept> is the Start time for the error/problem slice you noted from the previous step.",
      "pos": [
        16764,
        16874
      ]
    },
    {
      "content": "The date-time should be enclosed in double quotes.",
      "pos": [
        16875,
        16925
      ]
    },
    {
      "content": "You should see the output with details about the error (similar to the following):",
      "pos": [
        16929,
        17011
      ]
    },
    {
      "pos": [
        18195,
        18291
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"pighivewalkthrough\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Walkthrough: Troubleshooting an error with Hive/Pig processing"
    },
    {
      "content": "This walkthrough provides steps to troubleshoot an error with Hive/Pig processing by using both Azure Preview Portal and Azure PowerShell.",
      "pos": [
        18292,
        18430
      ]
    },
    {
      "content": "Walkthrough: Use Azure Portal to troubleshoot an error with Pig/Hive processing",
      "pos": [
        18438,
        18517
      ]
    },
    {
      "content": "In this scenario, data set is in an error state due to a failure in Hive processing on an HDInsight cluster.",
      "pos": [
        18518,
        18626
      ]
    },
    {
      "pos": [
        18631,
        18708
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>With errors<ept id=\"p1\">**</ept> on <bpt id=\"p2\">**</bpt>Datasets<ept id=\"p2\">**</ept> tile on the <bpt id=\"p3\">**</bpt>DATA FACTORY<ept id=\"p3\">**</ept> home page."
    },
    {
      "content": "With errors link on Datasets tile",
      "pos": [
        18716,
        18749
      ]
    },
    {
      "pos": [
        18818,
        18904
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Datasets with errors<ept id=\"p1\">**</ept> blade, click the <bpt id=\"p2\">**</bpt>table<ept id=\"p2\">**</ept> that you are interested in."
    },
    {
      "content": "Datasets with errors blade",
      "pos": [
        18912,
        18938
      ]
    },
    {
      "pos": [
        19011,
        19100
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>TABLE<ept id=\"p1\">**</ept> blade, Click on the <bpt id=\"p2\">**</bpt>problem slice<ept id=\"p2\">**</ept> with <bpt id=\"p3\">**</bpt>STATUS<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>Failed<ept id=\"p4\">**</ept>."
    },
    {
      "content": "Table with problem slices",
      "pos": [
        19108,
        19133
      ]
    },
    {
      "pos": [
        19211,
        19279
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade, click the <bpt id=\"p2\">**</bpt>Activity Run<ept id=\"p2\">**</ept> that failed."
    },
    {
      "content": "Data slice with failed runs",
      "pos": [
        19287,
        19314
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>ACTIVITY RUN DETAILS<ept id=\"p1\">**</ept> blade, you can download the files associated with the HDInsight processing.",
      "pos": [
        19386,
        19493
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Download<ept id=\"p1\">**</ept> for <bpt id=\"p2\">**</bpt>Status/stderr<ept id=\"p2\">**</ept> to download the error log file that contains details about the error.",
      "pos": [
        19494,
        19604
      ]
    },
    {
      "content": "Activity run details with download link",
      "pos": [
        19612,
        19651
      ]
    },
    {
      "content": "Walkthrough: Use Azure PowerShell to troubleshoot an error with Pig/Hive processing",
      "pos": [
        19717,
        19800
      ]
    },
    {
      "pos": [
        19805,
        19833
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        19839,
        19939
      ],
      "content": "Switch to <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode as the Data Factory cmdlets are available only in this mode."
    },
    {
      "content": "Run Get-AzureDataFactorySlice command to see the slices and their statuses.",
      "pos": [
        20001,
        20076
      ]
    },
    {
      "content": "You should see a slice with the status: Failed.",
      "pos": [
        20077,
        20124
      ]
    },
    {
      "pos": [
        20904,
        21000
      ],
      "content": "Now, run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryRun<ept id=\"p1\">**</ept> cmdlet to get details about activity run for the slice."
    },
    {
      "content": "The value of <bpt id=\"p1\">**</bpt>StartDateTime<ept id=\"p1\">**</ept> is the Start time for the error/problem slice you noted from the previous step.",
      "pos": [
        21182,
        21292
      ]
    },
    {
      "content": "The date-time should be enclosed in double quotes.",
      "pos": [
        21293,
        21343
      ]
    },
    {
      "content": "You should see the output with details about the error (similar to the following):",
      "pos": [
        21347,
        21429
      ]
    },
    {
      "pos": [
        22429,
        22598
      ],
      "content": "You can run <bpt id=\"p1\">**</bpt>Save-AzureDataFactoryLog<ept id=\"p1\">**</ept> cmdlet with Id value you see from the above output and download the log files using the <bpt id=\"p2\">**</bpt>-DownloadLogs<ept id=\"p2\">**</ept> option for the cmdlet."
    },
    {
      "content": "test",
      "pos": [
        24460,
        24464
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Troubleshoot Azure Data Factory issues\" \n    description=\"Learn how to troubleshoot issues with using Azure Data Factory.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/25/2015\" \n    ms.author=\"spelluru\"/>\n\n# Troubleshoot Data Factory issues\nYou can troubleshoot Azure Data Factory issues using Azure Portal (or) Azure PowerShell cmdlets. This topic has walkthroughs that show you how to use the Azure Portal to quickly troubleshoot errors that you encounter with Data Factory. \n\n## Problem: Not able to run Data Factory cmdlets\nTo resolve this issue, switch Azure mode to **AzureResourceManager**: \n\nLaunch **Azure PowerShell** and execute the following command to switch to the **AzureResourceManager** mode.The Azure Data Factory cmdlets are available in the **AzureResourceManager** mode.\n\n         switch-azuremode AzureResourceManager\n\n## Problem: Unauthorized error when running a Data Factory cmdlet\nYou are probably not using the right Azure account or subscription with the Azure PowerShell. Use the following cmdlets to select the right Azure account and subscription to use with the Azure PowerShell. \n\n1. Add-AzureAccount - Use the right user ID and password\n2. Get-AzureSubscription - View all the subscriptions for the account. \n3. Select-AzureSubscription <subscription name> - Select the right subscription. Use the same one you use to create a data factory on the Azure Preview Portal.\n\n## Problem: Fail to launch Data Gateway Express Setup from Azure Portal\nThe Express Setup for the Data Gateway requires Internet Explorer or a Microsoft ClickOnce compatible web browser. If you fails to start the Express Setup, you can\n\n1. Switch to Internet Explorer if you fails with other browsers. Or\n2. Use the \"Manual Setup\" links shown on the same blade in the portal to do the installation, and then copy the Key that is provided on the screen, and paste when the Data Management Gateway configuration is ready. If it doesn't launch, check your start menu for \"Microsoft Data Management Gateway\" and paste in the key when it launches. \n\n\n## Problem: Fail to launch Credentials Manager from Azure Portal\nWhen set up or update a SQL Server Linked Service via Azure Portal, the Credentials Manager application will be launched to guarantee security. It requires Internet Explorer or a Microsoft ClickOnce compatible web browser. You can switch to Internet Explorer if you fails with other browsers.\n\n## Problem: Fail to connect to on-premises SQL Server \nVerify that the SQL Server is reachable from the machine where the gateway is installed. On the machine on which the gateway is installed, you can\n\n1. Ping the machine where the SQL Server is installed. Or\n2. Try connecting to the SQL Server instance using the credentials you specified on the Azure Portal using SQL Server Management Studio (SSMS).\n\n\n## Problem: Input slices are in PendingExecution or PendingValidation state for ever\n\nThe slices could be in **PendingExecution** or **PendingValidation** state due to a number of reasons and one of the common reasons is that the **external** property is not set to **true**. Any dataset that is produced outside the scope of Azure Data Factory should be marked with **external** property . This indicates that the data is external and not backed by any pipelines within the data factory. The data slices are marked as **Ready** once the data is available in the respective store. \n\nSee the following example for the usage of the **external** property. You can optionally specify **externalData*** when you set external to true.. \n\nSee Tables topic in [JSON Scripting Reference][json-scripting-reference] for more details about this property.\n    \n    {\n      \"name\": \"CustomerTable\",\n      \"properties\": {\n        \"type\": \"AzureBlob\",\n        \"linkedServiceName\": \"MyLinkedService\",\n        \"typeProperties\": {\n          \"folderPath\": \"MyContainer/MySubFolder/\",\n          \"format\": {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \",\",\n            \"rowDelimiter\": \";\"\n          }\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"dataDelay\": \"00:10:00\",\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n To resolve the error, add the **external** property and the optional **externalData** section to the JSON definition of the input table and recreate the table. \n\n## Problem: Hybrid copy operation fails\nTo learn more details:\n\n1. Launch Data Management Gateway Configuration Manager on the machine on which gateway was installed. Verify that the **Gateway name** is set to the logical gateway name on the **Azure Portal**, **Gateway key status** is **registered** and **Service status** is **Started**. \n2. Launch **Event Viewer**. Expand **Applications and Services Logs** and click **Data Management Gateway**. See if there are any errors related to Data Management Gateway. \n\n## Problem: On Demand HDInsight Provisioning Fails with Error\n\nWhen using a linked service of type HDInsightOnDemandLinkedService, you should specify a linkedServiceName that points to  Azure Blob Storage. This storage account will be used to copy all the logs and supporting files for your on-demand HDInsight cluster.  Sometimes the activity that does the on-demand provisioning on HDInsight may fail with the following error:\n\n        Failed to create cluster. Exception: Unable to complete the cluster create operation. Operation failed with code '400'. Cluster left behind state: 'Error'. Message: 'StorageAccountNotColocated'.\n\nThis error usually indicates that the Location of the storage account specified in the linkedServiceName is not in the same data center location as where the HDInsight provisioning is happening. For example, if your Azure Data Factory location is West US, and the on-demand HDInsight provisioning happens in West US, but the Azure blob storage account location  is set to East US, the on-demand provisioning will fail.\n\nAdditionally, there is a second JSON property additionalLinkedServiceNames where additional storage accounts may be specified in on-demand HDInsight. Those additional linked storage accounts should be in the same location as the HDInsight cluster, or it will fail with the same error.\n\n\n\n## Problem: Custom Activity Fails\nWhen using a Custom Activity in Azure Data Factory (pipeline activity type CustomActivity), the custom application runs in the specified linked service to HDInsight as a Map only streaming MapReduce job. \n\nWhen the custom activity runs, Azure Data Factory will be able to capture that output from the HDInsight cluster, and save it in the *adfjobs* storage container in your Azure Blob Storage account. In case of an error, you can read the text from **stderr** output text file after a failure has occurred. The files are accessible and readable from the Azure portal itself in the web browser, or by using storage explorer tools to access the files kept in the storage container in Azure Blob Storage directly. \n\nTo enumerate and read the logs for a particular Custom Activity, you may follow one of the illustrated walkthroughs later on this page. In summary:\n\n1.  In the Azure portal **Browse** to locate your Data Factory.\n2.  Use the **Diagram** button to view the data factory diagram, and click on the **Dataset** Table that follows the specific **Pipeline** which has the Custom Activity. \n3.  In the **Table** blade, Click on the slice of interest in the **Problem slices** for the time frame to be investigated.\n4.  The detailed **Data Slice** blade will appear and it can list multiple **Activity runs** for the slice. Click on an **Activity** from the list. \n5.  The **Activity Run Details** blade will appear. It will list the **Error Message** in the middle of the blade, and several **Log files** listed at the bottom of the blade affiliated with that activity run.\n    - Logs/system-0.log\n    - Status\n    - Status/exit\n    - Status/stderr\n    - Status/stdout\n\n6. Click on the first **Log file** item in the list, and the log will open in a new blade with the full text displayed for you to read. Review the text of each log by clicking on each one. The text viewer blade will open. You can click the **Download** button to download the text file for optional offline viewing.  \n\nOne **common error** from a custom activity is \n        Package execution failed with exit code '1'. See 'wasb://adfjobs@storageaccount.blob.core.windows.net/PackageJobs/<guid>/<jobid>/Status/stderr' for more details.\n\nTo see more details for this kind of error, open the **stderr** file. One common error seen there is a timeout condition such as this:\n        INFO mapreduce.Job: Task Id : attempt_1424212573646_0168_m_000000_0, Status : FAILED \n        AttemptID:attempt_1424212573646_0168_m_000000_0 Timed out after 600 secs\n\nThis same error may appear multiple times, if the job has retried 3 times for example, over the span of 30 or more minutes. \n\nThis time out error indicates a 600 second (10 minute) timeout has happened. Typically this means the custom .Net application has not issued any status update for 10 minutes. If the application is hanging or stalled waiting on something for too long, the 10 minute timeout is a safety mechanism to prevent it from waiting forever and delaying your Azure Data Factory pipeline. \n\nThis time out originates in the configuration of HDInsight cluster that is linked in the custom activity. The setting is **mapred.task.timeout**, which defaults to 600000 milliseconds, as documented in the Apache default settings here: http://hadoop.apache.org/docs/r2.4.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml\n\nYou can overide this default by changing the defaults at the time of provisioning your HDInsight provisioning cluster. When using Azure Data Factory and **HDInsight On-demand** linked service, the JSON property can be added near your HDInsightOnDemandLinkedService JSON properties. For example, you can increase the value to 20 minutes using this JSON property.\n        \n        \"mapReduceConfiguration\" :\n        {\n            \"mapreduce.task.timeout\":\"1200000\"\n        }\n        \n\nFor more context and a full example of the JSON to edit these map reduce Configuration properties see Example #3 in the MSDN documentation here https://msdn.microsoft.com/library/azure/dn893526.aspx\n\n## Problem: PowerShell request fails with error error 400 Bad Request \"No registered resource provider found...\"\n\nAs of March 10, 2015, the Azure Data Factory PowerShell early private preview versions 2014-05-01-preview, 2014-07-01-preview, and 2014-08-01-preview will be discontinued. We recommend that you use the latest version of the ADF cmdlets, which are now part of the Azure PowerShell Download, such as the download from this URL http://go.microsoft.com/?linkid=9811175&clcid=0x409 \n\nIf you use the discontinued versions of the Azure PowerShell SDK you may receive the following errors:\n\n        HTTP/1.1 400 Bad Request\n        Cache-Control: no-cache\n        Pragma: no-cache\n        Content-Type: application/json; charset=utf-8\n        Expires: -1\n        x-ms-request-id: e07181e4-e421-46be-8a08-1f71d5e90494\n        x-ms-correlation-request-id: e07181e4-e421-46be-8a08-1f71d5e90494\n        x-ms-routing-request-id: WESTUS:20150306T234829Z:e07181e4-e421-46be-8a08-1f71d5e90494\n        Strict-Transport-Security: max-age=31536000; includeSubDomains\n        Date: Fri, 06 Mar 2015 23:48:29 GMT\n        Content-Length: 157\n        {\"error\":{\"code\":\"NoRegisteredProviderFound\",\"message\":\"No registered resource provider found for location 'west US' and API version '2014-05-01-preview'.\"}}\n\n\n## <a name=\"copywalkthrough\"></a> Walkthrough: Troubleshooting an error with copying data\nIn this walkthrough, you will introduce an error in the tutorial from Get started with Data Factory article and learn how you can use Azure Portal to troubleshoot the error.\n\n### Prerequisites\n1. Complete the Tutorial in the [Get started with Azure Data Factory][adfgetstarted] article.\n2. Confirm that the **ADFTutorialDataFactory** produces data in the **emp** table in the Azure SQL Database.  \n3. Now, delete the **emp** table (**drop table emp**) from the Azure SQL Database. This will introduce an error.\n4. Run the following command in the **Azure PowerShell** to update the active period for the pipeline so that it tries to write data to the **emp** table, which doesn’t exist anymore.\n\n         \n        Set-AzureDataFactoryPipelineActivePeriod -ResourceGroupName ADFTutorialResourceGroup -DataFactoryName ADFTutorialDataFactory -StartDateTime 2014-09-29 –EndDateTime 2014-09-30 –Name ADFTutorialPipeline\n    \n    Replace **StartDateTime** value with the current day and **EndDateTime** value with the next day. \n\n\n### Use Azure Preview Portal to troubleshoot the error\n\n1.  Login to [Azure Preview Portal][azure-preview-portal]. \n2.  Click **ADFTutorialDataFactory** from the **Startboard**. If you don’t see the data factory link on the **Startboard**, click **BROWSE** hub and click **Everything**. Click **Data factories…** in the **Browse** blade, and click **ADFTutorialDataFactory**.\n3.  Notice that you see **With errors** on the **Datasets** tile. Click **With errors**. You should see **Datasets with errors** blade.\n\n    ![Data Factory with Errors link][image-data-factory-troubleshoot-with-error-link]\n\n4. In the **Datasets** with errors blade, click **EmpSQLTable** to see the **TABLE** blade. \n\n    ![Datasets with errors blade][image-data-factory-troubleshoot-datasets-with-errors-blade]\n\n5. In the **TABLE** blade, you should see the problem slices, i.e., slices with an error in the **Problem slices** list at the bottom. You can also see any recent slices with errors in the **Recent slices** list. Click on a slice in the **Problem slices** list. \n\n    ![Table blade with problem slices][image-data-factory-troubleshoot-table-blade-with-problem-slices]\n\n    If you click **Problem slices** (not on a specific problem), you will see the **DATA SLICES** blade and then click a **specific problem slice** to see the **DATA SLICE** slide for the selected data slice.\n\n6. In the **DATA SLICE** blade for **EmpSQLTable**, you see all **activity runs** for the slice in the list at the bottom. Click on an **activity run** from the list that failed.\n\n    ![Data Slice blade with active runs][image-data-factory-troubleshoot-dataslice-blade-with-active-runs]\n\n\n7. In the **Activity Run Details** blade for the activity run you selected, you should see details about the error. In this scenario, you see: **Invalid object name ‘emp’**.\n\n    ![Activity run details with an error][image-data-factory-troubleshoot-activity-run-with-error]\n\nTo resolve this issue, create the **emp** table using the SQL script from [Get started with Data Factory][adfgetstarted] article.\n\n\n### Use Azure PowerShell cmdlets to troubleshoot the error\n1.  Launch **Azure PowerShell**. \n2.  Switch to **AzureResourceManager** mode as the Data Factory cmdlets are available only in this mode.\n\n         \n        switch-azuremode AzureResourceManager\n\n3. Run Get-AzureDataFactorySlice command to see the slices and their statuses. You should see a slice with the status: Failed.  \n\n         \n        Get-AzureDataFactorySlice -ResourceGroupName ADFTutorialResourceGroup -DataFactoryName ADFTutorialDataFactory -TableName EmpSQLTable -StartDateTime 2014-10-15\n\n    Replace **StartDateTime** with the StartDateTime value you specified for the **Set-AzureDataFactoryPipelineActivePeriod**. \n\n        ResourceGroupName       : ADFTutorialResourceGroup\n        DataFactoryName         : ADFTutorialDataFactory\n        TableName               : EmpSQLTable\n        Start                   : 10/15/2014 4:00:00 PM\n        End                     : 10/15/2014 5:00:00 PM\n        RetryCount              : 0\n        Status                  : Failed\n        LatencyStatus           :\n        LongRetryCount          : 0\n\n    Note the **Start** time for the problem slice (the slice with **Status** set to **Failed**) in the output. \n4. Now, run the **Get-AzureDataFactoryRun** cmdlet to get details about activity run for the slice.\n         \n        Get-AzureDataFactoryRun -ResourceGroupName ADFTutorialResourceGroup -DataFactoryName ADFTutorialDataFactory -TableName EmpSQLTable -StartDateTime \"10/15/2014 4:00:00 PM\"\n\n    The value of **StartDateTime** is the Start time for the error/problem slice you noted from the previous step. The date-time should be enclosed in double quotes.\n5. You should see the output with details about the error (similar to the following):\n\n        Id                      : 2b19475a-c546-473f-8da1-95a9df2357bc\n        ResourceGroupName       : ADFTutorialResourceGroup\n        DataFactoryName         : ADFTutorialDataFactory\n        TableName               : EmpSQLTable\n        ResumptionToken         :\n        ContinuationToken       :\n        ProcessingStartTime     : 10/15/2014 11:13:39 PM\n        ProcessingEndTime       : 10/15/2014 11:16:59 PM\n        PercentComplete         : 0\n        DataSliceStart          : 10/15/2014 4:00:00 PM\n        DataSliceEnd            : 10/15/2014 5:00:00 PM\n        Status                  : FailedExecution\n        Timestamp               : 10/15/2014 11:13:39 PM\n        RetryAttempt            : 0\n        Properties              : {}\n        ErrorMessage            : Unknown error in CopyActivity: System.Data.SqlClient.SqlException (0x80131904): **Invalid object name 'emp'.**\n                         at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean\n                      breakConnection, Action`1 wrapCloseInAction)\n                         at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning(TdsParserStateObject stateObj,\n\n \n\n## <a name=\"pighivewalkthrough\"></a> Walkthrough: Troubleshooting an error with Hive/Pig processing\nThis walkthrough provides steps to troubleshoot an error with Hive/Pig processing by using both Azure Preview Portal and Azure PowerShell. \n\n\n### Walkthrough: Use Azure Portal to troubleshoot an error with Pig/Hive processing\nIn this scenario, data set is in an error state due to a failure in Hive processing on an HDInsight cluster.\n\n1. Click **With errors** on **Datasets** tile on the **DATA FACTORY** home page.\n\n    ![With errors link on Datasets tile][image-data-factory-troubleshoot-walkthrough2-with-errors-link]\n\n2. In the **Datasets with errors** blade, click the **table** that you are interested in.\n\n    ![Datasets with errors blade][image-data-factory-troubleshoot-walkthrough2-datasets-with-errors]\n\n3. In the **TABLE** blade, Click on the **problem slice** with **STATUS** set to **Failed**.\n\n    ![Table with problem slices][image-data-factory-troubleshoot-walkthrough2-table-with-problem-slices]\n\n4. In the **DATA SLICE** blade, click the **Activity Run** that failed.\n\n    ![Data slice with failed runs][image-data-factory-troubleshoot-walkthrough2-slice-activity-runs]\n\n5. In the **ACTIVITY RUN DETAILS** blade, you can download the files associated with the HDInsight processing. Click **Download** for **Status/stderr** to download the error log file that contains details about the error.\n\n    ![Activity run details with download link][image-data-factory-troubleshoot-activity-run-details]\n\n    \n### Walkthrough: Use Azure PowerShell to troubleshoot an error with Pig/Hive processing\n1.  Launch **Azure PowerShell**. \n2.  Switch to **AzureResourceManager** mode as the Data Factory cmdlets are available only in this mode.\n\n         \n        switch-azuremode AzureResourceManager\n\n3. Run Get-AzureDataFactorySlice command to see the slices and their statuses. You should see a slice with the status: Failed.  \n\n         \n        Get-AzureDataFactorySlice -ResourceGroupName ADF -DataFactoryName LogProcessingFactory -TableName EnrichedGameEventsTable -StartDateTime 2014-05-04 20:00:00\n\n    Replace **StartDateTime** with the StartDateTime value you specified for the **Set-AzureDataFactoryPipelineActivePeriod**. \n\n        ResourceGroupName : ADF\n        DataFactoryName   : LogProcessingFactory\n        TableName         : EnrichedGameEventsTable\n        Start             : 5/5/2014 12:00:00 AM\n        End               : 5/6/2014 12:00:00 AM\n        RetryCount        : 0\n        Status            : Failed\n        LatencyStatus     :\n        LongRetryCount    : 0\n\n\n    Note the **Start** time for the problem slice (the slice with **Status** set to **Failed**) in the output. \n4. Now, run the **Get-AzureDataFactoryRun** cmdlet to get details about activity run for the slice.\n         \n        Get-AzureDataFactoryRun -ResourceGroupName ADF -DataFactoryName LogProcessingFactory -TableName EnrichedGameEventsTable -StartDateTime \"5/5/2014 12:00:00 AM\"\n\n    The value of **StartDateTime** is the Start time for the error/problem slice you noted from the previous step. The date-time should be enclosed in double quotes.\n5. You should see the output with details about the error (similar to the following):\n\n        Id                  : 841b77c9-d56c-48d1-99a3-8c16c3e77d39\n        ResourceGroupName   : ADF\n        DataFactoryName     : LogProcessingFactory3\n        TableName           : EnrichedGameEventsTable\n        ProcessingStartTime : 10/10/2014 3:04:52 AM\n        ProcessingEndTime   : 10/10/2014 3:06:49 AM\n        PercentComplete     : 0\n        DataSliceStart      : 5/5/2014 12:00:00 AM\n        DataSliceEnd        : 5/6/2014 12:00:00 AM\n        Status              : FailedExecution\n        Timestamp           : 10/10/2014 3:04:52 AM\n        RetryAttempt        : 0\n        Properties          : {}\n        ErrorMessage        : Pig script failed with exit code '5'. See 'wasb://adfjobs@spestore.blob.core.windows.net/PigQuery\n                                Jobs/841b77c9-d56c-48d1-99a3-8c16c3e77d39/10_10_2014_03_04_53_277/Status/stderr' for more details.\n        ActivityName        : PigEnrichLogs\n        PipelineName        : EnrichGameLogsPipeline\n        Type                :\n\n6. You can run **Save-AzureDataFactoryLog** cmdlet with Id value you see from the above output and download the log files using the **-DownloadLogs** option for the cmdlet.\n\n\n\n[adfgetstarted]: data-factory-get-started.md\n[adf-tutorial]: data-factory-tutorial.md\n[use-custom-activities]: data-factory-use-custom-activities.md\n[monitor-manage-using-powershell]: data-factory-monitor-manage-using-powershell.md\n[troubleshoot]: data-factory-troubleshoot.md\n[developer-reference]: http://go.microsoft.com/fwlink/?LinkId=516908\n[cmdlet-reference]: http://go.microsoft.com/fwlink/?LinkId=517456\n[json-scripting-reference]: http://go.microsoft.com/fwlink/?LinkId=516971\n\n[azure-preview-portal]: https://portal.azure.com/\n\n[image-data-factory-troubleshoot-with-error-link]: ./media/data-factory-troubleshoot/DataFactoryWithErrorLink.png\n\n[image-data-factory-troubleshoot-datasets-with-errors-blade]: ./media/data-factory-troubleshoot/DatasetsWithErrorsBlade.png\n\n[image-data-factory-troubleshoot-table-blade-with-problem-slices]: ./media/data-factory-troubleshoot/TableBladeWithProblemSlices.png\n\n[image-data-factory-troubleshoot-activity-run-with-error]: ./media/data-factory-troubleshoot/ActivityRunDetailsWithError.png\n\n[image-data-factory-troubleshoot-dataslice-blade-with-active-runs]: ./media/data-factory-troubleshoot/DataSliceBladeWithActivityRuns.png\n\n[image-data-factory-troubleshoot-walkthrough2-with-errors-link]: ./media/data-factory-troubleshoot/Walkthrough2WithErrorsLink.png\n\n[image-data-factory-troubleshoot-walkthrough2-datasets-with-errors]: ./media/data-factory-troubleshoot/Walkthrough2DataSetsWithErrors.png\n\n[image-data-factory-troubleshoot-walkthrough2-table-with-problem-slices]: ./media/data-factory-troubleshoot/Walkthrough2TableProblemSlices.png\n\n[image-data-factory-troubleshoot-walkthrough2-slice-activity-runs]: ./media/data-factory-troubleshoot/Walkthrough2DataSliceActivityRuns.png\n\n[image-data-factory-troubleshoot-activity-run-details]: ./media/data-factory-troubleshoot/Walkthrough2ActivityRunDetails.png\n \ntest\n"
}