{
  "nodes": [
    {
      "content": "Guide to the Net# Neural Networks Specification Language | Microsoft Azure",
      "pos": [
        28,
        102
      ]
    },
    {
      "content": "Syntax for the Net# neural networks specification language, together with examples of how to create a custom neural network model in Microsoft Azure ML using Net",
      "pos": [
        122,
        283
      ]
    },
    {
      "content": "Guide to Net# neural network specification language for Azure Machine Learning",
      "pos": [
        619,
        697
      ]
    },
    {
      "content": "Overview",
      "pos": [
        701,
        709
      ]
    },
    {
      "content": "Net# is a language developed by Microsoft that is used to define neural network architectures for neural network modules in Microsoft Azure Machine Learning.",
      "pos": [
        710,
        867
      ]
    },
    {
      "content": "In this article, you will learn:",
      "pos": [
        868,
        900
      ]
    },
    {
      "content": "Basic concepts related to neural networks",
      "pos": [
        908,
        949
      ]
    },
    {
      "content": "Neural network requirements and how to define the primary components",
      "pos": [
        954,
        1022
      ]
    },
    {
      "content": "The syntax and keywords of the Net# specification language",
      "pos": [
        1027,
        1085
      ]
    },
    {
      "content": "Examples of custom neural networks created using Net",
      "pos": [
        1090,
        1142
      ]
    },
    {
      "content": "Neural network basics",
      "pos": [
        1248,
        1269
      ]
    },
    {
      "content": "A neural network structure consists of <bpt id=\"p1\">***</bpt>nodes<ept id=\"p1\">***</ept> that are organized in <bpt id=\"p2\">***</bpt>layers<ept id=\"p2\">***</ept>, and weighted <bpt id=\"p3\">***</bpt>connections<ept id=\"p3\">***</ept> (or <bpt id=\"p4\">***</bpt>edges<ept id=\"p4\">***</ept>) between the nodes.",
      "pos": [
        1270,
        1423
      ]
    },
    {
      "content": "The connections are directional, and each connection has a <bpt id=\"p1\">***</bpt>source<ept id=\"p1\">***</ept> node and a <bpt id=\"p2\">***</bpt>destination<ept id=\"p2\">***</ept> node.",
      "pos": [
        1424,
        1530
      ]
    },
    {
      "content": "Each <bpt id=\"p1\">***</bpt>trainable layer<ept id=\"p1\">***</ept> (a hidden or an output layer) has one or more <bpt id=\"p2\">***</bpt>connection bundles<ept id=\"p2\">***</ept>.",
      "pos": [
        1534,
        1632
      ]
    },
    {
      "content": "A connection bundle consists of a source layer and a specification of the connections from that source layer.",
      "pos": [
        1633,
        1742
      ]
    },
    {
      "content": "All the connections in a given bundle share the same <bpt id=\"p1\">***</bpt>source layer<ept id=\"p1\">***</ept> and the same <bpt id=\"p2\">***</bpt>destination layer<ept id=\"p2\">***</ept>.",
      "pos": [
        1743,
        1852
      ]
    },
    {
      "content": "In Net#, a connection bundle is considered as belonging to the bundle's destination layer.",
      "pos": [
        1853,
        1943
      ]
    },
    {
      "content": "Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.",
      "pos": [
        1948,
        2093
      ]
    },
    {
      "pos": [
        2098,
        2242
      ],
      "content": "The default or standard bundle is a <bpt id=\"p1\">**</bpt>full bundle<ept id=\"p1\">**</ept>, in which each node in the source layer is connected to every node in the destination layer."
    },
    {
      "content": "Additionally, Net# supports the following four kinds of advanced connection bundles:",
      "pos": [
        2246,
        2330
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Filtered bundles<ept id=\"p1\">**</ept>.",
      "pos": [
        2338,
        2359
      ]
    },
    {
      "content": "The user can define a predicate by using the locations of the source layer node and the destination layer node.",
      "pos": [
        2360,
        2471
      ]
    },
    {
      "content": "Nodes are connected whenever the predicate is True.",
      "pos": [
        2472,
        2523
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Convolutional bundles<ept id=\"p1\">**</ept>.",
      "pos": [
        2528,
        2554
      ]
    },
    {
      "content": "The user can define small neighborhoods of nodes in the source layer.",
      "pos": [
        2555,
        2624
      ]
    },
    {
      "content": "Each node in the destination layer is connected to one neighborhood of nodes in the source layer.",
      "pos": [
        2625,
        2722
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Pooling bundles<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Response normalization bundles<ept id=\"p2\">**</ept>.",
      "pos": [
        2727,
        2786
      ]
    },
    {
      "content": "These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.",
      "pos": [
        2787,
        2904
      ]
    },
    {
      "content": "The difference is that the weights of the edges in these bundles are not trainable.",
      "pos": [
        2905,
        2988
      ]
    },
    {
      "content": "Instead, a predefined function is applied to the source node values to determine the destination node value.",
      "pos": [
        2989,
        3097
      ]
    },
    {
      "content": "Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.",
      "pos": [
        3101,
        3348
      ]
    },
    {
      "content": "Supported customizations",
      "pos": [
        3354,
        3378
      ]
    },
    {
      "content": "The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.",
      "pos": [
        3379,
        3507
      ]
    },
    {
      "content": "You can:",
      "pos": [
        3508,
        3516
      ]
    },
    {
      "content": "Create hidden layers and control the number of nodes in each layer.",
      "pos": [
        3524,
        3591
      ]
    },
    {
      "content": "Specify how layers are to be connected to each other.",
      "pos": [
        3596,
        3649
      ]
    },
    {
      "content": "Define special connectivity structures, such as convolutions and weight sharing bundles.",
      "pos": [
        3654,
        3742
      ]
    },
    {
      "content": "Specify different activation functions.",
      "pos": [
        3747,
        3786
      ]
    },
    {
      "content": "For details of the specification language syntax, see Structure Specification.",
      "pos": [
        3790,
        3868
      ]
    },
    {
      "content": "For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see Examples.",
      "pos": [
        3873,
        3992
      ]
    },
    {
      "content": "General requirements",
      "pos": [
        3998,
        4018
      ]
    },
    {
      "content": "There must be exactly one output layer, at least one input layer, and zero or more hidden layers.",
      "pos": [
        4023,
        4120
      ]
    },
    {
      "content": "Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.",
      "pos": [
        4126,
        4235
      ]
    },
    {
      "content": "Input layers have no associated trained parameters and represent the point where instance data enters the network.",
      "pos": [
        4241,
        4355
      ]
    },
    {
      "content": "Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.",
      "pos": [
        4361,
        4473
      ]
    },
    {
      "content": "The source and destination nodes must be in separate layers.",
      "pos": [
        4479,
        4539
      ]
    },
    {
      "content": "Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.",
      "pos": [
        4545,
        4669
      ]
    },
    {
      "content": "The output layer cannot be a source layer of a connection bundle.",
      "pos": [
        4674,
        4739
      ]
    },
    {
      "content": "Structure specifications",
      "pos": [
        4745,
        4769
      ]
    },
    {
      "content": "A neural network structure specification is composed of three sections: the <bpt id=\"p1\">**</bpt>constant declaration<ept id=\"p1\">**</ept>, the <bpt id=\"p2\">**</bpt>layer declaration<ept id=\"p2\">**</ept>, the <bpt id=\"p3\">**</bpt>connection declaration<ept id=\"p3\">**</ept>.",
      "pos": [
        4770,
        4930
      ]
    },
    {
      "content": "There is also an optional <bpt id=\"p1\">**</bpt>share declaration<ept id=\"p1\">**</ept> section.",
      "pos": [
        4931,
        4987
      ]
    },
    {
      "content": "The sections can be specified in any order.",
      "pos": [
        4988,
        5031
      ]
    },
    {
      "content": "Constant declaration",
      "pos": [
        5037,
        5057
      ]
    },
    {
      "content": "A constant declaration is optional.",
      "pos": [
        5059,
        5094
      ]
    },
    {
      "content": "It provides a means to define values used elsewhere in the neural network definition.",
      "pos": [
        5095,
        5180
      ]
    },
    {
      "content": "The declaration statement consists of an identifier followed by an equal sign and a value expression.",
      "pos": [
        5181,
        5282
      ]
    },
    {
      "pos": [
        5287,
        5349
      ],
      "content": "For example, the following statement defines a constant <bpt id=\"p1\">**</bpt>x<ept id=\"p1\">**</ept>:"
    },
    {
      "content": "To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.",
      "pos": [
        5375,
        5512
      ]
    },
    {
      "content": "For example:",
      "pos": [
        5513,
        5525
      ]
    },
    {
      "content": "The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.",
      "pos": [
        5561,
        5707
      ]
    },
    {
      "content": "For example:",
      "pos": [
        5708,
        5720
      ]
    },
    {
      "content": "Layer declaration",
      "pos": [
        5765,
        5782
      ]
    },
    {
      "content": "The layer declaration is required.",
      "pos": [
        5783,
        5817
      ]
    },
    {
      "content": "It defines the size and source of the layer, including its connection bundles and attributes.",
      "pos": [
        5818,
        5911
      ]
    },
    {
      "content": "The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).",
      "pos": [
        5912,
        6071
      ]
    },
    {
      "content": "For example:",
      "pos": [
        6072,
        6084
      ]
    },
    {
      "content": "The product of the dimensions is the number of nodes in the layer.",
      "pos": [
        6193,
        6259
      ]
    },
    {
      "content": "In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.",
      "pos": [
        6260,
        6356
      ]
    },
    {
      "content": "The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.",
      "pos": [
        6361,
        6551
      ]
    },
    {
      "content": "A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to <bpt id=\"p1\">**</bpt>sigmoid<ept id=\"p1\">**</ept>.",
      "pos": [
        7445,
        7628
      ]
    },
    {
      "content": "The following output functions are supported:",
      "pos": [
        7629,
        7674
      ]
    },
    {
      "content": "sigmoid",
      "pos": [
        7682,
        7689
      ]
    },
    {
      "content": "linear",
      "pos": [
        7694,
        7700
      ]
    },
    {
      "content": "softmax",
      "pos": [
        7705,
        7712
      ]
    },
    {
      "content": "rlinear",
      "pos": [
        7717,
        7724
      ]
    },
    {
      "content": "square",
      "pos": [
        7729,
        7735
      ]
    },
    {
      "content": "sqrt",
      "pos": [
        7740,
        7744
      ]
    },
    {
      "content": "srlinear",
      "pos": [
        7749,
        7757
      ]
    },
    {
      "content": "abs",
      "pos": [
        7762,
        7765
      ]
    },
    {
      "content": "tanh",
      "pos": [
        7770,
        7774
      ]
    },
    {
      "content": "brlinear",
      "pos": [
        7780,
        7788
      ]
    },
    {
      "pos": [
        7792,
        7861
      ],
      "content": "For example, the following declaration uses the <bpt id=\"p1\">**</bpt>softmax<ept id=\"p1\">**</ept> function:"
    },
    {
      "content": "Connection declaration",
      "pos": [
        7919,
        7941
      ]
    },
    {
      "content": "Immediately after defining the trainable layer, you must declare connections among the layers you have defined.",
      "pos": [
        7942,
        8053
      ]
    },
    {
      "content": "The connection bundle declaration starts with the keyword <bpt id=\"p1\">**</bpt>from<ept id=\"p1\">**</ept>, followed by the name of the bundle's source layer and the kind of connection bundle to create.",
      "pos": [
        8054,
        8216
      ]
    },
    {
      "content": "Currently, five kinds of connection bundles are supported:",
      "pos": [
        8221,
        8279
      ]
    },
    {
      "pos": [
        8287,
        8337
      ],
      "content": "<bpt id=\"p1\">**</bpt>Full<ept id=\"p1\">**</ept> bundles, indicated by the keyword <bpt id=\"p2\">**</bpt>all<ept id=\"p2\">**</ept>"
    },
    {
      "pos": [
        8342,
        8434
      ],
      "content": "<bpt id=\"p1\">**</bpt>Filtered<ept id=\"p1\">**</ept> bundles, indicated by the keyword <bpt id=\"p2\">**</bpt>where<ept id=\"p2\">**</ept>, followed by a predicate expression"
    },
    {
      "pos": [
        8439,
        8543
      ],
      "content": "<bpt id=\"p1\">**</bpt>Convolutional<ept id=\"p1\">**</ept> bundles, indicated by the keyword <bpt id=\"p2\">**</bpt>convolve<ept id=\"p2\">**</ept>, followed by the convolution attributes"
    },
    {
      "pos": [
        8548,
        8624
      ],
      "content": "<bpt id=\"p1\">**</bpt>Pooling<ept id=\"p1\">**</ept> bundles, indicated by the keywords <bpt id=\"p2\">**</bpt>max pool<ept id=\"p2\">**</ept> or <bpt id=\"p3\">**</bpt>mean pool<ept id=\"p3\">**</ept>"
    },
    {
      "pos": [
        8629,
        8707
      ],
      "content": "<bpt id=\"p1\">**</bpt>Response normalization<ept id=\"p1\">**</ept> bundles, indicated by the keyword <bpt id=\"p2\">**</bpt>response norm<ept id=\"p2\">**</ept>"
    },
    {
      "content": "Full bundles",
      "pos": [
        8717,
        8729
      ]
    },
    {
      "content": "A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.",
      "pos": [
        8733,
        8853
      ]
    },
    {
      "content": "This is the default network connection type.",
      "pos": [
        8854,
        8898
      ]
    },
    {
      "content": "Filtered bundles",
      "pos": [
        8904,
        8920
      ]
    },
    {
      "content": "A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.",
      "pos": [
        8921,
        9044
      ]
    },
    {
      "content": "The following example defines two filtered bundles:",
      "pos": [
        9045,
        9096
      ]
    },
    {
      "content": "In the predicate for ByRow, <bpt id=\"p1\">**</bpt>s<ept id=\"p1\">**</ept> is a parameter representing an index into the rectangular array of nodes of the input layer, Pixels, and <bpt id=\"p2\">**</bpt>d<ept id=\"p2\">**</ept> is a parameter representing an index into the array of nodes of the hidden layer, ByRow.",
      "pos": [
        9274,
        9507
      ]
    },
    {
      "content": "The type of both <bpt id=\"p1\">**</bpt>s<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>d<ept id=\"p2\">**</ept> is a tuple of integers of length two.",
      "pos": [
        9508,
        9578
      ]
    },
    {
      "content": "Conceptually, <bpt id=\"p1\">**</bpt>s<ept id=\"p1\">**</ept> ranges over all pairs of integers with <bpt id=\"p2\">_</bpt>0 &lt;= s[0] &lt; 10<ept id=\"p2\">_</ept> and <bpt id=\"p3\">_</bpt>0 &lt;= s[1] &lt; 20<ept id=\"p3\">_</ept>, and <bpt id=\"p4\">**</bpt>d<ept id=\"p4\">**</ept> ranges over all pairs of integers, with <bpt id=\"p5\">_</bpt>0 &lt;= d[0] &lt; 10<ept id=\"p5\">_</ept> and <bpt id=\"p6\">_</bpt>0 &lt;= d[1] &lt; 12<ept id=\"p6\">_</ept>.",
      "pos": [
        9579,
        9765
      ]
    },
    {
      "content": "On the right-hand side of the predicate expression, there is a condition.",
      "pos": [
        9771,
        9844
      ]
    },
    {
      "content": "In this example, for every value of <bpt id=\"p1\">**</bpt>s<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>d<ept id=\"p2\">**</ept> such that the condition is True, there is an edge from the source layer node to the destination layer node.",
      "pos": [
        9845,
        10004
      ]
    },
    {
      "content": "Thus, this filter expression indicates that the bundle includes a connection from the node defined by <bpt id=\"p1\">**</bpt>s<ept id=\"p1\">**</ept> to the node defined by <bpt id=\"p2\">**</bpt>d<ept id=\"p2\">**</ept> in all cases where s[0] is equal to d[0].",
      "pos": [
        10005,
        10183
      ]
    },
    {
      "content": "Optionally, you can specify a set of weights for a filtered bundle.",
      "pos": [
        10187,
        10254
      ]
    },
    {
      "content": "The value for the <bpt id=\"p1\">**</bpt>Weights<ept id=\"p1\">**</ept> attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.",
      "pos": [
        10255,
        10411
      ]
    },
    {
      "content": "By default, weights are randomly generated.",
      "pos": [
        10412,
        10455
      ]
    },
    {
      "content": "Weight values are grouped by the destination node index.",
      "pos": [
        10459,
        10515
      ]
    },
    {
      "content": "That is, if the first destination node is connected to K source nodes, the first <bpt id=\"p1\">_</bpt>K<ept id=\"p1\">_</ept> elements of the <bpt id=\"p2\">**</bpt>Weights<ept id=\"p2\">**</ept> tuple are the weights for the first destination node, in source index order.",
      "pos": [
        10516,
        10705
      ]
    },
    {
      "content": "The same applies for the remaining destination nodes.",
      "pos": [
        10706,
        10759
      ]
    },
    {
      "content": "Convolutional bundles",
      "pos": [
        10765,
        10786
      ]
    },
    {
      "content": "When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.",
      "pos": [
        10787,
        10924
      ]
    },
    {
      "content": "For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.",
      "pos": [
        10925,
        11027
      ]
    },
    {
      "content": "Convolutional bundles employ rectangular <bpt id=\"p1\">**</bpt>kernels<ept id=\"p1\">**</ept> that are slid through the dimensions.",
      "pos": [
        11031,
        11121
      ]
    },
    {
      "content": "Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as <bpt id=\"p1\">**</bpt>kernel applications<ept id=\"p1\">**</ept>.",
      "pos": [
        11122,
        11243
      ]
    },
    {
      "content": "Each kernel application corresponds to a node in the source layer, which is referred to as the <bpt id=\"p1\">**</bpt>central node<ept id=\"p1\">**</ept>.",
      "pos": [
        11244,
        11356
      ]
    },
    {
      "content": "The weights of a kernel are shared among many connections.",
      "pos": [
        11357,
        11415
      ]
    },
    {
      "content": "In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.",
      "pos": [
        11416,
        11516
      ]
    },
    {
      "content": "Convolutional bundles support the following attributes:",
      "pos": [
        11520,
        11575
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>InputShape<ept id=\"p1\">**</ept> defines the dimensionality of the source layer for the purposes of this convolutional bundle.",
      "pos": [
        11577,
        11685
      ]
    },
    {
      "content": "The value must be a tuple of positive integers.",
      "pos": [
        11686,
        11733
      ]
    },
    {
      "content": "The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.",
      "pos": [
        11734,
        11904
      ]
    },
    {
      "content": "The length of this tuple becomes the <bpt id=\"p1\">**</bpt>arity<ept id=\"p1\">**</ept> value for the convolutional bundle.",
      "pos": [
        11905,
        11987
      ]
    },
    {
      "content": "(Typically arity refers to the number of arguments or operands that a function can take.)",
      "pos": [
        11988,
        12077
      ]
    },
    {
      "pos": [
        12081,
        12223
      ],
      "content": "To define the shape and locations of the kernels, use the attributes <bpt id=\"p1\">**</bpt>KernelShape<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>Stride<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>Padding<ept id=\"p3\">**</ept>, <bpt id=\"p4\">**</bpt>LowerPad<ept id=\"p4\">**</ept>, and <bpt id=\"p5\">**</bpt>UpperPad<ept id=\"p5\">**</ept>:"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>KernelShape<ept id=\"p1\">**</ept>: (required) Defines the dimensionality of each kernel for the convolutional bundle.",
      "pos": [
        12232,
        12331
      ]
    },
    {
      "content": "The value must be a tuple of positive integers with a length that equals the arity of the bundle.",
      "pos": [
        12332,
        12429
      ]
    },
    {
      "content": "Each component of this tuple must be no greater than the corresponding component of <bpt id=\"p1\">**</bpt>InputShape<ept id=\"p1\">**</ept>.",
      "pos": [
        12430,
        12529
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Stride<ept id=\"p1\">**</ept>: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.",
      "pos": [
        12535,
        12691
      ]
    },
    {
      "content": "The value must be a tuple of positive integers with a length that is the arity of the bundle.",
      "pos": [
        12692,
        12785
      ]
    },
    {
      "content": "Each component of this tuple must be no greater than the corresponding component of <bpt id=\"p1\">**</bpt>KernelShape<ept id=\"p1\">**</ept>.",
      "pos": [
        12786,
        12886
      ]
    },
    {
      "content": "The default value is a tuple with all components equal to one.",
      "pos": [
        12887,
        12949
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Sharing<ept id=\"p1\">**</ept>: (optional) Defines the weight sharing for each dimension of the convolution.",
      "pos": [
        12955,
        13044
      ]
    },
    {
      "content": "The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.",
      "pos": [
        13045,
        13160
      ]
    },
    {
      "content": "A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.",
      "pos": [
        13161,
        13281
      ]
    },
    {
      "content": "The default value is a tuple that consists of all True values.",
      "pos": [
        13282,
        13344
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>MapCount<ept id=\"p1\">**</ept>: (optional) Defines the number of feature maps for the convolutional bundle.",
      "pos": [
        13350,
        13439
      ]
    },
    {
      "content": "The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.",
      "pos": [
        13440,
        13561
      ]
    },
    {
      "content": "A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.",
      "pos": [
        13562,
        13734
      ]
    },
    {
      "content": "The default value is one.",
      "pos": [
        13735,
        13760
      ]
    },
    {
      "content": "The total number of feature maps is the product of the components of the tuple.",
      "pos": [
        13761,
        13840
      ]
    },
    {
      "content": "The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.",
      "pos": [
        13841,
        13973
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Weights<ept id=\"p1\">**</ept>: (optional) Defines the initial weights for the bundle.",
      "pos": [
        13979,
        14046
      ]
    },
    {
      "content": "The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.",
      "pos": [
        14047,
        14215
      ]
    },
    {
      "content": "The default weights are randomly generated.",
      "pos": [
        14216,
        14259
      ]
    },
    {
      "content": "There are two sets of properties that control padding, which are mutually exclusive:",
      "pos": [
        14263,
        14347
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept>: (optional) Determines whether the input should be padded by using a <bpt id=\"p2\">**</bpt>default padding scheme<ept id=\"p2\">**</ept>.",
      "pos": [
        14353,
        14461
      ]
    },
    {
      "content": "The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.",
      "pos": [
        14462,
        14588
      ]
    },
    {
      "content": "A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.",
      "pos": [
        14589,
        14709
      ]
    },
    {
      "content": "If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.",
      "pos": [
        14710,
        15006
      ]
    },
    {
      "content": "Thus, the number of \"dummy\" nodes in each dimension is determined automatically, to fit exactly <bpt id=\"p1\">_</bpt>(InputShape[d] - 1) / Stride[d] + 1<ept id=\"p1\">_</ept> kernels into the padded source layer.",
      "pos": [
        15007,
        15178
      ]
    },
    {
      "content": "If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).",
      "pos": [
        15179,
        15339
      ]
    },
    {
      "content": "The default value of this attribute is a tuple with all components equal to False.",
      "pos": [
        15340,
        15422
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>UpperPad<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>LowerPad<ept id=\"p2\">**</ept>: (optional) Provide greater control over the amount of padding to use.",
      "pos": [
        15427,
        15527
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Important:<ept id=\"p1\">**</ept> These attributes can be defined if and only if the <bpt id=\"p2\">**</bpt>Padding<ept id=\"p2\">**</ept> property above is <bpt id=\"p3\">***</bpt>not<ept id=\"p3\">***</ept> defined.",
      "pos": [
        15528,
        15642
      ]
    },
    {
      "content": "The values should be integer-valued tuples with lengths that are the arity of the bundle.",
      "pos": [
        15643,
        15732
      ]
    },
    {
      "content": "When these attributes are specified, \"dummy\" nodes are added to the lower and upper ends of each dimension of the input layer.",
      "pos": [
        15733,
        15859
      ]
    },
    {
      "content": "The number of nodes added to the lower and upper ends in each dimension is determined by <bpt id=\"p1\">**</bpt>LowerPad<ept id=\"p1\">**</ept>[i] and <bpt id=\"p2\">**</bpt>UpperPad<ept id=\"p2\">**</ept>[i] respectively.",
      "pos": [
        15860,
        15998
      ]
    },
    {
      "content": "To ensure that kernels correspond only to \"real\" nodes and not to \"dummy\" nodes, the following conditions must be met:",
      "pos": [
        15999,
        16117
      ]
    },
    {
      "pos": [
        16126,
        16201
      ],
      "content": "Each component of <bpt id=\"p1\">**</bpt>LowerPad<ept id=\"p1\">**</ept> must be strictly less than KernelShape[d]/2."
    },
    {
      "pos": [
        16211,
        16283
      ],
      "content": "Each component of <bpt id=\"p1\">**</bpt>UpperPad<ept id=\"p1\">**</ept> must be no greater than KernelShape[d]/2."
    },
    {
      "content": "The default value of these attributes is a tuple with all components equal to 0.",
      "pos": [
        16293,
        16373
      ]
    },
    {
      "content": "The setting <bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept> = true allows as much padding as is needed to keep the \"center\" of the kernel inside the \"real\" input.",
      "pos": [
        16376,
        16502
      ]
    },
    {
      "content": "This changes the math a bit for computing the output size.",
      "pos": [
        16503,
        16561
      ]
    },
    {
      "content": "Generally, the output size <bpt id=\"p1\">_</bpt>D<ept id=\"p1\">_</ept> is computed as <bpt id=\"p2\">_</bpt>D = (I - K) / S + 1<ept id=\"p2\">_</ept>, where <bpt id=\"p3\">_</bpt>I<ept id=\"p3\">_</ept> is the input size, <bpt id=\"p4\">_</bpt>K<ept id=\"p4\">_</ept> is the kernel size, <bpt id=\"p5\">_</bpt>S<ept id=\"p5\">_</ept> is the stride, and <bpt id=\"p6\">_</bpt>/<ept id=\"p6\">_</ept> is integer division (round toward zero).",
      "pos": [
        16562,
        16751
      ]
    },
    {
      "content": "If you set UpperPad = [1, 1], the input size <bpt id=\"p1\">_</bpt>I<ept id=\"p1\">_</ept> is effectively 29, and thus <bpt id=\"p2\">_</bpt>D = (29 - 5) / 2 + 1 = 13<ept id=\"p2\">_</ept>.",
      "pos": [
        16752,
        16857
      ]
    },
    {
      "content": "However, when <bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept> = true, essentially <bpt id=\"p2\">_</bpt>I<ept id=\"p2\">_</ept> gets bumped up by <bpt id=\"p3\">_</bpt>K - 1<ept id=\"p3\">_</ept>; hence <bpt id=\"p4\">_</bpt>D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14<ept id=\"p4\">_</ept>.",
      "pos": [
        16858,
        16997
      ]
    },
    {
      "content": "By specifying values for <bpt id=\"p1\">**</bpt>UpperPad<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>LowerPad<ept id=\"p2\">**</ept> you get much more control over the padding than if you just set <bpt id=\"p3\">**</bpt>Padding<ept id=\"p3\">**</ept> = true.",
      "pos": [
        16998,
        17136
      ]
    },
    {
      "content": "For more information about convolutional networks and their applications, see these articles:",
      "pos": [
        17138,
        17231
      ]
    },
    {
      "content": "http://deeplearning.net/tutorial/lenet.html",
      "pos": [
        17240,
        17283
      ]
    },
    {
      "content": "http://research.microsoft.com/pubs/68920/icdar03.pdf",
      "pos": [
        17337,
        17389
      ]
    },
    {
      "content": "http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf",
      "pos": [
        17451,
        17506
      ]
    },
    {
      "content": "Pooling bundles",
      "pos": [
        17570,
        17585
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>pooling bundle<ept id=\"p1\">**</ept> applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.",
      "pos": [
        17586,
        17755
      ]
    },
    {
      "content": "Hence, pooling bundles have no trainable state (weights or biases).",
      "pos": [
        17756,
        17823
      ]
    },
    {
      "content": "Pooling bundles support all the convolutional attributes except <bpt id=\"p1\">**</bpt>Sharing<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>MapCount<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>Weights<ept id=\"p3\">**</ept>.",
      "pos": [
        17824,
        17931
      ]
    },
    {
      "content": "Typically, the kernels summarized by adjacent pooling units do not overlap.",
      "pos": [
        17935,
        18010
      ]
    },
    {
      "content": "If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.",
      "pos": [
        18011,
        18189
      ]
    },
    {
      "content": "Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.",
      "pos": [
        18190,
        18297
      ]
    },
    {
      "content": "The following example illustrates a pooling bundle:",
      "pos": [
        18301,
        18352
      ]
    },
    {
      "pos": [
        18529,
        18633
      ],
      "content": "The arity of the bundle is 3 (the length of the tuples <bpt id=\"p1\">**</bpt>InputShape<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>KernelShape<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>Stride<ept id=\"p3\">**</ept>)."
    },
    {
      "pos": [
        18639,
        18703
      ],
      "content": "The number of nodes in the source layer is <bpt id=\"p1\">_</bpt>5 * 24 * 24 = 2880<ept id=\"p1\">_</ept>."
    },
    {
      "pos": [
        18709,
        18800
      ],
      "content": "This is a traditional local pooling layer because <bpt id=\"p1\">**</bpt>KernelShape<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Stride<ept id=\"p2\">**</ept> are equal."
    },
    {
      "pos": [
        18806,
        18875
      ],
      "content": "The number of nodes in the destination layer is <bpt id=\"p1\">_</bpt>5 * 12 * 12 = 1440<ept id=\"p1\">_</ept>."
    },
    {
      "content": "For more information about pooling layers, see these articles:",
      "pos": [
        18883,
        18945
      ]
    },
    {
      "pos": [
        18953,
        19075
      ],
      "content": "<bpt id=\"p1\">[</bpt>http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf<ept id=\"p1\">](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)</ept> (Section 3.4)"
    },
    {
      "content": "http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf",
      "pos": [
        19081,
        19131
      ]
    },
    {
      "content": "http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf",
      "pos": [
        19191,
        19242
      ]
    },
    {
      "content": "Response normalization bundles",
      "pos": [
        19304,
        19334
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Response normalization<ept id=\"p1\">**</ept> is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in a paper titled ImageNet Classiﬁcation with Deep Convolutional Neural Networks (see section 3.3).",
      "pos": [
        19335,
        19546
      ]
    },
    {
      "content": "Response normalization is used to aid generalization in neural nets.",
      "pos": [
        19547,
        19615
      ]
    },
    {
      "content": "When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.",
      "pos": [
        19616,
        19771
      ]
    },
    {
      "content": "This is done by using three parameters (<bpt id=\"p1\">***</bpt>α<ept id=\"p1\">***</ept>, <bpt id=\"p2\">***</bpt>β<ept id=\"p2\">***</ept>, and <bpt id=\"p3\">***</bpt>k<ept id=\"p3\">***</ept>) and a convolutional structure (or neighborhood shape).",
      "pos": [
        19772,
        19897
      ]
    },
    {
      "content": "Every neuron in the destination layer <bpt id=\"p1\">***</bpt>y<ept id=\"p1\">***</ept> corresponds to a neuron <bpt id=\"p2\">***</bpt>x<ept id=\"p2\">***</ept> in the source layer.",
      "pos": [
        19898,
        19996
      ]
    },
    {
      "content": "The activation level of <bpt id=\"p1\">***</bpt>y<ept id=\"p1\">***</ept> is given by the following formula, where <bpt id=\"p2\">***</bpt>f<ept id=\"p2\">***</ept> is the activation level of a neuron, and <bpt id=\"p3\">***</bpt>Nx<ept id=\"p3\">***</ept> is the kernel (or the set that contains the neurons in the neighborhood of <bpt id=\"p4\">***</bpt>x<ept id=\"p4\">***</ept>), as defined by the following convolutional structure:",
      "pos": [
        19997,
        20265
      ]
    },
    {
      "content": "![][1]",
      "pos": [
        20269,
        20275
      ]
    },
    {
      "pos": [
        20279,
        20401
      ],
      "content": "Response normalization bundles support all the convolutional attributes except <bpt id=\"p1\">**</bpt>Sharing<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>MapCount<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>Weights<ept id=\"p3\">**</ept>."
    },
    {
      "content": "If the kernel contains neurons in the same map as <bpt id=\"p1\">***</bpt>x<ept id=\"p1\">***</ept>, the normalization scheme is referred to as <bpt id=\"p2\">**</bpt>same map normalization<ept id=\"p2\">**</ept>.",
      "pos": [
        20410,
        20539
      ]
    },
    {
      "content": "To define same map normalization, the first coordinate in <bpt id=\"p1\">**</bpt>InputShape<ept id=\"p1\">**</ept> must have the value 1.",
      "pos": [
        20540,
        20635
      ]
    },
    {
      "content": "If the kernel contains neurons in the same spatial position as <bpt id=\"p1\">***</bpt>x<ept id=\"p1\">***</ept>, but the neurons are in other maps, the normalization scheme is called <bpt id=\"p2\">**</bpt>across maps normalization<ept id=\"p2\">**</ept>.",
      "pos": [
        20640,
        20812
      ]
    },
    {
      "content": "This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.",
      "pos": [
        20813,
        21030
      ]
    },
    {
      "content": "To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.",
      "pos": [
        21031,
        21219
      ]
    },
    {
      "content": "Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).",
      "pos": [
        21223,
        21402
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Alert<ept id=\"p1\">**</ept>: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.",
      "pos": [
        21407,
        21517
      ]
    },
    {
      "content": "For example, if KernelShape[d] is odd, then <bpt id=\"p1\">_</bpt>KernelShape[d]/2<ept id=\"p1\">_</ept> corresponds to the central kernel node.",
      "pos": [
        21518,
        21620
      ]
    },
    {
      "content": "If <bpt id=\"p1\">_</bpt>KernelShape[d]<ept id=\"p1\">_</ept> is even, the central node is at <bpt id=\"p2\">_</bpt>KernelShape[d]/2 - 1<ept id=\"p2\">_</ept>.",
      "pos": [
        21621,
        21696
      ]
    },
    {
      "content": "Therefore, if <bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept>[d] is False, the first and the last <bpt id=\"p2\">_</bpt>KernelShape[d]/2<ept id=\"p2\">_</ept> nodes do not have corresponding nodes in the destination layer.",
      "pos": [
        21697,
        21841
      ]
    },
    {
      "content": "To avoid this situation, define <bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept> as [true, true, …, true].",
      "pos": [
        21842,
        21911
      ]
    },
    {
      "content": "In addition to the four attributes described earlier, response normalization bundles also support the following attributes:",
      "pos": [
        21915,
        22038
      ]
    },
    {
      "pos": [
        22046,
        22153
      ],
      "content": "<bpt id=\"p1\">**</bpt>Alpha<ept id=\"p1\">**</ept>: (required) Specifies a floating-point value that corresponds to <bpt id=\"p2\">***</bpt>α<ept id=\"p2\">***</ept> in the previous formula."
    },
    {
      "pos": [
        22159,
        22265
      ],
      "content": "<bpt id=\"p1\">**</bpt>Beta<ept id=\"p1\">**</ept>: (required) Specifies a floating-point value that corresponds to <bpt id=\"p2\">***</bpt>β<ept id=\"p2\">***</ept> in the previous formula."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Offset<ept id=\"p1\">**</ept>: (optional) Specifies a floating-point value that corresponds to <bpt id=\"p2\">***</bpt>k<ept id=\"p2\">***</ept> in the previous formula.",
      "pos": [
        22271,
        22379
      ]
    },
    {
      "content": "It defaults to 1.",
      "pos": [
        22380,
        22397
      ]
    },
    {
      "content": "The following example defines a response normalization bundle using these attributes:",
      "pos": [
        22401,
        22486
      ]
    },
    {
      "content": "The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.",
      "pos": [
        22678,
        22772
      ]
    },
    {
      "pos": [
        22778,
        22904
      ],
      "content": "The value of <bpt id=\"p1\">**</bpt>KernelShape<ept id=\"p1\">**</ept> indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle."
    },
    {
      "content": "The default value of <bpt id=\"p1\">**</bpt>Padding<ept id=\"p1\">**</ept> is False, thus the destination layer has only 10 nodes in each dimension.",
      "pos": [
        22910,
        23016
      ]
    },
    {
      "content": "To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].",
      "pos": [
        23017,
        23190
      ]
    },
    {
      "content": "Share declaration",
      "pos": [
        23196,
        23213
      ]
    },
    {
      "content": "Net# optionally supports defining multiple bundles with shared weights.",
      "pos": [
        23215,
        23286
      ]
    },
    {
      "content": "The weights of any two bundles can be shared if their structures are the same.",
      "pos": [
        23287,
        23365
      ]
    },
    {
      "content": "The following syntax defines bundles with shared weights:",
      "pos": [
        23366,
        23423
      ]
    },
    {
      "content": "For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:",
      "pos": [
        24007,
        24136
      ]
    },
    {
      "content": "The input features are partitioned into two equal sized input layers.",
      "pos": [
        24495,
        24564
      ]
    },
    {
      "content": "The hidden layers then compute higher level features on the two input layers.",
      "pos": [
        24570,
        24647
      ]
    },
    {
      "content": "The share-declaration specifies that H1 and H2 must be computed in the same way from their respective inputs.",
      "pos": [
        24653,
        24762
      ]
    },
    {
      "content": "Alternatively, this could be specified with two separate share-declarations as follows:",
      "pos": [
        24767,
        24854
      ]
    },
    {
      "content": "<ph id=\"ph1\">\n\n    share { 1 =&gt; H1, 1 =&gt; H2 } // share biases</ph>  \n\nYou can use the short form only when the layers contain a single bundle.",
      "pos": [
        24925,
        25049
      ]
    },
    {
      "content": "In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.",
      "pos": [
        25050,
        25209
      ]
    },
    {
      "content": "Examples of Net# usage",
      "pos": [
        25215,
        25237
      ]
    },
    {
      "content": "This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.",
      "pos": [
        25238,
        25415
      ]
    },
    {
      "content": "Define a simple custom neural network: \"Hello World\" example",
      "pos": [
        25423,
        25483
      ]
    },
    {
      "content": "This simple example demonstrates how to create a neural network model that has a single hidden layer.",
      "pos": [
        25484,
        25585
      ]
    },
    {
      "content": "input Data [100];\n    hidden H [200] from Data all;\n    output Out [10] sigmoid from H all;  \n\nThe example illustrates some basic commands as follows:  \n\n-   The first line defines the input layer (named Data), which has 100 nodes, and each node represents a feature in the input examples.",
      "pos": [
        25593,
        25882
      ]
    },
    {
      "content": "-   The second line creates the hidden layer.",
      "pos": [
        25883,
        25928
      ]
    },
    {
      "content": "The name H is assigned to the hidden layer, which has 200 nodes.",
      "pos": [
        25929,
        25993
      ]
    },
    {
      "content": "This layer is fully connected to the input layer.",
      "pos": [
        25994,
        26043
      ]
    },
    {
      "content": "-   The third line defines the output layer (named O), and it contains 10 output nodes.",
      "pos": [
        26044,
        26131
      ]
    },
    {
      "content": "For classification neural networks, there is one output node per class.",
      "pos": [
        26132,
        26203
      ]
    },
    {
      "content": "The keyword <bpt id=\"p1\">**</bpt>sigmoid<ept id=\"p1\">**</ept> indicates that the output function is applied to the output layer.",
      "pos": [
        26204,
        26294
      ]
    },
    {
      "content": "Define multiple hidden layers: computer vision example",
      "pos": [
        26302,
        26356
      ]
    },
    {
      "content": "The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.",
      "pos": [
        26357,
        26481
      ]
    },
    {
      "content": "<ph id=\"ph1\">// Define the input layers \n    input Pixels [10, 20];\n    input MetaData [7];\n    \n    // Define the first two hidden layers, using data only from the Pixels input\n    hidden ByRow [10, 12] from Pixels where (s,d) =&gt; s[0] == d[0];\n    hidden ByCol [5, 20] from Pixels where (s,d) =&gt; abs(s[1] - d[1]) </ph>",
      "pos": [
        26489,
        26790
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Guide to the Net# Neural Networks Specification Language | Microsoft Azure\" \n    description=\"Syntax for the Net# neural networks specification language, together with examples of how to create a custom neural network model in Microsoft Azure ML using Net#\" \n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"jeannt\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"06/29/2015\" \n    ms.author=\"jeannt\"/>\n\n\n\n# Guide to Net# neural network specification language for Azure Machine Learning\n\n##Overview\nNet# is a language developed by Microsoft that is used to define neural network architectures for neural network modules in Microsoft Azure Machine Learning. In this article, you will learn:  \n\n-   Basic concepts related to neural networks\n-   Neural network requirements and how to define the primary components\n-   The syntax and keywords of the Net# specification language\n-   Examples of custom neural networks created using Net# \n    \n[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]  \n\n##Neural network basics\nA neural network structure consists of ***nodes*** that are organized in ***layers***, and weighted ***connections*** (or ***edges***) between the nodes. The connections are directional, and each connection has a ***source*** node and a ***destination*** node.  \n\nEach ***trainable layer*** (a hidden or an output layer) has one or more ***connection bundles***. A connection bundle consists of a source layer and a specification of the connections from that source layer. All the connections in a given bundle share the same ***source layer*** and the same ***destination layer***. In Net#, a connection bundle is considered as belonging to the bundle's destination layer.  \n \nNet# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.   \n\nThe default or standard bundle is a **full bundle**, in which each node in the source layer is connected to every node in the destination layer.  \n\nAdditionally, Net# supports the following four kinds of advanced connection bundles:  \n\n-   **Filtered bundles**. The user can define a predicate by using the locations of the source layer node and the destination layer node. Nodes are connected whenever the predicate is True.\n-   **Convolutional bundles**. The user can define small neighborhoods of nodes in the source layer. Each node in the destination layer is connected to one neighborhood of nodes in the source layer.\n-   **Pooling bundles** and **Response normalization bundles**. These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer. The difference is that the weights of the edges in these bundles are not trainable. Instead, a predefined function is applied to the source node values to determine the destination node value.  \n\nUsing Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.  \n\n##Supported customizations\nThe architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#. You can:  \n\n-   Create hidden layers and control the number of nodes in each layer.\n-   Specify how layers are to be connected to each other.\n-   Define special connectivity structures, such as convolutions and weight sharing bundles.\n-   Specify different activation functions.  \n\nFor details of the specification language syntax, see Structure Specification.  \n \nFor examples of defining neural networks for some common machine learning tasks, from simplex to complex, see Examples.  \n\n##General requirements\n-   There must be exactly one output layer, at least one input layer, and zero or more hidden layers. \n-   Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions. \n-   Input layers have no associated trained parameters and represent the point where instance data enters the network. \n-   Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases. \n-   The source and destination nodes must be in separate layers. \n-   Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.\n-   The output layer cannot be a source layer of a connection bundle.  \n\n##Structure specifications\nA neural network structure specification is composed of three sections: the **constant declaration**, the **layer declaration**, the **connection declaration**. There is also an optional **share declaration** section. The sections can be specified in any order.  \n\n##Constant declaration \nA constant declaration is optional. It provides a means to define values used elsewhere in the neural network definition. The declaration statement consists of an identifier followed by an equal sign and a value expression.   \n\nFor example, the following statement defines a constant **x**:  \n\n\n    Const X = 28;  \n\nTo define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons. For example:  \n\n    Const { X = 28; Y = 4; }  \n\nThe right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression. For example:  \n\n    Const { X = 17 * 2; Y = true; }  \n\n##Layer declaration\nThe layer declaration is required. It defines the size and source of the layer, including its connection bundles and attributes. The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers). For example:  \n\n    input Data[784];\n    hidden Hidden[5,20] from Data all;\n    output Result[2] from Hidden all;  \n\n-   The product of the dimensions is the number of nodes in the layer. In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.\n-   The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.  \n\n<!-- REMOVED THIS CONTENT UNTIL THIS FEATURE IS SUPPORTED IN THE PRODUCT\nTo specify that the number of nodes in a layer be determined automatically, use the **auto** keyword. The **auto** keyword has different effects, depending on the layer:  \n\n-   In an input layer declaration, the number of nodes is the number of features in the input data.\n-   In a hidden layer declaration, the number of nodes is the number that is specified by the parameter value for **Number of hidden nodes**. \n-   In an output layer declaration, the number of nodes is 2 for two-class classification, 1 for regression, and equal to the number of output nodes for multiclass classification.   \n\nFor example, the following network definition allows the size of all layers to be automatically determined:  \n\n    input Data auto;\n    hidden Hidden auto from Data all;\n    output Result auto from Hidden all;  \n-->\n\nA layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to **sigmoid**. The following output functions are supported:  \n\n-   sigmoid\n-   linear\n-   softmax\n-   rlinear\n-   square\n-   sqrt\n-   srlinear\n-   abs\n-   tanh \n-   brlinear  \n\nFor example, the following declaration uses the **softmax** function:  \n\n    output Result [100] softmax from Hidden all;  \n\n##Connection declaration\nImmediately after defining the trainable layer, you must declare connections among the layers you have defined. The connection bundle declaration starts with the keyword **from**, followed by the name of the bundle's source layer and the kind of connection bundle to create.   \n\nCurrently, five kinds of connection bundles are supported:  \n\n-   **Full** bundles, indicated by the keyword **all**\n-   **Filtered** bundles, indicated by the keyword **where**, followed by a predicate expression\n-   **Convolutional** bundles, indicated by the keyword **convolve**, followed by the convolution attributes\n-   **Pooling** bundles, indicated by the keywords **max pool** or **mean pool**\n-   **Response normalization** bundles, indicated by the keyword **response norm**      \n\n##Full bundles  \n\nA full connection bundle includes a connection from each node in the source layer to each node in the destination layer. This is the default network connection type.  \n\n##Filtered bundles\nA filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression. The following example defines two filtered bundles:  \n\n    input Pixels [10, 20];\n    hidden ByRow[10, 12] from Pixels where (s,d) => s[0] == d[0];\n    hidden ByCol[5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;  \n\n-   In the predicate for ByRow, **s** is a parameter representing an index into the rectangular array of nodes of the input layer, Pixels, and **d** is a parameter representing an index into the array of nodes of the hidden layer, ByRow. The type of both **s** and **d** is a tuple of integers of length two. Conceptually, **s** ranges over all pairs of integers with _0 <= s[0] < 10_ and _0 <= s[1] < 20_, and **d** ranges over all pairs of integers, with _0 <= d[0] < 10_ and _0 <= d[1] < 12_. \n-   On the right-hand side of the predicate expression, there is a condition. In this example, for every value of **s** and **d** such that the condition is True, there is an edge from the source layer node to the destination layer node. Thus, this filter expression indicates that the bundle includes a connection from the node defined by **s** to the node defined by **d** in all cases where s[0] is equal to d[0].  \n\nOptionally, you can specify a set of weights for a filtered bundle. The value for the **Weights** attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle. By default, weights are randomly generated.  \n\nWeight values are grouped by the destination node index. That is, if the first destination node is connected to K source nodes, the first _K_ elements of the **Weights** tuple are the weights for the first destination node, in source index order. The same applies for the remaining destination nodes.  \n\n##Convolutional bundles\nWhen the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data. For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.  \n\nConvolutional bundles employ rectangular **kernels** that are slid through the dimensions. Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as **kernel applications**. Each kernel application corresponds to a node in the source layer, which is referred to as the **central node**. The weights of a kernel are shared among many connections. In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.  \n\nConvolutional bundles support the following attributes:\n\n**InputShape** defines the dimensionality of the source layer for the purposes of this convolutional bundle. The value must be a tuple of positive integers. The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer. The length of this tuple becomes the **arity** value for the convolutional bundle. (Typically arity refers to the number of arguments or operands that a function can take.)  \n\nTo define the shape and locations of the kernels, use the attributes **KernelShape**, **Stride**, **Padding**, **LowerPad**, and **UpperPad**:   \n\n-   **KernelShape**: (required) Defines the dimensionality of each kernel for the convolutional bundle. The value must be a tuple of positive integers with a length that equals the arity of the bundle. Each component of this tuple must be no greater than the corresponding component of **InputShape**. \n-   **Stride**: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes. The value must be a tuple of positive integers with a length that is the arity of the bundle. Each component of this tuple must be no greater than the corresponding component of **KernelShape**. The default value is a tuple with all components equal to one. \n-   **Sharing**: (optional) Defines the weight sharing for each dimension of the convolution. The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle. A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value. The default value is a tuple that consists of all True values. \n-   **MapCount**: (optional) Defines the number of feature maps for the convolutional bundle. The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle. A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one. The default value is one. The total number of feature maps is the product of the components of the tuple. The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes. \n-   **Weights**: (optional) Defines the initial weights for the bundle. The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article. The default weights are randomly generated.  \n\nThere are two sets of properties that control padding, which are mutually exclusive:\n\n-   **Padding**: (optional) Determines whether the input should be padded by using a **default padding scheme**. The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle. A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value. If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer. Thus, the number of \"dummy\" nodes in each dimension is determined automatically, to fit exactly _(InputShape[d] - 1) / Stride[d] + 1_ kernels into the padded source layer. If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1). The default value of this attribute is a tuple with all components equal to False.\n-   **UpperPad** and **LowerPad**: (optional) Provide greater control over the amount of padding to use. **Important:** These attributes can be defined if and only if the **Padding** property above is ***not*** defined. The values should be integer-valued tuples with lengths that are the arity of the bundle. When these attributes are specified, \"dummy\" nodes are added to the lower and upper ends of each dimension of the input layer. The number of nodes added to the lower and upper ends in each dimension is determined by **LowerPad**[i] and **UpperPad**[i] respectively. To ensure that kernels correspond only to \"real\" nodes and not to \"dummy\" nodes, the following conditions must be met:\n    -   Each component of **LowerPad** must be strictly less than KernelShape[d]/2. \n    -   Each component of **UpperPad** must be no greater than KernelShape[d]/2. \n    -   The default value of these attributes is a tuple with all components equal to 0. \n\nThe setting **Padding** = true allows as much padding as is needed to keep the \"center\" of the kernel inside the \"real\" input. This changes the math a bit for computing the output size. Generally, the output size _D_ is computed as _D = (I - K) / S + 1_, where _I_ is the input size, _K_ is the kernel size, _S_ is the stride, and _/_ is integer division (round toward zero). If you set UpperPad = [1, 1], the input size _I_ is effectively 29, and thus _D = (29 - 5) / 2 + 1 = 13_. However, when **Padding** = true, essentially _I_ gets bumped up by _K - 1_; hence _D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14_. By specifying values for **UpperPad** and **LowerPad** you get much more control over the padding than if you just set **Padding** = true.\n\nFor more information about convolutional networks and their applications, see these articles:  \n\n-   [http://deeplearning.net/tutorial/lenet.html ](http://deeplearning.net/tutorial/lenet.html )\n-   [http://research.microsoft.com/pubs/68920/icdar03.pdf](http://research.microsoft.com/pubs/68920/icdar03.pdf) \n-   [http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf](http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf)  \n\n##Pooling bundles\nA **pooling bundle** applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value. Hence, pooling bundles have no trainable state (weights or biases). Pooling bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.  \n\nTypically, the kernels summarized by adjacent pooling units do not overlap. If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks. Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.  \n\nThe following example illustrates a pooling bundle: \n\n    hidden P1 [5, 12, 12]\n      from C1 max pool {\n        InputShape  = [ 5, 24, 24];\n        KernelShape = [ 1,  2,  2];\n        Stride      = [ 1,  2,  2];\n      }  \n\n-   The arity of the bundle is 3 (the length of the tuples **InputShape**, **KernelShape**, and **Stride**). \n-   The number of nodes in the source layer is _5 * 24 * 24 = 2880_. \n-   This is a traditional local pooling layer because **KernelShape** and **Stride** are equal. \n-   The number of nodes in the destination layer is _5 * 12 * 12 = 1440_.  \n    \nFor more information about pooling layers, see these articles:  \n\n-   [http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Section 3.4)\n-   [http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf](http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf) \n-   [http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf)\n    \n##Response normalization bundles\n**Response normalization** is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in a paper titled ImageNet Classiﬁcation with Deep Convolutional Neural Networks (see section 3.3). Response normalization is used to aid generalization in neural nets. When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons. This is done by using three parameters (***α***, ***β***, and ***k***) and a convolutional structure (or neighborhood shape). Every neuron in the destination layer ***y*** corresponds to a neuron ***x*** in the source layer. The activation level of ***y*** is given by the following formula, where ***f*** is the activation level of a neuron, and ***Nx*** is the kernel (or the set that contains the neurons in the neighborhood of ***x***), as defined by the following convolutional structure:  \n\n![][1]  \n\nResponse normalization bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.  \n \n-   If the kernel contains neurons in the same map as ***x***, the normalization scheme is referred to as **same map normalization**. To define same map normalization, the first coordinate in **InputShape** must have the value 1.\n-   If the kernel contains neurons in the same spatial position as ***x***, but the neurons are in other maps, the normalization scheme is called **across maps normalization**. This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps. To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.  \n\nBecause response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).   \n\n**Alert**: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels. For example, if KernelShape[d] is odd, then _KernelShape[d]/2_ corresponds to the central kernel node. If _KernelShape[d]_ is even, the central node is at _KernelShape[d]/2 - 1_. Therefore, if **Padding**[d] is False, the first and the last _KernelShape[d]/2_ nodes do not have corresponding nodes in the destination layer. To avoid this situation, define **Padding** as [true, true, …, true].  \n\nIn addition to the four attributes described earlier, response normalization bundles also support the following attributes:  \n\n-   **Alpha**: (required) Specifies a floating-point value that corresponds to ***α*** in the previous formula. \n-   **Beta**: (required) Specifies a floating-point value that corresponds to ***β*** in the previous formula. \n-   **Offset**: (optional) Specifies a floating-point value that corresponds to ***k*** in the previous formula. It defaults to 1.  \n\nThe following example defines a response normalization bundle using these attributes:  \n\n    hidden RN1 [5, 10, 10]\n      from P1 response norm {\n        InputShape  = [ 5, 12, 12];\n        KernelShape = [ 1,  3,  3];\n        Alpha = 0.001;\n        Beta = 0.75;\n      }  \n\n-   The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes. \n-   The value of **KernelShape** indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle. \n-   The default value of **Padding** is False, thus the destination layer has only 10 nodes in each dimension. To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].  \n\n##Share declaration \nNet# optionally supports defining multiple bundles with shared weights. The weights of any two bundles can be shared if their structures are the same. The following syntax defines bundles with shared weights:  \n\n    share-declaration:\n        share    {    layer-list    }\n        share    {    bundle-list    }\n       share    {    bias-list    }\n    \n    layer-list:\n        layer-name    ,    layer-name\n        layer-list    ,    layer-name\n    \n    bundle-list:\n       bundle-spec    ,    bundle-spec\n        bundle-list    ,    bundle-spec\n    \n    bundle-spec:\n       layer-name    =>     layer-name\n    \n    bias-list:\n        bias-spec    ,    bias-spec\n        bias-list    ,    bias-spec\n    \n    bias-spec:\n        1    =>    layer-name\n    \n    layer-name:\n        identifier  \n\nFor example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:  \n\n    Const {\n      InputSize = 37;\n      HiddenSize = 50;\n    }\n    input {\n      Data1 [InputSize];\n      Data2 [InputSize];\n    }\n    hidden {\n      H1 [HiddenSize] from Data1 all;\n      H2 [HiddenSize] from Data2 all;\n    }\n    output Result [2] {\n      from H1 all;\n      from H2 all;\n    }\n    share { H1, H2 } // share both weights and biases  \n\n-   The input features are partitioned into two equal sized input layers. \n-   The hidden layers then compute higher level features on the two input layers. \n-   The share-declaration specifies that H1 and H2 must be computed in the same way from their respective inputs.  \n \nAlternatively, this could be specified with two separate share-declarations as follows:  \n\n    share { Data1 => H1, Data2 => H2 } // share weights  \n\n<!-- -->\n\n    share { 1 => H1, 1 => H2 } // share biases  \n\nYou can use the short form only when the layers contain a single bundle. In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.  \n\n##Examples of Net# usage\nThis section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.   \n\n###Define a simple custom neural network: \"Hello World\" example\nThis simple example demonstrates how to create a neural network model that has a single hidden layer.  \n\n    input Data [100];\n    hidden H [200] from Data all;\n    output Out [10] sigmoid from H all;  \n\nThe example illustrates some basic commands as follows:  \n\n-   The first line defines the input layer (named Data), which has 100 nodes, and each node represents a feature in the input examples.\n-   The second line creates the hidden layer. The name H is assigned to the hidden layer, which has 200 nodes. This layer is fully connected to the input layer.\n-   The third line defines the output layer (named O), and it contains 10 output nodes. For classification neural networks, there is one output node per class. The keyword **sigmoid** indicates that the output function is applied to the output layer.   \n\n###Define multiple hidden layers: computer vision example\nThe following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.  \n\n    // Define the input layers \n    input Pixels [10, 20];\n    input MetaData [7];\n    \n    // Define the first two hidden layers, using data only from the Pixels input\n    hidden ByRow [10, 12] from Pixels where (s,d) => s[0] == d[0];\n    hidden ByCol [5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;\n    \n    // Define the third hidden layer, which uses as source the hidden layers ByRow and ByCol\n    hidden Gather [100] \n    {\n      from ByRow all;\n      from ByCol all;\n    }\n    \n    // Define the output layer and its sources\n    output Result [10]  \n    {\n      from Gather all;\n      from MetaData all;\n    }  \n\nThis example illustrates several features of the neural networks specification language:  \n\n-   The structure has two input layers, Pixels and MetaData.\n-   The Pixels layer is a source layer for two connection bundles, with destination layers, ByRow and ByCol.\n-   The layers, Gather and Result, are destination layers in multiple connection bundles.\n-   The output layer, Result, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.\n-   The hidden layers, ByRow and ByCol, specify filtered connectivity by using predicate expressions. More precisely, the node in ByRow at [x, y] is connected to those nodes in Pixels by having the first index coordinate equal to the node's first coordinate, x. Similarly, the node in ByCol at [x, y] is connected to those nodes in Pixels by having the second index coordinate within one of the node's second coordinate, y.  \n\n###Define a convolutional network for multiclass classification: digit recognition example\nThe definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.  \n\n    input Image [29, 29];\n    hidden Conv1 [5, 13, 13] from Image convolve \n    {\n       InputShape  = [29, 29];\n       KernelShape = [ 5,  5];\n       Stride      = [ 2,  2];\n       MapCount    = 5;\n    }\n    hidden Conv2 [50, 5, 5]\n    from Conv1 convolve \n    {\n       InputShape  = [ 5, 13, 13];\n       KernelShape = [ 1,  5,  5];\n       Stride      = [ 1,  2,  2];\n       Sharing     = [false, true, true];\n       MapCount    = 10;\n    }\n    hidden Hid3 [100] from Conv2 all;\n    output Digit [10] from Hid3 all;  \n\n\n-   The structure has a single input layer, Image.\n-   The keyword **convolve** indicates that Conv1 and Conv2 are convolutional layers. Each of these layer declarations is followed by a list of the convolution attributes.\n-   The net has a third hidden layer, Hid3, which is fully connected to the second hidden layer, Conv2.\n-   The output layer, Digit, is connected only to the third hidden layer, Hid3. The keyword **all** indicates that the output layer is fully connected to Hid3.\n-   The arity of the convolution is three (the length of the tuples **InputShape**, **KernelShape**, **Stride**, and **Sharing**). \n-   The number of weights per kernel is _1 + **KernelShape**\\[0] * **KernelShape**\\[1] * **KernelShape**\\[2] = 1 + 1 * 5 * 5 = 26. Or 26 * 50 = 1300_.\n-   You can calculate the nodes in each hidden layer as follows:\n    -   **NodeCount**\\[0] = (5 - 1) / 1 + 1 = 5.\n    -   **NodeCount**\\[1] = (13 - 5) / 2 + 1 = 5. \n    -   **NodeCount**\\[2] = (13 - 5) / 2 + 1 = 5. \n-   The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: _**MapCount** * **NodeCount**\\[0] * **NodeCount**\\[1] * **NodeCount**\\[2] = 10 * 5 * 5 * 5_\n-   Because **Sharing**[d] is False only for _d == 0_, the number of kernels is _**MapCount** * **NodeCount**\\[0] = 10 * 5 = 50_. \n\n[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif\n \n\ntest\n"
}