<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure</source>
          <target state="new">Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud.</source>
          <target state="new">Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory</source>
          <target state="new">Move data from an on-premise SQL server to SQL Azure with Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>This topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF).</source>
          <target state="new">This topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF).</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="intro"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Introduction: What is ADF and when should it be used to migrate data?</source>
          <target state="new"><ph id="ph1">&lt;a name="intro"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Introduction: What is ADF and when should it be used to migrate data?</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data.</source>
          <target state="new">Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>The key concept in the ADF model is pipeline.</source>
          <target state="new">The key concept in the ADF model is pipeline.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets.</source>
          <target state="new">A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Linked services are used to define the information needed for Data Factory to connect to the data resources.</source>
          <target state="new">Linked services are used to define the information needed for Data Factory to connect to the data resources.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud.</source>
          <target state="new">With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies.</source>
          <target state="new">These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.</source>
          <target state="new">Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.</source>
          <target state="new">Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis.</source>
          <target state="new">ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>ADF also has other capabilities such as support for complex operations.</source>
          <target state="new">ADF also has other capabilities such as support for complex operations.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>For more information on ADF, see the documentation at <bpt id="p1">[</bpt>Azure Data Factory (ADF)<ept id="p1">](http://azure.microsoft.com/services/data-factory/)</ept>.</source>
          <target state="new">For more information on ADF, see the documentation at <bpt id="p1">[</bpt>Azure Data Factory (ADF)<ept id="p1">](http://azure.microsoft.com/services/data-factory/)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="scenario"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>The Scenario</source>
          <target state="new"><ph id="ph1">&lt;a name="scenario"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>The Scenario</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud.</source>
          <target state="new">We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The two activities are:</source>
          <target state="new">The two activities are:</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>copy data from an on-premise SQL Server database to an Azure Blob Storage account</source>
          <target state="new">copy data from an on-premise SQL Server database to an Azure Blob Storage account</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>copy data from the Azure Blob Storage account to an Azure SQL Database.</source>
          <target state="new">copy data from the Azure Blob Storage account to an Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Reference<ept id="p1">**</ept>: The steps shown here have been adapted from the more detailed tutorial <bpt id="p2">[</bpt>Enable your pipelines to work with on-premises data<ept id="p2">](data-factory-use-onpremises-datasources.md)</ept> provided by the ADF team and references to the relevant sections of that topic are provided when appropriate.</source>
          <target state="new"><bpt id="p1">**</bpt>Reference<ept id="p1">**</ept>: The steps shown here have been adapted from the more detailed tutorial <bpt id="p2">[</bpt>Enable your pipelines to work with on-premises data<ept id="p2">](data-factory-use-onpremises-datasources.md)</ept> provided by the ADF team and references to the relevant sections of that topic are provided when appropriate.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="prereqs"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</source>
          <target state="new"><ph id="ph1">&lt;a name="prereqs"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>This tutorial assumes you have:</source>
          <target state="new">This tutorial assumes you have:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>An <bpt id="p1">**</bpt>Azure subscription<ept id="p1">**</ept>.</source>
          <target state="new">An <bpt id="p1">**</bpt>Azure subscription<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>If you do not have a subscription, you can sign up for a <bpt id="p1">[</bpt>free trial<ept id="p1">](https://azure.microsoft.com/pricing/free-trial/)</ept>.</source>
          <target state="new">If you do not have a subscription, you can sign up for a <bpt id="p1">[</bpt>free trial<ept id="p1">](https://azure.microsoft.com/pricing/free-trial/)</ept>.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>An <bpt id="p1">**</bpt>Azure storage account<ept id="p1">**</ept>.</source>
          <target state="new">An <bpt id="p1">**</bpt>Azure storage account<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>You will use an Azure storage account for storing the data in this tutorial.</source>
          <target state="new">You will use an Azure storage account for storing the data in this tutorial.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>If you don't have an Azure storage account, see the <bpt id="p1">[</bpt>Create a storage account<ept id="p1">](storage-create-storage-account.md#create-a-storage-account)</ept> article.</source>
          <target state="new">If you don't have an Azure storage account, see the <bpt id="p1">[</bpt>Create a storage account<ept id="p1">](storage-create-storage-account.md#create-a-storage-account)</ept> article.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>After you have created the storage account, you will need to obtain the account key used to access the storage.</source>
          <target state="new">After you have created the storage account, you will need to obtain the account key used to access the storage.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>View, copy and regenerate storage access keys<ept id="p1">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.</source>
          <target state="new">See <bpt id="p1">[</bpt>View, copy and regenerate storage access keys<ept id="p1">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Access to an <bpt id="p1">**</bpt>Azure SQL Database<ept id="p1">**</ept>.</source>
          <target state="new">Access to an <bpt id="p1">**</bpt>Azure SQL Database<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>If you must setup an Azure SQL Database, <bpt id="p1">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id="p1">](sql-database-get-started.md)</ept> provides information on how to provision a new instance of a Azure SQL Database.</source>
          <target state="new">If you must setup an Azure SQL Database, <bpt id="p1">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id="p1">](sql-database-get-started.md)</ept> provides information on how to provision a new instance of a Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Installed and configured <bpt id="p1">**</bpt>Azure PowerShell<ept id="p1">**</ept> locally.</source>
          <target state="new">Installed and configured <bpt id="p1">**</bpt>Azure PowerShell<ept id="p1">**</ept> locally.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p1">[</bpt>How to install and configure Azure PowerShell<ept id="p1">](powershell-install-configure.md)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p1">[</bpt>How to install and configure Azure PowerShell<ept id="p1">](powershell-install-configure.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This procedure uses the <bpt id="p1">[</bpt>Azure preview portal<ept id="p1">](https://ms.portal.azure.com/)</ept>.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This procedure uses the <bpt id="p1">[</bpt>Azure preview portal<ept id="p1">](https://ms.portal.azure.com/)</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="upload-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Upload the data to your on-premise SQL Server</source>
          <target state="new"><ph id="ph1">&lt;a name="upload-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Upload the data to your on-premise SQL Server</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>We use the <bpt id="p1">[</bpt>NYC Taxi dataset<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept> to demonstrate the migration process.</source>
          <target state="new">We use the <bpt id="p1">[</bpt>NYC Taxi dataset<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept> to demonstrate the migration process.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id="p1">[</bpt>here<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept>.</source>
          <target state="new">The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id="p1">[</bpt>here<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip.</source>
          <target state="new">The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>A sample and description of these files are provided in <bpt id="p1">[</bpt>NYC Taxi Trips Dataset Description<ept id="p1">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.</source>
          <target state="new">A sample and description of these files are provided in <bpt id="p1">[</bpt>NYC Taxi Trips Dataset Description<ept id="p1">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset.</source>
          <target state="new">You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id="p1">[</bpt>Bulk Import Data into SQL Server Database<ept id="p1">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>.</source>
          <target state="new">To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id="p1">[</bpt>Bulk Import Data into SQL Server Database<ept id="p1">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.</source>
          <target state="new">These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="create-adf"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Create an Azure Data Factory</source>
          <target state="new"><ph id="ph1">&lt;a name="create-adf"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Create an Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>The instructions for creating a new Azure Data Factory and a resource group in the <bpt id="p1">[</bpt>Azure preview portal<ept id="p1">](https://ms.portal.azure.com/)</ept> are provided <bpt id="p2">[</bpt>here<ept id="p2">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>.</source>
          <target state="new">The instructions for creating a new Azure Data Factory and a resource group in the <bpt id="p1">[</bpt>Azure preview portal<ept id="p1">](https://ms.portal.azure.com/)</ept> are provided <bpt id="p2">[</bpt>here<ept id="p2">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Name the new ADF instance <bpt id="p1">*</bpt>adfdsp<ept id="p1">*</ept> and name the resource group created <bpt id="p2">*</bpt>adfdsprg<ept id="p2">*</ept>.</source>
          <target state="new">Name the new ADF instance <bpt id="p1">*</bpt>adfdsp<ept id="p1">*</ept> and name the resource group created <bpt id="p2">*</bpt>adfdsprg<ept id="p2">*</ept>.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Install and configure up the Data Management Gateway</source>
          <target state="new">Install and configure up the Data Management Gateway</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory.</source>
          <target state="new">To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway.</source>
          <target state="new">To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.</source>
          <target state="new">The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>For set-up instructions and details on Data Management Gateway, see <bpt id="p1">[</bpt>Enable your pipelines to work with on-premises data<ept id="p1">](data-factory-use-onpremises-datasources.md)</ept></source>
          <target state="new">For set-up instructions and details on Data Management Gateway, see <bpt id="p1">[</bpt>Enable your pipelines to work with on-premises data<ept id="p1">](data-factory-use-onpremises-datasources.md)</ept></target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adflinkedservices"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create linked services to connect to the data resources</source>
          <target state="new"><ph id="ph1">&lt;a name="adflinkedservices"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create linked services to connect to the data resources</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>A linked service defines the information needed for Azure Data Factory to connect to a data resource.</source>
          <target state="new">A linked service defines the information needed for Azure Data Factory to connect to a data resource.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>The step-by-step procedure for creating linked services is provided in <bpt id="p1">[</bpt>Create linked services<ept id="p1">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.</source>
          <target state="new">The step-by-step procedure for creating linked services is provided in <bpt id="p1">[</bpt>Create linked services<ept id="p1">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>We have three resources in this scenario for which linked services are needed.</source>
          <target state="new">We have three resources in this scenario for which linked services are needed.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Linked service for on-premise SQL Server</source>
          <target state="new">Linked service for on-premise SQL Server</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Linked service for Azure Blob Storage</source>
          <target state="new">Linked service for Azure Blob Storage</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Linked service for Azure SQL database</source>
          <target state="new">Linked service for Azure SQL database</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-linked-service-onprem-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-linked-service-onprem-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>To create the linked service for the on-premise SQL Server, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>SQL<ept id="p2">*</ept> and enter the credentials for the <bpt id="p3">*</bpt>username<ept id="p3">*</ept> and <bpt id="p4">*</bpt>password<ept id="p4">*</ept> for the on-premise SQL Server.</source>
          <target state="new">To create the linked service for the on-premise SQL Server, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>SQL<ept id="p2">*</ept> and enter the credentials for the <bpt id="p3">*</bpt>username<ept id="p3">*</ept> and <bpt id="p4">*</bpt>password<ept id="p4">*</ept> for the on-premise SQL Server.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>You need to enter the servername as a <bpt id="p1">**</bpt>fully qualified servername backslash instance name (servername\instancename)<ept id="p1">**</ept>.</source>
          <target state="new">You need to enter the servername as a <bpt id="p1">**</bpt>fully qualified servername backslash instance name (servername\instancename)<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Name the linked service <bpt id="p1">*</bpt>adfonpremsql<ept id="p1">*</ept>.</source>
          <target state="new">Name the linked service <bpt id="p1">*</bpt>adfonpremsql<ept id="p1">*</ept>.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-linked-service-blob-store"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for Blob</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-linked-service-blob-store"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for Blob</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>To create the linked service for the Azure Blob Storage account, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>Azure Storage Account<ept id="p2">*</ept> and enter the Azure Blob Storage account key and container name.</source>
          <target state="new">To create the linked service for the Azure Blob Storage account, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>Azure Storage Account<ept id="p2">*</ept> and enter the Azure Blob Storage account key and container name.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Name the link service <bpt id="p1">*</bpt>adfds<ept id="p1">*</ept>.</source>
          <target state="new">Name the link service <bpt id="p1">*</bpt>adfds<ept id="p1">*</ept>.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-linked-service-azure-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for Azure SQL database</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-linked-service-azure-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Linked service for Azure SQL database</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>To create the linked service for the Azure SQL Database, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>Azure SQL<ept id="p2">*</ept> and enter the credentials for the <bpt id="p3">*</bpt>username<ept id="p3">*</ept> and <bpt id="p4">*</bpt>password<ept id="p4">*</ept> for the Azure SQL Database.</source>
          <target state="new">To create the linked service for the Azure SQL Database, click on the <bpt id="p1">**</bpt>Data Store<ept id="p1">**</ept> in the ADF landing page on Azure Portal, select <bpt id="p2">*</bpt>Azure SQL<ept id="p2">*</ept> and enter the credentials for the <bpt id="p3">*</bpt>username<ept id="p3">*</ept> and <bpt id="p4">*</bpt>password<ept id="p4">*</ept> for the Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>username<ept id="p1">*</ept> must be specified as <bpt id="p2">*</bpt>user@servername<ept id="p2">*</ept>.</source>
          <target state="new">The <bpt id="p1">*</bpt>username<ept id="p1">*</ept> must be specified as <bpt id="p2">*</bpt>user@servername<ept id="p2">*</ept>.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Define and create tables to specify how to access the datasets</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Define and create tables to specify how to access the datasets</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Create tables that specify the structure, location and availability of the datasets with the following script-based procedures.</source>
          <target state="new">Create tables that specify the structure, location and availability of the datasets with the following script-based procedures.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>JSON files are used to define the tables.</source>
          <target state="new">JSON files are used to define the tables.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>For more information on the structure of these files, see <bpt id="p1">[</bpt>Datasets<ept id="p1">](data-factory-create-datasets.md)</ept>.</source>
          <target state="new">For more information on the structure of these files, see <bpt id="p1">[</bpt>Datasets<ept id="p1">](data-factory-create-datasets.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph>  You should execute <ph id="ph2">`Switch-AzureMode -Name AzureResourceManager`</ph> and the <ph id="ph3">`Add-AzureAccount`</ph> cmdlets prior to executing the <bpt id="p1">[</bpt>New-AzureDataFactoryTable<ept id="p1">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept> cmdlet to confirm that the Azure PowerShell cmdlets are available and that the right Azure subscription is selected for the command execution.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph>  You should execute <ph id="ph2">`Switch-AzureMode -Name AzureResourceManager`</ph> and the <ph id="ph3">`Add-AzureAccount`</ph> cmdlets prior to executing the <bpt id="p1">[</bpt>New-AzureDataFactoryTable<ept id="p1">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept> cmdlet to confirm that the Azure PowerShell cmdlets are available and that the right Azure subscription is selected for the command execution.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>For documentation of these cmdlets, see <bpt id="p1">[</bpt>Switch-AzureMode<ept id="p1">](https://msdn.microsoft.com/library/dn722470.aspx)</ept> and <bpt id="p2">[</bpt>Add-AzureAccoun<ept id="p2">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.</source>
          <target state="new">For documentation of these cmdlets, see <bpt id="p1">[</bpt>Switch-AzureMode<ept id="p1">](https://msdn.microsoft.com/library/dn722470.aspx)</ept> and <bpt id="p2">[</bpt>Add-AzureAccoun<ept id="p2">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>The JSON-based definitions in the tables use the following names:</source>
          <target state="new">The JSON-based definitions in the tables use the following names:</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>the <bpt id="p1">**</bpt>table name<ept id="p1">**</ept> in the on-premise SQL server is <bpt id="p2">*</bpt>nyctaxi_data<ept id="p2">*</ept></source>
          <target state="new">the <bpt id="p1">**</bpt>table name<ept id="p1">**</ept> in the on-premise SQL server is <bpt id="p2">*</bpt>nyctaxi_data<ept id="p2">*</ept></target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>the <bpt id="p1">**</bpt>container name<ept id="p1">**</ept> in the Azure Blob Storage account is <bpt id="p2">*</bpt>containername<ept id="p2">*</ept></source>
          <target state="new">the <bpt id="p1">**</bpt>container name<ept id="p1">**</ept> in the Azure Blob Storage account is <bpt id="p2">*</bpt>containername<ept id="p2">*</ept></target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Three table definitions are needed for this ADF pipeline:</source>
          <target state="new">Three table definitions are needed for this ADF pipeline:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>SQL on-premise Table</source>
          <target state="new">SQL on-premise Table</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Blob Table</source>
          <target state="new">Blob Table</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>SQL Azure Table</source>
          <target state="new">SQL Azure Table</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph>  The following procedures use Azure PowerShell to define and create the ADF activities.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph>  The following procedures use Azure PowerShell to define and create the ADF activities.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>But these tasks can also be accomplished using the Azure preview portal.</source>
          <target state="new">But these tasks can also be accomplished using the Azure preview portal.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>For details, see <bpt id="p1">[</bpt>Create input and output datasets<ept id="p1">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.</source>
          <target state="new">For details, see <bpt id="p1">[</bpt>Create input and output datasets<ept id="p1">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-table-onprem-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>SQL on-premise Table</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-table-onprem-sql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>SQL on-premise Table</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>The table definition for the on-premise SQL Server is specified in the following JSON file.</source>
          <target state="new">The table definition for the on-premise SQL Server is specified in the following JSON file.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Note that the column names were not included here, you can sub-select on the column names by including them here (for details please check the <bpt id="p1">[</bpt>ADF documentation<ept id="p1">](data-factory-copy-activity.md)</ept>).</source>
          <target state="new">Note that the column names were not included here, you can sub-select on the column names by including them here (for details please check the <bpt id="p1">[</bpt>ADF documentation<ept id="p1">](data-factory-copy-activity.md)</ept>).</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>onpremtabledef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\onpremtabledef.json<ept id="p2">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>onpremtabledef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\onpremtabledef.json<ept id="p2">*</ept>).</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-table-blob-store"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Blob Table</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-table-blob-store"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Blob Table</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Definition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):</source>
          <target state="new">Definition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>bloboutputtabledef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\bloboutputtabledef.json<ept id="p2">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>bloboutputtabledef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\bloboutputtabledef.json<ept id="p2">*</ept>).</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-table-azure-sq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>SQL Azure Table</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-table-azure-sq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>SQL Azure Table</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Definition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):</source>
          <target state="new">Definition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>AzureSqlTable.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\AzureSqlTable.json<ept id="p2">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p1">*</bpt>AzureSqlTable.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\AzureSqlTable.json<ept id="p2">*</ept>).</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-pipeline"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Define and create the pipeline</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-pipeline"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Define and create the pipeline</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures.</source>
          <target state="new">Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>A JSON file is used to define the pipeline properties.</source>
          <target state="new">A JSON file is used to define the pipeline properties.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>The script assumes that the <bpt id="p1">**</bpt>pipeline name<ept id="p1">**</ept> is <bpt id="p2">*</bpt>AMLDSProcessPipeline<ept id="p2">*</ept>.</source>
          <target state="new">The script assumes that the <bpt id="p1">**</bpt>pipeline name<ept id="p1">**</ept> is <bpt id="p2">*</bpt>AMLDSProcessPipeline<ept id="p2">*</ept>.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC).</source>
          <target state="new">Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC).</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph>  The following procedures use Azure PowerShell to define and create the ADF pipeline.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph>  The following procedures use Azure PowerShell to define and create the ADF pipeline.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>But this task can also be accomplished using the Azure preview portal.</source>
          <target state="new">But this task can also be accomplished using the Azure preview portal.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>For details, see <bpt id="p1">[</bpt>Create and run a pipeline<ept id="p1">](data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.</source>
          <target state="new">For details, see <bpt id="p1">[</bpt>Create and run a pipeline<ept id="p1">](data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Using the table definitions provided above, the pipeline definition for the ADF is specified as follows:</source>
          <target state="new">Using the table definitions provided above, the pipeline definition for the ADF is specified as follows:</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Copy this JSON definition of the pipeline into a file called <bpt id="p1">*</bpt>pipelinedef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\pipelinedef.json<ept id="p2">*</ept>).</source>
          <target state="new">Copy this JSON definition of the pipeline into a file called <bpt id="p1">*</bpt>pipelinedef.json<ept id="p1">*</ept> file and save it to a known location (here assumed to be <bpt id="p2">*</bpt>C:\temp\pipelinedef.json<ept id="p2">*</ept>).</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Create the pipeline in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the pipeline in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Confirm that you can see the pipeline on the ADF in the Azure portal show up as following (when you click on the diagram)</source>
          <target state="new">Confirm that you can see the pipeline on the ADF in the Azure portal show up as following (when you click on the diagram)</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="adf-pipeline-start"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Start the Pipeline</source>
          <target state="new"><ph id="ph1">&lt;a name="adf-pipeline-start"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Start the Pipeline</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>The pipeline can now be run using the following command:</source>
          <target state="new">The pipeline can now be run using the following command:</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>startdate<ept id="p1">*</ept> and <bpt id="p2">*</bpt>enddate<ept id="p2">*</ept> parameter values need to be replaced with the actual dates between which you want the pipeline to run.</source>
          <target state="new">The <bpt id="p1">*</bpt>startdate<ept id="p1">*</ept> and <bpt id="p2">*</bpt>enddate<ept id="p2">*</ept> parameter values need to be replaced with the actual dates between which you want the pipeline to run.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Once the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day.</source>
          <target state="new">Once the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Note that we have not leveraged the functionality provided by ADF to pipe data incrementally.</source>
          <target state="new">Note that we have not leveraged the functionality provided by ADF to pipe data incrementally.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>For more details on how to do this and other capabilities provided by ADF, see the <bpt id="p1">[</bpt>ADF documentation<ept id="p1">](http://azure.microsoft.com/services/data-factory/)</ept>.</source>
          <target state="new">For more details on how to do this and other capabilities provided by ADF, see the <bpt id="p1">[</bpt>ADF documentation<ept id="p1">](http://azure.microsoft.com/services/data-factory/)</ept>.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">62874ba85f0a5ba017e96338414f99ecf7f63388</xliffext:olfilehash>
  </header>
</xliff>