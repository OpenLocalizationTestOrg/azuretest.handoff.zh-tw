{
  "nodes": [
    {
      "content": "How to work with 'big data' data sources",
      "pos": [
        26,
        66
      ]
    },
    {
      "content": "How-to article highlighting patters for using Azure Data Catalog  with 'big data' data sources, including Azure Blob Storage and Hadoop HDFS.",
      "pos": [
        84,
        225
      ]
    },
    {
      "content": "How to work with big data sources in Azure Data Catalog",
      "pos": [
        529,
        584
      ]
    },
    {
      "content": "Introduction",
      "pos": [
        589,
        601
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Microsoft Azure Data Catalog<ept id=\"p1\">**</ept> is a fully managed cloud service that serves as a system of registration and system of discovery for enterprise data sources.",
      "pos": [
        602,
        760
      ]
    },
    {
      "content": "In other words, <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept> is all about helping people discover, understand, and use data sources, and helping organizations to get more value from their existing data sources, including big data.",
      "pos": [
        761,
        969
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept> supports the registration of Azure Blog Storage blobs and directories as well as Hadoop HDFS files and directories.",
      "pos": [
        971,
        1109
      ]
    },
    {
      "content": "The semi-structured nature of these data sources provides great flexibility, but it also means that users must consider how the data sources are organized in order to get the most value from registering them with <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept>.",
      "pos": [
        1110,
        1346
      ]
    },
    {
      "content": "Directories as logical data sets",
      "pos": [
        1351,
        1383
      ]
    },
    {
      "content": "A common pattern for organizing big data sources is to treat directories as logical data sets.",
      "pos": [
        1385,
        1479
      ]
    },
    {
      "content": "Top-level directories are used to define a data set, while subfolders define partitions, and the files they contain store the data itself.",
      "pos": [
        1480,
        1618
      ]
    },
    {
      "content": "An example of this pattern might be:",
      "pos": [
        1620,
        1656
      ]
    },
    {
      "content": "In this example, vehicle_maintenance_events and location_tracking_events represent logical data sets.",
      "pos": [
        1945,
        2046
      ]
    },
    {
      "content": "Each of these folders contains data files which are organized by year and month into subfolders.",
      "pos": [
        2047,
        2143
      ]
    },
    {
      "content": "Each of these folders could potentially contain hundreds or thousands of files.",
      "pos": [
        2144,
        2223
      ]
    },
    {
      "content": "In this pattern, registering individual files with <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept> probably does not make sense.",
      "pos": [
        2225,
        2328
      ]
    },
    {
      "content": "Instead, register the directories that represent the data sets that will be meaningful to the users working with the data.",
      "pos": [
        2329,
        2451
      ]
    },
    {
      "content": "Reference data files",
      "pos": [
        2456,
        2476
      ]
    },
    {
      "content": "A complementary pattern is to store reference data sets as individual files.",
      "pos": [
        2478,
        2554
      ]
    },
    {
      "content": "These data sets may be thought of as the 'small' side of big data, and are often similar to dimensions in an analytical data model.",
      "pos": [
        2555,
        2686
      ]
    },
    {
      "content": "Reference data files contain records that are used to provide context for the bulk of the data files stored elsewhere in the big data store.",
      "pos": [
        2687,
        2827
      ]
    },
    {
      "content": "An example of this pattern might be:",
      "pos": [
        2829,
        2865
      ]
    },
    {
      "content": "When an analyst or data scientist is working with the data contained in the larger directory structures, the data in these reference files can be used to provide more detailed information for entities that are referred to only by name or ID in the larger data set.",
      "pos": [
        2945,
        3209
      ]
    },
    {
      "content": "In this pattern, it will make sense to register the individual reference data files with <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept>.",
      "pos": [
        3211,
        3323
      ]
    },
    {
      "content": "Each file represents a data set, and each one can be annotated and discovered individually.",
      "pos": [
        3324,
        3415
      ]
    },
    {
      "content": "Alternate patterns",
      "pos": [
        3420,
        3438
      ]
    },
    {
      "content": "The patterns described above are just two possible ways a big data store may be organized, but each implementation will be different.",
      "pos": [
        3440,
        3573
      ]
    },
    {
      "content": "Regardless of how your data sources are structured, when registering big data sources with <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept>, focus on registering the files and directories that represent the data sets that will be of value to others within your organization.",
      "pos": [
        3574,
        3822
      ]
    },
    {
      "content": "Registering all files and directories can clutter the catalog, making it harder for users to find what they need.",
      "pos": [
        3823,
        3936
      ]
    },
    {
      "content": "Summary",
      "pos": [
        3941,
        3948
      ]
    },
    {
      "content": "Registering data sources with <bpt id=\"p1\">**</bpt>Azure Data Catalog<ept id=\"p1\">**</ept> makes them easier to discover and understand.",
      "pos": [
        3949,
        4047
      ]
    },
    {
      "content": "By registering and annotating the big data files and directories that represent logical data sets, you can help users find and use the big data sources they need.",
      "pos": [
        4048,
        4210
      ]
    },
    {
      "content": "test",
      "pos": [
        4212,
        4216
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"How to work with 'big data' data sources\"\n   description=\"How-to article highlighting patters for using Azure Data Catalog  with 'big data' data sources, including Azure Blob Storage and Hadoop HDFS.\"\n   services=\"data-catalog\"\n   documentationCenter=\"\"\n   authors=\"steelanddata\"\n   manager=\"NA\"\n   editor=\"\"\n   tags=\"\"/>\n<tags\n   ms.service=\"data-catalog\"\n   ms.devlang=\"NA\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"NA\"\n   ms.workload=\"data-catalog\"\n   ms.date=\"08/25/2015\"\n   ms.author=\"maroche\"/>\n\n\n# How to work with big data sources in Azure Data Catalog\n\n## Introduction\n**Microsoft Azure Data Catalog** is a fully managed cloud service that serves as a system of registration and system of discovery for enterprise data sources. In other words, **Azure Data Catalog** is all about helping people discover, understand, and use data sources, and helping organizations to get more value from their existing data sources, including big data.\n\n**Azure Data Catalog** supports the registration of Azure Blog Storage blobs and directories as well as Hadoop HDFS files and directories. The semi-structured nature of these data sources provides great flexibility, but it also means that users must consider how the data sources are organized in order to get the most value from registering them with **Azure Data Catalog**.\n\n## Directories as logical data sets\n\nA common pattern for organizing big data sources is to treat directories as logical data sets. Top-level directories are used to define a data set, while subfolders define partitions, and the files they contain store the data itself.\n\nAn example of this pattern might be:\n\n    \\vehicle_maintenance_events\n        \\2013\n        \\2014\n        \\2015\n            \\01\n                \\2015-01-trailer01.csv\n                \\2015-01-trailer92.csv\n                \\2015-01-canister9635.csv\n                ...\n    \\location_tracking_events\n        \\2013\n        ...\n\nIn this example, vehicle_maintenance_events and location_tracking_events represent logical data sets. Each of these folders contains data files which are organized by year and month into subfolders. Each of these folders could potentially contain hundreds or thousands of files.\n\nIn this pattern, registering individual files with **Azure Data Catalog** probably does not make sense. Instead, register the directories that represent the data sets that will be meaningful to the users working with the data.\n\n## Reference data files\n\nA complementary pattern is to store reference data sets as individual files. These data sets may be thought of as the 'small' side of big data, and are often similar to dimensions in an analytical data model. Reference data files contain records that are used to provide context for the bulk of the data files stored elsewhere in the big data store.\n\nAn example of this pattern might be:\n\n    \\vehicles.csv\n    \\maintenance_facilities.csv\n    \\maintenance_types.csv\n\nWhen an analyst or data scientist is working with the data contained in the larger directory structures, the data in these reference files can be used to provide more detailed information for entities that are referred to only by name or ID in the larger data set.\n\nIn this pattern, it will make sense to register the individual reference data files with **Azure Data Catalog**. Each file represents a data set, and each one can be annotated and discovered individually.\n\n## Alternate patterns\n\nThe patterns described above are just two possible ways a big data store may be organized, but each implementation will be different. Regardless of how your data sources are structured, when registering big data sources with **Azure Data Catalog**, focus on registering the files and directories that represent the data sets that will be of value to others within your organization. Registering all files and directories can clutter the catalog, making it harder for users to find what they need.\n\n## Summary\nRegistering data sources with **Azure Data Catalog** makes them easier to discover and understand. By registering and annotating the big data files and directories that represent logical data sets, you can help users find and use the big data sources they need.\n\ntest\n"
}