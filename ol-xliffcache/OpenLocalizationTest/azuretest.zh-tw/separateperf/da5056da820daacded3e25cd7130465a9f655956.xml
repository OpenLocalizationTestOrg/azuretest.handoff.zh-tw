{
  "nodes": [
    {
      "content": "Develop Java MapReduce programs for Hadoop | Microsoft Azure",
      "pos": [
        27,
        87
      ]
    },
    {
      "content": "Learn how to develop Java MapReduce programs on HDInsight emulator, how to deploy them to HDInsight.",
      "pos": [
        106,
        206
      ]
    },
    {
      "content": "Develop Java MapReduce programs for Hadoop in HDInsight",
      "pos": [
        512,
        567
      ]
    },
    {
      "content": "This tutorial walks you through an end-to-end scenario for developing a word-counting Hadoop MapReduce job in Java by using Apache Maven.",
      "pos": [
        655,
        792
      ]
    },
    {
      "content": "The tutorial also shows how to test the application on the HDInsight Emulator for Azure and then deploy and run it on an Windows-based HDInsight cluster.",
      "pos": [
        793,
        946
      ]
    },
    {
      "pos": [
        950,
        991
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"prerequisites\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "Before you begin this tutorial, you must have completed the following:",
      "pos": [
        993,
        1063
      ]
    },
    {
      "content": "Install the HDInsight Emulator.",
      "pos": [
        1067,
        1098
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Get started using HDInsight Emulator<ept id=\"p1\">][hdinsight-emulator]</ept>.",
      "pos": [
        1099,
        1180
      ]
    },
    {
      "content": "Make sure all the required services are running.",
      "pos": [
        1181,
        1229
      ]
    },
    {
      "content": "On the computer that has HDInsight Emulator installed, launch the Hadoop Command line from the Desktop shortcut, navigate to <bpt id=\"p1\">**</bpt>C:\\hdp<ept id=\"p1\">**</ept>, and run the command <bpt id=\"p2\">**</bpt>start_local_hdp_services.cmd<ept id=\"p2\">**</ept>.",
      "pos": [
        1230,
        1420
      ]
    },
    {
      "content": "Install Azure PowerShell on the emulator computer.",
      "pos": [
        1423,
        1473
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Install and configure Azure PowerShell<ept id=\"p1\">][powershell-install-configure]</ept>.",
      "pos": [
        1474,
        1567
      ]
    },
    {
      "content": "Install Java platform JDK 7 or higher on the emulator computer.",
      "pos": [
        1570,
        1633
      ]
    },
    {
      "content": "This is already available on the emulator computer.",
      "pos": [
        1634,
        1685
      ]
    },
    {
      "pos": [
        1688,
        1751
      ],
      "content": "Install and configure <bpt id=\"p1\">[</bpt>Apache Maven<ept id=\"p1\">](http://maven.apache.org/)</ept>."
    },
    {
      "content": "Obtain an Azure subscription.",
      "pos": [
        1754,
        1783
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Purchase Options<ept id=\"p1\">][azure-purchase-options]</ept>, <bpt id=\"p2\">[</bpt>Member Offers<ept id=\"p2\">][azure-member-offers]</ept>, or <bpt id=\"p3\">[</bpt>Free Trial<ept id=\"p3\">][azure-free-trial]</ept>.",
      "pos": [
        1784,
        1922
      ]
    },
    {
      "pos": [
        1927,
        2003
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"develop\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Use Apache Maven to create a MapReduce program in Java"
    },
    {
      "content": "Create a word-counting MapReduce application.",
      "pos": [
        2005,
        2050
      ]
    },
    {
      "content": "It is a simple application that counts the occurrences of each word in a given input set.",
      "pos": [
        2051,
        2140
      ]
    },
    {
      "content": "In this section, we will performing the following tasks:",
      "pos": [
        2141,
        2197
      ]
    },
    {
      "content": "Create a project by using Apache Maven",
      "pos": [
        2202,
        2240
      ]
    },
    {
      "content": "Update the Project Object Model (POM)",
      "pos": [
        2244,
        2281
      ]
    },
    {
      "content": "Create the word-counting MapReduce application",
      "pos": [
        2285,
        2331
      ]
    },
    {
      "content": "Build and package the application",
      "pos": [
        2335,
        2368
      ]
    },
    {
      "content": "To create a project by using Maven",
      "pos": [
        2372,
        2406
      ]
    },
    {
      "pos": [
        2413,
        2464
      ],
      "content": "Create a directory *<bpt id=\"p1\">*</bpt>C:\\Tutorials\\WordCountJava\\*<ept id=\"p1\">*</ept>."
    },
    {
      "content": "From the command line in your development environment, change directories to the location you created.",
      "pos": [
        2468,
        2570
      ]
    },
    {
      "pos": [
        2574,
        2674
      ],
      "content": "Use the <bpt id=\"p1\">__</bpt>mvn<ept id=\"p1\">__</ept> command, which is installed with Maven, to generate the scaffolding for the project."
    },
    {
      "pos": [
        2849,
        3049
      ],
      "content": "This will create a new directory in the current directory, with the name specified by the <bpt id=\"p1\">__</bpt>artifactID<ept id=\"p1\">__</ept> parameter (<bpt id=\"p2\">**</bpt>wordcountjava<ept id=\"p2\">**</ept> in this example.) This directory will contain the following items:"
    },
    {
      "pos": [
        3057,
        3256
      ],
      "content": "<bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> - The <bpt id=\"p2\">[</bpt>Project Object Model (POM)<ept id=\"p2\">](http://maven.apache.org/guides/introduction/introduction-to-the-pom.html)</ept> that contains information and configuration details used to build the project."
    },
    {
      "pos": [
        3264,
        3396
      ],
      "content": "<bpt id=\"p1\">__</bpt>src<ept id=\"p1\">__</ept> - The directory that contains the <bpt id=\"p2\">__</bpt>main\\java\\org\\apache\\hadoop\\examples<ept id=\"p2\">__</ept> directory, where you will author the application."
    },
    {
      "pos": [
        3400,
        3514
      ],
      "content": "Delete the <bpt id=\"p1\">__</bpt>src\\test\\java\\org\\apache\\hadoop\\examples\\apptest.java<ept id=\"p1\">__</ept> file, as it will not be used in this example."
    },
    {
      "content": "To update the POM",
      "pos": [
        3518,
        3535
      ]
    },
    {
      "pos": [
        3542,
        3626
      ],
      "content": "Edit the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file and add the following inside the <ph id=\"ph1\">`&lt;dependencies&gt;`</ph> section:"
    },
    {
      "content": "This tells Maven that the project requires the libraries (listed within &lt;artifactId\\&gt;) with a specific version (listed within &lt;version\\&gt;).",
      "pos": [
        4184,
        4322
      ]
    },
    {
      "content": "At compile time, this will be downloaded from the default Maven repository.",
      "pos": [
        4323,
        4398
      ]
    },
    {
      "content": "You can use the <bpt id=\"p1\">[</bpt>Maven repository search<ept id=\"p1\">](http://search.maven.org/#artifactdetails%7Corg.apache.hadoop%7Chadoop-mapreduce-examples%7C2.5.1%7Cjar)</ept> to view more.",
      "pos": [
        4399,
        4558
      ]
    },
    {
      "content": "Add the following to the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file.",
      "pos": [
        4563,
        4605
      ]
    },
    {
      "content": "This must be inside the <ph id=\"ph1\">`&lt;project&gt;...&lt;/project&gt;`</ph> tags in the file; for example, between <ph id=\"ph2\">`&lt;/dependencies&gt;`</ph> and <ph id=\"ph3\">`&lt;/project&gt;`</ph>.",
      "pos": [
        4606,
        4729
      ]
    },
    {
      "content": "This configures the <bpt id=\"p1\">[</bpt>Maven Shade Plugin<ept id=\"p1\">](http://maven.apache.org/plugins/maven-shade-plugin/)</ept>, which is used to prevent license duplication in the JAR file that is built by Maven.",
      "pos": [
        5536,
        5715
      ]
    },
    {
      "content": "The reason this is used is that the duplicate license files cause an error at run time on the HDInsight cluster.",
      "pos": [
        5716,
        5828
      ]
    },
    {
      "content": "Using the Maven Shade Plugin with the <ph id=\"ph1\">`ApacheLicenseResourceTransformer`</ph> implementation prevents this error.",
      "pos": [
        5829,
        5937
      ]
    },
    {
      "content": "The Maven Shade Plugin will also produce an uberjar (sometimes called a fatjar), which contains all the dependencies required by the application.",
      "pos": [
        5943,
        6088
      ]
    },
    {
      "pos": [
        6093,
        6119
      ],
      "content": "Save the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file."
    },
    {
      "content": "To create the word-counting application",
      "pos": [
        6123,
        6162
      ]
    },
    {
      "pos": [
        6169,
        6303
      ],
      "content": "Go to the <bpt id=\"p1\">__</bpt>wordcountjava\\src\\main\\java\\org\\apache\\hadoop\\examples<ept id=\"p1\">__</ept> directory and rename the <bpt id=\"p2\">__</bpt>app.java<ept id=\"p2\">__</ept> file to <bpt id=\"p3\">__</bpt>WordCount.java<ept id=\"p3\">__</ept>."
    },
    {
      "content": "Open Notepad.",
      "pos": [
        6307,
        6320
      ]
    },
    {
      "content": "Copy and paste the following program into Notepad:",
      "pos": [
        6324,
        6374
      ]
    },
    {
      "content": "Notice the package name is <bpt id=\"p1\">**</bpt>org.apache.hadoop.examples<ept id=\"p1\">**</ept> and the class name is <bpt id=\"p2\">**</bpt>WordCount<ept id=\"p2\">**</ept>.",
      "pos": [
        9240,
        9334
      ]
    },
    {
      "content": "You will use these names when you submit the MapReduce job.",
      "pos": [
        9335,
        9394
      ]
    },
    {
      "content": "Save the file.",
      "pos": [
        9399,
        9413
      ]
    },
    {
      "content": "To build and package the application",
      "pos": [
        9417,
        9453
      ]
    },
    {
      "pos": [
        9460,
        9540
      ],
      "content": "Open a command prompt and change directories to the <bpt id=\"p1\">__</bpt>wordcountjava<ept id=\"p1\">__</ept> directory."
    },
    {
      "content": "Use the following command to build a JAR file containing the application:",
      "pos": [
        9545,
        9618
      ]
    },
    {
      "content": "This will clean any previous build artifacts, download any dependencies that have not already been installed, and then build and package the application.",
      "pos": [
        9651,
        9804
      ]
    },
    {
      "pos": [
        9809,
        9936
      ],
      "content": "Once the command finishes, the <bpt id=\"p1\">__</bpt>wordcountjava\\target<ept id=\"p1\">__</ept> directory will contain a file named <bpt id=\"p2\">__</bpt>wordcountjava-1.0-SNAPSHOT.jar<ept id=\"p2\">__</ept>."
    },
    {
      "pos": [
        9944,
        10015
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The <bpt id=\"p1\">__</bpt>wordcountjava-1.0-SNAPSHOT.jar<ept id=\"p1\">__</ept> file is an uberjar."
    },
    {
      "pos": [
        10020,
        10071
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"test\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Test the program on the emulator"
    },
    {
      "content": "Testing the MapReduce job on the HDInsight Emulator includes the following procedures:",
      "pos": [
        10073,
        10159
      ]
    },
    {
      "content": "Upload the data files to the Hadoop Distributed File System (HDFS) on the emulator",
      "pos": [
        10164,
        10246
      ]
    },
    {
      "content": "Create a local user group",
      "pos": [
        10250,
        10275
      ]
    },
    {
      "content": "Run a word-counting MapReduce job",
      "pos": [
        10279,
        10312
      ]
    },
    {
      "content": "Retrieve the job results",
      "pos": [
        10316,
        10340
      ]
    },
    {
      "content": "By default, the HDInsight Emulator uses HDFS as the file system.",
      "pos": [
        10342,
        10406
      ]
    },
    {
      "content": "Optionally, you can configure the HDInsight Emulator to use Azure Blob storage.",
      "pos": [
        10407,
        10486
      ]
    },
    {
      "content": "For details, see <bpt id=\"p1\">[</bpt>Get started with HDInsight Emulator<ept id=\"p1\">][hdinsight-emulator-wasb]</ept>.",
      "pos": [
        10487,
        10567
      ]
    },
    {
      "content": "In this tutorial, you will use the HDFS <bpt id=\"p1\">**</bpt>copyFromLocal<ept id=\"p1\">**</ept> command to upload the data files to HDFS.",
      "pos": [
        10569,
        10668
      ]
    },
    {
      "content": "The next section shows you how to upload files to Azure Blob storage by using Azure PowerShell.",
      "pos": [
        10669,
        10764
      ]
    },
    {
      "content": "For other methods for uploading files to Azure Blob storage, see <bpt id=\"p1\">[</bpt>Upload data to HDInsight<ept id=\"p1\">][hdinsight-upload-data]</ept>.",
      "pos": [
        10765,
        10880
      ]
    },
    {
      "content": "This tutorial uses the following HDFS folder structure:",
      "pos": [
        10882,
        10937
      ]
    },
    {
      "content": "Folder",
      "pos": [
        10939,
        10945
      ]
    },
    {
      "content": "Note",
      "pos": [
        10946,
        10950
      ]
    },
    {
      "content": "/WordCount",
      "pos": [
        10959,
        10969
      ]
    },
    {
      "content": "The root folder for the word-counting project.",
      "pos": [
        10970,
        11016
      ]
    },
    {
      "content": "/WordCount/Apps",
      "pos": [
        11018,
        11033
      ]
    },
    {
      "content": "The folder for the mapper and reducer executables.",
      "pos": [
        11034,
        11084
      ]
    },
    {
      "content": "/WordCount/Input",
      "pos": [
        11085,
        11101
      ]
    },
    {
      "content": "The MapReduce source file folder.",
      "pos": [
        11102,
        11135
      ]
    },
    {
      "content": "/WordCount/Output",
      "pos": [
        11136,
        11153
      ]
    },
    {
      "content": "The MapReduce output file folder.",
      "pos": [
        11154,
        11187
      ]
    },
    {
      "content": "/WordCount/MRStatusOutput",
      "pos": [
        11188,
        11213
      ]
    },
    {
      "content": "The job output folder.",
      "pos": [
        11214,
        11236
      ]
    },
    {
      "content": "This tutorial uses the .txt files located in the %hadoop_home% directory as the data files.",
      "pos": [
        11239,
        11330
      ]
    },
    {
      "pos": [
        11334,
        11391
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The Hadoop HDFS commands are case sensitive."
    },
    {
      "content": "To copy the data files to the emulator HDFS",
      "pos": [
        11395,
        11438
      ]
    },
    {
      "content": "Open the Hadoop command line from your desktop.",
      "pos": [
        11445,
        11492
      ]
    },
    {
      "content": "The Hadoop command line is installed by the emulator installer.",
      "pos": [
        11493,
        11556
      ]
    },
    {
      "content": "From the Hadoop command-line window, run the following command to make a directory for the input files:",
      "pos": [
        11560,
        11663
      ]
    },
    {
      "content": "The path used here is the relative path.",
      "pos": [
        11748,
        11788
      ]
    },
    {
      "content": "It is equivalent to the following:",
      "pos": [
        11789,
        11823
      ]
    },
    {
      "content": "Run the following command to copy some text files to the input folder on HDFS:",
      "pos": [
        11892,
        11970
      ]
    },
    {
      "content": "The MapReduce job will count the words in these files.",
      "pos": [
        12090,
        12144
      ]
    },
    {
      "content": "Run the following command to list and verify the uploaded files:",
      "pos": [
        12149,
        12213
      ]
    },
    {
      "content": "To create a local user group",
      "pos": [
        12257,
        12285
      ]
    },
    {
      "content": "To successfully run the MapReduce job on the cluster, you must create a user group called hdfs.",
      "pos": [
        12289,
        12384
      ]
    },
    {
      "content": "To this group, you must also add a user called hadoop and the local user with which you log on to the emulator.",
      "pos": [
        12385,
        12496
      ]
    },
    {
      "content": "Use the following commands from an elevated command prompt:",
      "pos": [
        12497,
        12556
      ]
    },
    {
      "content": "To run the MapReduce job by using the Hadoop command line",
      "pos": [
        12811,
        12868
      ]
    },
    {
      "content": "Open the Hadoop command line from your desktop.",
      "pos": [
        12875,
        12922
      ]
    },
    {
      "content": "Run the following command to delete the /WordCount/Output folder structure from HDFS.",
      "pos": [
        12926,
        13011
      ]
    },
    {
      "content": "/WordCount/Output is the output folder of the word-counting MapReduce job.",
      "pos": [
        13012,
        13086
      ]
    },
    {
      "content": "The MapReduce job will fail if the folder already exists.",
      "pos": [
        13087,
        13144
      ]
    },
    {
      "content": "This step is necessary if this is the second time you're running the job.",
      "pos": [
        13145,
        13218
      ]
    },
    {
      "content": "Run the following command:",
      "pos": [
        13268,
        13294
      ]
    },
    {
      "content": "If the job finishes successfully, you should get an output similar to the following screenshot:",
      "pos": [
        13471,
        13566
      ]
    },
    {
      "content": "HDI.EMulator.WordCount.Run",
      "pos": [
        13574,
        13600
      ]
    },
    {
      "content": "From the screenshot, you can see both map and reduce finished 100%.",
      "pos": [
        13637,
        13704
      ]
    },
    {
      "content": "It also lists the job ID.",
      "pos": [
        13705,
        13730
      ]
    },
    {
      "content": "The same report can be retrieved by opening the <bpt id=\"p1\">**</bpt>Hadoop MapReduce status<ept id=\"p1\">**</ept> shortcut from your desktop, and looking for the same job ID.",
      "pos": [
        13731,
        13867
      ]
    },
    {
      "content": "The other option for running a MapReduce job is using Azure PowerShell.",
      "pos": [
        13869,
        13940
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Get started with the HDInsight Emulator<ept id=\"p1\">][hdinsight-emulator]</ept>.",
      "pos": [
        13941,
        14025
      ]
    },
    {
      "content": "To display the output from HDFS",
      "pos": [
        14029,
        14060
      ]
    },
    {
      "content": "Open the Hadoop command line.",
      "pos": [
        14067,
        14096
      ]
    },
    {
      "content": "Run the following commands to display the output:",
      "pos": [
        14100,
        14149
      ]
    },
    {
      "content": "You can append \"|more\" at the end of the command to get the page view.",
      "pos": [
        14251,
        14321
      ]
    },
    {
      "content": "Or use the <bpt id=\"p1\">**</bpt>findstr<ept id=\"p1\">**</ept> command to find a string pattern:",
      "pos": [
        14322,
        14378
      ]
    },
    {
      "content": "You have now developed a word-counting MapReduce job and tested it successfully on the emulator.",
      "pos": [
        14453,
        14549
      ]
    },
    {
      "content": "The next step is to deploy and run it on Azure HDInsight.",
      "pos": [
        14550,
        14607
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"upload\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Upload data and application to Azure Blob storage",
      "pos": [
        14613,
        14681
      ]
    },
    {
      "content": "Azure HDInsight uses Azure Blob storage for data storage.",
      "pos": [
        14682,
        14739
      ]
    },
    {
      "content": "When an HDInsight cluster is provisioned, an Azure Blob storage container is used to store the system files.",
      "pos": [
        14740,
        14848
      ]
    },
    {
      "content": "You can use either this default container or a different container (either on the same Azure Storage account or on a different Storage account located in the same datacenter as the cluster) for storing the data files.",
      "pos": [
        14849,
        15066
      ]
    },
    {
      "content": "In this tutorial, you will create a container on a separate Storage account for the data files and the MapReduce application.",
      "pos": [
        15068,
        15193
      ]
    },
    {
      "content": "The data files are the text files in the <bpt id=\"p1\">**</bpt>C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common<ept id=\"p1\">**</ept> directory on your emulator workstation.",
      "pos": [
        15194,
        15335
      ]
    },
    {
      "content": "To create a Blob storage account and a container",
      "pos": [
        15339,
        15387
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        15394,
        15416
      ]
    },
    {
      "content": "Set the variables, and then run the commands:",
      "pos": [
        15420,
        15465
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$subscripionName<ept id=\"p1\">**</ept> variable is associated with your Azure subscription.",
      "pos": [
        15709,
        15786
      ]
    },
    {
      "content": "You must name <bpt id=\"p1\">**</bpt>$storageAccountName\\_Data<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>$containerName\\_Data<ept id=\"p2\">**</ept>.",
      "pos": [
        15787,
        15860
      ]
    },
    {
      "content": "For the naming restrictions, see <bpt id=\"p1\">[</bpt>Naming and Referencing Containers, Blobs, and Metadata<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsazure/dd135715.aspx)</ept>.",
      "pos": [
        15861,
        16013
      ]
    },
    {
      "content": "Run the following commands to create a Storage account and a Blob storage container on the account:",
      "pos": [
        16018,
        16117
      ]
    },
    {
      "content": "Run the following commands to verify the Storage account and the container:",
      "pos": [
        16695,
        16770
      ]
    },
    {
      "content": "To upload the data files",
      "pos": [
        16908,
        16932
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        16939,
        16961
      ]
    },
    {
      "content": "Set the first three variables, and then run the commands:",
      "pos": [
        16965,
        17022
      ]
    },
    {
      "pos": [
        17320,
        17443
      ],
      "content": "The <bpt id=\"p1\">**</bpt>$storageAccountName\\_Data<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>$containerName\\_Data<ept id=\"p2\">**</ept> variables are the same as you defined in the last procedure."
    },
    {
      "pos": [
        17449,
        17569
      ],
      "content": "Notice the source file folder is <bpt id=\"p1\">**</bpt>c:\\Hadoop\\hadoop-1.1.0-SNAPSHOT<ept id=\"p1\">**</ept>, and the destination folder is <bpt id=\"p2\">**</bpt>WordCount/Input<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Run the following commands to get a list of the .txt files in the source file folder:",
      "pos": [
        17574,
        17659
      ]
    },
    {
      "content": "Run the following commands to create a storage context object:",
      "pos": [
        17815,
        17877
      ]
    },
    {
      "content": "Run the following commands to copy the files:",
      "pos": [
        18196,
        18241
      ]
    },
    {
      "content": "Run the following commands to list the uploaded files:",
      "pos": [
        18642,
        18696
      ]
    },
    {
      "content": "You should see about the uploaded text data files.",
      "pos": [
        18938,
        18988
      ]
    },
    {
      "content": "To upload the word-counting application",
      "pos": [
        18992,
        19031
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        19038,
        19060
      ]
    },
    {
      "content": "Set the first three variables, and then run the commands:",
      "pos": [
        19064,
        19121
      ]
    },
    {
      "pos": [
        19435,
        19676
      ],
      "content": "The <bpt id=\"p1\">**</bpt>$storageAccountName\\_Data<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>$containerName\\_Data<ept id=\"p2\">**</ept> variables are the same as you defined in the last procedure, which means you will upload both the data file and the application to the same container on the same Storage account."
    },
    {
      "pos": [
        19682,
        19734
      ],
      "content": "Notice the destination folder is <bpt id=\"p1\">**</bpt>WordCount/jars<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following commands to create a storage context object:",
      "pos": [
        19739,
        19801
      ]
    },
    {
      "content": "Run the following command to copy the applications:",
      "pos": [
        20120,
        20171
      ]
    },
    {
      "content": "Run the following commands to list the uploaded files:",
      "pos": [
        20315,
        20369
      ]
    },
    {
      "content": "You should see the JAR file listed there.",
      "pos": [
        20617,
        20658
      ]
    },
    {
      "pos": [
        20662,
        20720
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run the MapReduce job on Azure HDInsight"
    },
    {
      "content": "In this section, you will create an Azure PowerShell script that performs the following tasks:",
      "pos": [
        20722,
        20816
      ]
    },
    {
      "content": "Provisions an HDInsight cluster",
      "pos": [
        20821,
        20852
      ]
    },
    {
      "content": "Create a Storage account that will be used as the default HDInsight cluster file system",
      "pos": [
        20861,
        20948
      ]
    },
    {
      "content": "Create a Blob storage container",
      "pos": [
        20956,
        20987
      ]
    },
    {
      "content": "Create an HDInsight cluster",
      "pos": [
        20995,
        21022
      ]
    },
    {
      "content": "Submits the MapReduce job",
      "pos": [
        21027,
        21052
      ]
    },
    {
      "content": "Create a MapReduce job definition",
      "pos": [
        21061,
        21094
      ]
    },
    {
      "content": "Submit a MapReduce job",
      "pos": [
        21102,
        21124
      ]
    },
    {
      "content": "Wait for the job to finish",
      "pos": [
        21132,
        21158
      ]
    },
    {
      "content": "Display standard error",
      "pos": [
        21166,
        21188
      ]
    },
    {
      "content": "Display standard output",
      "pos": [
        21196,
        21219
      ]
    },
    {
      "content": "Deletes the cluster",
      "pos": [
        21224,
        21243
      ]
    },
    {
      "content": "Delete the HDInsight cluster",
      "pos": [
        21252,
        21280
      ]
    },
    {
      "content": "Delete the Storage account used as the default HDInsight cluster file system",
      "pos": [
        21288,
        21364
      ]
    },
    {
      "content": "To run the Azure PowerShell script",
      "pos": [
        21369,
        21403
      ]
    },
    {
      "content": "Open Notepad.",
      "pos": [
        21410,
        21423
      ]
    },
    {
      "content": "Copy and paste the following code:",
      "pos": [
        21427,
        21461
      ]
    },
    {
      "content": "Set the first six variables in the script.",
      "pos": [
        25866,
        25908
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$stringPrefix<ept id=\"p1\">**</ept> variable is used to prefix the specified string to the HDInsight cluster name, the Storage account name, and the Blob storage container name.",
      "pos": [
        25909,
        26072
      ]
    },
    {
      "content": "Because the names for these must be 3 to 24 characters, make sure the string you specify and the names this script uses, together, do not exceed the character limit for the name.",
      "pos": [
        26073,
        26251
      ]
    },
    {
      "content": "You must use all lowercase for <bpt id=\"p1\">**</bpt>$stringPrefix<ept id=\"p1\">**</ept>.",
      "pos": [
        26252,
        26301
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$storageAccountName\\_Data<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>$containerName\\_Data<ept id=\"p2\">**</ept> variables are the Storage account and container that are used for storing the data files and the application.",
      "pos": [
        26307,
        26479
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$location<ept id=\"p1\">**</ept> variable must match the data Storage account location.",
      "pos": [
        26480,
        26552
      ]
    },
    {
      "content": "Review the rest of the variables.",
      "pos": [
        26557,
        26590
      ]
    },
    {
      "content": "Save the script file.",
      "pos": [
        26594,
        26615
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        26619,
        26641
      ]
    },
    {
      "content": "Run the following command to set the execution policy to RemoteSigned:",
      "pos": [
        26645,
        26715
      ]
    },
    {
      "content": "When prompted, enter the user name and password for the HDInsight cluster.",
      "pos": [
        26787,
        26861
      ]
    },
    {
      "content": "Because you will delete the cluster at the end of the script and you will not need the user name and password anymore, the user name and password can be any strings.",
      "pos": [
        26862,
        27027
      ]
    },
    {
      "content": "If you don't want to get prompted for the credentials, see <bpt id=\"p1\">[</bpt>Working with Passwords, Secure Strings and Credentials in Windows PowerShell<ept id=\"p1\">][powershell-PSCredential]</ept>.",
      "pos": [
        27028,
        27191
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"retrieve\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Retrieve the MapReduce job output",
      "pos": [
        27196,
        27252
      ]
    },
    {
      "content": "This section shows you how to download and display the output.",
      "pos": [
        27253,
        27315
      ]
    },
    {
      "content": "For the information on displaying the results in Excel, see <bpt id=\"p1\">[</bpt>Connect Excel to HDInsight with the Microsoft Hive ODBC Driver<ept id=\"p1\">][hdinsight-ODBC]</ept> and <bpt id=\"p2\">[</bpt>Connect Excel to HDInsight with Power Query<ept id=\"p2\">][hdinsight-power-query]</ept>.",
      "pos": [
        27316,
        27530
      ]
    },
    {
      "content": "To retrieve the output",
      "pos": [
        27535,
        27557
      ]
    },
    {
      "content": "Open the Azure PowerShell window.",
      "pos": [
        27564,
        27597
      ]
    },
    {
      "content": "Change the directory to <bpt id=\"p1\">**</bpt>C:\\Tutorials\\WordCountJava<ept id=\"p1\">**</ept>.",
      "pos": [
        27601,
        27656
      ]
    },
    {
      "content": "The default Azure PowerShell folder is <bpt id=\"p1\">**</bpt>C:\\Windows\\System32\\WindowsPowerShell\\v1.0<ept id=\"p1\">**</ept>.",
      "pos": [
        27657,
        27743
      ]
    },
    {
      "content": "The cmdlets you will run will download the output file to the current folder.",
      "pos": [
        27744,
        27821
      ]
    },
    {
      "content": "You don't have permissions to download the files to the system folders.",
      "pos": [
        27822,
        27893
      ]
    },
    {
      "content": "Run the following commands to set the values:",
      "pos": [
        27897,
        27942
      ]
    },
    {
      "content": "Run the following commands to create an Azure storage context object:",
      "pos": [
        28185,
        28254
      ]
    },
    {
      "content": "Run the following commands to download and display the output:",
      "pos": [
        28538,
        28600
      ]
    },
    {
      "pos": [
        28762,
        28935
      ],
      "content": "After the job is completed, you have the option to export the data to SQL Server or Azure SQL Database by using <bpt id=\"p1\">[</bpt>Sqoop<ept id=\"p1\">][hdinsight-use-sqoop]</ept>, or to export the data to Excel."
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps",
      "pos": [
        28941,
        28973
      ]
    },
    {
      "content": "In this tutorial, you have learned how to develop a Java MapReduce job, how to test the application on the HDInsight Emulator, and how to write an Azure PowerShell script to provision an HDInsight cluster and run a MapReduce job on the cluster.",
      "pos": [
        28974,
        29218
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        29219,
        29261
      ]
    },
    {
      "content": "Develop C# Hadoop streaming MapReduce programs for HDInsight",
      "pos": [
        29266,
        29326
      ]
    },
    {
      "content": "Get started with Azure HDInsight",
      "pos": [
        29360,
        29392
      ]
    },
    {
      "content": "Get started with the HDInsight Emulator",
      "pos": [
        29420,
        29459
      ]
    },
    {
      "content": "Use Azure Blob storage with HDInsight",
      "pos": [
        29484,
        29521
      ]
    },
    {
      "content": "Administer HDInsight using Azure PowerShell",
      "pos": [
        29545,
        29588
      ]
    },
    {
      "content": "Upload data to HDInsight",
      "pos": [
        29621,
        29645
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        29673,
        29696
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        29721,
        29743
      ]
    },
    {
      "content": "Connect Excel to HDInsight with Power Query",
      "pos": [
        29767,
        29810
      ]
    },
    {
      "content": "Connect Excel to HDInsight with the Microsoft Hive ODBC Driver",
      "pos": [
        29838,
        29900
      ]
    },
    {
      "content": "test",
      "pos": [
        31389,
        31393
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Develop Java MapReduce programs for Hadoop | Microsoft Azure\"\n    description=\"Learn how to develop Java MapReduce programs on HDInsight emulator, how to deploy them to HDInsight.\"\n    services=\"hdinsight\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    authors=\"nitinme\"\n    documentationCenter=\"\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"Java\"\n    ms.topic=\"article\"\n    ms.date=\"07/11/2015\"\n    ms.author=\"nitinme\"/>\n\n# Develop Java MapReduce programs for Hadoop in HDInsight\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-maven-mapreduce-selector.md)]\n\nThis tutorial walks you through an end-to-end scenario for developing a word-counting Hadoop MapReduce job in Java by using Apache Maven. The tutorial also shows how to test the application on the HDInsight Emulator for Azure and then deploy and run it on an Windows-based HDInsight cluster.\n\n##<a name=\"prerequisites\"></a>Prerequisites\n\nBefore you begin this tutorial, you must have completed the following:\n\n- Install the HDInsight Emulator. For instructions, see [Get started using HDInsight Emulator][hdinsight-emulator]. Make sure all the required services are running. On the computer that has HDInsight Emulator installed, launch the Hadoop Command line from the Desktop shortcut, navigate to **C:\\hdp**, and run the command **start_local_hdp_services.cmd**.\n- Install Azure PowerShell on the emulator computer. For instructions, see [Install and configure Azure PowerShell][powershell-install-configure].\n- Install Java platform JDK 7 or higher on the emulator computer. This is already available on the emulator computer.\n- Install and configure [Apache Maven](http://maven.apache.org/).\n- Obtain an Azure subscription. For instructions, see [Purchase Options][azure-purchase-options], [Member Offers][azure-member-offers], or [Free Trial][azure-free-trial].\n\n\n##<a name=\"develop\"></a>Use Apache Maven to create a MapReduce program in Java\n\nCreate a word-counting MapReduce application. It is a simple application that counts the occurrences of each word in a given input set. In this section, we will performing the following tasks:\n\n1. Create a project by using Apache Maven\n2. Update the Project Object Model (POM)\n3. Create the word-counting MapReduce application\n4. Build and package the application\n\n**To create a project by using Maven**\n\n1. Create a directory **C:\\Tutorials\\WordCountJava\\**.\n2. From the command line in your development environment, change directories to the location you created.\n3. Use the __mvn__ command, which is installed with Maven, to generate the scaffolding for the project.\n\n        mvn archetype:generate -DgroupId=org.apache.hadoop.examples -DartifactId=wordcountjava -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n    This will create a new directory in the current directory, with the name specified by the __artifactID__ parameter (**wordcountjava** in this example.) This directory will contain the following items:\n\n    * __pom.xml__ - The [Project Object Model (POM)](http://maven.apache.org/guides/introduction/introduction-to-the-pom.html) that contains information and configuration details used to build the project.\n\n    * __src__ - The directory that contains the __main\\java\\org\\apache\\hadoop\\examples__ directory, where you will author the application.\n3. Delete the __src\\test\\java\\org\\apache\\hadoop\\examples\\apptest.java__ file, as it will not be used in this example.\n\n**To update the POM**\n\n1. Edit the __pom.xml__ file and add the following inside the `<dependencies>` section:\n\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-mapreduce-examples</artifactId>\n          <version>2.5.1</version>\n        </dependency>\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-mapreduce-client-common</artifactId>\n          <version>2.5.1</version>\n        </dependency>\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-common</artifactId>\n          <version>2.5.1</version>\n        </dependency>\n\n    This tells Maven that the project requires the libraries (listed within <artifactId\\>) with a specific version (listed within <version\\>). At compile time, this will be downloaded from the default Maven repository. You can use the [Maven repository search](http://search.maven.org/#artifactdetails%7Corg.apache.hadoop%7Chadoop-mapreduce-examples%7C2.5.1%7Cjar) to view more.\n\n2. Add the following to the __pom.xml__ file. This must be inside the `<project>...</project>` tags in the file; for example, between `</dependencies>` and `</project>`.\n\n        <build>\n          <plugins>\n            <plugin>\n              <groupId>org.apache.maven.plugins</groupId>\n              <artifactId>maven-shade-plugin</artifactId>\n              <version>2.3</version>\n              <configuration>\n                <transformers>\n                  <transformer implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\">\n                  </transformer>\n                </transformers>\n              </configuration>\n              <executions>\n                <execution>\n                  <phase>package</phase>\n                    <goals>\n                      <goal>shade</goal>\n                    </goals>\n                </execution>\n              </executions>\n            </plugin>\n          </plugins>\n        </build>\n\n    This configures the [Maven Shade Plugin](http://maven.apache.org/plugins/maven-shade-plugin/), which is used to prevent license duplication in the JAR file that is built by Maven. The reason this is used is that the duplicate license files cause an error at run time on the HDInsight cluster. Using the Maven Shade Plugin with the `ApacheLicenseResourceTransformer` implementation prevents this error.\n\n    The Maven Shade Plugin will also produce an uberjar (sometimes called a fatjar), which contains all the dependencies required by the application.\n\n3. Save the __pom.xml__ file.\n\n**To create the word-counting application**\n\n1. Go to the __wordcountjava\\src\\main\\java\\org\\apache\\hadoop\\examples__ directory and rename the __app.java__ file to __WordCount.java__.\n2. Open Notepad.\n2. Copy and paste the following program into Notepad:\n\n        package org.apache.hadoop.examples;\n\n        import java.io.IOException;\n        import java.util.StringTokenizer;\n        import org.apache.hadoop.conf.Configuration;\n        import org.apache.hadoop.fs.Path;\n        import org.apache.hadoop.io.IntWritable;\n        import org.apache.hadoop.io.Text;\n        import org.apache.hadoop.mapreduce.Job;\n        import org.apache.hadoop.mapreduce.Mapper;\n        import org.apache.hadoop.mapreduce.Reducer;\n        import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n        import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n        import org.apache.hadoop.util.GenericOptionsParser;\n\n        public class WordCount {\n\n          public static class TokenizerMapper\n               extends Mapper<Object, Text, Text, IntWritable>{\n\n            private final static IntWritable one = new IntWritable(1);\n            private Text word = new Text();\n\n            public void map(Object key, Text value, Context context\n                            ) throws IOException, InterruptedException {\n              StringTokenizer itr = new StringTokenizer(value.toString());\n              while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n              }\n            }\n          }\n\n          public static class IntSumReducer\n               extends Reducer<Text,IntWritable,Text,IntWritable> {\n            private IntWritable result = new IntWritable();\n\n            public void reduce(Text key, Iterable<IntWritable> values,\n                               Context context\n                               ) throws IOException, InterruptedException {\n              int sum = 0;\n              for (IntWritable val : values) {\n                sum += val.get();\n              }\n              result.set(sum);\n              context.write(key, result);\n            }\n          }\n\n          public static void main(String[] args) throws Exception {\n            Configuration conf = new Configuration();\n            String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n            if (otherArgs.length != 2) {\n              System.err.println(\"Usage: wordcount <in> <out>\");\n              System.exit(2);\n            }\n            Job job = new Job(conf, \"word count\");\n            job.setJarByClass(WordCount.class);\n            job.setMapperClass(TokenizerMapper.class);\n            job.setCombinerClass(IntSumReducer.class);\n            job.setReducerClass(IntSumReducer.class);\n            job.setOutputKeyClass(Text.class);\n            job.setOutputValueClass(IntWritable.class);\n            FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n            FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n            System.exit(job.waitForCompletion(true) ? 0 : 1);\n          }\n        }\n\n    Notice the package name is **org.apache.hadoop.examples** and the class name is **WordCount**. You will use these names when you submit the MapReduce job.\n\n3. Save the file.\n\n**To build and package the application**\n\n1. Open a command prompt and change directories to the __wordcountjava__ directory.\n\n2. Use the following command to build a JAR file containing the application:\n\n        mvn clean package\n\n    This will clean any previous build artifacts, download any dependencies that have not already been installed, and then build and package the application.\n\n3. Once the command finishes, the __wordcountjava\\target__ directory will contain a file named __wordcountjava-1.0-SNAPSHOT.jar__.\n\n    > [AZURE.NOTE] The __wordcountjava-1.0-SNAPSHOT.jar__ file is an uberjar.\n\n\n##<a name=\"test\"></a>Test the program on the emulator\n\nTesting the MapReduce job on the HDInsight Emulator includes the following procedures:\n\n1. Upload the data files to the Hadoop Distributed File System (HDFS) on the emulator\n2. Create a local user group\n3. Run a word-counting MapReduce job\n4. Retrieve the job results\n\nBy default, the HDInsight Emulator uses HDFS as the file system. Optionally, you can configure the HDInsight Emulator to use Azure Blob storage. For details, see [Get started with HDInsight Emulator][hdinsight-emulator-wasb].\n\nIn this tutorial, you will use the HDFS **copyFromLocal** command to upload the data files to HDFS. The next section shows you how to upload files to Azure Blob storage by using Azure PowerShell. For other methods for uploading files to Azure Blob storage, see [Upload data to HDInsight][hdinsight-upload-data].\n\nThis tutorial uses the following HDFS folder structure:\n\nFolder|Note\n---|---\n/WordCount|The root folder for the word-counting project. \n/WordCount/Apps|The folder for the mapper and reducer executables.\n/WordCount/Input|The MapReduce source file folder.\n/WordCount/Output|The MapReduce output file folder.\n/WordCount/MRStatusOutput|The job output folder.\n\n\nThis tutorial uses the .txt files located in the %hadoop_home% directory as the data files.\n\n> [AZURE.NOTE] The Hadoop HDFS commands are case sensitive.\n\n**To copy the data files to the emulator HDFS**\n\n1. Open the Hadoop command line from your desktop. The Hadoop command line is installed by the emulator installer.\n2. From the Hadoop command-line window, run the following command to make a directory for the input files:\n\n        hadoop fs -mkdir /WordCount\n        hadoop fs -mkdir /WordCount/Input\n\n    The path used here is the relative path. It is equivalent to the following:\n\n        hadoop fs -mkdir hdfs://localhost:8020/WordCount/Input\n\n3. Run the following command to copy some text files to the input folder on HDFS:\n\n        hadoop fs -copyFromLocal C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common\\*.txt /WordCount/Input\n\n    The MapReduce job will count the words in these files.\n\n4. Run the following command to list and verify the uploaded files:\n\n        hadoop fs -ls /WordCount/Input\n\n**To create a local user group**\n\nTo successfully run the MapReduce job on the cluster, you must create a user group called hdfs. To this group, you must also add a user called hadoop and the local user with which you log on to the emulator. Use the following commands from an elevated command prompt:\n\n        # Add a user group called hdfs\n        net localgroup hdfs /add\n\n        # Adds a user called hadoop to the group\n        net localgroup hdfs hadoop /add\n\n        # Adds the local user to the group\n        net localgroup hdfs <username> /add\n\n**To run the MapReduce job by using the Hadoop command line**\n\n1. Open the Hadoop command line from your desktop.\n2. Run the following command to delete the /WordCount/Output folder structure from HDFS. /WordCount/Output is the output folder of the word-counting MapReduce job. The MapReduce job will fail if the folder already exists. This step is necessary if this is the second time you're running the job.\n\n        hadoop fs -rm - r /WordCount/Output\n\n2. Run the following command:\n\n        hadoop jar C:\\Tutorials\\WordCountJava\\wordcountjava\\target\\wordcountjava-1.0-SNAPSHOT.jar org.apache.hadoop.examples.WordCount /WordCount/Input /WordCount/Output\n\n    If the job finishes successfully, you should get an output similar to the following screenshot:\n\n    ![HDI.EMulator.WordCount.Run][image-emulator-wordcount-run]\n\n    From the screenshot, you can see both map and reduce finished 100%. It also lists the job ID. The same report can be retrieved by opening the **Hadoop MapReduce status** shortcut from your desktop, and looking for the same job ID.\n\nThe other option for running a MapReduce job is using Azure PowerShell. For instructions, see [Get started with the HDInsight Emulator][hdinsight-emulator].\n\n**To display the output from HDFS**\n\n1. Open the Hadoop command line.\n2. Run the following commands to display the output:\n\n        hadoop fs -ls /WordCount/Output/\n        hadoop fs -cat /WordCount/Output/part-r-00000\n\n    You can append \"|more\" at the end of the command to get the page view. Or use the **findstr** command to find a string pattern:\n\n        hadoop fs -cat /WordCount/Output/part-r-00000 | findstr \"there\"\n\nYou have now developed a word-counting MapReduce job and tested it successfully on the emulator. The next step is to deploy and run it on Azure HDInsight.\n\n\n\n##<a id=\"upload\"></a>Upload data and application to Azure Blob storage\nAzure HDInsight uses Azure Blob storage for data storage. When an HDInsight cluster is provisioned, an Azure Blob storage container is used to store the system files. You can use either this default container or a different container (either on the same Azure Storage account or on a different Storage account located in the same datacenter as the cluster) for storing the data files.\n\nIn this tutorial, you will create a container on a separate Storage account for the data files and the MapReduce application. The data files are the text files in the **C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common** directory on your emulator workstation.\n\n**To create a Blob storage account and a container**\n\n1. Open Azure PowerShell.\n2. Set the variables, and then run the commands:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName_Data = \"<AzureStorageAccountName>\"  \n        $containerName_Data = \"<ContainerName>\"\n        $location = \"<MicrosoftDataCenter>\"  # For example, \"East US\"\n\n    The **$subscripionName** variable is associated with your Azure subscription. You must name **$storageAccountName\\_Data** and **$containerName\\_Data**. For the naming restrictions, see [Naming and Referencing Containers, Blobs, and Metadata](http://msdn.microsoft.com/library/windowsazure/dd135715.aspx).\n\n3. Run the following commands to create a Storage account and a Blob storage container on the account:\n\n        # Select an Azure subscription\n        Select-AzureSubscription $subscriptionName\n\n        # Create a Storage account\n        New-AzureStorageAccount -StorageAccountName $storageAccountName_Data -location $location\n\n        # Create a Blob storage container\n        $storageAccountKey = Get-AzureStorageKey $storageAccountName_Data | %{ $_.Primary }\n        $destContext = New-AzureStorageContext –StorageAccountName $storageAccountName_Data –StorageAccountKey $storageAccountKey  \n        New-AzureStorageContainer -Name $containerName_Data -Context $destContext\n\n4. Run the following commands to verify the Storage account and the container:\n\n        Get-AzureStorageAccount -StorageAccountName $storageAccountName_Data\n        Get-AzureStorageContainer -Context $destContext\n\n**To upload the data files**\n\n1. Open Azure PowerShell.\n2. Set the first three variables, and then run the commands:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName_Data = \"<AzureStorageAccountName>\"  \n        $containerName_Data = \"<ContainerName>\"\n\n        $localFolder = \"C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common\\\"\n        $destFolder = \"WordCount/Input\"\n\n    The **$storageAccountName\\_Data** and **$containerName\\_Data** variables are the same as you defined in the last procedure.\n\n    Notice the source file folder is **c:\\Hadoop\\hadoop-1.1.0-SNAPSHOT**, and the destination folder is **WordCount/Input**.\n\n3. Run the following commands to get a list of the .txt files in the source file folder:\n\n        # Get a list of the .txt files\n        $filesAll = Get-ChildItem $localFolder\n        $filesTxt = $filesAll | where {$_.Extension -eq \".txt\"}\n\n4. Run the following commands to create a storage context object:\n\n        # Create a storage context object\n        Select-AzureSubscription $subscriptionName\n        $storageaccountkey = get-azurestoragekey $storageAccountName_Data | %{$_.Primary}\n        $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName_Data -StorageAccountKey $storageaccountkey\n\n5. Run the following commands to copy the files:\n\n        # Copy the files from the local workstation to the Blob container\n        foreach ($file in $filesTxt){\n\n            $fileName = \"$localFolder\\$file\"\n            $blobName = \"$destFolder/$file\"\n\n            write-host \"Copying $fileName to $blobName\"\n\n            Set-AzureStorageBlobContent -File $fileName -Container $containerName_Data -Blob $blobName -Context $destContext\n        }\n\n6. Run the following commands to list the uploaded files:\n\n        # List the uploaded files in the Blob storage container\n        Write-Host \"The Uploaded data files:\" -BackgroundColor Green\n        Get-AzureStorageBlob -Container $containerName_Data -Context $destContext -Prefix $destFolder\n\n    You should see about the uploaded text data files.\n\n**To upload the word-counting application**\n\n1. Open Azure PowerShell.\n2. Set the first three variables, and then run the commands:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName_Data = \"<AzureStorageAccountName>\"  \n        $containerName_Data = \"<ContainerName>\"\n\n        $jarFile = \"C:\\Tutorials\\WordCountJava\\wordcountjava\\target\\wordcountjava-1.0-SNAPSHOT.jar\"\n        $blobFolder = \"WordCount/jars\"\n\n    The **$storageAccountName\\_Data** and **$containerName\\_Data** variables are the same as you defined in the last procedure, which means you will upload both the data file and the application to the same container on the same Storage account.\n\n    Notice the destination folder is **WordCount/jars**.\n\n4. Run the following commands to create a storage context object:\n\n        # Create a storage context object\n        Select-AzureSubscription $subscriptionName\n        $storageaccountkey = get-azurestoragekey $storageAccountName_Data | %{$_.Primary}\n        $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName_Data -StorageAccountKey $storageaccountkey\n\n5. Run the following command to copy the applications:\n\n        Set-AzureStorageBlobContent -File $jarFile -Container $containerName_Data -Blob \"$blobFolder/WordCount.jar\" -Context $destContext\n\n6. Run the following commands to list the uploaded files:\n\n        # List the uploaded files in the Blob storage container\n        Write-Host \"The Uploaded application file:\" -BackgroundColor Green\n        Get-AzureStorageBlob -Container $containerName_Data -Context $destContext -Prefix $blobFolder\n\n    You should see the JAR file listed there.\n\n##<a name=\"run\"></a>Run the MapReduce job on Azure HDInsight\n\nIn this section, you will create an Azure PowerShell script that performs the following tasks:\n\n1. Provisions an HDInsight cluster\n\n    1. Create a Storage account that will be used as the default HDInsight cluster file system\n    2. Create a Blob storage container\n    3. Create an HDInsight cluster\n\n2. Submits the MapReduce job\n\n    1. Create a MapReduce job definition\n    2. Submit a MapReduce job\n    3. Wait for the job to finish\n    4. Display standard error\n    5. Display standard output\n\n3. Deletes the cluster\n\n    1. Delete the HDInsight cluster\n    2. Delete the Storage account used as the default HDInsight cluster file system\n\n\n**To run the Azure PowerShell script**\n\n1. Open Notepad.\n2. Copy and paste the following code:\n\n        # The Storage account and the HDInsight cluster variables\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $stringPrefix = \"<StringForPrefix>\"\n        $location = \"<MicrosoftDataCenter>\"     ### Must match the data Storage account location\n        $clusterNodes = <NumberOFNodesInTheCluster>\n\n        $storageAccountName_Data = \"<TheDataStorageAccountName>\"\n        $containerName_Data = \"<TheDataBlobStorageContainerName>\"\n\n        $clusterName = $stringPrefix + \"hdicluster\"\n\n        $storageAccountName_Default = $stringPrefix + \"hdistore\"\n        $containerName_Default =  $stringPrefix + \"hdicluster\"\n\n        # The MapReduce job variables\n        $jarFile = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/jars/WordCount.jar\"\n        $className = \"org.apache.hadoop.examples.WordCount\"\n        $mrInput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Input/\"\n        $mrOutput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Output/\"\n        $mrStatusOutput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/MRStatusOutput/\"\n\n        # Create a PSCredential object. The user name and password are hardcoded here. You can change them if you want.\n        $password = ConvertTo-SecureString \"Pass@word1\" -AsPlainText -Force\n        $creds = New-Object System.Management.Automation.PSCredential (\"Admin\", $password)\n\n        Select-AzureSubscription $subscriptionName\n\n        #=============================\n        # Create a Storage account used as the default file system\n        Write-Host \"Create a storage account\" -ForegroundColor Green\n        New-AzureStorageAccount -StorageAccountName $storageAccountName_Default -location $location\n\n        #=============================\n        # Create a Blob storage container used as the default file system\n        Write-Host \"Create a Blob storage container\" -ForegroundColor Green\n        $storageAccountKey_Default = Get-AzureStorageKey $storageAccountName_Default | %{ $_.Primary }\n        $destContext = New-AzureStorageContext –StorageAccountName $storageAccountName_Default –StorageAccountKey $storageAccountKey_Default\n\n        New-AzureStorageContainer -Name $containerName_Default -Context $destContext\n\n        #=============================\n        # Create an HDInsight cluster\n        Write-Host \"Create an HDInsight cluster\" -ForegroundColor Green\n        $storageAccountKey_Data = Get-AzureStorageKey $storageAccountName_Data | %{ $_.Primary }\n\n        $config = New-AzureHDInsightClusterConfig -ClusterSizeInNodes $clusterNodes |\n            Set-AzureHDInsightDefaultStorage -StorageAccountName \"$storageAccountName_Default.blob.core.windows.net\" -StorageAccountKey $storageAccountKey_Default -StorageContainerName $containerName_Default |\n            Add-AzureHDInsightStorage -StorageAccountName \"$storageAccountName_Data.blob.core.windows.net\" -StorageAccountKey $storageAccountKey_Data\n\n        New-AzureHDInsightCluster -Name $clusterName -Location $location -Credential $creds -Config $config\n\n        #=============================\n        # Create a MapReduce job definition\n        Write-Host \"Create a MapReduce job definition\" -ForegroundColor Green\n        $mrJobDef = New-AzureHDInsightMapReduceJobDefinition -JobName mrWordCountJob  -JarFile $jarFile -ClassName $className -Arguments $mrInput, $mrOutput -StatusFolder /WordCountStatus\n\n        #=============================\n        # Run the MapReduce job\n        Write-Host \"Run the MapReduce job\" -ForegroundColor Green\n        $mrJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $mrJobDef\n        Wait-AzureHDInsightJob -Job $mrJob -WaitTimeoutInSeconds 3600\n\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardError\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardOutput\n\n        #=============================\n        # Delete the HDInsight cluster\n        Write-Host \"Delete the HDInsight cluster\" -ForegroundColor Green\n        Remove-AzureHDInsightCluster -Name $clusterName  \n\n        # Delete the default file system Storage account\n        Write-Host \"Delete the storage account\" -ForegroundColor Green\n        Remove-AzureStorageAccount -StorageAccountName $storageAccountName_Default\n\n3. Set the first six variables in the script. The **$stringPrefix** variable is used to prefix the specified string to the HDInsight cluster name, the Storage account name, and the Blob storage container name. Because the names for these must be 3 to 24 characters, make sure the string you specify and the names this script uses, together, do not exceed the character limit for the name. You must use all lowercase for **$stringPrefix**.\n\n    The **$storageAccountName\\_Data** and **$containerName\\_Data** variables are the Storage account and container that are used for storing the data files and the application. The **$location** variable must match the data Storage account location.\n\n4. Review the rest of the variables.\n5. Save the script file.\n6. Open Azure PowerShell.\n7. Run the following command to set the execution policy to RemoteSigned:\n\n        PowerShell -File <FileName> -ExecutionPolicy RemoteSigned\n\n8. When prompted, enter the user name and password for the HDInsight cluster. Because you will delete the cluster at the end of the script and you will not need the user name and password anymore, the user name and password can be any strings. If you don't want to get prompted for the credentials, see [Working with Passwords, Secure Strings and Credentials in Windows PowerShell][powershell-PSCredential].\n\n\n##<a name=\"retrieve\"></a>Retrieve the MapReduce job output\nThis section shows you how to download and display the output. For the information on displaying the results in Excel, see [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-ODBC] and [Connect Excel to HDInsight with Power Query][hdinsight-power-query].\n\n\n**To retrieve the output**\n\n1. Open the Azure PowerShell window.\n2. Change the directory to **C:\\Tutorials\\WordCountJava**. The default Azure PowerShell folder is **C:\\Windows\\System32\\WindowsPowerShell\\v1.0**. The cmdlets you will run will download the output file to the current folder. You don't have permissions to download the files to the system folders.\n2. Run the following commands to set the values:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName_Data = \"<TheDataStorageAccountName>\"\n        $containerName_Data = \"<TheDataBlobStorageContainerName>\"\n        $blobName = \"WordCount/Output/part-r-00000\"\n\n3. Run the following commands to create an Azure storage context object:\n\n        Select-AzureSubscription $subscriptionName\n        $storageAccountKey = Get-AzureStorageKey $storageAccountName_Data | %{ $_.Primary }\n        $storageContext = New-AzureStorageContext –StorageAccountName $storageAccountName_Data –StorageAccountKey $storageAccountKey  \n\n4. Run the following commands to download and display the output:\n\n        Get-AzureStorageBlobContent -Container $containerName_Data -Blob $blobName -Context $storageContext -Force\n        cat \"./$blobName\" | findstr \"there\"\n\nAfter the job is completed, you have the option to export the data to SQL Server or Azure SQL Database by using [Sqoop][hdinsight-use-sqoop], or to export the data to Excel.  \n\n##<a id=\"nextsteps\"></a>Next steps\nIn this tutorial, you have learned how to develop a Java MapReduce job, how to test the application on the HDInsight Emulator, and how to write an Azure PowerShell script to provision an HDInsight cluster and run a MapReduce job on the cluster. To learn more, see the following articles:\n\n- [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-streaming]\n- [Get started with Azure HDInsight][hdinsight-get-started]\n- [Get started with the HDInsight Emulator][hdinsight-emulator]\n- [Use Azure Blob storage with HDInsight][hdinsight-storage]\n- [Administer HDInsight using Azure PowerShell][hdinsight-admin-powershell]\n- [Upload data to HDInsight][hdinsight-upload-data]\n- [Use Hive with HDInsight][hdinsight-use-hive]\n- [Use Pig with HDInsight][hdinsight-use-pig]\n- [Connect Excel to HDInsight with Power Query][hdinsight-power-query]\n- [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-ODBC]\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[hdinsight-use-sqoop]: hdinsight-use-sqoop.md\n[hdinsight-ODBC]: hdinsight-connect-excel-hive-ODBC-driver.md\n[hdinsight-power-query]: hdinsight-connect-excel-power-query.md\n\n[hdinsight-develop-streaming]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n\n[hdinsight-get-started]: ../hdinsight-get-started.md\n[hdinsight-emulator]: ../hdinsight-get-started-emulator.md\n[hdinsight-emulator-wasb]: ../hdinsight-get-started-emulator.md#blobstorage\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-storage]: ../hdinsight-use-blob-storage.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-power-query]: hdinsight-connect-excel-power-query.md\n\n[powershell-PSCredential]: http://social.technet.microsoft.com/wiki/contents/articles/4546.working-with-passwords-secure-strings-and-credentials-in-windows-powershell.aspx\n[powershell-install-configure]: ../install-configure-powershell.md\n\n\n\n[image-emulator-wordcount-compile]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Compile-Java-MapReduce.png\n[image-emulator-wordcount-run]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Run-Java-MapReduce.png\n\ntest\n"
}