<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Create Stream Analytics Inputs | Microsoft Azure</source>
          <target state="new">Create Stream Analytics Inputs | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to connect to and configure the input sources for Stream Analytics solutions.</source>
          <target state="new">Learn how to connect to and configure the input sources for Stream Analytics solutions.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create Stream Analytics inputs</source>
          <target state="new">Create Stream Analytics inputs</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Understanding Stream Analytics inputs</source>
          <target state="new">Understanding Stream Analytics inputs</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Stream Analytics inputs are defined as a connection to a data source.</source>
          <target state="new">Stream Analytics inputs are defined as a connection to a data source.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Stream Analytics has first class integration with the Azure sources Event Hub and Blob storage from within and outside of the job subscription.</source>
          <target state="new">Stream Analytics has first class integration with the Azure sources Event Hub and Blob storage from within and outside of the job subscription.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>As data is sent to that data source, it is consumed by the Stream Analytics job and processed in real time.</source>
          <target state="new">As data is sent to that data source, it is consumed by the Stream Analytics job and processed in real time.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Inputs are divided into two distinct types: data stream inputs and reference data inputs.</source>
          <target state="new">Inputs are divided into two distinct types: data stream inputs and reference data inputs.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Data stream inputs</source>
          <target state="new">Data stream inputs</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Stream Analytics jobs must include at least one data stream input to be consumed and transformed by the job.</source>
          <target state="new">Stream Analytics jobs must include at least one data stream input to be consumed and transformed by the job.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Azure Blob storage and Azure Event Hubs are supported as data stream input sources.</source>
          <target state="new">Azure Blob storage and Azure Event Hubs are supported as data stream input sources.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Azure Event Hubs are used to collect event streams from multiple devices and services, such as social media activity feeds, stock trade information or data from sensors.</source>
          <target state="new">Azure Event Hubs are used to collect event streams from multiple devices and services, such as social media activity feeds, stock trade information or data from sensors.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Alternately, Azure Blob storage can be used as an input source for ingesting bulk data.</source>
          <target state="new">Alternately, Azure Blob storage can be used as an input source for ingesting bulk data.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Reference data inputs</source>
          <target state="new">Reference data inputs</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Stream Analytics supports a second type of input known as reference data.</source>
          <target state="new">Stream Analytics supports a second type of input known as reference data.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>This is auxiliary data which is typically used for performing correlation and look-ups, and the data here is usually static or infrequently changed.</source>
          <target state="new">This is auxiliary data which is typically used for performing correlation and look-ups, and the data here is usually static or infrequently changed.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Azure Blob storage is currently the only supported input source for reference data.</source>
          <target state="new">Azure Blob storage is currently the only supported input source for reference data.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Reference data source blobs are limited to 50MB in size.</source>
          <target state="new">Reference data source blobs are limited to 50MB in size.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Creating an Event Hub data input stream</source>
          <target state="new">Creating an Event Hub data input stream</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Overview of Event Hubs</source>
          <target state="new">Overview of Event Hubs</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Event Hubs are a highly scalable event ingestor, and are the most common method of data ingestion to a Stream Analytics job.</source>
          <target state="new">Event Hubs are a highly scalable event ingestor, and are the most common method of data ingestion to a Stream Analytics job.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Event Hubs and Stream Analytics together provide customers an end to end solution for real time analytics -- Event Hubs allow customers to feed events into Azure in real time, and Stream Analytics jobs can process them in real time.</source>
          <target state="new">Event Hubs and Stream Analytics together provide customers an end to end solution for real time analytics -- Event Hubs allow customers to feed events into Azure in real time, and Stream Analytics jobs can process them in real time.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>For example, customers can send web clicks, sensor readings, online log events to Event Hubs, and create Stream Analytics jobs to use Event Hubs as the input data streams for real time filtering, aggregating and joining.</source>
          <target state="new">For example, customers can send web clicks, sensor readings, online log events to Event Hubs, and create Stream Analytics jobs to use Event Hubs as the input data streams for real time filtering, aggregating and joining.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Event Hubs can be used for data egress also.</source>
          <target state="new">Event Hubs can be used for data egress also.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>For further details on Event Hubs see the <bpt id="p1">[</bpt>Event Hubs<ept id="p1">]</ept><bpt id="p2">(https://azure.microsoft.com/services/event-hubs/ "</bpt>Event Hubs<ept id="p2">")</ept> documentation.</source>
          <target state="new">For further details on Event Hubs see the <bpt id="p1">[</bpt>Event Hubs<ept id="p1">]</ept><bpt id="p2">(https://azure.microsoft.com/services/event-hubs/ "</bpt>Event Hubs<ept id="p2">")</ept> documentation.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Consumer groups</source>
          <target state="new">Consumer groups</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Each Stream Analytics Event Hub input should be configured to have its own consumer group.</source>
          <target state="new">Each Stream Analytics Event Hub input should be configured to have its own consumer group.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>When a job contains a self-join or multiple inputs, some input may be read by more than one reader downstream, which impacts the number of readers in a single consumer group.</source>
          <target state="new">When a job contains a self-join or multiple inputs, some input may be read by more than one reader downstream, which impacts the number of readers in a single consumer group.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>To avoid exceeding Event Hub limit of 5 readers per consumer group per partition, it is a best practice to designate a consumer group for each Stream Analytics job.</source>
          <target state="new">To avoid exceeding Event Hub limit of 5 readers per consumer group per partition, it is a best practice to designate a consumer group for each Stream Analytics job.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Note that there is also a limit of 20 consumer groups per Event Hub.</source>
          <target state="new">Note that there is also a limit of 20 consumer groups per Event Hub.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>For details, see the <bpt id="p1">[</bpt>Event Hubs Programming Guide<ept id="p1">]</ept><bpt id="p2">(https://msdn.microsoft.com/library/azure/dn789972.aspx "</bpt>Event Hubs Programming Guide<ept id="p2">")</ept>.</source>
          <target state="new">For details, see the <bpt id="p1">[</bpt>Event Hubs Programming Guide<ept id="p1">]</ept><bpt id="p2">(https://msdn.microsoft.com/library/azure/dn789972.aspx "</bpt>Event Hubs Programming Guide<ept id="p2">")</ept>.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Creating an Event Hub input data stream</source>
          <target state="new">Creating an Event Hub input data stream</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Adding an Event Hub as a data stream input</source>
          <target state="new">Adding an Event Hub as a data stream input</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select the default option, <bpt id="p2">**</bpt>Data stream<ept id="p2">**</ept>, and click the right button.</source>
          <target state="new">On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select the default option, <bpt id="p2">**</bpt>Data stream<ept id="p2">**</ept>, and click the right button.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>image1</source>
          <target state="new">image1</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Next select <bpt id="p1">**</bpt>Event Hub<ept id="p1">**</ept>.</source>
          <target state="new">Next select <bpt id="p1">**</bpt>Event Hub<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>image6</source>
          <target state="new">image6</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Type or select the following fields and click the right button:</source>
          <target state="new">Type or select the following fields and click the right button:</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that will be used in the job query to reference this input</source>
          <target state="new"><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that will be used in the job query to reference this input</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Service Bus Namespace<ept id="p1">**</ept>: A Service Bus namespace is a container for a set of messaging entities.</source>
          <target state="new"><bpt id="p1">**</bpt>Service Bus Namespace<ept id="p1">**</ept>: A Service Bus namespace is a container for a set of messaging entities.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>When you created a new Event Hub, you also created a Service Bus namespace.</source>
          <target state="new">When you created a new Event Hub, you also created a Service Bus namespace.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Hub<ept id="p1">**</ept>: The name of your Event Hub input</source>
          <target state="new"><bpt id="p1">**</bpt>Event Hub<ept id="p1">**</ept>: The name of your Event Hub input</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Hub Policy Name<ept id="p1">**</ept>: The shared access policy, which can be created on the Event Hub Configure tab.</source>
          <target state="new"><bpt id="p1">**</bpt>Event Hub Policy Name<ept id="p1">**</ept>: The shared access policy, which can be created on the Event Hub Configure tab.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Each shared access policy will have a name, permissions that you set, and access keys.</source>
          <target state="new">Each shared access policy will have a name, permissions that you set, and access keys.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Hub Consumer Group<ept id="p1">**</ept> (Optional): The Consumer Group to ingest data from the Event Hub.</source>
          <target state="new"><bpt id="p1">**</bpt>Event Hub Consumer Group<ept id="p1">**</ept> (Optional): The Consumer Group to ingest data from the Event Hub.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>If not specified, Stream Analytics jobs will use the Default Consumer Group to ingest data from the Event Hub.</source>
          <target state="new">If not specified, Stream Analytics jobs will use the Default Consumer Group to ingest data from the Event Hub.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>It is recommended to use a distinct consumer Group for each Stream Analytics job.</source>
          <target state="new">It is recommended to use a distinct consumer Group for each Stream Analytics job.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>image7</source>
          <target state="new">image7</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Specify the following settings:</source>
          <target state="new">Specify the following settings:</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</source>
          <target state="new"><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</source>
          <target state="new"><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>image8</source>
          <target state="new">image8</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Event Hub.</source>
          <target state="new">Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Event Hub.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Creating a Blob storage data stream input</source>
          <target state="new">Creating a Blob storage data stream input</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>For scenarios with large amounts of unstructured data to store in the cloud, Blob storage offers a cost-effective and scalable solution.</source>
          <target state="new">For scenarios with large amounts of unstructured data to store in the cloud, Blob storage offers a cost-effective and scalable solution.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Data in Blob storage is generally considered data “at rest” but it can be processed as a data stream by Stream Analytics.</source>
          <target state="new">Data in Blob storage is generally considered data “at rest” but it can be processed as a data stream by Stream Analytics.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>One common scenario for Blob storage inputs with Stream Analytics is log processing, where telemetry is captured from a system and needs to be parsed and processed to extract meaningful data.</source>
          <target state="new">One common scenario for Blob storage inputs with Stream Analytics is log processing, where telemetry is captured from a system and needs to be parsed and processed to extract meaningful data.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>It is important to note that the default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was created.</source>
          <target state="new">It is important to note that the default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was created.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>To process the data as a stream using a timestamp in the event payload, the <bpt id="p1">[</bpt>TIMESTAMP BY<ept id="p1">](https://msdn.microsoft.com/library/azure/dn834998.aspx)</ept> keyword must be used.</source>
          <target state="new">To process the data as a stream using a timestamp in the event payload, the <bpt id="p1">[</bpt>TIMESTAMP BY<ept id="p1">](https://msdn.microsoft.com/library/azure/dn834998.aspx)</ept> keyword must be used.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>For further information on Blob storage see the <bpt id="p1">[</bpt>Blob storage<ept id="p1">](http://azure.microsoft.com/services/storage/blobs/)</ept> documentation.</source>
          <target state="new">For further information on Blob storage see the <bpt id="p1">[</bpt>Blob storage<ept id="p1">](http://azure.microsoft.com/services/storage/blobs/)</ept> documentation.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Adding Blob storage as a data stream input</source>
          <target state="new">Adding Blob storage as a data stream input</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select the default option, <bpt id="p2">**</bpt>Data stream<ept id="p2">**</ept>, and click the right button.</source>
          <target state="new">On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select the default option, <bpt id="p2">**</bpt>Data stream<ept id="p2">**</ept>, and click the right button.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>image1</source>
          <target state="new">image1</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Select <bpt id="p1">**</bpt>Blob storage<ept id="p1">**</ept> and click the right button.</source>
          <target state="new">Select <bpt id="p1">**</bpt>Blob storage<ept id="p1">**</ept> and click the right button.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>image2</source>
          <target state="new">image2</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Type or select the following fields:</source>
          <target state="new">Type or select the following fields:</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that  will be used in the job query to reference this input</source>
          <target state="new"><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that  will be used in the job query to reference this input</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Storage Account<ept id="p1">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.</source>
          <target state="new"><bpt id="p1">**</bpt>Storage Account<ept id="p1">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Storage Container<ept id="p1">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.</source>
          <target state="new"><bpt id="p1">**</bpt>Storage Container<ept id="p1">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>When you upload a blob to the Blob service, you must specify a container for that blob.</source>
          <target state="new">When you upload a blob to the Blob service, you must specify a container for that blob.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>image3</source>
          <target state="new">image3</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Click the <bpt id="p1">**</bpt>Configure Advanced Settings<ept id="p1">**</ept> box for the option to configure the Path Prefix Pattern for readings blobs in a customized path.</source>
          <target state="new">Click the <bpt id="p1">**</bpt>Configure Advanced Settings<ept id="p1">**</ept> box for the option to configure the Path Prefix Pattern for readings blobs in a customized path.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>If this field is not specified, Stream Analytics will read all blobs in the container.</source>
          <target state="new">If this field is not specified, Stream Analytics will read all blobs in the container.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>image4</source>
          <target state="new">image4</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Choose the following settings:</source>
          <target state="new">Choose the following settings:</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</source>
          <target state="new"><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</source>
          <target state="new"><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.</source>
          <target state="new">Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Creating a Blob storage reference data</source>
          <target state="new">Creating a Blob storage reference data</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Blob storage can be used to define reference data for a Stream Analytics job.</source>
          <target state="new">Blob storage can be used to define reference data for a Stream Analytics job.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>This is static or slow-changing data that is used for performing lookups or correlating data.</source>
          <target state="new">This is static or slow-changing data that is used for performing lookups or correlating data.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Support for refreshing reference data can be enabled by specifying a path pattern in the input configuration using the {date} and {time} tokens.</source>
          <target state="new">Support for refreshing reference data can be enabled by specifying a path pattern in the input configuration using the {date} and {time} tokens.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Stream Analytics will update reference data definitions based on this path pattern.</source>
          <target state="new">Stream Analytics will update reference data definitions based on this path pattern.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>For example, a pattern of <ph id="ph1">`"/sample/{date}/{time}/products.csv"`</ph> with a date format of “YYYY-MM-DD” and a time format of “HH:mm” tells Stream Analytics to pick up the updated blob <ph id="ph2">`"/sample/2015-04-16/17:30/products.csv"`</ph> at 5:30 PM on April 16th 2015 UTC time zone .</source>
          <target state="new">For example, a pattern of <ph id="ph1">`"/sample/{date}/{time}/products.csv"`</ph> with a date format of “YYYY-MM-DD” and a time format of “HH:mm” tells Stream Analytics to pick up the updated blob <ph id="ph2">`"/sample/2015-04-16/17:30/products.csv"`</ph> at 5:30 PM on April 16th 2015 UTC time zone .</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Currently Stream Analytics jobs look for reference blob refresh data only when the time coincides with the time encoded in the blob name: e.g. jobs look for /sample/2015-04-16/17:30/products.csv between 5:30 PM and 5:30:59.999999999PM on April 16th 2015 UTC time zone.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Currently Stream Analytics jobs look for reference blob refresh data only when the time coincides with the time encoded in the blob name: e.g. jobs look for /sample/2015-04-16/17:30/products.csv between 5:30 PM and 5:30:59.999999999PM on April 16th 2015 UTC time zone.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>When the clock strikes 5:31PM it stops looking for /sample/2015-04-16/17:30/products.csv and starts looking for /sample/2015-04-16/17:31/products.csv.</source>
          <target state="new">When the clock strikes 5:31PM it stops looking for /sample/2015-04-16/17:30/products.csv and starts looking for /sample/2015-04-16/17:31/products.csv.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>The only time previous reference data blobs are considered is when the job starts.</source>
          <target state="new">The only time previous reference data blobs are considered is when the job starts.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>At that time the job is looking for the blob which has a latest date/time encoded in its name with a value before than the job start time (the newest reference data blob from before the job start time).</source>
          <target state="new">At that time the job is looking for the blob which has a latest date/time encoded in its name with a value before than the job start time (the newest reference data blob from before the job start time).</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>This is done to ensure there is a non-empty reference data set at the start of the job.</source>
          <target state="new">This is done to ensure there is a non-empty reference data set at the start of the job.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>If one cannot be found, the job will fail and display a diagnostic notice to the user:</source>
          <target state="new">If one cannot be found, the job will fail and display a diagnostic notice to the user:</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Adding Blob storage as reference data</source>
          <target state="new">Adding Blob storage as reference data</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select <bpt id="p2">**</bpt>Reference data<ept id="p2">**</ept> and click the right button.</source>
          <target state="new">On the inputs tab of the Stream Analytics job, click <bpt id="p1">**</bpt>ADD INPUT<ept id="p1">**</ept> and then select <bpt id="p2">**</bpt>Reference data<ept id="p2">**</ept> and click the right button.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>image9</source>
          <target state="new">image9</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Type or select the following fields:</source>
          <target state="new">Type or select the following fields:</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that  will be used in the job query to reference this input</source>
          <target state="new"><bpt id="p1">**</bpt>Input Alias<ept id="p1">**</ept>: A friendly name that  will be used in the job query to reference this input</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Storage Account<ept id="p1">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.</source>
          <target state="new"><bpt id="p1">**</bpt>Storage Account<ept id="p1">**</ept>: If the storage account is in a different subscription than the streaming job the Storage Account Name and Storage Account Key will be required.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Storage Container<ept id="p1">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.</source>
          <target state="new"><bpt id="p1">**</bpt>Storage Container<ept id="p1">**</ept>: Containers provide a logical grouping for blobs stored in the Microsoft Azure Blob service.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>When you upload a blob to the Blob service, you must specify a container for that blob.</source>
          <target state="new">When you upload a blob to the Blob service, you must specify a container for that blob.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Path Pattern<ept id="p1">**</ept>: The file path used to locate your blobs within the specified container.</source>
          <target state="new"><bpt id="p1">**</bpt>Path Pattern<ept id="p1">**</ept>: The file path used to locate your blobs within the specified container.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Within the path, you may choose to specify one or more instances of the following 2 variables: {date}, {time}</source>
          <target state="new">Within the path, you may choose to specify one or more instances of the following 2 variables: {date}, {time}</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>image10</source>
          <target state="new">image10</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Choose the following settings:</source>
          <target state="new">Choose the following settings:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</source>
          <target state="new"><bpt id="p1">**</bpt>Event Serialization Format<ept id="p1">**</ept>: To make sure your queries work the way you expect, Stream Analytics needs to know which serialization format (JSON, CSV, or Avro) you're using for incoming data streams.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</source>
          <target state="new"><bpt id="p1">**</bpt>Encoding<ept id="p1">**</ept>: UTF-8 is the only supported encoding format at this time.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>image12</source>
          <target state="new">image12</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.</source>
          <target state="new">Click the check button to complete the wizard and verify that Stream Analytics can successfully connect to the Blob storage account.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>image11</source>
          <target state="new">image11</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Get help</source>
          <target state="new">Get help</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>For further assistance, try our <bpt id="p1">[</bpt>Azure Stream Analytics forum<ept id="p1">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept></source>
          <target state="new">For further assistance, try our <bpt id="p1">[</bpt>Azure Stream Analytics forum<ept id="p1">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept></target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Introduction to Azure Stream Analytics</source>
          <target state="new">Introduction to Azure Stream Analytics</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Get started using Azure Stream Analytics</source>
          <target state="new">Get started using Azure Stream Analytics</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Scale Azure Stream Analytics jobs</source>
          <target state="new">Scale Azure Stream Analytics jobs</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Azure Stream Analytics Query Language Reference</source>
          <target state="new">Azure Stream Analytics Query Language Reference</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Azure Stream Analytics Management REST API Reference</source>
          <target state="new">Azure Stream Analytics Management REST API Reference</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">15461c70c0ca8e01701a05c965c8011ae301431c</xliffext:olfilehash>
  </header>
</xliff>