{
  "nodes": [
    {
      "content": "Provision a Spark cluster on HDInsight and use Spark SQL from Zeppelin and Jupyter for interactive analysis | Microsoft Azure",
      "pos": [
        28,
        153
      ]
    },
    {
      "content": "Step-by-step instructions on how to quickly provision an Apache Spark cluster in HDInsight and then use Spark SQL from Zeppelin and Jupyter notebooks to run interactive queries.",
      "pos": [
        173,
        350
      ]
    },
    {
      "content": "Quick Start: Provision Apache Spark on HDInsight and run interactive queries using Spark SQL",
      "pos": [
        691,
        783
      ]
    },
    {
      "content": "Provision Apache Spark on HDInsight and run interactive queries using Spark SQL",
      "pos": [
        888,
        967
      ]
    },
    {
      "pos": [
        1036,
        1306
      ],
      "content": "Learn how to provision an Apache Spark cluster in HDInsight using the Quick Create option and then use the web-based <bpt id=\"p1\">[</bpt>Zeppelin<ept id=\"p1\">](https://zeppelin.incubator.apache.org)</ept> and <bpt id=\"p2\">[</bpt>Jupyter<ept id=\"p2\">](https://jupyter.org)</ept> notebooks to run Spark SQL interactive queries on the Spark cluster."
    },
    {
      "content": "Get started using Apache Spark in HDInsight",
      "pos": [
        1314,
        1357
      ]
    },
    {
      "content": "Prerequisites:",
      "pos": [
        1607,
        1621
      ]
    },
    {
      "content": "Before you begin this tutorial, you must have an Azure subscription.",
      "pos": [
        1625,
        1693
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Get Azure free trial<ept id=\"p1\">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "pos": [
        1694,
        1824
      ]
    },
    {
      "pos": [
        1829,
        1889
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"provision\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Provision an HDInsight Spark cluster"
    },
    {
      "content": "In this section, you provision an HDInsight version 3.2 cluster, which is based on Spark version 1.3.1.",
      "pos": [
        1891,
        1994
      ]
    },
    {
      "content": "For information about HDInsight versions and their SLAs, see <bpt id=\"p1\">[</bpt>HDInsight component versioning<ept id=\"p1\">](hdinsight-component-versioning.md)</ept>.",
      "pos": [
        1995,
        2124
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The steps in this article create an Apache Spark cluster in HDInsight by using basic configuration settings.",
      "pos": [
        2127,
        2248
      ]
    },
    {
      "content": "For information about other cluster configuration settings (such as using additional storage, Azure virtual network, or a metastore for Hive), see <bpt id=\"p1\">[</bpt>Provision HDInsight clusters using custom options<ept id=\"p1\">](hdinsight-apache-spark-provision-clusters.md)</ept>.",
      "pos": [
        2249,
        2494
      ]
    },
    {
      "content": "To provision a Spark cluster",
      "pos": [
        2499,
        2527
      ]
    },
    {
      "pos": [
        2535,
        2603
      ],
      "content": "Sign in to the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](https://ms.portal.azure.com/)</ept>."
    },
    {
      "pos": [
        2609,
        2679
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept>, Click <bpt id=\"p2\">**</bpt>Data Analytics<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>HDInsight<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Creating a new cluster in the Azure Preview Portal",
      "pos": [
        2687,
        2737
      ]
    },
    {
      "content": "Enter a <bpt id=\"p1\">**</bpt>Cluster Name<ept id=\"p1\">**</ept>, select <bpt id=\"p2\">**</bpt>Hadoop<ept id=\"p2\">**</ept> for the <bpt id=\"p3\">**</bpt>Cluster Type<ept id=\"p3\">**</ept>, and from the <bpt id=\"p4\">**</bpt>Cluster Operating System<ept id=\"p4\">**</ept> drop-down, select <bpt id=\"p5\">**</bpt>Windows Server 2012 R2 Datacenter<ept id=\"p5\">**</ept>.",
      "pos": [
        2888,
        3056
      ]
    },
    {
      "content": "A green check will appear beside the cluster name if it is available.",
      "pos": [
        3057,
        3126
      ]
    },
    {
      "content": "Enter cluster name and type",
      "pos": [
        3134,
        3161
      ]
    },
    {
      "pos": [
        3289,
        3429
      ],
      "content": "If you have more than one subscription, click the <bpt id=\"p1\">**</bpt>Subscription<ept id=\"p1\">**</ept> entry to select the Azure subscription that will be used for the cluster."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Resource Group<ept id=\"p1\">**</ept> to see a list of existing resource groups and then select the one to create the cluster in.",
      "pos": [
        3434,
        3550
      ]
    },
    {
      "content": "Or, you can click <bpt id=\"p1\">**</bpt>Create New<ept id=\"p1\">**</ept> and then enter the name of the new resource group.",
      "pos": [
        3551,
        3634
      ]
    },
    {
      "content": "A green check will appear to indicate if the new group name is available.",
      "pos": [
        3635,
        3708
      ]
    },
    {
      "pos": [
        3716,
        3815
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> This entry will default to one of your existing resource groups, if any are available."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Credentials<ept id=\"p1\">**</ept>, then enter a <bpt id=\"p2\">**</bpt>Cluster Login Username<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>Cluster Login Password<ept id=\"p3\">**</ept>.",
      "pos": [
        3820,
        3914
      ]
    },
    {
      "content": "If you want to enable remote desktop on the cluster node, for <bpt id=\"p1\">**</bpt>Enable Remote Desktop<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Yes<ept id=\"p2\">**</ept>, and then specify the required values.",
      "pos": [
        3915,
        4056
      ]
    },
    {
      "content": "This tutorial does not require remote desktop so you can skip this.",
      "pos": [
        4057,
        4124
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> at the bottom to save the credentials configuration.",
      "pos": [
        4125,
        4194
      ]
    },
    {
      "content": "Provide cluster credentials",
      "pos": [
        4202,
        4229
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Data Source<ept id=\"p1\">**</ept> to choose an existing data source for the cluster, or create a new one.",
      "pos": [
        4357,
        4450
      ]
    },
    {
      "content": "When you provision a Hadoop cluster in HDInsight, you specify an Azure Storage account.",
      "pos": [
        4451,
        4538
      ]
    },
    {
      "content": "A specific Blob storage container from that account is designated as the default file system, like in the Hadoop distributed file system (HDFS).",
      "pos": [
        4539,
        4683
      ]
    },
    {
      "content": "By default, the HDInsight cluster is provisioned in the same data center as the storage account you specify.",
      "pos": [
        4684,
        4792
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Use Azure Blob storage with HDInsight<ept id=\"p1\">][hdinsight-storage]</ept>",
      "pos": [
        4793,
        4877
      ]
    },
    {
      "content": "Data source blade",
      "pos": [
        4885,
        4902
      ]
    },
    {
      "content": "Currently you can select an Azure Storage Account as the data source for an HDInsight cluster.",
      "pos": [
        5024,
        5118
      ]
    },
    {
      "content": "Use the following to understand the entries on the <bpt id=\"p1\">**</bpt>Data Source<ept id=\"p1\">**</ept> blade.",
      "pos": [
        5119,
        5192
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Selection Method<ept id=\"p1\">**</ept>: Set this to <bpt id=\"p2\">**</bpt>From all subscriptions<ept id=\"p2\">**</ept> to enable browsing of storage accounts from all your subscriptions.",
      "pos": [
        5200,
        5328
      ]
    },
    {
      "content": "Set this to <bpt id=\"p1\">**</bpt>Access Key<ept id=\"p1\">**</ept> if you want to enter the <bpt id=\"p2\">**</bpt>Storage Name<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>Access Key<ept id=\"p3\">**</ept> of an existing storage account.",
      "pos": [
        5329,
        5448
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Select storage account / Create New<ept id=\"p1\">**</ept>: Click <bpt id=\"p2\">**</bpt>Select storage account<ept id=\"p2\">**</ept> to browse and select an existing storage account you want to associate with the cluster.",
      "pos": [
        5456,
        5618
      ]
    },
    {
      "content": "Or, click <bpt id=\"p1\">**</bpt>Create New<ept id=\"p1\">**</ept> to create a new storage account.",
      "pos": [
        5619,
        5676
      ]
    },
    {
      "content": "Use the field that appears to enter the name of the storage account.",
      "pos": [
        5677,
        5745
      ]
    },
    {
      "content": "A green check will appear if the name is available.",
      "pos": [
        5746,
        5797
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Choose Default Container<ept id=\"p1\">**</ept>: Use this to enter the name of the default container to use for the cluster.",
      "pos": [
        5805,
        5910
      ]
    },
    {
      "content": "While you can enter any name here, we recommend using the same name as the cluster so that you can easily recognize that the container is used for this specific cluster.",
      "pos": [
        5911,
        6080
      ]
    },
    {
      "pos": [
        6089,
        6179
      ],
      "content": "<bpt id=\"p1\">**</bpt>Location<ept id=\"p1\">**</ept>: The geographic region that the storage account is in, or will be created in."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Selecting the location for the default data source will also set the location of the HDInsight cluster.",
      "pos": [
        6191,
        6312
      ]
    },
    {
      "content": "The cluster and default data source must be located in the same region.",
      "pos": [
        6313,
        6384
      ]
    },
    {
      "pos": [
        6394,
        6449
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> to save the data source configuration."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Node Pricing Tiers<ept id=\"p1\">**</ept> to display information about the nodes that will be created for this cluster.",
      "pos": [
        6454,
        6560
      ]
    },
    {
      "content": "Set the number of worker nodes that you need for the cluster.",
      "pos": [
        6561,
        6622
      ]
    },
    {
      "content": "The estimated cost of the cluster will be shown within the blade.",
      "pos": [
        6623,
        6688
      ]
    },
    {
      "content": "Node pricing tiers blade",
      "pos": [
        6696,
        6720
      ]
    },
    {
      "pos": [
        6853,
        6909
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> to save the node pricing configuration."
    },
    {
      "content": "On the <bpt id=\"p1\">**</bpt>New HDInsight Cluster<ept id=\"p1\">**</ept> blade, ensure that <bpt id=\"p2\">**</bpt>Pin to Startboard<ept id=\"p2\">**</ept> is selected, and then click <bpt id=\"p3\">**</bpt>Create<ept id=\"p3\">**</ept>.",
      "pos": [
        6914,
        7027
      ]
    },
    {
      "content": "This will create the cluster and add a tile for it to the Startboard of your Azure Portal.",
      "pos": [
        7028,
        7118
      ]
    },
    {
      "content": "The icon will indicate that the cluster is provisioning, and will change to display the HDInsight icon once provisioning has completed.",
      "pos": [
        7119,
        7254
      ]
    },
    {
      "content": "While provisioning",
      "pos": [
        7262,
        7280
      ]
    },
    {
      "content": "Provisioning complete",
      "pos": [
        7283,
        7304
      ]
    },
    {
      "content": "Provisioning indicator on startboard",
      "pos": [
        7366,
        7402
      ]
    },
    {
      "content": "Provisioned cluster tile",
      "pos": [
        7493,
        7517
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> It will take some time for the cluster to be created, usually around 15 minutes.",
      "pos": [
        7612,
        7705
      ]
    },
    {
      "content": "Use the tile on the Startboard, or the <bpt id=\"p1\">**</bpt>Notifications<ept id=\"p1\">**</ept> entry on the left of the page to check on the provisioning process.",
      "pos": [
        7706,
        7830
      ]
    },
    {
      "content": "Once the provisioning completes, click the tile for the Spark cluster from the Startboard to launch the cluster blade.",
      "pos": [
        7836,
        7954
      ]
    },
    {
      "pos": [
        7959,
        8041
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"zeppelin\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run interactive Spark SQL queries using a Zeppelin notebook"
    },
    {
      "content": "After you have provisioned a cluster, you can use a web-based Zeppelin notebook to run Spark SQL interactive queries against the Spark HDInsight cluster.",
      "pos": [
        8043,
        8196
      ]
    },
    {
      "content": "In this section, we will use a sample data file (hvac.csv) available by default on the cluster to run some interactive Spark SQL queries.",
      "pos": [
        8197,
        8334
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The notebook you create following the instructions below is also available by default on the cluster.",
      "pos": [
        8337,
        8451
      ]
    },
    {
      "content": "After you have launched Zeppelin, you will find this notebook by the name <bpt id=\"p1\">**</bpt>Zeppelin HVAC tutorial<ept id=\"p1\">**</ept>.",
      "pos": [
        8452,
        8553
      ]
    },
    {
      "content": "Launch the Zeppelin notebook.",
      "pos": [
        8558,
        8587
      ]
    },
    {
      "content": "From the Spark cluster blade, click <bpt id=\"p1\">**</bpt>Quick Links<ept id=\"p1\">**</ept>, and then from the <bpt id=\"p2\">**</bpt>Cluster Dashboard<ept id=\"p2\">**</ept> blade, click <bpt id=\"p3\">**</bpt>Zeppelin Notebook<ept id=\"p3\">**</ept>.",
      "pos": [
        8588,
        8716
      ]
    },
    {
      "content": "When prompted, enter the admin credentials for the cluster.",
      "pos": [
        8717,
        8776
      ]
    },
    {
      "content": "Follow the instructions on the page that opens up to launch the notebook.",
      "pos": [
        8777,
        8850
      ]
    },
    {
      "content": "Create a new notebook.",
      "pos": [
        8855,
        8877
      ]
    },
    {
      "content": "From the header pane, click <bpt id=\"p1\">**</bpt>Notebook<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Create New Note<ept id=\"p2\">**</ept>.",
      "pos": [
        8878,
        8955
      ]
    },
    {
      "content": "Create a new Zeppelin notebook",
      "pos": [
        8963,
        8993
      ]
    },
    {
      "content": "On the same page, under the <bpt id=\"p1\">**</bpt>Notebook<ept id=\"p1\">**</ept> heading, you should see a new notebook with the name starting with <bpt id=\"p2\">**</bpt>Note XXXXXXXXX<ept id=\"p2\">**</ept>.",
      "pos": [
        9129,
        9256
      ]
    },
    {
      "content": "Click the new notebook.",
      "pos": [
        9257,
        9280
      ]
    },
    {
      "content": "On the web page for the new notebook, click the heading, and change the name of the notebook if you want to.",
      "pos": [
        9285,
        9393
      ]
    },
    {
      "content": "Press ENTER to save the name change.",
      "pos": [
        9394,
        9430
      ]
    },
    {
      "content": "Also, make sure the notebook header shows a <bpt id=\"p1\">**</bpt>Connected<ept id=\"p1\">**</ept> status in the top-right corner.",
      "pos": [
        9431,
        9520
      ]
    },
    {
      "content": "Zeppelin notebook status",
      "pos": [
        9528,
        9552
      ]
    },
    {
      "content": "Load sample data into a temporary table.",
      "pos": [
        9685,
        9725
      ]
    },
    {
      "content": "When you provision a Spark cluster in HDInsight, the sample data file, <bpt id=\"p1\">**</bpt>hvac.csv<ept id=\"p1\">**</ept>, is copied to the associated storage account under <bpt id=\"p2\">**</bpt>\\HdiSamples\\SensorSampleData\\hvac<ept id=\"p2\">**</ept>.",
      "pos": [
        9726,
        9899
      ]
    },
    {
      "content": "In the empty paragraph that is created by default in the new notebook, paste the following snippet.",
      "pos": [
        9905,
        10004
      ]
    },
    {
      "content": "Press <bpt id=\"p1\">**</bpt>SHIFT + ENTER<ept id=\"p1\">**</ept> or click the <bpt id=\"p2\">**</bpt>Play<ept id=\"p2\">**</ept> button for the paragraph to run the snippet.",
      "pos": [
        10746,
        10836
      ]
    },
    {
      "content": "The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED.",
      "pos": [
        10837,
        10942
      ]
    },
    {
      "content": "The output shows up at the bottom of the same paragraph.",
      "pos": [
        10943,
        10999
      ]
    },
    {
      "content": "The screenshot looks like the following:",
      "pos": [
        11000,
        11040
      ]
    },
    {
      "content": "Create a temporary table from raw data",
      "pos": [
        11048,
        11086
      ]
    },
    {
      "content": "You can also provide a title to each paragraph.",
      "pos": [
        11239,
        11286
      ]
    },
    {
      "content": "From the right-hand corner, click the <bpt id=\"p1\">**</bpt>Settings<ept id=\"p1\">**</ept> icon, and then click <bpt id=\"p2\">**</bpt>Show title<ept id=\"p2\">**</ept>.",
      "pos": [
        11287,
        11374
      ]
    },
    {
      "content": "You can now run Spark SQL statements on the <bpt id=\"p1\">**</bpt>hvac<ept id=\"p1\">**</ept> table.",
      "pos": [
        11379,
        11438
      ]
    },
    {
      "content": "Paste the following query in a new paragraph.",
      "pos": [
        11439,
        11484
      ]
    },
    {
      "content": "The query retrieves the building ID and the difference between the target and actual temperatures for each building on a given date.",
      "pos": [
        11485,
        11617
      ]
    },
    {
      "content": "Press <bpt id=\"p1\">**</bpt>SHIFT + ENTER<ept id=\"p1\">**</ept>.",
      "pos": [
        11618,
        11642
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>%sql<ept id=\"p1\">**</ept> statement at the beginning tells the notebook to use the Spark  SQL interpreter.",
      "pos": [
        11784,
        11877
      ]
    },
    {
      "content": "You can look at the defined interpreters from the <bpt id=\"p1\">**</bpt>Interpreter<ept id=\"p1\">**</ept> tab in the notebook header.",
      "pos": [
        11878,
        11971
      ]
    },
    {
      "content": "The following screenshot shows the output.",
      "pos": [
        11977,
        12019
      ]
    },
    {
      "content": "Run a Spark SQL statement using the notebook",
      "pos": [
        12027,
        12071
      ]
    },
    {
      "content": "Click the display options (highlighted in rectangle) to switch between different representations for the same output.",
      "pos": [
        12228,
        12345
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Settings<ept id=\"p1\">**</ept> to choose what consitutes the key and values in the output.",
      "pos": [
        12346,
        12424
      ]
    },
    {
      "content": "The screen capture above uses <bpt id=\"p1\">**</bpt>buildingID<ept id=\"p1\">**</ept> as the key and the average of <bpt id=\"p2\">**</bpt>temp_diff<ept id=\"p2\">**</ept> as the value.",
      "pos": [
        12425,
        12527
      ]
    },
    {
      "content": "You can also run Spark SQL statements using variables in the query.",
      "pos": [
        12537,
        12604
      ]
    },
    {
      "content": "The next snippet shows how to define a variable, <bpt id=\"p1\">**</bpt>Temp<ept id=\"p1\">**</ept>, in the query with the possible values you want to query with.",
      "pos": [
        12605,
        12725
      ]
    },
    {
      "content": "When you first run the query, a drop-down is automatically populated with the values you specified for the variable.",
      "pos": [
        12726,
        12842
      ]
    },
    {
      "content": "Paste this snippet in a new paragraph and press <bpt id=\"p1\">**</bpt>SHIFT + ENTER<ept id=\"p1\">**</ept>.",
      "pos": [
        13016,
        13082
      ]
    },
    {
      "content": "The following screenshot shows the output.",
      "pos": [
        13083,
        13125
      ]
    },
    {
      "content": "Run a Spark SQL statement using the notebook",
      "pos": [
        13133,
        13177
      ]
    },
    {
      "content": "For subsequent queries, you can select a new value from the drop-down and run the query again.",
      "pos": [
        13333,
        13427
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Settings<ept id=\"p1\">**</ept> to choose what consitutes the key and values in the output.",
      "pos": [
        13428,
        13506
      ]
    },
    {
      "content": "The screen capture above uses <bpt id=\"p1\">**</bpt>buildingID<ept id=\"p1\">**</ept> as the key, the average of <bpt id=\"p2\">**</bpt>temp_diff<ept id=\"p2\">**</ept> as the value, and <bpt id=\"p3\">**</bpt>targettemp<ept id=\"p3\">**</ept> as the group.",
      "pos": [
        13507,
        13639
      ]
    },
    {
      "content": "Restart the Spark SQL interpreter to exit the application.",
      "pos": [
        13644,
        13702
      ]
    },
    {
      "content": "Click the <bpt id=\"p1\">**</bpt>Interpreter<ept id=\"p1\">**</ept> tab at the top, and for the Spark interpreter, click <bpt id=\"p2\">**</bpt>Restart<ept id=\"p2\">**</ept>.",
      "pos": [
        13703,
        13794
      ]
    },
    {
      "content": "Restart the Zeppelin intepreter",
      "pos": [
        13802,
        13833
      ]
    },
    {
      "pos": [
        13983,
        14051
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"jupyter\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run Spark SQL queries using a Jupyter notebook"
    },
    {
      "content": "In this section, you use a Jupyter notebook to run Spark SQL queries against a Spark cluster.",
      "pos": [
        14053,
        14146
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The notebook you create following the instructions below is also available by default on the cluster.",
      "pos": [
        14149,
        14263
      ]
    },
    {
      "content": "After you have launched Jupyter, you will find this notebook by the name <bpt id=\"p1\">**</bpt>HVACTutorial.ipynb<ept id=\"p1\">**</ept>.",
      "pos": [
        14264,
        14360
      ]
    },
    {
      "content": "Launch the Jupyter notebook.",
      "pos": [
        14365,
        14393
      ]
    },
    {
      "content": "From the Spark cluster blade, click <bpt id=\"p1\">**</bpt>Quick Links<ept id=\"p1\">**</ept>, and then from the <bpt id=\"p2\">**</bpt>Cluster Dashboard<ept id=\"p2\">**</ept> blade, click <bpt id=\"p3\">**</bpt>Jupyter Notebook<ept id=\"p3\">**</ept>.",
      "pos": [
        14394,
        14521
      ]
    },
    {
      "content": "When prompted, enter the admin credentials for the Spark cluster.",
      "pos": [
        14522,
        14587
      ]
    },
    {
      "content": "Create a new notebook.",
      "pos": [
        14592,
        14614
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>New<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Python2<ept id=\"p2\">**</ept>.",
      "pos": [
        14615,
        14657
      ]
    },
    {
      "content": "Create a new Jupyter notebook",
      "pos": [
        14665,
        14694
      ]
    },
    {
      "content": "A new notebook is created and opened with the name Untitled.pynb.",
      "pos": [
        14842,
        14907
      ]
    },
    {
      "content": "Click the notebook name at the top, and enter a friendly name.",
      "pos": [
        14908,
        14970
      ]
    },
    {
      "content": "Provide a name for the notebook",
      "pos": [
        14978,
        15009
      ]
    },
    {
      "content": "Import the required modules and create the Spark and SQL contexts.",
      "pos": [
        15158,
        15224
      ]
    },
    {
      "content": "Paste the following snippet in an empty cell, and then press <bpt id=\"p1\">**</bpt>SHIFT + ENTER<ept id=\"p1\">**</ept>.",
      "pos": [
        15225,
        15304
      ]
    },
    {
      "content": "Everytime you run a job in Jupyter, your web browser window title will show a <bpt id=\"p1\">**</bpt>(Busy)<ept id=\"p1\">**</ept> status along with the notebook title.",
      "pos": [
        15578,
        15704
      ]
    },
    {
      "content": "You will also see a solid circle next to the <bpt id=\"p1\">**</bpt>Python 2<ept id=\"p1\">**</ept> text in the top-right corner.",
      "pos": [
        15705,
        15792
      ]
    },
    {
      "content": "After the job completes, this will change to a hollow circle.",
      "pos": [
        15793,
        15854
      ]
    },
    {
      "content": "Status of a Jupyter notebook job",
      "pos": [
        15863,
        15895
      ]
    },
    {
      "content": "Load sample data into a temporary table.",
      "pos": [
        16037,
        16077
      ]
    },
    {
      "content": "When you provision a Spark cluster in HDInsight, the sample data file, <bpt id=\"p1\">**</bpt>hvac.csv<ept id=\"p1\">**</ept>, is copied to the associated storage account under <bpt id=\"p2\">**</bpt>\\HdiSamples\\SensorSampleData\\hvac<ept id=\"p2\">**</ept>.",
      "pos": [
        16078,
        16251
      ]
    },
    {
      "content": "In an empty cell, paste the following snippet and press <bpt id=\"p1\">**</bpt>SHIFT + ENTER<ept id=\"p1\">**</ept>.",
      "pos": [
        16257,
        16331
      ]
    },
    {
      "content": "This snippet registers the data into a temporary table called <bpt id=\"p1\">**</bpt>hvac<ept id=\"p1\">**</ept>.",
      "pos": [
        16332,
        16403
      ]
    },
    {
      "content": "Once the job completes successfully, the following output is displayed.",
      "pos": [
        17453,
        17524
      ]
    },
    {
      "content": "Restart the kernel to exit the application.",
      "pos": [
        18286,
        18329
      ]
    },
    {
      "content": "From the top menu bar, click <bpt id=\"p1\">**</bpt>Kernel<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Restart<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Restart<ept id=\"p3\">**</ept> again at the prompt.",
      "pos": [
        18330,
        18437
      ]
    },
    {
      "content": "Restart the Jupyter Kernel",
      "pos": [
        18445,
        18471
      ]
    },
    {
      "pos": [
        18611,
        18641
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"seealso\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>See also"
    },
    {
      "content": "Overview: Apache Spark on Azure HDInsight",
      "pos": [
        18647,
        18688
      ]
    },
    {
      "content": "Provision a Spark on HDInsight cluster",
      "pos": [
        18729,
        18767
      ]
    },
    {
      "content": "Perform interactive data analysis using Spark in HDInsight with BI tools",
      "pos": [
        18818,
        18890
      ]
    },
    {
      "content": "Use Spark in HDInsight for building machine learning applications",
      "pos": [
        18935,
        19000
      ]
    },
    {
      "content": "Use Spark in HDInsight for building real-time streaming applications",
      "pos": [
        19066,
        19134
      ]
    },
    {
      "content": "Manage resources for the Apache Spark cluster in Azure HDInsight",
      "pos": [
        19208,
        19272
      ]
    },
    {
      "content": "test",
      "pos": [
        19828,
        19832
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Provision a Spark cluster on HDInsight and use Spark SQL from Zeppelin and Jupyter for interactive analysis | Microsoft Azure\" \n    description=\"Step-by-step instructions on how to quickly provision an Apache Spark cluster in HDInsight and then use Spark SQL from Zeppelin and Jupyter notebooks to run interactive queries.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/07/2015\" \n    ms.author=\"nitinme\"/>\n\n\n# Quick Start: Provision Apache Spark on HDInsight and run interactive queries using Spark SQL\n\n[AZURE.INCLUDE [hdinsight-azure-preview-portal](../../includes/hdinsight-azure-preview-portal.md)]\n\n* [Provision Apache Spark on HDInsight and run interactive queries using Spark SQL](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql-v1.md)\n\nLearn how to provision an Apache Spark cluster in HDInsight using the Quick Create option and then use the web-based [Zeppelin](https://zeppelin.incubator.apache.org) and [Jupyter](https://jupyter.org) notebooks to run Spark SQL interactive queries on the Spark cluster.\n\n\n   ![Get started using Apache Spark in HDInsight](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.GetStartedFlow.Spark.png  \"Get started using Apache Spark in HDInsight tutorial. Steps illustrated: create a storage account; provision a cluster; run Spark SQL statements\")\n\n**Prerequisites:**\n\nBefore you begin this tutorial, you must have an Azure subscription. See [Get Azure free trial](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n\n\n##<a name=\"provision\"></a>Provision an HDInsight Spark cluster\n\nIn this section, you provision an HDInsight version 3.2 cluster, which is based on Spark version 1.3.1. For information about HDInsight versions and their SLAs, see [HDInsight component versioning](hdinsight-component-versioning.md).\n\n>[AZURE.NOTE] The steps in this article create an Apache Spark cluster in HDInsight by using basic configuration settings. For information about other cluster configuration settings (such as using additional storage, Azure virtual network, or a metastore for Hive), see [Provision HDInsight clusters using custom options](hdinsight-apache-spark-provision-clusters.md).\n\n\n**To provision a Spark cluster** \n\n1. Sign in to the [Azure Preview Portal](https://ms.portal.azure.com/). \n\n2. Click **NEW**, Click **Data Analytics**, and then click **HDInsight**.\n\n    ![Creating a new cluster in the Azure Preview Portal](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.CreateCluster.1.png \"Creating a new cluster in the Azure Preview Portal\")\n\n3. Enter a **Cluster Name**, select **Hadoop** for the **Cluster Type**, and from the **Cluster Operating System** drop-down, select **Windows Server 2012 R2 Datacenter**. A green check will appear beside the cluster name if it is available.\n\n    ![Enter cluster name and type](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.CreateCluster.2.png \"Enter cluster name and type\")\n\n4. If you have more than one subscription, click the **Subscription** entry to select the Azure subscription that will be used for the cluster.\n\n5. Click **Resource Group** to see a list of existing resource groups and then select the one to create the cluster in. Or, you can click **Create New** and then enter the name of the new resource group. A green check will appear to indicate if the new group name is available.\n\n    > [AZURE.NOTE] This entry will default to one of your existing resource groups, if any are available.\n\n6. Click **Credentials**, then enter a **Cluster Login Username** and **Cluster Login Password**. If you want to enable remote desktop on the cluster node, for **Enable Remote Desktop**, click **Yes**, and then specify the required values. This tutorial does not require remote desktop so you can skip this. Click **Select** at the bottom to save the credentials configuration.\n\n    ![Provide cluster credentials](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.CreateCluster.3.png \"Provide cluster credentials\")\n\n7. Click **Data Source** to choose an existing data source for the cluster, or create a new one. When you provision a Hadoop cluster in HDInsight, you specify an Azure Storage account. A specific Blob storage container from that account is designated as the default file system, like in the Hadoop distributed file system (HDFS). By default, the HDInsight cluster is provisioned in the same data center as the storage account you specify. For more information, see [Use Azure Blob storage with HDInsight][hdinsight-storage]\n\n    ![Data source blade](./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.CreateCluster.4.png \"Provide data source configuration\")\n\n    Currently you can select an Azure Storage Account as the data source for an HDInsight cluster. Use the following to understand the entries on the **Data Source** blade.\n\n    - **Selection Method**: Set this to **From all subscriptions** to enable browsing of storage accounts from all your subscriptions. Set this to **Access Key** if you want to enter the **Storage Name** and **Access Key** of an existing storage account.\n\n    - **Select storage account / Create New**: Click **Select storage account** to browse and select an existing storage account you want to associate with the cluster. Or, click **Create New** to create a new storage account. Use the field that appears to enter the name of the storage account. A green check will appear if the name is available.\n\n    - **Choose Default Container**: Use this to enter the name of the default container to use for the cluster. While you can enter any name here, we recommend using the same name as the cluster so that you can easily recognize that the container is used for this specific cluster. \n\n    - **Location**: The geographic region that the storage account is in, or will be created in.\n\n        > [AZURE.IMPORTANT] Selecting the location for the default data source will also set the location of the HDInsight cluster. The cluster and default data source must be located in the same region.\n    \n    Click **Select** to save the data source configuration.\n\n8. Click **Node Pricing Tiers** to display information about the nodes that will be created for this cluster. Set the number of worker nodes that you need for the cluster. The estimated cost of the cluster will be shown within the blade.\n\n    ![Node pricing tiers blade](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.CreateCluster.5.png \"Specify number of cluster nodes\")\n\n    Click **Select** to save the node pricing configuration.\n\n9. On the **New HDInsight Cluster** blade, ensure that **Pin to Startboard** is selected, and then click **Create**. This will create the cluster and add a tile for it to the Startboard of your Azure Portal. The icon will indicate that the cluster is provisioning, and will change to display the HDInsight icon once provisioning has completed.\n\n    | While provisioning | Provisioning complete |\n    | ------------------ | --------------------- |\n    | ![Provisioning indicator on startboard](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/provisioning.png) | ![Provisioned cluster tile](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/provisioned.png) |\n\n    > [AZURE.NOTE] It will take some time for the cluster to be created, usually around 15 minutes. Use the tile on the Startboard, or the **Notifications** entry on the left of the page to check on the provisioning process.\n\n10. Once the provisioning completes, click the tile for the Spark cluster from the Startboard to launch the cluster blade.\n\n\n##<a name=\"zeppelin\"></a>Run interactive Spark SQL queries using a Zeppelin notebook\n\nAfter you have provisioned a cluster, you can use a web-based Zeppelin notebook to run Spark SQL interactive queries against the Spark HDInsight cluster. In this section, we will use a sample data file (hvac.csv) available by default on the cluster to run some interactive Spark SQL queries.\n\n>[AZURE.NOTE] The notebook you create following the instructions below is also available by default on the cluster. After you have launched Zeppelin, you will find this notebook by the name **Zeppelin HVAC tutorial**.\n\n1. Launch the Zeppelin notebook. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Zeppelin Notebook**. When prompted, enter the admin credentials for the cluster. Follow the instructions on the page that opens up to launch the notebook.\n\n2. Create a new notebook. From the header pane, click **Notebook**, and then click **Create New Note**.\n\n    ![Create a new Zeppelin notebook](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.CreateNewNote.png \"Create a new Zeppelin notebook\")\n\n    On the same page, under the **Notebook** heading, you should see a new notebook with the name starting with **Note XXXXXXXXX**. Click the new notebook.\n\n3. On the web page for the new notebook, click the heading, and change the name of the notebook if you want to. Press ENTER to save the name change. Also, make sure the notebook header shows a **Connected** status in the top-right corner.\n\n    ![Zeppelin notebook status](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.NewNote.Connected.png \"Zeppelin notebook status\")\n\n4. Load sample data into a temporary table. When you provision a Spark cluster in HDInsight, the sample data file, **hvac.csv**, is copied to the associated storage account under **\\HdiSamples\\SensorSampleData\\hvac**.\n\n    In the empty paragraph that is created by default in the new notebook, paste the following snippet.\n\n        // Create an RDD using the default Spark context, sc\n        val hvacText = sc.textFile(\"wasb:///HdiSamples/SensorSampleData/hvac/HVAC.csv\")\n        \n        // Define a schema\n        case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)\n        \n        // Map the values in the .csv file to the schema\n        val hvac = hvacText.map(s => s.split(\",\")).filter(s => s(0) != \"Date\").map(\n            s => Hvac(s(0), \n                    s(1),\n                    s(2).toInt,\n                    s(3).toInt,\n                    s(6)\n            )\n        ).toDF()\n        \n        // Register as a temporary table called \"hvac\"\n        hvac.registerTempTable(\"hvac\")\n        \n    Press **SHIFT + ENTER** or click the **Play** button for the paragraph to run the snippet. The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED. The output shows up at the bottom of the same paragraph. The screenshot looks like the following:\n\n    ![Create a temporary table from raw data](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Note.LoadDataIntoTable.png \"Create a temporary table from raw data\")\n\n    You can also provide a title to each paragraph. From the right-hand corner, click the **Settings** icon, and then click **Show title**.\n\n5. You can now run Spark SQL statements on the **hvac** table. Paste the following query in a new paragraph. The query retrieves the building ID and the difference between the target and actual temperatures for each building on a given date. Press **SHIFT + ENTER**.\n\n        %sql\n        select buildingID, (targettemp - actualtemp) as temp_diff, date \n        from hvac\n        where date = \"6/1/13\" \n\n    The **%sql** statement at the beginning tells the notebook to use the Spark  SQL interpreter. You can look at the defined interpreters from the **Interpreter** tab in the notebook header.\n\n    The following screenshot shows the output.\n\n    ![Run a Spark SQL statement using the notebook](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Note.SparkSQLQuery1.png \"Run a Spark SQL statement using the notebook\")\n\n     Click the display options (highlighted in rectangle) to switch between different representations for the same output. Click **Settings** to choose what consitutes the key and values in the output. The screen capture above uses **buildingID** as the key and the average of **temp_diff** as the value.\n\n    \n6. You can also run Spark SQL statements using variables in the query. The next snippet shows how to define a variable, **Temp**, in the query with the possible values you want to query with. When you first run the query, a drop-down is automatically populated with the values you specified for the variable.\n\n        %sql\n        select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff\n        from hvac\n        where targettemp > \"${Temp = 65,65|75|85}\" \n\n    Paste this snippet in a new paragraph and press **SHIFT + ENTER**. The following screenshot shows the output.\n\n    ![Run a Spark SQL statement using the notebook](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Note.SparkSQLQuery2.png \"Run a Spark SQL statement using the notebook\")\n\n    For subsequent queries, you can select a new value from the drop-down and run the query again. Click **Settings** to choose what consitutes the key and values in the output. The screen capture above uses **buildingID** as the key, the average of **temp_diff** as the value, and **targettemp** as the group.\n\n7. Restart the Spark SQL interpreter to exit the application. Click the **Interpreter** tab at the top, and for the Spark interpreter, click **Restart**.\n\n    ![Restart the Zeppelin intepreter](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Zeppelin.Restart.Interpreter.png \"Restart the Zeppelin intepreter\")\n\n##<a name=\"jupyter\"></a>Run Spark SQL queries using a Jupyter notebook\n\nIn this section, you use a Jupyter notebook to run Spark SQL queries against a Spark cluster.\n\n>[AZURE.NOTE] The notebook you create following the instructions below is also available by default on the cluster. After you have launched Jupyter, you will find this notebook by the name **HVACTutorial.ipynb**.\n\n1. Launch the Jupyter notebook. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Jupyter Notebook**. When prompted, enter the admin credentials for the Spark cluster.\n\n2. Create a new notebook. Click **New**, and then click **Python2**.\n\n    ![Create a new Jupyter notebook](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Note.Jupyter.CreateNotebook.png \"Create a new Jupyter notebook\")\n\n3. A new notebook is created and opened with the name Untitled.pynb. Click the notebook name at the top, and enter a friendly name.\n\n    ![Provide a name for the notebook](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Note.Jupyter.Notebook.Name.png \"Provide a name for the notebook\")\n\n4. Import the required modules and create the Spark and SQL contexts. Paste the following snippet in an empty cell, and then press **SHIFT + ENTER**.\n\n        from pyspark import SparkContext\n        from pyspark.sql import SQLContext\n        from pyspark.sql.types import *\n\n        # Create Spark and SQL contexts\n        sc = SparkContext('spark://headnodehost:7077', 'pyspark')\n        sqlContext = SQLContext(sc)\n\n    Everytime you run a job in Jupyter, your web browser window title will show a **(Busy)** status along with the notebook title. You will also see a solid circle next to the **Python 2** text in the top-right corner. After the job completes, this will change to a hollow circle.\n\n     ![Status of a Jupyter notebook job](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Jupyter.Job.Status.png \"Status of a Jupyter notebook job\")\n\n4. Load sample data into a temporary table. When you provision a Spark cluster in HDInsight, the sample data file, **hvac.csv**, is copied to the associated storage account under **\\HdiSamples\\SensorSampleData\\hvac**.\n\n    In an empty cell, paste the following snippet and press **SHIFT + ENTER**. This snippet registers the data into a temporary table called **hvac**.\n\n\n        # Load the data\n        hvacText = sc.textFile(\"wasb:///HdiSamples/SensorSampleData/hvac/HVAC.csv\")\n        \n        # Create the schema\n        hvacSchema = StructType([StructField(\"date\", StringType(), False),StructField(\"time\", StringType(), False),StructField(\"targettemp\", IntegerType(), False),StructField(\"actualtemp\", IntegerType(), False),StructField(\"buildingID\", StringType(), False)])\n        \n        # Parse the data in hvacText\n        hvac = hvacText.map(lambda s: s.split(\",\")).filter(lambda s: s[0] != \"Date\").map(lambda s:(str(s[0]), str(s[1]), int(s[2]), int(s[3]), str(s[6]) ))\n        \n        # Create a data frame\n        hvacdf = sqlContext.createDataFrame(hvac,hvacSchema)\n        \n        # Register the data fram as a table to run queries against\n        hvacdf.registerAsTable(\"hvac\")\n        \n        # Run queries against the table and display the data\n        data = sqlContext.sql(\"select buildingID, (targettemp - actualtemp) as temp_diff, date from hvac where date = \\\"6/1/13\\\"\")\n        data.show()\n\n5. Once the job completes successfully, the following output is displayed.\n\n        buildingID temp_diff date  \n        4          8         6/1/13\n        3          2         6/1/13\n        7          -10       6/1/13\n        12         3         6/1/13\n        7          9         6/1/13\n        7          5         6/1/13\n        3          11        6/1/13\n        8          -7        6/1/13\n        17         14        6/1/13\n        16         -3        6/1/13\n        8          -8        6/1/13\n        1          -1        6/1/13\n        12         11        6/1/13\n        3          14        6/1/13\n        6          -4        6/1/13\n        1          4         6/1/13\n        19         4         6/1/13\n        19         12        6/1/13\n        9          -9        6/1/13\n        15         -10       6/1/13\n\n6. Restart the kernel to exit the application. From the top menu bar, click **Kernel**, click **Restart**, and then click **Restart** again at the prompt.\n\n    ![Restart the Jupyter Kernel](./media/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql/HDI.Spark.Jupyter.Restart.Kernel.png \"Restart the Jupyter Kernel\")\n\n\n##<a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n* [Provision a Spark on HDInsight cluster](hdinsight-apache-spark-provision-clusters.md)\n* [Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n* [Use Spark in HDInsight for building machine learning applications](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n* [Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming.md)\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)\n\n\n[hdinsight-versions]: ../hdinsight-component-versioning/\n[hdinsight-upload-data]: ../hdinsight-upload-data/\n[hdinsight-storage]: ../hdinsight-use-blob-storage/\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n[azure-management-portal]: https://manage.windowsazure.com/\n[azure-create-storageaccount]: ../storage-create-storage-account/ \n\ntest\n"
}