{
  "nodes": [
    {
      "content": "Parallel Bulk Data Import Using SQL Partition Tables | Microsoft Azure",
      "pos": [
        28,
        98
      ]
    },
    {
      "content": "Parallel Bulk Data Import Using SQL Partition Tables",
      "pos": [
        118,
        170
      ]
    },
    {
      "content": "Parallel Bulk Data Import Using SQL Partition Tables",
      "pos": [
        525,
        577
      ]
    },
    {
      "content": "For big data loading/transfer to an SQL database, importing data to the SQL DB and subsequent queries can be improved by using <bpt id=\"p1\">_</bpt>Partitioned Tables and Views<ept id=\"p1\">_</ept>.",
      "pos": [
        579,
        737
      ]
    },
    {
      "content": "This document describes how to build partitioned table(s) for fast parallel bluk importing of data to a SQL Server database.",
      "pos": [
        738,
        862
      ]
    },
    {
      "content": "Create a new database and a set of filegroups",
      "pos": [
        868,
        913
      ]
    },
    {
      "pos": [
        917,
        1009
      ],
      "content": "<bpt id=\"p1\">[</bpt>Create a new database<ept id=\"p1\">](https://technet.microsoft.com/library/ms176061.aspx)</ept> (if not exists)"
    },
    {
      "content": "Add database filegroups to the database which will hold the partitioned physical files",
      "pos": [
        1012,
        1098
      ]
    },
    {
      "pos": [
        1101,
        1303
      ],
      "content": "Note: This can be done with <bpt id=\"p1\">[</bpt>CREATE DATABASE<ept id=\"p1\">](https://technet.microsoft.com/library/ms176061.aspx)</ept> if new or <bpt id=\"p2\">[</bpt>ALTER DATABASE<ept id=\"p2\">](https://msdn.microsoft.com/library/bb522682.aspx)</ept> if database exists already"
    },
    {
      "content": "Add one or more files (as needed) to each database filegroup",
      "pos": [
        1307,
        1367
      ]
    },
    {
      "pos": [
        1372,
        1533
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Specify the target filegroup which will hold data for this partition and the physical database file name(s) where the filegroup data will be stored."
    },
    {
      "content": "The following example creates a new database with three filegroups other than the primary and log groups, containing one physical file in each.",
      "pos": [
        1536,
        1679
      ]
    },
    {
      "content": "The database files are created in the default SQL Server Data folder, as configured in the SQL Server instance.",
      "pos": [
        1680,
        1791
      ]
    },
    {
      "content": "For more information about the default file locations, see <bpt id=\"p1\">[</bpt>File Locations for Default and Named Instances of SQL Server<ept id=\"p1\">](https://msdn.microsoft.com/library/ms143547.aspx)</ept>.",
      "pos": [
        1792,
        1964
      ]
    },
    {
      "content": "Create a partitioned table",
      "pos": [
        3036,
        3062
      ]
    },
    {
      "content": "Create partitioned table(s) according to the data schema, mapped to the database filegroups created in the previous step.",
      "pos": [
        3064,
        3185
      ]
    },
    {
      "content": "When data is bulk imported to the partitioned table(s), records will be distributed among the filegroups according to a partition scheme, as described below.",
      "pos": [
        3186,
        3343
      ]
    },
    {
      "content": "To create a partition table, you need to:",
      "pos": [
        3347,
        3388
      ]
    },
    {
      "pos": [
        3394,
        3645
      ],
      "content": "<bpt id=\"p1\">[</bpt>Create a partition function<ept id=\"p1\">](https://msdn.microsoft.com/library/ms187802.aspx)</ept> which defines the range of values/boundaries to be included in each individual partition table, e.g., to limit partitions by month(some\\_datetime\\_field) in the year 2013:"
    },
    {
      "pos": [
        3928,
        4094
      ],
      "content": "<bpt id=\"p1\">[</bpt>Create a partition scheme<ept id=\"p1\">](https://msdn.microsoft.com/library/ms179854.aspx)</ept> which maps each partition range in the partition function to a physical filegroup, e.g.:"
    },
    {
      "content": "Tip: To verify the ranges in effect in each partition according to the function/scheme, run the following query:",
      "pos": [
        4409,
        4521
      ]
    },
    {
      "pos": [
        4918,
        5117
      ],
      "content": "<bpt id=\"p1\">[</bpt>Create partitioned table<ept id=\"p1\">](https://msdn.microsoft.com/library/ms174979.aspx)</ept>(s) according to your data schema, and specify the partition scheme and constraint field used to partition the table, e.g.:"
    },
    {
      "pos": [
        5238,
        5354
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Create Partitioned Tables and Indexes<ept id=\"p1\">](https://msdn.microsoft.com/library/ms188730.aspx)</ept>."
    },
    {
      "content": "Bulk import the data for each individual partition table",
      "pos": [
        5359,
        5415
      ]
    },
    {
      "content": "You may use BCP, BULK INSERT, or other methods such as <bpt id=\"p1\">[</bpt>SQL Server Migration Wizard<ept id=\"p1\">](http://sqlazuremw.codeplex.com/)</ept>.",
      "pos": [
        5419,
        5537
      ]
    },
    {
      "content": "The example provided uses the BCP method.",
      "pos": [
        5538,
        5579
      ]
    },
    {
      "pos": [
        5583,
        5744
      ],
      "content": "<bpt id=\"p1\">[</bpt>Alter the database<ept id=\"p1\">](https://msdn.microsoft.com/library/bb522682.aspx)</ept> to change transaction logging scheme to BULK_LOGGED to minimize overhead of logging, e.g.:"
    },
    {
      "content": "To expedite data loading, launch the bulk import operations in parallel.",
      "pos": [
        5813,
        5885
      ]
    },
    {
      "content": "For tips on expediting bulk importing of big data into SQL Server databases, see <bpt id=\"p1\">[</bpt>Load 1TB in less than 1 hour<ept id=\"p1\">](http://blogs.msdn.com/b/sqlcat/archive/2006/05/19/602142.aspx)</ept>.",
      "pos": [
        5886,
        6061
      ]
    },
    {
      "content": "The following PowerShell script is an example of parallel data loading using BCP.",
      "pos": [
        6063,
        6144
      ]
    },
    {
      "content": "Create indexes to optimize joins and query performance",
      "pos": [
        8801,
        8855
      ]
    },
    {
      "content": "If you will extract data for modeling from multiple tables, create indexes on the join keys to improve the join performance.",
      "pos": [
        8859,
        8983
      ]
    },
    {
      "pos": [
        8987,
        9144
      ],
      "content": "<bpt id=\"p1\">[</bpt>Create indexes<ept id=\"p1\">](https://technet.microsoft.com/library/ms188783.aspx)</ept> (clustered or non-clustered) targeting the same filegroup for each partition, for e.g.:"
    },
    {
      "content": "or,",
      "pos": [
        9282,
        9285
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You may choose to create the indexes before bulk importing the data.",
      "pos": [
        9417,
        9498
      ]
    },
    {
      "content": "Index creation before bulk importing will slow down the data loading.",
      "pos": [
        9499,
        9568
      ]
    },
    {
      "content": "Advanced Analytics Process and Technology in Action Example",
      "pos": [
        9574,
        9633
      ]
    },
    {
      "pos": [
        9635,
        9890
      ],
      "content": "For an end-to-end walkthrough example using the Advanced Analytics Process and Technology (ADAPT) with a public dataset, see <bpt id=\"p1\">[</bpt>Advanced Analytics Process and Technology in Action: using SQL Server<ept id=\"p1\">](machine-learning-data-science-process-sql-walkthrough.md)</ept>."
    },
    {
      "content": "test",
      "pos": [
        9893,
        9897
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Parallel Bulk Data Import Using SQL Partition Tables | Microsoft Azure\" \n    description=\"Parallel Bulk Data Import Using SQL Partition Tables\" \n    services=\"machine-learning\" \n    solutions=\"\" \n    documentationCenter=\"\" \n    authors=\"msolhab\"\n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"09/01/2015\" \n    ms.author=\"msolhab\" /> \n\n# Parallel Bulk Data Import Using SQL Partition Tables\n\nFor big data loading/transfer to an SQL database, importing data to the SQL DB and subsequent queries can be improved by using _Partitioned Tables and Views_. This document describes how to build partitioned table(s) for fast parallel bluk importing of data to a SQL Server database.\n\n\n## Create a new database and a set of filegroups\n\n- [Create a new database](https://technet.microsoft.com/library/ms176061.aspx) (if not exists)\n- Add database filegroups to the database which will hold the partitioned physical files\n- Note: This can be done with [CREATE DATABASE](https://technet.microsoft.com/library/ms176061.aspx) if new or [ALTER DATABASE](https://msdn.microsoft.com/library/bb522682.aspx) if database exists already\n\n- Add one or more files (as needed) to each database filegroup\n\n > [AZURE.NOTE] Specify the target filegroup which will hold data for this partition and the physical database file name(s) where the filegroup data will be stored.\n \nThe following example creates a new database with three filegroups other than the primary and log groups, containing one physical file in each. The database files are created in the default SQL Server Data folder, as configured in the SQL Server instance. For more information about the default file locations, see [File Locations for Default and Named Instances of SQL Server](https://msdn.microsoft.com/library/ms143547.aspx).\n\n    DECLARE @data_path nvarchar(256);\n    SET @data_path = (SELECT SUBSTRING(physical_name, 1, CHARINDEX(N'master.mdf', LOWER(physical_name)) - 1)\n      FROM master.sys.master_files\n      WHERE database_id = 1 AND file_id = 1);\n    \n    EXECUTE ('\n        CREATE DATABASE <database_name>\n        ON  PRIMARY \n        ( NAME = ''Primary'', FILENAME = ''' + @data_path + '<primary_file_name>.mdf'', SIZE = 4096KB , FILEGROWTH = 1024KB ), \n        FILEGROUP [filegroup_1] \n        ( NAME = ''FileGroup1'', FILENAME = ''' + @data_path + '<file_name_1>.ndf'' , SIZE = 4096KB , FILEGROWTH = 1024KB ), \n        FILEGROUP [filegroup_2] \n        ( NAME = ''FileGroup1'', FILENAME = ''' + @data_path + '<file_name_2>.ndf'' , SIZE = 4096KB , FILEGROWTH = 1024KB ), \n        FILEGROUP [filegroup_3] \n        ( NAME = ''FileGroup1'', FILENAME = ''' + @data_path + '<file_name>.ndf'' , SIZE = 102400KB , FILEGROWTH = 10240KB ), \n        LOG ON \n        ( NAME = ''LogFileGroup'', FILENAME = ''' + @data_path + '<log_file_name>.ldf'' , SIZE = 1024KB , FILEGROWTH = 10%)\n    ')\n    \n## Create a partitioned table\n\nCreate partitioned table(s) according to the data schema, mapped to the database filegroups created in the previous step. When data is bulk imported to the partitioned table(s), records will be distributed among the filegroups according to a partition scheme, as described below.\n\n**To create a partition table, you need to:**\n\n- [Create a partition function](https://msdn.microsoft.com/library/ms187802.aspx) which defines the range of values/boundaries to be included in each individual partition table, e.g., to limit partitions by month(some\\_datetime\\_field) in the year 2013:\n\n        CREATE PARTITION FUNCTION <DatetimeFieldPFN>(<datetime_field>)  \n        AS RANGE RIGHT FOR VALUES (\n            '20130201', '20130301', '20130401',\n            '20130501', '20130601', '20130701', '20130801',\n            '20130901', '20131001', '20131101', '20131201' )\n\n- [Create a partition scheme](https://msdn.microsoft.com/library/ms179854.aspx) which maps each partition range in the partition function to a physical filegroup, e.g.:\n\n        CREATE PARTITION SCHEME <DatetimeFieldPScheme> AS  \n        PARTITION <DatetimeFieldPFN> TO (\n        <filegroup_1>, <filegroup_2>, <filegroup_3>, <filegroup_4>,\n        <filegroup_5>, <filegroup_6>, <filegroup_7>, <filegroup_8>,\n        <filegroup_9>, <filegroup_10>, <filegroup_11>, <filegroup_12> )\n\n- Tip: To verify the ranges in effect in each partition according to the function/scheme, run the following query:\n\n        SELECT psch.name as PartitionScheme,\n            prng.value AS ParitionValue,\n            prng.boundary_id AS BoundaryID\n        FROM sys.partition_functions AS pfun\n        INNER JOIN sys.partition_schemes psch ON pfun.function_id = psch.function_id\n        INNER JOIN sys.partition_range_values prng ON prng.function_id=pfun.function_id\n        WHERE pfun.name = <DatetimeFieldPFN>\n\n- [Create partitioned table](https://msdn.microsoft.com/library/ms174979.aspx)(s) according to your data schema, and specify the partition scheme and constraint field used to partition the table, e.g.:\n\n        CREATE TABLE <table_name> ( [include schema definition here] )\n        ON <TablePScheme>(<partition_field>)\n\n- For more information, see [Create Partitioned Tables and Indexes](https://msdn.microsoft.com/library/ms188730.aspx).\n\n## Bulk import the data for each individual partition table\n\n- You may use BCP, BULK INSERT, or other methods such as [SQL Server Migration Wizard](http://sqlazuremw.codeplex.com/). The example provided uses the BCP method.\n\n- [Alter the database](https://msdn.microsoft.com/library/bb522682.aspx) to change transaction logging scheme to BULK_LOGGED to minimize overhead of logging, e.g.:\n\n        ALTER DATABASE <database_name> SET RECOVERY BULK_LOGGED\n\n- To expedite data loading, launch the bulk import operations in parallel. For tips on expediting bulk importing of big data into SQL Server databases, see [Load 1TB in less than 1 hour](http://blogs.msdn.com/b/sqlcat/archive/2006/05/19/602142.aspx).\n\nThe following PowerShell script is an example of parallel data loading using BCP.\n\n    # Set database name, input data directory, and output log directory\n    # This example loads comma-separated input data files\n    # The example assumes the partitioned data files are named as <base_file_name>_<partition_number>.csv\n    # Assumes the input data files include a header line. Loading starts at line number 2.\n\n    $dbname = \"<database_name>\"\n    $indir  = \"<path_to_data_files>\"\n    $logdir = \"<path_to_log_directory>\"\n\n    # Select authentication mode\n    $sqlauth = 0\n    \n    # For SQL authentication, set the server and user credentials\n    $sqlusr = \"<user@server>\"\n    $server = \"<tcp:serverdns>\"\n    $pass   = \"<password>\"\n\n    # Set number of partitions per table - Should match the number of input data files per table\n    $numofparts = <number_of_partitions>\n       \n    # Set table name to be loaded, basename of input data files, input format file, and number of partitions\n    $tbname = \"<table_name>\"\n    $basename = \"<base_input_data_filename_no_extension>\"\n    $fmtfile = \"<full_path_to_format_file>\"\n   \n    # Create log directory if it does not exist\n    New-Item -ErrorAction Ignore -ItemType directory -Path $logdir\n      \n    # BCP example using Windows authentication\n    $ScriptBlock1 = {\n       param($dbname, $tbname, $basename, $fmtfile, $indir, $logdir, $num)\n       bcp ($dbname + \"..\" + $tbname) in ($indir + \"\\\" + $basename + \"_\" + $num + \".csv\") -o ($logdir + \"\\\" + $tbname + \"_\" + $num + \".txt\") -h \"TABLOCK\" -F 2 -C \"RAW\" -f ($fmtfile) -T -b 2500 -t \",\" -r \\n\n    }\n    \n    # BCP example using SQL authentication\n    $ScriptBlock2 = {\n       param($dbname, $tbname, $basename, $fmtfile, $indir, $logdir, $num, $sqlusr, $server, $pass)\n       bcp ($dbname + \"..\" + $tbname) in ($indir + \"\\\" + $basename + \"_\" + $num + \".csv\") -o ($logdir + \"\\\" + $tbname + \"_\" + $num + \".txt\") -h \"TABLOCK\" -F 2 -C \"RAW\" -f ($fmtfile) -U $sqlusr -S $server -P $pass -b 2500 -t \",\" -r \\n\n    }\n    \n    # Background processing of all partitions\n    for ($i=1; $i -le $numofparts; $i++)\n    {\n       Write-Output \"Submit loading trip and fare partitions # $i\"\n       if ($sqlauth -eq 0) {\n          # Use Windows authentication\n          Start-Job -ScriptBlock $ScriptBlock1 -Arg ($dbname, $tbname, $basename, $fmtfile, $indir, $logdir, $i)\n       } \n       else {\n          # Use SQL authentication\n          Start-Job -ScriptBlock $ScriptBlock2 -Arg ($dbname, $tbname, $basename, $fmtfile, $indir, $logdir, $i, $sqlusr, $server, $pass)\n       }\n    }\n    \n    Get-Job\n    \n    # Optional - Wait till all jobs complete and report date and time\n    date\n    While (Get-Job -State \"Running\") { Start-Sleep 10 }\n    date\n\n## Create indexes to optimize joins and query performance\n\n- If you will extract data for modeling from multiple tables, create indexes on the join keys to improve the join performance.\n\n- [Create indexes](https://technet.microsoft.com/library/ms188783.aspx) (clustered or non-clustered) targeting the same filegroup for each partition, for e.g.:\n\n        CREATE CLUSTERED INDEX <table_idx> ON <table_name>( [include index columns here] )\n        ON <TablePScheme>(<partition)field>)\nor,\n\n        CREATE INDEX <table_idx> ON <table_name>( [include index columns here] )\n        ON <TablePScheme>(<partition)field>)\n\n > [AZURE.NOTE] You may choose to create the indexes before bulk importing the data. Index creation before bulk importing will slow down the data loading.\n\n### Advanced Analytics Process and Technology in Action Example\n\nFor an end-to-end walkthrough example using the Advanced Analytics Process and Technology (ADAPT) with a public dataset, see [Advanced Analytics Process and Technology in Action: using SQL Server](machine-learning-data-science-process-sql-walkthrough.md).\n \ntest\n"
}