{
  "nodes": [
    {
      "content": "PolyBase in SQL Data Warehouse Tutorial | Microsoft Azure",
      "pos": [
        26,
        83
      ]
    },
    {
      "content": "Learn what PolyBase is and how to use it for data warehousing scenarios.",
      "pos": [
        101,
        173
      ]
    },
    {
      "content": "Load data with PolyBase",
      "pos": [
        505,
        528
      ]
    },
    {
      "content": "PolyBase technology allows you to query and join data from multiple sources, all by using Transact-SQL commands.",
      "pos": [
        529,
        641
      ]
    },
    {
      "content": "Using PolyBase, you can query data stored in Azure blob storage and load it into SQL Data",
      "pos": [
        644,
        733
      ]
    },
    {
      "content": "Warehouse database with the following steps:",
      "pos": [
        735,
        779
      ]
    },
    {
      "content": "Create database master key and credential.",
      "pos": [
        783,
        825
      ]
    },
    {
      "content": "Create PolyBase objects: external data source, external file format, and external table.",
      "pos": [
        828,
        916
      ]
    },
    {
      "content": "Query data stored in Azure blob storage.",
      "pos": [
        920,
        960
      ]
    },
    {
      "content": "Load data from Azure blob storage into SQL Data Warehouse.",
      "pos": [
        963,
        1021
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        1027,
        1040
      ]
    },
    {
      "content": "To step through this tutorial, you need:",
      "pos": [
        1041,
        1081
      ]
    },
    {
      "content": "Azure storage account",
      "pos": [
        1085,
        1106
      ]
    },
    {
      "content": "Your data stored in Azure blob storage as delimited text files",
      "pos": [
        1109,
        1171
      ]
    },
    {
      "content": "First, you will create the objects that PolyBase requires for connecting to and querying data in Azure blob storage.",
      "pos": [
        1173,
        1289
      ]
    },
    {
      "content": "Create database master key",
      "pos": [
        1294,
        1320
      ]
    },
    {
      "content": "Connect to user database on your server to create a database master key.",
      "pos": [
        1321,
        1393
      ]
    },
    {
      "content": "This key is used to encrypt your credential secret in the next step.",
      "pos": [
        1394,
        1462
      ]
    },
    {
      "pos": [
        1516,
        1570
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>CREATE MASTER KEY (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Create a database scoped credential",
      "pos": [
        1575,
        1610
      ]
    },
    {
      "content": "To access Azure blob storage, you need to create a database scoped credential that stores authentication information for your Azure storage account.",
      "pos": [
        1611,
        1759
      ]
    },
    {
      "content": "Connect to your data warehouse database and create a database scoped credential for each Azure storage account you want to access.",
      "pos": [
        1760,
        1890
      ]
    },
    {
      "content": "Specify an identity name and your Azure storage account key as the Secret.",
      "pos": [
        1891,
        1965
      ]
    },
    {
      "content": "The identity name does not affect authentication to Azure Storage.",
      "pos": [
        1966,
        2032
      ]
    },
    {
      "content": "To see if a database-scoped credential already exists, use   sys.database_credentials, not sys.credentials which only shows the server credentials.",
      "pos": [
        2034,
        2181
      ]
    },
    {
      "pos": [
        2436,
        2490
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>CREATE CREDENTIAL (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "To drop a database scoped credential you simply use the following syntax:",
      "pos": [
        2492,
        2565
      ]
    },
    {
      "pos": [
        2643,
        2695
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>DROP CREDENTIAL (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Create an external data source",
      "pos": [
        2700,
        2730
      ]
    },
    {
      "content": "The external data source is a database object that stores the location of the Azure blob storage data and your access information.",
      "pos": [
        2731,
        2861
      ]
    },
    {
      "content": "You need to define an external data source for each Azure Storage container you want to access.",
      "pos": [
        2862,
        2957
      ]
    },
    {
      "pos": [
        3191,
        3255
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>CREATE EXTERNAL DATA SOURCE (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "To drop the external data source the syntax is as follows:",
      "pos": [
        3257,
        3315
      ]
    },
    {
      "pos": [
        3401,
        3463
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>DROP EXTERNAL DATA SOURCE (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Create an external file format",
      "pos": [
        3468,
        3498
      ]
    },
    {
      "content": "The external file format is a database object that specifies the format of the external data.",
      "pos": [
        3499,
        3592
      ]
    },
    {
      "content": "In this example, we have uncompressed data in a text file and the fields are separated with the pipe character ('|').",
      "pos": [
        3593,
        3710
      ]
    },
    {
      "content": "PolyBase can work with compressed and uncompressed data in delimited text, Hive RCFILE and HIVE ORC formats.",
      "pos": [
        4009,
        4117
      ]
    },
    {
      "pos": [
        4120,
        4184
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>CREATE EXTERNAL FILE FORMAT (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "To drop an external file format the syntax is as follows:",
      "pos": [
        4186,
        4243
      ]
    },
    {
      "pos": [
        4334,
        4396
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>DROP EXTERNAL FILE FORMAT (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Create an external table",
      "pos": [
        4401,
        4425
      ]
    },
    {
      "content": "The external table definition is similar to a relational table definition.",
      "pos": [
        4427,
        4501
      ]
    },
    {
      "content": "The key difference is the location and the format of the data.",
      "pos": [
        4502,
        4564
      ]
    },
    {
      "content": "The external table definition is stored in the SQL Data Warehouse database.",
      "pos": [
        4565,
        4640
      ]
    },
    {
      "content": "The data is stored in the location specified by the data source.",
      "pos": [
        4641,
        4705
      ]
    },
    {
      "content": "The LOCATION option specifies the path to the data from the root of the data source.",
      "pos": [
        4707,
        4791
      ]
    },
    {
      "content": "In this example, the data is located at 'wasbs://mycontainer@ test.blob.core.windows.net/path/Demo/'.",
      "pos": [
        4792,
        4893
      ]
    },
    {
      "content": "All the files for the same table need to be under the same logical folder in Azure BLOB.",
      "pos": [
        4894,
        4982
      ]
    },
    {
      "pos": [
        5414,
        5507
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Please note that you cannot create statistics on an external table at this time."
    },
    {
      "pos": [
        5509,
        5567
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>CREATE EXTERNAL TABLE (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "The objects you just created are stored in SQL Data Warehouse database.",
      "pos": [
        5569,
        5640
      ]
    },
    {
      "content": "You can view them in the SQL Server Data Tools (SSDT) Object Explorer.",
      "pos": [
        5641,
        5711
      ]
    },
    {
      "content": "To drop an external table you need to use the following syntax:",
      "pos": [
        5714,
        5777
      ]
    },
    {
      "pos": [
        5861,
        5973
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> When dropping an external table you must use <ph id=\"ph2\">`DROP EXTERNAL TABLE`</ph> you <bpt id=\"p1\">**</bpt>cannot<ept id=\"p1\">**</ept> use <ph id=\"ph3\">`DROP TABLE`</ph>."
    },
    {
      "pos": [
        5976,
        6032
      ],
      "content": "Reference topic: <bpt id=\"p1\">[</bpt>DROP EXTERNAL TABLE (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "pos": [
        6034,
        6173
      ],
      "content": "It is also worth noting that external tables are visible in both <ph id=\"ph1\">`sys.tables`</ph> and more specifically in <ph id=\"ph2\">`sys.external_tables`</ph> catalog views."
    },
    {
      "content": "Rotating storage keys",
      "pos": [
        6178,
        6199
      ]
    },
    {
      "content": "From time to time you will want to change the access key to your blob storage for security reasons.",
      "pos": [
        6201,
        6300
      ]
    },
    {
      "content": "The most elegant way to perform this task is to follow a process known as \"rotating the keys\".",
      "pos": [
        6303,
        6397
      ]
    },
    {
      "content": "You may have noticed that you have two storage keys for your blob storage account.",
      "pos": [
        6398,
        6480
      ]
    },
    {
      "content": "This is so that you can transition",
      "pos": [
        6481,
        6515
      ]
    },
    {
      "content": "Rotating your Azure storage account keys is a simple three step process",
      "pos": [
        6518,
        6589
      ]
    },
    {
      "content": "Create second database scoped credential based on the secondary storage access key",
      "pos": [
        6594,
        6676
      ]
    },
    {
      "content": "Create second external data source based off this new credential",
      "pos": [
        6680,
        6744
      ]
    },
    {
      "content": "Drop and create the external table(s) pointing to the new external data source",
      "pos": [
        6748,
        6826
      ]
    },
    {
      "content": "When you have migrated all your external tables to the new external data source then you can perform the clean up tasks:",
      "pos": [
        6828,
        6948
      ]
    },
    {
      "content": "Drop first external data source",
      "pos": [
        6954,
        6985
      ]
    },
    {
      "content": "Drop first database scoped credential based on the primary storage access key",
      "pos": [
        6989,
        7066
      ]
    },
    {
      "content": "Log into Azure and regenerate the primary access key ready for the next time",
      "pos": [
        7070,
        7146
      ]
    },
    {
      "content": "Query Azure blob storage data",
      "pos": [
        7151,
        7180
      ]
    },
    {
      "content": "Queries against external tables simply use the table name as though it was a relational table.",
      "pos": [
        7181,
        7275
      ]
    },
    {
      "content": "This is an ad-hoc query that joins insurance customer data stored in SQL Data Warehouse, with automobile sensor data stored in Azure storage blob.",
      "pos": [
        7278,
        7424
      ]
    },
    {
      "content": "The result shows the drivers that drive faster than others.",
      "pos": [
        7425,
        7484
      ]
    },
    {
      "content": "Load data from Azure blob storage",
      "pos": [
        7940,
        7973
      ]
    },
    {
      "content": "This example loads data from Azure blob storage to SQL Data Warehouse database.",
      "pos": [
        7974,
        8053
      ]
    },
    {
      "content": "Storing data directly removes the data transfer time for queries.",
      "pos": [
        8055,
        8120
      ]
    },
    {
      "content": "Storing data with a columnstore index improves query performance for analysis queries by up to 10x.",
      "pos": [
        8121,
        8220
      ]
    },
    {
      "content": "This example uses the CREATE TABLE AS SELECT statement to load data.",
      "pos": [
        8222,
        8290
      ]
    },
    {
      "content": "The new table inherits the columns named in the query.",
      "pos": [
        8291,
        8345
      ]
    },
    {
      "content": "It inherits the data types of those columns from the external table definition.",
      "pos": [
        8346,
        8425
      ]
    },
    {
      "content": "CREATE TABLE AS SELECT is a highly performant Transact-SQL statement  that replaces INSERT...SELECT.",
      "pos": [
        8428,
        8528
      ]
    },
    {
      "content": "It was originally developed for  the massively parallel processing (MPP) engine in Analytics Platform System and is now in SQL Data Warehouse.",
      "pos": [
        8530,
        8672
      ]
    },
    {
      "pos": [
        8927,
        8973
      ],
      "content": "See <bpt id=\"p1\">[</bpt>CREATE TABLE AS SELECT (Transact-SQL)<ept id=\"p1\">][]</ept>."
    },
    {
      "content": "Working around the PolyBase UTF-8 requirement",
      "pos": [
        8979,
        9024
      ]
    },
    {
      "content": "At present PolyBase supports loading data files that have been UTF-8 encoded.",
      "pos": [
        9025,
        9102
      ]
    },
    {
      "content": "As UTF-8 uses the same character encoding as ASCII PolyBase will also support loading data that is ASCII encoded.",
      "pos": [
        9103,
        9216
      ]
    },
    {
      "content": "However, PolyBase does not support other character encoding such as UTF-16 / Unicode or extended ASCII characters.",
      "pos": [
        9217,
        9331
      ]
    },
    {
      "content": "Extended ASCII includes characters with accents such as the umlaut which is common in German.",
      "pos": [
        9332,
        9425
      ]
    },
    {
      "content": "To work around this requirement the best answer is to re-write to UTF-8 encoding.",
      "pos": [
        9427,
        9508
      ]
    },
    {
      "content": "There are several ways to do this.",
      "pos": [
        9510,
        9544
      ]
    },
    {
      "content": "Below are two approaches using Powershell:",
      "pos": [
        9545,
        9587
      ]
    },
    {
      "content": "Simple example for small files",
      "pos": [
        9594,
        9624
      ]
    },
    {
      "content": "Below is a simple one line Powershell script that creates the file.",
      "pos": [
        9626,
        9693
      ]
    },
    {
      "content": "However, whilst this is a simple way to re-encode the data it is by no means the most efficient.",
      "pos": [
        9801,
        9897
      ]
    },
    {
      "content": "The io streaming example below is much, much faster and achieves the same result.",
      "pos": [
        9898,
        9979
      ]
    },
    {
      "content": "IO Streaming example for larger files",
      "pos": [
        9985,
        10022
      ]
    },
    {
      "content": "The code sample below is more complex but as it streams the rows of data from source to target it is much more efficient.",
      "pos": [
        10024,
        10145
      ]
    },
    {
      "content": "Use this approach for larger files.",
      "pos": [
        10146,
        10181
      ]
    },
    {
      "content": "Next steps",
      "pos": [
        11025,
        11035
      ]
    },
    {
      "pos": [
        11036,
        11092
      ],
      "content": "For more development tips, see <bpt id=\"p1\">[</bpt>development overview<ept id=\"p1\">][]</ept>."
    }
  ],
  "content": "<properties\n   pageTitle=\"PolyBase in SQL Data Warehouse Tutorial | Microsoft Azure\"\n   description=\"Learn what PolyBase is and how to use it for data warehousing scenarios.\"\n   services=\"sql-data-warehouse\"\n   documentationCenter=\"NA\"\n   authors=\"barbkess\"\n   manager=\"jhubbard\"\n   editor=\"jrowlandjones\"/>\n\n<tags\n   ms.service=\"sql-data-warehouse\"\n   ms.devlang=\"NA\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"NA\"\n   ms.workload=\"data-services\"\n   ms.date=\"05/09/2015\"\n   ms.author=\"sahajs;barbkess\"/>\n\n\n# Load data with PolyBase\nPolyBase technology allows you to query and join data from multiple sources, all by using Transact-SQL commands. \n\nUsing PolyBase, you can query data stored in Azure blob storage and load it into SQL Data \nWarehouse database with the following steps:\n\n- Create database master key and credential.\n- Create PolyBase objects: external data source, external file format, and external table. \n- Query data stored in Azure blob storage.\n- Load data from Azure blob storage into SQL Data Warehouse.\n\n\n## Prerequisites\nTo step through this tutorial, you need:\n\n- Azure storage account\n- Your data stored in Azure blob storage as delimited text files\n\nFirst, you will create the objects that PolyBase requires for connecting to and querying data in Azure blob storage.\n\n## Create database master key\nConnect to user database on your server to create a database master key. This key is used to encrypt your credential secret in the next step. \n\n```\n-- Creating master key\nCREATE MASTER KEY;\n```\n\nReference topic: [CREATE MASTER KEY (Transact-SQL)][].\n\n## Create a database scoped credential\nTo access Azure blob storage, you need to create a database scoped credential that stores authentication information for your Azure storage account. Connect to your data warehouse database and create a database scoped credential for each Azure storage account you want to access. Specify an identity name and your Azure storage account key as the Secret. The identity name does not affect authentication to Azure Storage.\n\nTo see if a database-scoped credential already exists, use   sys.database_credentials, not sys.credentials which only shows the server credentials.\n\n```\n-- Check for existing database-scoped credentials.\nSELECT * FROM sys.database_credentials;\n\n-- Create a database scoped credential\nCREATE DATABASE SCOPED CREDENTIAL ASBSecret \nWITH IDENTITY = 'joe'\n,    Secret = '<azure_storage_account_key>'\n;\n```\n\nReference topic: [CREATE CREDENTIAL (Transact-SQL)][].\n\nTo drop a database scoped credential you simply use the following syntax:\n\n```\n-- Dropping credential\nDROP DATABASE SCOPED CREDENTIAL ASBSecret\n;\n```\n\nReference topic: [DROP CREDENTIAL (Transact-SQL)][].\n\n## Create an external data source\nThe external data source is a database object that stores the location of the Azure blob storage data and your access information. You need to define an external data source for each Azure Storage container you want to access.\n\n```\n-- Creating external data source (Azure Blob Storage) \nCREATE EXTERNAL DATA SOURCE azure_storage \nWITH\n(\n    TYPE = HADOOP\n,   LOCATION ='wasbs://mycontainer@ test.blob.core.windows.net/path'\n,   CREDENTIAL = ASBSecret\n)\n;\n```\n\nReference topic: [CREATE EXTERNAL DATA SOURCE (Transact-SQL)][].\n\nTo drop the external data source the syntax is as follows:\n\n```\n-- Dropping external data source\nDROP EXTERNAL DATA SOURCE azure_storage\n;\n```\n\nReference topic: [DROP EXTERNAL DATA SOURCE (Transact-SQL)][].\n\n## Create an external file format\nThe external file format is a database object that specifies the format of the external data. In this example, we have uncompressed data in a text file and the fields are separated with the pipe character ('|'). \n\n```\n-- Creating external file format (delimited text file)\nCREATE EXTERNAL FILE FORMAT text_file_format \nWITH \n(   \n    FORMAT_TYPE = DELIMITEDTEXT \n,   FORMAT_OPTIONS  (\n                        FIELD_TERMINATOR ='|'\n                    ,   USE_TYPE_DEFAULT = TRUE\n                    )\n)\n;\n```\n\nPolyBase can work with compressed and uncompressed data in delimited text, Hive RCFILE and HIVE ORC formats. \n\nReference topic: [CREATE EXTERNAL FILE FORMAT (Transact-SQL)][].\n\nTo drop an external file format the syntax is as follows:\n\n```\n-- Dropping external file format...\nDROP EXTERNAL FILE FORMAT text_file_format\n;\n```\nReference topic: [DROP EXTERNAL FILE FORMAT (Transact-SQL)][].\n\n## Create an external table\n\nThe external table definition is similar to a relational table definition. The key difference is the location and the format of the data. The external table definition is stored in the SQL Data Warehouse database. The data is stored in the location specified by the data source.\n\nThe LOCATION option specifies the path to the data from the root of the data source. In this example, the data is located at 'wasbs://mycontainer@ test.blob.core.windows.net/path/Demo/'. All the files for the same table need to be under the same logical folder in Azure BLOB.\n\n```\n-- Creating external table pointing to file stored in Azure Storage\nCREATE EXTERNAL TABLE [ext].[CarSensor_Data] \n(\n     [SensorKey]     int    NOT NULL \n,    [CustomerKey]   int    NOT NULL \n,    [GeographyKey]  int        NULL \n,    [Speed]         float  NOT NULL \n,    [YearMeasured]  int    NOT NULL\n)\nWITH \n(\n    LOCATION    = '/Demo/'\n,   DATA_SOURCE = azure_storage\n,   FILE_FORMAT = text_file_format      \n)\n;\n```\n\n> [AZURE.NOTE] Please note that you cannot create statistics on an external table at this time.\n\nReference topic: [CREATE EXTERNAL TABLE (Transact-SQL)][].\n\nThe objects you just created are stored in SQL Data Warehouse database. You can view them in the SQL Server Data Tools (SSDT) Object Explorer. \n\nTo drop an external table you need to use the following syntax:\n\n```\n--Dropping external table\nDROP EXTERNAL TABLE [ext].[CarSensor_Data]\n;\n```\n\n> [AZURE.NOTE] When dropping an external table you must use `DROP EXTERNAL TABLE` you **cannot** use `DROP TABLE`. \n\nReference topic: [DROP EXTERNAL TABLE (Transact-SQL)][].\n\nIt is also worth noting that external tables are visible in both `sys.tables` and more specifically in `sys.external_tables` catalog views.\n\n## Rotating storage keys\n\nFrom time to time you will want to change the access key to your blob storage for security reasons. \n\nThe most elegant way to perform this task is to follow a process known as \"rotating the keys\". You may have noticed that you have two storage keys for your blob storage account. This is so that you can transition \n\nRotating your Azure storage account keys is a simple three step process\n\n1. Create second database scoped credential based on the secondary storage access key\n2. Create second external data source based off this new credential\n3. Drop and create the external table(s) pointing to the new external data source\n\nWhen you have migrated all your external tables to the new external data source then you can perform the clean up tasks:\n \n1. Drop first external data source\n2. Drop first database scoped credential based on the primary storage access key\n3. Log into Azure and regenerate the primary access key ready for the next time\n\n## Query Azure blob storage data\nQueries against external tables simply use the table name as though it was a relational table. \n\nThis is an ad-hoc query that joins insurance customer data stored in SQL Data Warehouse, with automobile sensor data stored in Azure storage blob. The result shows the drivers that drive faster than others.\n\n```\n-- Join SQL Data Warehouse relational data with Azure storage data. \nSELECT \n      [Insured_Customers].[FirstName]\n,     [Insured_Customers].[LastName]\n,     [Insured_Customers].[YearlyIncome]\n,     [CarSensor_Data].[Speed]\nFROM  [dbo].[Insured_Customers] \nJOIN  [ext].[CarSensor_Data]         ON [Insured_Customers].[CustomerKey] = [CarSensor_Data].[CustomerKey]\nWHERE [CarSensor_Data].[Speed] > 60 \nORDER BY [CarSensor_Data].[Speed] DESC\n;\n```\n\n## Load data from Azure blob storage\nThis example loads data from Azure blob storage to SQL Data Warehouse database.\n\nStoring data directly removes the data transfer time for queries. Storing data with a columnstore index improves query performance for analysis queries by up to 10x.\n\nThis example uses the CREATE TABLE AS SELECT statement to load data. The new table inherits the columns named in the query. It inherits the data types of those columns from the external table definition. \n\nCREATE TABLE AS SELECT is a highly performant Transact-SQL statement  that replaces INSERT...SELECT.  It was originally developed for  the massively parallel processing (MPP) engine in Analytics Platform System and is now in SQL Data Warehouse.\n\n```\n-- Load data from Azure blob storage to SQL Data Warehouse \n\nCREATE TABLE [dbo].[Customer_Speed]\nWITH \n(   \n    CLUSTERED COLUMNSTORE INDEX\n,   DISTRIBUTION = HASH([CarSensor_Data].[CustomerKey])\n)\nAS \nSELECT * \nFROM   [ext].[CarSensor_Data]\n;\n```\n\nSee [CREATE TABLE AS SELECT (Transact-SQL)][].\n\n\n## Working around the PolyBase UTF-8 requirement\nAt present PolyBase supports loading data files that have been UTF-8 encoded. As UTF-8 uses the same character encoding as ASCII PolyBase will also support loading data that is ASCII encoded. However, PolyBase does not support other character encoding such as UTF-16 / Unicode or extended ASCII characters. Extended ASCII includes characters with accents such as the umlaut which is common in German.\n\nTo work around this requirement the best answer is to re-write to UTF-8 encoding.\n\nThere are several ways to do this. Below are two approaches using Powershell: \n\n### Simple example for small files\n\nBelow is a simple one line Powershell script that creates the file.\n \n```\nGet-Content <input_file_name> -Encoding Unicode | Set-Content <output_file_name> -Encoding utf8\n```\n\nHowever, whilst this is a simple way to re-encode the data it is by no means the most efficient. The io streaming example below is much, much faster and achieves the same result.\n\n### IO Streaming example for larger files\n\nThe code sample below is more complex but as it streams the rows of data from source to target it is much more efficient. Use this approach for larger files.\n\n```\n#Static variables\n$ascii = [System.Text.Encoding]::ASCII\n$utf16le = [System.Text.Encoding]::Unicode\n$utf8 = [System.Text.Encoding]::UTF8\n$ansi = [System.Text.Encoding]::Default\n$append = $False\n\n#Set source file path and file name\n$src = [System.IO.Path]::Combine(\"C:\\input_file_path\\\",\"input_file_name.txt\")\n\n#Set source file encoding (using list above)\n$src_enc = $ansi\n\n#Set target file path and file name\n$tgt = [System.IO.Path]::Combine(\"C:\\output_file_path\\\",\"output_file_name.txt\")\n\n#Set target file encoding (using list above)\n$tgt_enc = $utf8\n\n$read = New-Object System.IO.StreamReader($src,$src_enc)\n$write = New-Object System.IO.StreamWriter($tgt,$append,$tgt_enc)\n\nwhile ($read.Peek() -ne -1)\n{\n    $line = $read.ReadLine();\n    $write.WriteLine($line);\n}\n$read.Close()\n$read.Dispose()\n$write.Close()\n$write.Dispose()\n```\n\n## Next steps\nFor more development tips, see [development overview][].\n\n<!--Image references-->\n\n<!--Article references-->\n[Load data with bcp]: sql-data-warehouse-load-with-bcp.md\n[Load with PolyBase]: sql-data-warehouse-load-with-polybase.md\n[solution partners]: sql-data-warehouse-solution-partners.md\n[development overview]: sql-data-warehouse-overview-develop.md\n\n<!--MSDN references-->\n[supported source/sink]: https://msdn.microsoft.com/library/dn894007.aspx\n[copy activity]: https://msdn.microsoft.com/library/dn835035.aspx\n[SQL Server destination adapter]: https://msdn.microsoft.com/library/ms141095.aspx\n[SSIS]: https://msdn.microsoft.com/library/ms141026.aspx\n\n\n<!-- External Links -->\n[CREATE EXTERNAL DATA SOURCE (Transact-SQL)]:https://msdn.microsoft.com/library/dn935022(v=sql.130).aspx\n[CREATE EXTERNAL FILE FORMAT (Transact-SQL)]:https://msdn.microsoft.com/library/dn935026(v=sql.130).aspx\n[CREATE EXTERNAL TABLE (Transact-SQL)]:https://msdn.microsoft.com/library/dn935021(v=sql.130).aspx\n\n[DROP EXTERNAL DATA SOURCE (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/mt146367.aspx\n[DROP EXTERNAL FILE FORMAT (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/mt146379.aspx\n[DROP EXTERNAL TABLE (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/mt130698.aspx\n\n[CREATE TABLE AS SELECT (Transact-SQL)]:https://msdn.microsoft.com/library/mt204041.aspx\n[CREATE MASTER KEY (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/ms174382.aspx\n[CREATE CREDENTIAL (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/ms189522.aspx\n[DROP CREDENTIAL (Transact-SQL)]:https://msdn.microsoft.com/en-us/library/ms189450.aspx\n\n\n"
}