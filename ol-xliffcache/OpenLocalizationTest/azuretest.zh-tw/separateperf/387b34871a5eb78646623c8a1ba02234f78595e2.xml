{
  "nodes": [
    {
      "content": "Introduction to Apache Storm on HDInsight | Microsoft Azure",
      "pos": [
        27,
        86
      ]
    },
    {
      "content": "Get an introduction to Apache Storm, and learn how you can use Storm on HDInsight to build real-time data analytics solutions in the cloud.",
      "pos": [
        105,
        244
      ]
    },
    {
      "content": "Introduction to Apache Storm on HDInsight: Real-time analytics for Hadoop",
      "pos": [
        566,
        639
      ]
    },
    {
      "pos": [
        641,
        807
      ],
      "content": "Apache Storm on HDInsight allows you to create distributed, real-time analytics solutions in the Azure environment by using <bpt id=\"p1\">[</bpt>Apache Hadoop<ept id=\"p1\">](http://hadoop.apache.org)</ept>."
    },
    {
      "content": "What is Apache Storm?",
      "pos": [
        811,
        832
      ]
    },
    {
      "content": "Apache Storm is a distributed, fault-tolerant, open-source computation system that allows you to process data in real-time with Hadoop.",
      "pos": [
        834,
        969
      ]
    },
    {
      "content": "Storm solutions can also provide guaranteed processing of data, with the ability to replay data that was not successfully processed the first time.",
      "pos": [
        970,
        1117
      ]
    },
    {
      "content": "Why use Storm on HDInsight?",
      "pos": [
        1121,
        1148
      ]
    },
    {
      "content": "Apache Storm on HDInsight is a managed cluster integrated into the Azure environment.",
      "pos": [
        1150,
        1235
      ]
    },
    {
      "content": "It provides the following key benefits:",
      "pos": [
        1236,
        1275
      ]
    },
    {
      "content": "Performs as a managed service with an SLA of 99.9% up time",
      "pos": [
        1279,
        1337
      ]
    },
    {
      "pos": [
        1341,
        1455
      ],
      "content": "Use the language of your choice: Provides support for Storm components written in <bpt id=\"p1\">**</bpt>Java<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>C#<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>Python<ept id=\"p3\">**</ept>"
    },
    {
      "content": "Supports a mix of programming languages: Read data using Java, then process it using C",
      "pos": [
        1463,
        1549
      ]
    },
    {
      "pos": [
        1558,
        1758
      ],
      "content": "Use the <bpt id=\"p1\">**</bpt>Trident<ept id=\"p1\">**</ept> Java interface to create Storm topologies that support \"exactly once\" processing of messages, \"transactional\" datastore persistence, and a set of common stream analytics operations"
    },
    {
      "content": "Includes built-in scale-up and scale-down features: Scale an HDInsight cluster with no impact to running Storm topologies",
      "pos": [
        1762,
        1883
      ]
    },
    {
      "content": "Integrate with other Azure services, including Event Hub, Azure Virtual Network, SQL Database, Blob storage, and DocumentDB",
      "pos": [
        1887,
        2010
      ]
    },
    {
      "content": "Combine the capabilities of multiple HDInsight clusters by using Azure Virtual Network: Create analytic pipelines that use HDInsight, HBase, or Hadoop clusters",
      "pos": [
        2018,
        2177
      ]
    },
    {
      "pos": [
        2179,
        2363
      ],
      "content": "For a list of companies that are using Apache Storm for their real-time analytics solutions, see <bpt id=\"p1\">[</bpt>Companies Using Apache Storm<ept id=\"p1\">](https://storm.apache.org/documentation/Powered-By.html)</ept>."
    },
    {
      "pos": [
        2365,
        2451
      ],
      "content": "To get started using Storm, see <bpt id=\"p1\">[</bpt>Get started with Storm on HDInsight<ept id=\"p1\">][gettingstarted]</ept>."
    },
    {
      "content": "Ease of provisioning",
      "pos": [
        2456,
        2476
      ]
    },
    {
      "content": "You can provision a new Storm on HDInsight cluster in minutes.",
      "pos": [
        2478,
        2540
      ]
    },
    {
      "content": "Specify the cluster name, size, administrator account, and the storage account.",
      "pos": [
        2541,
        2620
      ]
    },
    {
      "content": "Azure will create the cluster, including sample topologies and a web-management dashboard.",
      "pos": [
        2621,
        2711
      ]
    },
    {
      "pos": [
        2715,
        2866
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You can also provision Storm clusters by using the <bpt id=\"p1\">[</bpt>Azure CLI<ept id=\"p1\">](../xplat-cli.md)</ept> or <bpt id=\"p2\">[</bpt>Azure PowerShell<ept id=\"p2\">](../powershell-install-configure.md)</ept>."
    },
    {
      "content": "Within 15 minutes of submitting the request, you will have a new Storm cluster running and ready for your first real-time analytics pipeline.",
      "pos": [
        2868,
        3009
      ]
    },
    {
      "content": "Ease of use",
      "pos": [
        3014,
        3025
      ]
    },
    {
      "content": "If you use Visual Studio, the HDInsight Tools for Visual Studio allow you to create C# and hybrid C#/Java topologies, and then submit them to your Storm on HDInsight cluster.",
      "pos": [
        3027,
        3201
      ]
    },
    {
      "content": "Storm Project creation",
      "pos": [
        3207,
        3229
      ]
    },
    {
      "content": "HDInsight Tools for Visual Studio also provides an interface that allows you to monitor and manage Storm topologies on a cluster.",
      "pos": [
        3284,
        3413
      ]
    },
    {
      "content": "Storm management",
      "pos": [
        3417,
        3433
      ]
    },
    {
      "pos": [
        3484,
        3694
      ],
      "content": "For an example of using the HDInsight Tools to create a Storm application, see <bpt id=\"p1\">[</bpt>Develop C# Storm topologies with the HDInsight Tools for Visual Studio<ept id=\"p1\">](hdinsight-storm-develop-csharp-visual-studio-topology.md)</ept>."
    },
    {
      "pos": [
        3696,
        3890
      ],
      "content": "For more information about the HDInsight Tools for Visual Studio, see <bpt id=\"p1\">[</bpt>Get started using the HDInsight Tools for Visual Studio<ept id=\"p1\">](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md)</ept>."
    },
    {
      "content": "Each Storm on HDInsight cluster also provides a web-based Storm Dashboard that allows you to submit, monitor, and manage Storm topologies running on the cluster.",
      "pos": [
        3892,
        4053
      ]
    },
    {
      "content": "Storm dashboard",
      "pos": [
        4057,
        4072
      ]
    },
    {
      "pos": [
        4123,
        4282
      ],
      "content": "For more information about using the Storm Dashboard, see <bpt id=\"p1\">[</bpt>Deploy and manage Apache Storm topologies on HDInsight<ept id=\"p1\">](hdinsight-storm-deploy-monitor-topology.md)</ept>."
    },
    {
      "content": "Storm on HDInsight also provides easy integration with Azure Event Hubs through the <bpt id=\"p1\">**</bpt>Event Hub Spout<ept id=\"p1\">**</ept>.",
      "pos": [
        4284,
        4388
      ]
    },
    {
      "content": "This is available on each storm cluster at <bpt id=\"p1\">**</bpt>%STORM_HOME%\\examples\\eventhubspout\\eventhubs-storm-spout-0.9-jar-with-dependencies.jar<ept id=\"p1\">**</ept>.",
      "pos": [
        4389,
        4524
      ]
    },
    {
      "content": "For examples of using this spout in a Storm topology, see <bpt id=\"p1\">[</bpt>Getting started with Event Hubs<ept id=\"p1\">](service-bus-event-hubs-c-storm-getstarted.MD)</ept> and <bpt id=\"p2\">[</bpt>Analyzing sensor data with Storm and HBase<ept id=\"p2\">](hdinsight-storm-sensor-data-analysis.MD)</ept>.",
      "pos": [
        4525,
        4753
      ]
    },
    {
      "content": "Reliability",
      "pos": [
        4758,
        4769
      ]
    },
    {
      "content": "Apache Storm always guarantees that each incoming message will be fully processed, even when the data analysis is spread over hundreds of nodes.",
      "pos": [
        4771,
        4915
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Nimbus node<ept id=\"p1\">**</ept> provides similar functionality to the Hadoop JobTracker, and it assigns tasks to other nodes in the cluster through <bpt id=\"p2\">**</bpt>Zookeeper<ept id=\"p2\">**</ept>.",
      "pos": [
        4917,
        5067
      ]
    },
    {
      "content": "Zookeeper nodes provide coordination for the cluster and facilitate communication between Nimbus and the <bpt id=\"p1\">**</bpt>Supervisor<ept id=\"p1\">**</ept> process on the worker nodes.",
      "pos": [
        5068,
        5216
      ]
    },
    {
      "content": "If one processing node goes down, the Nimbus node is informed, and it assigns the task and associated data to another node.",
      "pos": [
        5217,
        5340
      ]
    },
    {
      "content": "The default configuration for Apache Storm is to have only one Nimbus node.",
      "pos": [
        5342,
        5417
      ]
    },
    {
      "content": "Storm on HDInsight runs two Nimbus nodes.",
      "pos": [
        5418,
        5459
      ]
    },
    {
      "content": "If the primary node fails, the HDInsight cluster will switch to the secondary node while the primary node is recovered.",
      "pos": [
        5460,
        5579
      ]
    },
    {
      "content": "Diagram of nimbus, zookeeper, and supervisor",
      "pos": [
        5583,
        5627
      ]
    },
    {
      "content": "Scale",
      "pos": [
        5678,
        5683
      ]
    },
    {
      "content": "Although you can specify the number of nodes in your cluster during creation, you may want to grow or shrink the cluster to match workload.",
      "pos": [
        5685,
        5824
      ]
    },
    {
      "content": "All HDInsight clusters allow you to change the number of nodes in the cluster, even while processing data.",
      "pos": [
        5825,
        5931
      ]
    },
    {
      "pos": [
        5935,
        6086
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> To take advantage of new nodes added through scaling, you will need to rebalance topologies started before the cluster size was increased."
    },
    {
      "content": "Support",
      "pos": [
        6091,
        6098
      ]
    },
    {
      "content": "Storm on HDInsight comes with full enterprise-level 24/7 support.",
      "pos": [
        6100,
        6165
      ]
    },
    {
      "content": "Storm on HDInsight also has an SLA of 99.9%.",
      "pos": [
        6166,
        6210
      ]
    },
    {
      "content": "That means we guarantee that the cluster will have external connectivity at least 99.9% of the time.",
      "pos": [
        6211,
        6311
      ]
    },
    {
      "content": "Common use cases for real-time analytics",
      "pos": [
        6315,
        6355
      ]
    },
    {
      "content": "The following are some common scenarios for which you might use Apache storm on HDInsight.",
      "pos": [
        6357,
        6447
      ]
    },
    {
      "content": "For information about real-world scenarios, read <bpt id=\"p1\">[</bpt>How companies are using Storm<ept id=\"p1\">](https://storm.incubator.apache.org/documentation/Powered-By.html)</ept>.",
      "pos": [
        6448,
        6595
      ]
    },
    {
      "content": "Internet of Things (IoT)",
      "pos": [
        6599,
        6623
      ]
    },
    {
      "content": "Fraud detection",
      "pos": [
        6626,
        6641
      ]
    },
    {
      "content": "Social analytics",
      "pos": [
        6644,
        6660
      ]
    },
    {
      "content": "Extract, Transform, Load (ETL)",
      "pos": [
        6663,
        6693
      ]
    },
    {
      "content": "Network monitoring",
      "pos": [
        6696,
        6714
      ]
    },
    {
      "content": "Search",
      "pos": [
        6717,
        6723
      ]
    },
    {
      "content": "Mobile engagement",
      "pos": [
        6726,
        6743
      ]
    },
    {
      "content": "How is data in HDInsight Storm processed?",
      "pos": [
        6747,
        6788
      ]
    },
    {
      "content": "Apache Storm runs <bpt id=\"p1\">**</bpt>topologies<ept id=\"p1\">**</ept> instead of the MapReduce jobs that you may be familiar with in HDInsight or Hadoop.",
      "pos": [
        6790,
        6906
      ]
    },
    {
      "content": "A Storm on HDInsight cluster contains two types of nodes: head nodes that run <bpt id=\"p1\">**</bpt>Nimbus<ept id=\"p1\">**</ept> and worker nodes that run <bpt id=\"p2\">**</bpt>Supervisor<ept id=\"p2\">**</ept>.",
      "pos": [
        6907,
        7037
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Nimbus<ept id=\"p1\">**</ept>: Similar to the JobTracker in Hadoop, it is responsible for distributing code throughout the cluster, assigning tasks to virtual machines, and monitoring for failure.",
      "pos": [
        7041,
        7218
      ]
    },
    {
      "content": "HDInsight provides two Nimbus nodes, so there is no single point of failure for Storm on HDInsight",
      "pos": [
        7219,
        7317
      ]
    },
    {
      "pos": [
        7321,
        7447
      ],
      "content": "<bpt id=\"p1\">**</bpt>Supervisor<ept id=\"p1\">**</ept>: The supervisor for each worker node is responsible for starting and stopping <bpt id=\"p2\">**</bpt>worker processes<ept id=\"p2\">**</ept> on the node."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Worker process<ept id=\"p1\">**</ept>: Runs a subset of a <bpt id=\"p2\">**</bpt>topology<ept id=\"p2\">**</ept>.",
      "pos": [
        7451,
        7503
      ]
    },
    {
      "content": "A running topology is distributed across many worker processes throughout the cluster.",
      "pos": [
        7504,
        7590
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Topology<ept id=\"p1\">**</ept>: Defines a graph of computation that processes <bpt id=\"p2\">**</bpt>streams<ept id=\"p2\">**</ept> of data.",
      "pos": [
        7594,
        7674
      ]
    },
    {
      "content": "Unlike MapReduce jobs, topologies run until you stop them.",
      "pos": [
        7675,
        7733
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Stream<ept id=\"p1\">**</ept>: An unbound collection of <bpt id=\"p2\">**</bpt>tuples<ept id=\"p2\">**</ept>.",
      "pos": [
        7737,
        7785
      ]
    },
    {
      "content": "Streams are produced by <bpt id=\"p1\">**</bpt>spouts<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>bolts<ept id=\"p2\">**</ept>, and they are consumed by <bpt id=\"p3\">**</bpt>bolts<ept id=\"p3\">**</ept>.",
      "pos": [
        7786,
        7871
      ]
    },
    {
      "pos": [
        7875,
        7927
      ],
      "content": "<bpt id=\"p1\">**</bpt>Tuple<ept id=\"p1\">**</ept>: A named list of dynamically typed values."
    },
    {
      "pos": [
        7931,
        8009
      ],
      "content": "<bpt id=\"p1\">**</bpt>Spout<ept id=\"p1\">**</ept>: Consumes data from a data source and emits one or more <bpt id=\"p2\">**</bpt>streams<ept id=\"p2\">**</ept>."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> In many cases, data is read from a queue, such as Kafka, Azure Service Bus queues, or Event hubs.",
      "pos": [
        8017,
        8127
      ]
    },
    {
      "content": "The queue ensures that data is persisted if there is an outage.",
      "pos": [
        8128,
        8191
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Bolt<ept id=\"p1\">**</ept>: Consumes <bpt id=\"p2\">**</bpt>streams<ept id=\"p2\">**</ept>, performs processing on <bpt id=\"p3\">**</bpt>tuples<ept id=\"p3\">**</ept>, and may emit <bpt id=\"p4\">**</bpt>streams<ept id=\"p4\">**</ept>.",
      "pos": [
        8195,
        8287
      ]
    },
    {
      "content": "Bolts are also responsible for writing data to external storage, such as a queue, HDInsight, HBase, a blob, or other data store.",
      "pos": [
        8288,
        8416
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Apache Thrift<ept id=\"p1\">**</ept>: A software framework for scalable cross-language service development.",
      "pos": [
        8420,
        8508
      ]
    },
    {
      "content": "It allows you to build services that work between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, and other languages.",
      "pos": [
        8509,
        8675
      ]
    },
    {
      "pos": [
        8683,
        8839
      ],
      "content": "<bpt id=\"p1\">**</bpt>Nimbus<ept id=\"p1\">**</ept> is a Thrift service, and a <bpt id=\"p2\">**</bpt>topology<ept id=\"p2\">**</ept> is a Thrift definition, so it is possible to develop topologies using a variety of programming languages."
    },
    {
      "pos": [
        8841,
        8941
      ],
      "content": "For more information about Storm components, see the <bpt id=\"p1\">[</bpt>Storm tutorial<ept id=\"p1\">][apachetutorial]</ept> at apache.org."
    },
    {
      "content": "What programming languages can I use?",
      "pos": [
        8946,
        8983
      ]
    },
    {
      "content": "The Storm on HDInsight cluster provides support for C#, Java, and Python.",
      "pos": [
        8985,
        9058
      ]
    },
    {
      "content": "C&amp;#35;",
      "pos": [
        9064,
        9070
      ]
    },
    {
      "content": "The HDInsight Tools for Visual Studio allow .NET developers to design and implement a topology in C#.",
      "pos": [
        9072,
        9173
      ]
    },
    {
      "content": "You can also create hybrid topologies that use Java and C# components.",
      "pos": [
        9174,
        9244
      ]
    },
    {
      "pos": [
        9246,
        9404
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Develop C# topologies for Apache Storm on HDInsight using Visual Studio<ept id=\"p1\">](hdinsight-storm-develop-csharp-visual-studio-topology.md)</ept>."
    },
    {
      "content": "Java",
      "pos": [
        9409,
        9413
      ]
    },
    {
      "content": "Most Java examples you encounter will be plain Java or Trident.",
      "pos": [
        9415,
        9478
      ]
    },
    {
      "content": "Trident is a high-level abstraction that makes it easier to do things such as joins, aggregations, grouping, and filtering.",
      "pos": [
        9479,
        9602
      ]
    },
    {
      "content": "However, Trident acts on batches of tuples, whereas a raw Java solution processes a stream one tuple at a time.",
      "pos": [
        9603,
        9714
      ]
    },
    {
      "pos": [
        9716,
        9865
      ],
      "content": "For more information about Trident, see the <bpt id=\"p1\">[</bpt>Trident tutorial<ept id=\"p1\">](https://storm.incubator.apache.org/documentation/Trident-tutorial.html)</ept> at apache.org."
    },
    {
      "pos": [
        9867,
        10005
      ],
      "content": "For examples of raw Java and Trident topologies, see the <bpt id=\"p1\">**</bpt>%storm_home%\\contrib\\storm-starter<ept id=\"p1\">**</ept> directory on your HDInsight Storm cluster."
    },
    {
      "content": "What are some common development patterns?",
      "pos": [
        10009,
        10051
      ]
    },
    {
      "content": "Guaranteed message processing",
      "pos": [
        10056,
        10085
      ]
    },
    {
      "content": "Storm can provide different levels of guaranteed message processing.",
      "pos": [
        10087,
        10155
      ]
    },
    {
      "content": "For example, a basic Storm application can guarantee at-least-once processing, and Trident can guarantee exactly-once processing.",
      "pos": [
        10156,
        10285
      ]
    },
    {
      "pos": [
        10287,
        10423
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Guarantees on data processing<ept id=\"p1\">](https://storm.apache.org/about/guarantees-data-processing.html)</ept> at apache.org."
    },
    {
      "content": "IBasicBolt",
      "pos": [
        10428,
        10438
      ]
    },
    {
      "pos": [
        10440,
        10747
      ],
      "content": "The pattern of reading an input tuple, emitting zero or more tuples, and then acking the input tuple immediately at the end of the execute method is very common, and Storm provides the <bpt id=\"p1\">[</bpt>IBasicBolt<ept id=\"p1\">](https://storm.apache.org/apidocs/backtype/storm/topology/IBasicBolt.html)</ept> interface to automate this pattern."
    },
    {
      "content": "Joins",
      "pos": [
        10752,
        10757
      ]
    },
    {
      "content": "Joining two streams of data will vary between applications.",
      "pos": [
        10759,
        10818
      ]
    },
    {
      "content": "For example, you could join each tuple from multiple streams into one new stream, or you could join only batches of tuples for a specific window.",
      "pos": [
        10819,
        10964
      ]
    },
    {
      "content": "Either way, joining can be accomplished by using <bpt id=\"p1\">[</bpt>fieldsGrouping<ept id=\"p1\">](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29)</ept>, which is a way of defining how tuples are routed to bolts.",
      "pos": [
        10965,
        11266
      ]
    },
    {
      "pos": [
        11268,
        11412
      ],
      "content": "In the following Java example, fieldsGrouping is used to route tuples that originate from components \"1\", \"2\", and \"3\" to the <bpt id=\"p1\">**</bpt>MyJoiner<ept id=\"p1\">**</ept> bolt."
    },
    {
      "content": "Batching",
      "pos": [
        11659,
        11667
      ]
    },
    {
      "content": "Batching can be accomplished several ways.",
      "pos": [
        11669,
        11711
      ]
    },
    {
      "content": "With a basic Storm Java topology, you might use simple counter to batch X number of tuples before emitting them, or use an internal timing mechanism known as a \"tick tuple\" to emit a batch every X seconds.",
      "pos": [
        11712,
        11917
      ]
    },
    {
      "pos": [
        11919,
        12059
      ],
      "content": "For an example of using tick tuples, see <bpt id=\"p1\">[</bpt>Analyzing sensor data with Storm and HBase on HDInsight<ept id=\"p1\">](hdinsight-storm-sensor-data-analysis.md)</ept>."
    },
    {
      "content": "If you are using Trident, it is based on processing batches of tuples.",
      "pos": [
        12061,
        12131
      ]
    },
    {
      "content": "Caching",
      "pos": [
        12136,
        12143
      ]
    },
    {
      "content": "In-memory caching is often used as a mechanism for speeding up processing because it keeps frequently used assets in memory.",
      "pos": [
        12145,
        12269
      ]
    },
    {
      "content": "Because a topology is distributed across multiple nodes, and multiple processes within each node, you should consider using <bpt id=\"p1\">[</bpt>fieldsGrouping<ept id=\"p1\">](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29)</ept> to ensure that tuples containing the fields that are used for cache lookup are always routed to the same process.",
      "pos": [
        12270,
        12700
      ]
    },
    {
      "content": "This avoids duplication of cache entries across processes.",
      "pos": [
        12701,
        12759
      ]
    },
    {
      "content": "Streaming top N",
      "pos": [
        12764,
        12779
      ]
    },
    {
      "content": "When your topology depends on calculating a \"top N\" value, such as the top 5 trends on Twitter, you should calculate the top N value in parallel and then merge the output from those calculations into a global value.",
      "pos": [
        12781,
        12996
      ]
    },
    {
      "content": "This can be done by using <bpt id=\"p1\">[</bpt>fieldsGrouping<ept id=\"p1\">](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29)</ept> to route by field to the parallel bolts (which partitions the data by field value), and then route to a bolt that globally determines the top N value.",
      "pos": [
        12997,
        13366
      ]
    },
    {
      "pos": [
        13368,
        13526
      ],
      "content": "For an example of this, see the <bpt id=\"p1\">[</bpt>RollingTopWords<ept id=\"p1\">](https://github.com/nathanmarz/storm-starter/blob/master/src/jvm/storm/starter/RollingTopWords.java)</ept> example."
    },
    {
      "content": "Next steps",
      "pos": [
        13530,
        13540
      ]
    },
    {
      "content": "Learn more about real-time analytics solutions with Apache Storm in HDInsight:",
      "pos": [
        13542,
        13620
      ]
    },
    {
      "content": "Getting Started with Storm on HDInsight",
      "pos": [
        13625,
        13664
      ]
    },
    {
      "content": "Example topologies for Storm on HDInsight",
      "pos": [
        13686,
        13727
      ]
    },
    {
      "content": "test",
      "pos": [
        14102,
        14106
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Introduction to Apache Storm on HDInsight | Microsoft Azure\"\n    description=\"Get an introduction to Apache Storm, and learn how you can use Storm on HDInsight to build real-time data analytics solutions in the cloud.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/24/2015\"\n   ms.author=\"larryfr\"/>\n\n#Introduction to Apache Storm on HDInsight: Real-time analytics for Hadoop\n\nApache Storm on HDInsight allows you to create distributed, real-time analytics solutions in the Azure environment by using [Apache Hadoop](http://hadoop.apache.org).\n\n##What is Apache Storm?\n\nApache Storm is a distributed, fault-tolerant, open-source computation system that allows you to process data in real-time with Hadoop. Storm solutions can also provide guaranteed processing of data, with the ability to replay data that was not successfully processed the first time.\n\n##Why use Storm on HDInsight?\n\nApache Storm on HDInsight is a managed cluster integrated into the Azure environment. It provides the following key benefits:\n\n* Performs as a managed service with an SLA of 99.9% up time\n\n* Use the language of your choice: Provides support for Storm components written in **Java**, **C#**, and **Python**\n\n    * Supports a mix of programming languages: Read data using Java, then process it using C#\n\n    * Use the **Trident** Java interface to create Storm topologies that support \"exactly once\" processing of messages, \"transactional\" datastore persistence, and a set of common stream analytics operations\n\n* Includes built-in scale-up and scale-down features: Scale an HDInsight cluster with no impact to running Storm topologies\n\n* Integrate with other Azure services, including Event Hub, Azure Virtual Network, SQL Database, Blob storage, and DocumentDB\n\n    * Combine the capabilities of multiple HDInsight clusters by using Azure Virtual Network: Create analytic pipelines that use HDInsight, HBase, or Hadoop clusters\n\nFor a list of companies that are using Apache Storm for their real-time analytics solutions, see [Companies Using Apache Storm](https://storm.apache.org/documentation/Powered-By.html).\n\nTo get started using Storm, see [Get started with Storm on HDInsight][gettingstarted].\n\n###Ease of provisioning\n\nYou can provision a new Storm on HDInsight cluster in minutes. Specify the cluster name, size, administrator account, and the storage account. Azure will create the cluster, including sample topologies and a web-management dashboard.\n\n> [AZURE.NOTE] You can also provision Storm clusters by using the [Azure CLI](../xplat-cli.md) or [Azure PowerShell](../powershell-install-configure.md).\n\nWithin 15 minutes of submitting the request, you will have a new Storm cluster running and ready for your first real-time analytics pipeline.\n\n###Ease of use\n\nIf you use Visual Studio, the HDInsight Tools for Visual Studio allow you to create C# and hybrid C#/Java topologies, and then submit them to your Storm on HDInsight cluster.  \n\n![Storm Project creation](./media/hdinsight-storm-overview/createproject.png)\n\nHDInsight Tools for Visual Studio also provides an interface that allows you to monitor and manage Storm topologies on a cluster.\n\n![Storm management](./media/hdinsight-storm-overview/stormview.png)\n\nFor an example of using the HDInsight Tools to create a Storm application, see [Develop C# Storm topologies with the HDInsight Tools for Visual Studio](hdinsight-storm-develop-csharp-visual-studio-topology.md).\n\nFor more information about the HDInsight Tools for Visual Studio, see [Get started using the HDInsight Tools for Visual Studio](../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md).\n\nEach Storm on HDInsight cluster also provides a web-based Storm Dashboard that allows you to submit, monitor, and manage Storm topologies running on the cluster.\n\n![Storm dashboard](./media/hdinsight-storm-overview/dashboard.png)\n\nFor more information about using the Storm Dashboard, see [Deploy and manage Apache Storm topologies on HDInsight](hdinsight-storm-deploy-monitor-topology.md).\n\nStorm on HDInsight also provides easy integration with Azure Event Hubs through the **Event Hub Spout**. This is available on each storm cluster at **%STORM_HOME%\\examples\\eventhubspout\\eventhubs-storm-spout-0.9-jar-with-dependencies.jar**. For examples of using this spout in a Storm topology, see [Getting started with Event Hubs](service-bus-event-hubs-c-storm-getstarted.MD) and [Analyzing sensor data with Storm and HBase](hdinsight-storm-sensor-data-analysis.MD).\n\n###Reliability\n\nApache Storm always guarantees that each incoming message will be fully processed, even when the data analysis is spread over hundreds of nodes.\n\nThe **Nimbus node** provides similar functionality to the Hadoop JobTracker, and it assigns tasks to other nodes in the cluster through **Zookeeper**. Zookeeper nodes provide coordination for the cluster and facilitate communication between Nimbus and the **Supervisor** process on the worker nodes. If one processing node goes down, the Nimbus node is informed, and it assigns the task and associated data to another node.\n\nThe default configuration for Apache Storm is to have only one Nimbus node. Storm on HDInsight runs two Nimbus nodes. If the primary node fails, the HDInsight cluster will switch to the secondary node while the primary node is recovered.\n\n![Diagram of nimbus, zookeeper, and supervisor](./media/hdinsight-storm-overview/nimbus.png)\n\n###Scale\n\nAlthough you can specify the number of nodes in your cluster during creation, you may want to grow or shrink the cluster to match workload. All HDInsight clusters allow you to change the number of nodes in the cluster, even while processing data.\n\n> [AZURE.NOTE] To take advantage of new nodes added through scaling, you will need to rebalance topologies started before the cluster size was increased.\n\n###Support\n\nStorm on HDInsight comes with full enterprise-level 24/7 support. Storm on HDInsight also has an SLA of 99.9%. That means we guarantee that the cluster will have external connectivity at least 99.9% of the time.\n\n##Common use cases for real-time analytics\n\nThe following are some common scenarios for which you might use Apache storm on HDInsight. For information about real-world scenarios, read [How companies are using Storm](https://storm.incubator.apache.org/documentation/Powered-By.html).\n\n* Internet of Things (IoT)\n* Fraud detection\n* Social analytics\n* Extract, Transform, Load (ETL)\n* Network monitoring\n* Search\n* Mobile engagement\n\n##How is data in HDInsight Storm processed?\n\nApache Storm runs **topologies** instead of the MapReduce jobs that you may be familiar with in HDInsight or Hadoop. A Storm on HDInsight cluster contains two types of nodes: head nodes that run **Nimbus** and worker nodes that run **Supervisor**.\n\n* **Nimbus**: Similar to the JobTracker in Hadoop, it is responsible for distributing code throughout the cluster, assigning tasks to virtual machines, and monitoring for failure. HDInsight provides two Nimbus nodes, so there is no single point of failure for Storm on HDInsight\n\n* **Supervisor**: The supervisor for each worker node is responsible for starting and stopping **worker processes** on the node.\n\n* **Worker process**: Runs a subset of a **topology**. A running topology is distributed across many worker processes throughout the cluster.\n\n* **Topology**: Defines a graph of computation that processes **streams** of data. Unlike MapReduce jobs, topologies run until you stop them.\n\n* **Stream**: An unbound collection of **tuples**. Streams are produced by **spouts** and **bolts**, and they are consumed by **bolts**.\n\n* **Tuple**: A named list of dynamically typed values.\n\n* **Spout**: Consumes data from a data source and emits one or more **streams**.\n\n    > [AZURE.NOTE] In many cases, data is read from a queue, such as Kafka, Azure Service Bus queues, or Event hubs. The queue ensures that data is persisted if there is an outage.\n\n* **Bolt**: Consumes **streams**, performs processing on **tuples**, and may emit **streams**. Bolts are also responsible for writing data to external storage, such as a queue, HDInsight, HBase, a blob, or other data store.\n\n* **Apache Thrift**: A software framework for scalable cross-language service development. It allows you to build services that work between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, and other languages.\n\n    * **Nimbus** is a Thrift service, and a **topology** is a Thrift definition, so it is possible to develop topologies using a variety of programming languages.\n\nFor more information about Storm components, see the [Storm tutorial][apachetutorial] at apache.org.\n\n\n##What programming languages can I use?\n\nThe Storm on HDInsight cluster provides support for C#, Java, and Python.\n\n### C&#35;\n\nThe HDInsight Tools for Visual Studio allow .NET developers to design and implement a topology in C#. You can also create hybrid topologies that use Java and C# components.\n\nFor more information, see [Develop C# topologies for Apache Storm on HDInsight using Visual Studio](hdinsight-storm-develop-csharp-visual-studio-topology.md).\n\n###Java\n\nMost Java examples you encounter will be plain Java or Trident. Trident is a high-level abstraction that makes it easier to do things such as joins, aggregations, grouping, and filtering. However, Trident acts on batches of tuples, whereas a raw Java solution processes a stream one tuple at a time.\n\nFor more information about Trident, see the [Trident tutorial](https://storm.incubator.apache.org/documentation/Trident-tutorial.html) at apache.org.\n\nFor examples of raw Java and Trident topologies, see the **%storm_home%\\contrib\\storm-starter** directory on your HDInsight Storm cluster.\n\n##What are some common development patterns?\n\n###Guaranteed message processing\n\nStorm can provide different levels of guaranteed message processing. For example, a basic Storm application can guarantee at-least-once processing, and Trident can guarantee exactly-once processing.\n\nFor more information, see [Guarantees on data processing](https://storm.apache.org/about/guarantees-data-processing.html) at apache.org.\n\n###IBasicBolt\n\nThe pattern of reading an input tuple, emitting zero or more tuples, and then acking the input tuple immediately at the end of the execute method is very common, and Storm provides the [IBasicBolt](https://storm.apache.org/apidocs/backtype/storm/topology/IBasicBolt.html) interface to automate this pattern.\n\n###Joins\n\nJoining two streams of data will vary between applications. For example, you could join each tuple from multiple streams into one new stream, or you could join only batches of tuples for a specific window. Either way, joining can be accomplished by using [fieldsGrouping](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29), which is a way of defining how tuples are routed to bolts.\n\nIn the following Java example, fieldsGrouping is used to route tuples that originate from components \"1\", \"2\", and \"3\" to the **MyJoiner** bolt.\n\n    builder.setBolt(\"join\", new MyJoiner(), parallelism) .fieldsGrouping(\"1\", new Fields(\"joinfield1\", \"joinfield2\")) .fieldsGrouping(\"2\", new Fields(\"joinfield1\", \"joinfield2\")) .fieldsGrouping(\"3\", new Fields(\"joinfield1\", \"joinfield2\"));\n\n###Batching\n\nBatching can be accomplished several ways. With a basic Storm Java topology, you might use simple counter to batch X number of tuples before emitting them, or use an internal timing mechanism known as a \"tick tuple\" to emit a batch every X seconds.\n\nFor an example of using tick tuples, see [Analyzing sensor data with Storm and HBase on HDInsight](hdinsight-storm-sensor-data-analysis.md).\n\nIf you are using Trident, it is based on processing batches of tuples.\n\n###Caching\n\nIn-memory caching is often used as a mechanism for speeding up processing because it keeps frequently used assets in memory. Because a topology is distributed across multiple nodes, and multiple processes within each node, you should consider using [fieldsGrouping](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29) to ensure that tuples containing the fields that are used for cache lookup are always routed to the same process. This avoids duplication of cache entries across processes.\n\n###Streaming top N\n\nWhen your topology depends on calculating a \"top N\" value, such as the top 5 trends on Twitter, you should calculate the top N value in parallel and then merge the output from those calculations into a global value. This can be done by using [fieldsGrouping](http://javadox.com/org.apache.storm/storm-core/0.9.1-incubating/backtype/storm/topology/InputDeclarer.html#fieldsGrouping%28java.lang.String,%20backtype.storm.tuple.Fields%29) to route by field to the parallel bolts (which partitions the data by field value), and then route to a bolt that globally determines the top N value.\n\nFor an example of this, see the [RollingTopWords](https://github.com/nathanmarz/storm-starter/blob/master/src/jvm/storm/starter/RollingTopWords.java) example.\n\n##Next steps\n\nLearn more about real-time analytics solutions with Apache Storm in HDInsight:\n\n* [Getting Started with Storm on HDInsight][gettingstarted]\n\n* [Example topologies for Storm on HDInsight](hdinsight-storm-example-topology.md)\n\n[stormtrident]: https://storm.incubator.apache.org/documentation/Trident-API-Overview.html\n[samoa]: http://yahooeng.tumblr.com/post/65453012905/introducing-samoa-an-open-source-platform-for-mining\n[apachetutorial]: https://storm.incubator.apache.org/documentation/Tutorial.html\n[gettingstarted]: ../hdinsight-storm-getting-started.md\n\ntest\n"
}