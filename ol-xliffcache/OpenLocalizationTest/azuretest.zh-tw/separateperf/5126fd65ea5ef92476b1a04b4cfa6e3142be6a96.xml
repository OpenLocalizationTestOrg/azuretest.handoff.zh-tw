{
  "nodes": [
    {
      "content": "MapReduce with Hadoop on HDInsight | Microsoft Azure",
      "pos": [
        26,
        78
      ]
    },
    {
      "content": "Learn how to run MapReduce jobs on Hadoop in HDInsight clusters.",
      "pos": [
        96,
        160
      ]
    },
    {
      "content": "You'll run a basic word count operation implemented as a Java MapReduce job.",
      "pos": [
        161,
        237
      ]
    },
    {
      "content": "Use MapReduce in Hadoop on HDInsight",
      "pos": [
        555,
        591
      ]
    },
    {
      "content": "In this article, you will learn how to run MapReduce jobs on Hadoop in HDInsight clusters.",
      "pos": [
        683,
        773
      ]
    },
    {
      "content": "We run a basic word count operation implemented as a Java MapReduce job.",
      "pos": [
        774,
        846
      ]
    },
    {
      "pos": [
        850,
        887
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"whatis\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>What is MapReduce?"
    },
    {
      "content": "Hadoop MapReduce is a software framework for writing jobs that process vast amounts of data.",
      "pos": [
        889,
        981
      ]
    },
    {
      "content": "Input data is split into independent chunks, which are then processed in parallel across the nodes in your cluster.",
      "pos": [
        982,
        1097
      ]
    },
    {
      "content": "A MapReduce job consist of two functions:",
      "pos": [
        1098,
        1139
      ]
    },
    {
      "pos": [
        1143,
        1268
      ],
      "content": "<bpt id=\"p1\">**</bpt>Mapper<ept id=\"p1\">**</ept>: Consumes input data, analyzes it (usually with filter and sorting operations), and emits tuples (key-value pairs)"
    },
    {
      "pos": [
        1271,
        1415
      ],
      "content": "<bpt id=\"p1\">**</bpt>Reducer<ept id=\"p1\">**</ept>: Consumes tuples emitted by the Mapper and performs a summary operation that creates a smaller, combined result from the Mapper data"
    },
    {
      "content": "A basic word count MapReduce job example is illustrated in the following diagram:",
      "pos": [
        1417,
        1498
      ]
    },
    {
      "content": "HDI.WordCountDiagram",
      "pos": [
        1502,
        1522
      ]
    },
    {
      "content": "The output of this job is a count of how many times each word occurred in the text that was analyzed.",
      "pos": [
        1553,
        1654
      ]
    },
    {
      "content": "The mapper takes each line from the input text as an input and breaks it into words.",
      "pos": [
        1658,
        1742
      ]
    },
    {
      "content": "It emits a key/value pair each time a word occurs of the word is followed by a 1.",
      "pos": [
        1743,
        1824
      ]
    },
    {
      "content": "The output is sorted before sending it to reducer.",
      "pos": [
        1825,
        1875
      ]
    },
    {
      "content": "The reducer sums these individual counts for each word and emits a single key/value pair that contains the word followed by the sum of its occurrences.",
      "pos": [
        1879,
        2030
      ]
    },
    {
      "content": "MapReduce can be implemented in a variety of languages.",
      "pos": [
        2032,
        2087
      ]
    },
    {
      "content": "Java is the most common implementation, and is used for demonstration purposes in this document.",
      "pos": [
        2088,
        2184
      ]
    },
    {
      "content": "Hadoop Streaming",
      "pos": [
        2190,
        2206
      ]
    },
    {
      "content": "Languages or frameworks that are based on Java and the Java Virtual Machine (for example, Scalding or Cascading,) can be ran directly as a MapReduce job, similar to a Java application.",
      "pos": [
        2208,
        2392
      ]
    },
    {
      "content": "Others, such as C# or Python, or standalone executables, must use Hadoop streaming.",
      "pos": [
        2393,
        2476
      ]
    },
    {
      "content": "Hadoop streaming communicates with the mapper and reducer over STDIN and STDOUT - the mapper and reducer read data a line at a time from STDIN, and write the output to STDOUT.",
      "pos": [
        2478,
        2653
      ]
    },
    {
      "content": "Each line read or emitted by the mapper and reducer must be in the format of a key/value pair, delimited by a tab charaacter:",
      "pos": [
        2654,
        2779
      ]
    },
    {
      "pos": [
        2801,
        2899
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Hadoop Streaming<ept id=\"p1\">](http://hadoop.apache.org/docs/r1.2.1/streaming.html)</ept>."
    },
    {
      "content": "For examples of using Hadoop streaming with HDInsight, see the following:",
      "pos": [
        2901,
        2974
      ]
    },
    {
      "content": "Develop C# Hadoop streaming programs",
      "pos": [
        2979,
        3015
      ]
    },
    {
      "content": "Develop Python MapReduce jobs",
      "pos": [
        3072,
        3101
      ]
    },
    {
      "pos": [
        3144,
        3182
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"data\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>About the sample data"
    },
    {
      "content": "In this example, for sample data, you will use the notebooks of Leonardo Da Vinci, which are provided as a text document in your HDInsight cluster.",
      "pos": [
        3184,
        3331
      ]
    },
    {
      "content": "The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters.",
      "pos": [
        3333,
        3450
      ]
    },
    {
      "content": "HDInsight can access files stored in Blob storage by using the <bpt id=\"p1\">**</bpt>wasb<ept id=\"p1\">**</ept> prefix.",
      "pos": [
        3451,
        3530
      ]
    },
    {
      "content": "For example, to access the sample.log file, you would use the following syntax:",
      "pos": [
        3531,
        3610
      ]
    },
    {
      "pos": [
        3660,
        3803
      ],
      "content": "Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id=\"p1\">**</bpt>/example/data/gutenberg/davinci.txt<ept id=\"p1\">**</ept>."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> In the previous syntax, <bpt id=\"p1\">**</bpt>wasb:///<ept id=\"p1\">**</ept> is used to access files that are stored in the default storage container for your HDInsight cluster.",
      "pos": [
        3807,
        3957
      ]
    },
    {
      "content": "If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address.",
      "pos": [
        3958,
        4178
      ]
    },
    {
      "content": "For example, <bpt id=\"p1\">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/gutenberg/davinci.txt<ept id=\"p1\">**</ept>.",
      "pos": [
        4179,
        4282
      ]
    },
    {
      "pos": [
        4286,
        4329
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"job\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>About the example MapReduce"
    },
    {
      "content": "The MapReduce job that is used in this example is located at <bpt id=\"p1\">**</bpt>wasb://example/jars/hadoop-mapreduce-examples.jar<ept id=\"p1\">**</ept>, and it is provided with your HDInsight cluster.",
      "pos": [
        4331,
        4494
      ]
    },
    {
      "content": "This contains a word count example that you will run against <bpt id=\"p1\">**</bpt>davinci.txt<ept id=\"p1\">**</ept>.",
      "pos": [
        4495,
        4572
      ]
    },
    {
      "pos": [
        4576,
        4682
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> On HDInsight 2.1 clusters, the file location is <bpt id=\"p1\">**</bpt>wasb:///example/jars/hadoop-examples.jar<ept id=\"p1\">**</ept>."
    },
    {
      "content": "For reference, the following is the Java code for the word count MapReduce job:",
      "pos": [
        4684,
        4763
      ]
    },
    {
      "pos": [
        7386,
        7532
      ],
      "content": "For instructions to write your own MapReduce job, see <bpt id=\"p1\">[</bpt>Develop Java MapReduce programs for HDInsight<ept id=\"p1\">](hdinsight-develop-deploy-java-mapreduce.md)</ept>."
    },
    {
      "pos": [
        7536,
        7569
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run the MapReduce"
    },
    {
      "content": "HDInsight can run HiveQL jobs by using a variety of methods.",
      "pos": [
        7571,
        7631
      ]
    },
    {
      "content": "Use the following table to decide which method is right for you, then follow the link for a walkthrough.",
      "pos": [
        7632,
        7736
      ]
    },
    {
      "pos": [
        7740,
        7755
      ],
      "content": "<bpt id=\"p1\">**</bpt>Use this<ept id=\"p1\">**</ept>..."
    },
    {
      "content": "...to do this",
      "pos": [
        7811,
        7824
      ]
    },
    {
      "pos": [
        7867,
        7908
      ],
      "content": "...with this <bpt id=\"p1\">**</bpt>cluster operating system<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        7911,
        7951
      ],
      "content": "...from this <bpt id=\"p1\">**</bpt>client operating system<ept id=\"p1\">**</ept>"
    },
    {
      "content": "SSH",
      "pos": [
        8173,
        8176
      ]
    },
    {
      "pos": [
        8241,
        8279
      ],
      "content": "Use the Hadoop command through <bpt id=\"p1\">**</bpt>SSH<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Linux",
      "pos": [
        8299,
        8304
      ]
    },
    {
      "content": "Linux, Unix, Mac OS X, or Windows",
      "pos": [
        8343,
        8376
      ]
    },
    {
      "content": "Curl",
      "pos": [
        8389,
        8393
      ]
    },
    {
      "pos": [
        8457,
        8498
      ],
      "content": "Submit the job remotely by using <bpt id=\"p1\">**</bpt>REST<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Linux or Windows",
      "pos": [
        8515,
        8531
      ]
    },
    {
      "content": "Linux, Unix, Mac OS X, or Windows",
      "pos": [
        8559,
        8592
      ]
    },
    {
      "content": "Windows PowerShell",
      "pos": [
        8605,
        8623
      ]
    },
    {
      "pos": [
        8673,
        8728
      ],
      "content": "Submit the job remotely by using <bpt id=\"p1\">**</bpt>Windows PowerShell<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Linux or Windows",
      "pos": [
        8731,
        8747
      ]
    },
    {
      "content": "Windows",
      "pos": [
        8775,
        8782
      ]
    },
    {
      "content": "Remote Desktop",
      "pos": [
        8821,
        8835
      ]
    },
    {
      "pos": [
        8889,
        8938
      ],
      "content": "Use the Hadoop command through <bpt id=\"p1\">**</bpt>Remote Desktop<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Windows",
      "pos": [
        8947,
        8954
      ]
    },
    {
      "content": "Windows",
      "pos": [
        8991,
        8998
      ]
    },
    {
      "pos": [
        9037,
        9069
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "Although MapReduce provides powerful diagnostic abilities, it can be a bit challenging to master.",
      "pos": [
        9071,
        9168
      ]
    },
    {
      "content": "There are several Java-based frameworks that make it easier to define MapReduce applications, as well as technologies such as Pig and Hive, which provide an easier way to work with data in HDInsight.",
      "pos": [
        9169,
        9368
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        9369,
        9411
      ]
    },
    {
      "content": "Develop Java MapReduce programs for HDInsight",
      "pos": [
        9416,
        9461
      ]
    },
    {
      "content": "Develop Python streaming MapReduce programs for HDInsight",
      "pos": [
        9511,
        9568
      ]
    },
    {
      "content": "Develop C# Hadoop streaming MapReduce programs for HDInsight",
      "pos": [
        9612,
        9672
      ]
    },
    {
      "content": "Develop Scalding MapReduce jobs with Apache Hadoop on HDInsight",
      "pos": [
        9707,
        9770
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        9816,
        9839
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        9865,
        9887
      ]
    },
    {
      "content": "Run the HDInsight Samples",
      "pos": [
        9912,
        9937
      ]
    },
    {
      "content": "test",
      "pos": [
        10567,
        10571
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"MapReduce with Hadoop on HDInsight | Microsoft Azure\"\n   description=\"Learn how to run MapReduce jobs on Hadoop in HDInsight clusters. You'll run a basic word count operation implemented as a Java MapReduce job.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/06/2015\"\n   ms.author=\"larryfr\"/>\n\n# Use MapReduce in Hadoop on HDInsight\n\n[AZURE.INCLUDE [mapreduce-selector](../../includes/hdinsight-selector-use-mapreduce.md)]\n\nIn this article, you will learn how to run MapReduce jobs on Hadoop in HDInsight clusters. We run a basic word count operation implemented as a Java MapReduce job.\n\n##<a id=\"whatis\"></a>What is MapReduce?\n\nHadoop MapReduce is a software framework for writing jobs that process vast amounts of data. Input data is split into independent chunks, which are then processed in parallel across the nodes in your cluster. A MapReduce job consist of two functions:\n\n* **Mapper**: Consumes input data, analyzes it (usually with filter and sorting operations), and emits tuples (key-value pairs)\n* **Reducer**: Consumes tuples emitted by the Mapper and performs a summary operation that creates a smaller, combined result from the Mapper data\n\nA basic word count MapReduce job example is illustrated in the following diagram:\n\n![HDI.WordCountDiagram][image-hdi-wordcountdiagram]\n\nThe output of this job is a count of how many times each word occurred in the text that was analyzed.\n\n* The mapper takes each line from the input text as an input and breaks it into words. It emits a key/value pair each time a word occurs of the word is followed by a 1. The output is sorted before sending it to reducer.\n\n* The reducer sums these individual counts for each word and emits a single key/value pair that contains the word followed by the sum of its occurrences.\n\nMapReduce can be implemented in a variety of languages. Java is the most common implementation, and is used for demonstration purposes in this document.\n\n### Hadoop Streaming\n\nLanguages or frameworks that are based on Java and the Java Virtual Machine (for example, Scalding or Cascading,) can be ran directly as a MapReduce job, similar to a Java application. Others, such as C# or Python, or standalone executables, must use Hadoop streaming.\n\nHadoop streaming communicates with the mapper and reducer over STDIN and STDOUT - the mapper and reducer read data a line at a time from STDIN, and write the output to STDOUT. Each line read or emitted by the mapper and reducer must be in the format of a key/value pair, delimited by a tab charaacter:\n\n    [key]/t[value]\n\nFor more information, see [Hadoop Streaming](http://hadoop.apache.org/docs/r1.2.1/streaming.html).\n\nFor examples of using Hadoop streaming with HDInsight, see the following:\n\n* [Develop C# Hadoop streaming programs](hdinsight-hadoop-develop-deploy-streaming-jobs.md)\n\n* [Develop Python MapReduce jobs](hdinsight-hadoop-streaming-python.md)\n\n##<a id=\"data\"></a>About the sample data\n\nIn this example, for sample data, you will use the notebooks of Leonardo Da Vinci, which are provided as a text document in your HDInsight cluster.\n\nThe sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters. HDInsight can access files stored in Blob storage by using the **wasb** prefix. For example, to access the sample.log file, you would use the following syntax:\n\n    wasb:///example/data/gutenberg/davinci.txt\n\nBecause Azure Blob storage is the default storage for HDInsight, you can also access the file by using **/example/data/gutenberg/davinci.txt**.\n\n> [AZURE.NOTE] In the previous syntax, **wasb:///** is used to access files that are stored in the default storage container for your HDInsight cluster. If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address. For example, **wasb://mycontainer@mystorage.blob.core.windows.net/example/data/gutenberg/davinci.txt**.\n\n##<a id=\"job\"></a>About the example MapReduce\n\nThe MapReduce job that is used in this example is located at **wasb://example/jars/hadoop-mapreduce-examples.jar**, and it is provided with your HDInsight cluster. This contains a word count example that you will run against **davinci.txt**.\n\n> [AZURE.NOTE] On HDInsight 2.1 clusters, the file location is **wasb:///example/jars/hadoop-examples.jar**.\n\nFor reference, the following is the Java code for the word count MapReduce job:\n\n    package org.apache.hadoop.examples;\n\n    import java.io.IOException;\n    import java.util.StringTokenizer;\n\n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.IntWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.Mapper;\n    import org.apache.hadoop.mapreduce.Reducer;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    import org.apache.hadoop.util.GenericOptionsParser;\n\n    public class WordCount {\n\n      public static class TokenizerMapper\n           extends Mapper<Object, Text, Text, IntWritable>{\n\n        private final static IntWritable one = new IntWritable(1);\n        private Text word = new Text();\n\n        public void map(Object key, Text value, Context context\n                        ) throws IOException, InterruptedException {\n          StringTokenizer itr = new StringTokenizer(value.toString());\n          while (itr.hasMoreTokens()) {\n            word.set(itr.nextToken());\n            context.write(word, one);\n          }\n        }\n      }\n\n      public static class IntSumReducer\n           extends Reducer<Text,IntWritable,Text,IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n                           ) throws IOException, InterruptedException {\n          int sum = 0;\n          for (IntWritable val : values) {\n            sum += val.get();\n          }\n          result.set(sum);\n          context.write(key, result);\n        }\n      }\n\n      public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length != 2) {\n          System.err.println(\"Usage: wordcount <in> <out>\");\n          System.exit(2);\n        }\n        Job job = new Job(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(IntSumReducer.class);\n        job.setReducerClass(IntSumReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n      }\n    }\n\nFor instructions to write your own MapReduce job, see [Develop Java MapReduce programs for HDInsight](hdinsight-develop-deploy-java-mapreduce.md).\n\n##<a id=\"run\"></a>Run the MapReduce\n\nHDInsight can run HiveQL jobs by using a variety of methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.\n\n| **Use this**...                                                    | **...to do this**                                       | ...with this **cluster operating system** | ...from this **client operating system** |\n|:-------------------------------------------------------------------|:--------------------------------------------------------|:------------------------------------------|:-----------------------------------------|\n| [SSH](hdinsight-hadoop-use-mapreduce-ssh.md)                       | Use the Hadoop command through **SSH**                  | Linux                                     | Linux, Unix, Mac OS X, or Windows        |\n| [Curl](hdinsight-hadoop-use-mapreduce-curl.md)                     | Submit the job remotely by using **REST**               | Linux or Windows                          | Linux, Unix, Mac OS X, or Windows        |\n| [Windows PowerShell](hdinsight-hadoop-use-mapreduce-powershell.md) | Submit the job remotely by using **Windows PowerShell** | Linux or Windows                          | Windows                                  |\n| [Remote Desktop](hdinsight-hadoop-use-mapreduce-remote-desktop)    | Use the Hadoop command through **Remote Desktop**       | Windows                                   | Windows                                  |\n\n##<a id=\"nextsteps\"></a>Next steps\n\nAlthough MapReduce provides powerful diagnostic abilities, it can be a bit challenging to master. There are several Java-based frameworks that make it easier to define MapReduce applications, as well as technologies such as Pig and Hive, which provide an easier way to work with data in HDInsight. To learn more, see the following articles:\n\n* [Develop Java MapReduce programs for HDInsight](hdinsight-develop-deploy-java-mapreduce.md)\n\n* [Develop Python streaming MapReduce programs for HDInsight](hdinsight-hadoop-streaming-python.md)\n\n* [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-streaming]\n\n* [Develop Scalding MapReduce jobs with Apache Hadoop on HDInsight](hdinsight-hadoop-mapreduce-scalding.md)\n\n* [Use Hive with HDInsight][hdinsight-use-hive]\n\n* [Use Pig with HDInsight][hdinsight-use-pig]\n\n* [Run the HDInsight Samples][hdinsight-samples]\n\n\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-get-started]: ../hdinsight-get-started.md\n[hdinsight-develop-mapreduce-jobs]: hdinsight-develop-deploy-java-mapreduce.md\n[hdinsight-develop-streaming]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-samples]: hdinsight-run-samples.md\n[hdinsight-provision]: hdinsight-provision-clusters.md\n\n[powershell-install-configure]: ../powershell-install-configure.md\n\n[image-hdi-wordcountdiagram]: ./media/hdinsight-use-mapreduce/HDI.WordCountDiagram.gif\n\ntest\n"
}