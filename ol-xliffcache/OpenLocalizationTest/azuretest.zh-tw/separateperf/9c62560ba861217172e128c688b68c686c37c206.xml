{
  "nodes": [
    {
      "content": "Create Stream Analytics Outputs | Microsoft Azure",
      "pos": [
        28,
        77
      ]
    },
    {
      "content": "Learn how to connect to and configure the output targets for Stream Analytics solutions.",
      "pos": [
        97,
        185
      ]
    },
    {
      "content": "Create Stream Analytics outputs",
      "pos": [
        525,
        556
      ]
    },
    {
      "content": "Understanding Stream Analytics outputs",
      "pos": [
        561,
        599
      ]
    },
    {
      "content": "When creating a Stream Analytics job, one of the considerations is how the output of the job is consumed.",
      "pos": [
        607,
        712
      ]
    },
    {
      "content": "How are the consumers of the data transformation viewing the results of the Stream Analytics job?",
      "pos": [
        713,
        810
      ]
    },
    {
      "content": "What tool(s) will they be using to analyze the output?",
      "pos": [
        811,
        865
      ]
    },
    {
      "content": "Is data retention or warehousing a requirement?",
      "pos": [
        866,
        913
      ]
    },
    {
      "content": "Azure Stream Analytics provides seven different methods for storing and viewing job outputs.",
      "pos": [
        915,
        1007
      ]
    },
    {
      "content": "SQL Database, Blob storage, Event Hubs, Service Bus Queues, Service Bus Topics, Power BI and Table storage.",
      "pos": [
        1008,
        1115
      ]
    },
    {
      "content": "This provides for both ease of viewing job output and flexibility in the consumption and storage of the job output for data warehousing and other purposes.",
      "pos": [
        1116,
        1271
      ]
    },
    {
      "content": "Using a SQL Database as an output",
      "pos": [
        1276,
        1309
      ]
    },
    {
      "content": "A SQL Database can be used as an output for data that is relational in nature or for applications that depend on content being hosted in a relational database.",
      "pos": [
        1317,
        1476
      ]
    },
    {
      "content": "For more information on Azure SQL databases see <bpt id=\"p1\">[</bpt>Azure SQL Databases<ept id=\"p1\">](http://azure.microsoft.com/services/sql-database/)</ept>.",
      "pos": [
        1477,
        1598
      ]
    },
    {
      "content": "Parameters",
      "pos": [
        1604,
        1614
      ]
    },
    {
      "content": "To start using a SQL Database, the following information will be needed about the SQL Database:",
      "pos": [
        1620,
        1715
      ]
    },
    {
      "content": "Server Name",
      "pos": [
        1720,
        1731
      ]
    },
    {
      "content": "Database Name",
      "pos": [
        1735,
        1748
      ]
    },
    {
      "content": "A valid username/password combination",
      "pos": [
        1752,
        1789
      ]
    },
    {
      "content": "Output table name.",
      "pos": [
        1793,
        1811
      ]
    },
    {
      "content": "This table must already exist, the job will not create it.",
      "pos": [
        1812,
        1870
      ]
    },
    {
      "content": "Adding SQL Database as an output",
      "pos": [
        1876,
        1908
      ]
    },
    {
      "content": "graphic1",
      "pos": [
        1916,
        1924
      ]
    },
    {
      "pos": [
        1937,
        2026
      ],
      "content": "Go to the outputs tab of the job, and click on the <bpt id=\"p1\">**</bpt>ADD OUTPUT<ept id=\"p1\">**</ept> command and click next."
    },
    {
      "content": "graphic2",
      "pos": [
        2030,
        2038
      ]
    },
    {
      "pos": [
        2051,
        2089
      ],
      "content": "Choose <bpt id=\"p1\">**</bpt>SQL Database<ept id=\"p1\">**</ept> as the output."
    },
    {
      "content": "graphic3",
      "pos": [
        2093,
        2101
      ]
    },
    {
      "content": "Enter the database information on the next page.",
      "pos": [
        2114,
        2162
      ]
    },
    {
      "content": "The output alias is a friendly name used in queries to direct the query output to this database.",
      "pos": [
        2163,
        2259
      ]
    },
    {
      "content": "The default alias if if only a single output is present is \"output\".",
      "pos": [
        2260,
        2328
      ]
    },
    {
      "content": "graphic4",
      "pos": [
        2332,
        2340
      ]
    },
    {
      "content": "If the database exists within the same subscription as the Stream Analytics job, the option to select \"Use SQL Database from Current Subscription\", is available.",
      "pos": [
        2353,
        2514
      ]
    },
    {
      "content": "Then select the database from the drop down list.",
      "pos": [
        2515,
        2564
      ]
    },
    {
      "content": "graphic5",
      "pos": [
        2568,
        2576
      ]
    },
    {
      "content": "Click next to add this output.",
      "pos": [
        2589,
        2619
      ]
    },
    {
      "content": "Two operations should start, the first is to add the output.",
      "pos": [
        2620,
        2680
      ]
    },
    {
      "content": "graphic6",
      "pos": [
        2684,
        2692
      ]
    },
    {
      "content": "The second operation is to test the connection.",
      "pos": [
        2705,
        2752
      ]
    },
    {
      "content": "Azure Stream Analytics will try to connect to the SQL Database and verify the credentials entered are correct and that the table is accessible.",
      "pos": [
        2753,
        2896
      ]
    },
    {
      "content": "Using Blob storage as an output",
      "pos": [
        2901,
        2932
      ]
    },
    {
      "pos": [
        2940,
        3096
      ],
      "content": "For an introduction on Azure Blob storage and its usage, see the documentation at <bpt id=\"p1\">[</bpt>How to use Blobs<ept id=\"p1\">](./articles/storage/storage-dotnet-how-to-use-blobs.md)</ept>."
    },
    {
      "content": "Parameters",
      "pos": [
        3102,
        3112
      ]
    },
    {
      "content": "To start using a Blob storage output, the following information will be needed:",
      "pos": [
        3118,
        3197
      ]
    },
    {
      "content": "If the storage account is in a different subscription than the streaming job then fields will appear to provide the Storage Account Name and Storage Account Key.",
      "pos": [
        3202,
        3363
      ]
    },
    {
      "content": "The container name.",
      "pos": [
        3367,
        3386
      ]
    },
    {
      "content": "The file name prefix.",
      "pos": [
        3390,
        3411
      ]
    },
    {
      "content": "What serialization format is utilized for the data (Avro, CSV, JSON).",
      "pos": [
        3415,
        3484
      ]
    },
    {
      "content": "Adding Blob storage as an output",
      "pos": [
        3490,
        3522
      ]
    },
    {
      "content": "Select output to Blob storage.",
      "pos": [
        3528,
        3558
      ]
    },
    {
      "content": "graphic20",
      "pos": [
        3562,
        3571
      ]
    },
    {
      "content": "Then supply the details as shown below:",
      "pos": [
        3585,
        3624
      ]
    },
    {
      "content": "graphic21",
      "pos": [
        3628,
        3637
      ]
    },
    {
      "content": "Using an Event Hub as an output",
      "pos": [
        3654,
        3685
      ]
    },
    {
      "content": "Overview",
      "pos": [
        3697,
        3705
      ]
    },
    {
      "content": "Event Hubs are a highly scalable event ingestor, and typically are the most common way for Stream Analytics data ingress.",
      "pos": [
        3712,
        3833
      ]
    },
    {
      "content": "Their robust handling of high numbers of events also make them perfect for job output.",
      "pos": [
        3834,
        3920
      ]
    },
    {
      "content": "One use of an Event Hub as output is when the output of an Stream Analytics job will be the input of another streaming job.",
      "pos": [
        3922,
        4045
      ]
    },
    {
      "content": "For further details on Event Hubs visit the portal at <bpt id=\"p1\">[</bpt>Event Hubs<ept id=\"p1\">]</ept><bpt id=\"p2\">(https://azure.microsoft.com/services/event-hubs/ \"</bpt>Event Hubs<ept id=\"p2\">\")</ept>.",
      "pos": [
        4046,
        4176
      ]
    },
    {
      "content": "Parameters",
      "pos": [
        4183,
        4193
      ]
    },
    {
      "content": "There are a few parameters that are needed to configure Event Hub data streams.",
      "pos": [
        4199,
        4278
      ]
    },
    {
      "content": "Service Bus Namespace: Service Bus Namespace of the Event Hub.",
      "pos": [
        4283,
        4345
      ]
    },
    {
      "content": "A Service Bus namespace is a container for a set of messaging entities.",
      "pos": [
        4346,
        4417
      ]
    },
    {
      "content": "When creating a new Event Hub, a Service Bus namespace is also created.",
      "pos": [
        4418,
        4489
      ]
    },
    {
      "content": "Event Hub Name: Name of the Event Hub.",
      "pos": [
        4494,
        4532
      ]
    },
    {
      "content": "It’s the name specified when creating a new Event Hub.",
      "pos": [
        4534,
        4588
      ]
    },
    {
      "content": "Event Hub Policy Name: The name of the shared access policy for accessing the Event Hub.",
      "pos": [
        4593,
        4681
      ]
    },
    {
      "content": "Shared access policies can be configured for an Event Hub on the Configure tab.",
      "pos": [
        4683,
        4762
      ]
    },
    {
      "content": "Each shared access policy will have a name, permissions set, and access keys generated.",
      "pos": [
        4763,
        4850
      ]
    },
    {
      "content": "Event Hub Policy Key:  The primary or secondary key of the shared access policy for accessing the Event Hub.",
      "pos": [
        4854,
        4962
      ]
    },
    {
      "content": "Partition Key Column:  Optional parameter for Event Hub outputs.",
      "pos": [
        4968,
        5032
      ]
    },
    {
      "content": "This column contains the partition key for Event Hub output.",
      "pos": [
        5033,
        5093
      ]
    },
    {
      "content": "Adding an Event Hub as an output",
      "pos": [
        5099,
        5131
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Output<ept id=\"p1\">**</ept> from the top of the page, and then click <bpt id=\"p2\">**</bpt>Add Output<ept id=\"p2\">**</ept>.",
      "pos": [
        5140,
        5213
      ]
    },
    {
      "content": "Select Event Hub as the output option and click the right button at the bottom of the window.",
      "pos": [
        5214,
        5307
      ]
    },
    {
      "content": "graphic38",
      "pos": [
        5315,
        5324
      ]
    },
    {
      "content": "Provide the relevant information into the fields for the output and when finished click the right button at the bottom of the window to proceed.",
      "pos": [
        5341,
        5485
      ]
    },
    {
      "content": "graphic36",
      "pos": [
        5493,
        5502
      ]
    },
    {
      "content": "Validate the Event Serialization Format, Encoding type and Format are set to the appropriate values and click the check box to complete the action.",
      "pos": [
        5519,
        5666
      ]
    },
    {
      "content": "graphic37",
      "pos": [
        5674,
        5683
      ]
    },
    {
      "content": "Using Power BI as an output",
      "pos": [
        5700,
        5727
      ]
    },
    {
      "content": "Overview",
      "pos": [
        5739,
        5747
      ]
    },
    {
      "content": "Power BI can be utilized as an output for a Stream Analytics job to provide for a rich visualization experience for Stream Analytics users.",
      "pos": [
        5752,
        5891
      ]
    },
    {
      "content": "This capability can be utilized for operational dashboards, report generation and metric driven reporting.",
      "pos": [
        5892,
        5998
      ]
    },
    {
      "content": "For more information on Power BI visit the <bpt id=\"p1\">[</bpt>Power BI<ept id=\"p1\">](https://powerbi.microsoft.com/)</ept> site.",
      "pos": [
        5999,
        6090
      ]
    },
    {
      "content": "Parameters",
      "pos": [
        6096,
        6106
      ]
    },
    {
      "content": "There are a few parameters that are needed to configure a Power BI output.",
      "pos": [
        6112,
        6186
      ]
    },
    {
      "content": "Output Alias – Any friendly-named output alias that is easy to refer to.",
      "pos": [
        6191,
        6263
      ]
    },
    {
      "content": "This output alias is particularly helpful if it is decided to have multiple outputs for a job.",
      "pos": [
        6264,
        6358
      ]
    },
    {
      "content": "In that case, this alias will be referred to in your query.",
      "pos": [
        6359,
        6418
      ]
    },
    {
      "content": "For example, use the output alias value = “OutPbi”.",
      "pos": [
        6419,
        6470
      ]
    },
    {
      "content": "Dataset Name - Provide a dataset name that it is desired for the Power BI output to use.",
      "pos": [
        6474,
        6562
      ]
    },
    {
      "content": "For example, use “pbidemo”.",
      "pos": [
        6563,
        6590
      ]
    },
    {
      "content": "Table Name - Provide a table name under the dataset of the Power BI output.",
      "pos": [
        6594,
        6669
      ]
    },
    {
      "content": "For example, use “pbidemo”.",
      "pos": [
        6670,
        6697
      ]
    },
    {
      "content": "Currently, Power BI output from Stream Analytics jobs may only have one table in a dataset.",
      "pos": [
        6700,
        6791
      ]
    },
    {
      "content": "**",
      "pos": [
        6791,
        6793
      ]
    },
    {
      "content": "Adding Power BI as an output",
      "pos": [
        6799,
        6827
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Output<ept id=\"p1\">**</ept> from the top of the page, and then click <bpt id=\"p2\">**</bpt>Add Output<ept id=\"p2\">**</ept>.",
      "pos": [
        6837,
        6910
      ]
    },
    {
      "content": "Select Power BI as the output option.",
      "pos": [
        6911,
        6948
      ]
    },
    {
      "content": "graphic22",
      "pos": [
        6956,
        6965
      ]
    },
    {
      "content": "A screen like the following is presented.",
      "pos": [
        6983,
        7024
      ]
    },
    {
      "content": "graphic23",
      "pos": [
        7032,
        7041
      ]
    },
    {
      "content": "In this step, provide the work or school account for authorizing the Power BI output.",
      "pos": [
        7059,
        7144
      ]
    },
    {
      "content": "If you are not already signed up for Power BI, choose <bpt id=\"p1\">**</bpt>Sign up now<ept id=\"p1\">**</ept>.",
      "pos": [
        7145,
        7215
      ]
    },
    {
      "content": "Next a screen like the following will be presented.",
      "pos": [
        7220,
        7271
      ]
    },
    {
      "content": "graphic24",
      "pos": [
        7279,
        7288
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> One should not explicitly create the dataset and table in the Power BI dashboard.",
      "pos": [
        7307,
        7401
      ]
    },
    {
      "content": "The dataset and table will be automatically populated when the job is started and the job starts pumping output into Power BI.",
      "pos": [
        7402,
        7528
      ]
    },
    {
      "content": "Note that if the job query doesn’t return any results, the dataset and table will not be created.",
      "pos": [
        7529,
        7626
      ]
    },
    {
      "content": "Also be aware that if Power BI already had a dataset and table with the same name as the one provided in this Stream Analytics job, the existing data will be overwritten.",
      "pos": [
        7627,
        7797
      ]
    },
    {
      "pos": [
        7803,
        7882
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>Test Connection<ept id=\"p2\">**</ept> and now the output configuration is complete."
    },
    {
      "content": "Using Azure Table storage for an output",
      "pos": [
        7888,
        7927
      ]
    },
    {
      "content": "Table storage offers highly available, massively scalable storage, so that an application can automatically scale to meet user demand.",
      "pos": [
        7935,
        8069
      ]
    },
    {
      "content": "Table storage is Microsoft’s NoSQL key/attribute store which one can leverage for structured data with less constraints on the schema.",
      "pos": [
        8070,
        8204
      ]
    },
    {
      "content": "Azure Table storage can be used to store data for persistence and efficient retrieval.",
      "pos": [
        8205,
        8291
      ]
    },
    {
      "content": "For further information on Azure Table storage visit <bpt id=\"p1\">[</bpt>Azure Table storage<ept id=\"p1\">](./articles/storage/storage-introduction.md)</ept>.",
      "pos": [
        8292,
        8411
      ]
    },
    {
      "content": "Parameters",
      "pos": [
        8417,
        8427
      ]
    },
    {
      "content": "To start using an Azure Table storage, the following information is needed:",
      "pos": [
        8433,
        8508
      ]
    },
    {
      "content": "Storage account name (if this storage is in a different subscription from the streaming job).",
      "pos": [
        8513,
        8606
      ]
    },
    {
      "content": "Storage account key (if this storage is in a different subscription from the streaming job).",
      "pos": [
        8610,
        8702
      ]
    },
    {
      "content": "Output table name (will be created if not exist).",
      "pos": [
        8706,
        8755
      ]
    },
    {
      "content": "Partition Key (required).",
      "pos": [
        8759,
        8784
      ]
    },
    {
      "content": "Row key",
      "pos": [
        8788,
        8795
      ]
    },
    {
      "content": "For a better design of Partition key and Row key, please refer article below",
      "pos": [
        8797,
        8873
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Designing a Scalable Partitioning Strategy for Azure Table Storage<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/hh508997.aspx)</ept>.",
      "pos": [
        8874,
        8999
      ]
    },
    {
      "content": "Adding Azure Table storage as an output",
      "pos": [
        9005,
        9044
      ]
    },
    {
      "content": "graphic11",
      "pos": [
        9052,
        9061
      ]
    },
    {
      "pos": [
        9075,
        9164
      ],
      "content": "Go to the outputs tab of the job, and click on the <bpt id=\"p1\">**</bpt>ADD OUTPUT<ept id=\"p1\">**</ept> command and click next."
    },
    {
      "content": "graphic12",
      "pos": [
        9168,
        9177
      ]
    },
    {
      "pos": [
        9191,
        9230
      ],
      "content": "Choose <bpt id=\"p1\">**</bpt>Table storage<ept id=\"p1\">**</ept> as the output."
    },
    {
      "content": "graphic13",
      "pos": [
        9234,
        9243
      ]
    },
    {
      "content": "Enter the Azure Table information on the next page.",
      "pos": [
        9257,
        9308
      ]
    },
    {
      "content": "The output alias is the name that can be used in the query to direct the query output to this table.",
      "pos": [
        9309,
        9409
      ]
    },
    {
      "content": "graphic14",
      "pos": [
        9413,
        9422
      ]
    },
    {
      "content": "graphic15",
      "pos": [
        9438,
        9447
      ]
    },
    {
      "content": "Batch Size is the number of records for a batch operation.",
      "pos": [
        9461,
        9519
      ]
    },
    {
      "content": "Typically the default is sufficient for most jobs, refer to the <bpt id=\"p1\">[</bpt>Table Batch Operation spec<ept id=\"p1\">](https://msdn.microsoft.com/library/microsoft.windowsazure.storage.table.tablebatchoperation.aspx)</ept> for more details on modifying this setting.",
      "pos": [
        9520,
        9754
      ]
    },
    {
      "content": "If an Azure Storage account exists within the same subscription being used to create the job, select \"Use Storage Account from Current Subscription\", and select the Storage Account from the drop down.",
      "pos": [
        9756,
        9956
      ]
    },
    {
      "content": "Click next to add this output.",
      "pos": [
        9958,
        9988
      ]
    },
    {
      "content": "Two two operations should start, the first is to add the output.",
      "pos": [
        9989,
        10053
      ]
    },
    {
      "content": "graphic16",
      "pos": [
        10057,
        10066
      ]
    },
    {
      "content": "The second one is to test the connection.",
      "pos": [
        10080,
        10121
      ]
    },
    {
      "content": "Azure Stream Analytics will try to connect to that Storage Account and verify the credentials entered are correct.",
      "pos": [
        10122,
        10236
      ]
    },
    {
      "content": "Service Bus Queues",
      "pos": [
        10241,
        10259
      ]
    },
    {
      "content": "Introduction to Service Bus Queues concepts",
      "pos": [
        10271,
        10314
      ]
    },
    {
      "content": "Service Bus Queues offer a First In, First Out (FIFO) message delivery to one or more competing consumers.",
      "pos": [
        10319,
        10425
      ]
    },
    {
      "content": "Typically, messages are expected to be received and processed by the receivers in the temporal order in which they were added to the queue, and each message is received and processed by only one message consumer.",
      "pos": [
        10426,
        10638
      ]
    },
    {
      "pos": [
        10640,
        11023
      ],
      "content": "For further information on Service Bus Queues see <bpt id=\"p1\">[</bpt>Service Bus Queues, Topics, and Subscriptions<ept id=\"p1\">]</ept><bpt id=\"p2\">(https://msdn.microsoft.com/library/azure/hh367516.aspx \"</bpt>Service Bus Queues, Topics, and Subscriptions<ept id=\"p2\">\")</ept> and <bpt id=\"p3\">[</bpt>An Introduction to Service Bus Queues<ept id=\"p3\">]</ept><bpt id=\"p4\">(http://blogs.msdn.com/b/appfabric/archive/2011/05/17/an-introduction-to-service-bus-queues.aspx \"</bpt>An Introduction to Service Bus Queues<ept id=\"p4\">\")</ept>."
    },
    {
      "content": "Parameters",
      "pos": [
        11029,
        11039
      ]
    },
    {
      "content": "To start using a Service Bus Queues output, the following information will be needed:",
      "pos": [
        11045,
        11130
      ]
    },
    {
      "content": "Output Alias – Any friendly-named output alias that is easy to refer to.",
      "pos": [
        11135,
        11207
      ]
    },
    {
      "content": "This output alias is particularly helpful if it is decided to have multiple outputs for a job.",
      "pos": [
        11208,
        11302
      ]
    },
    {
      "content": "In that case, this alias will be referred to in the job query.",
      "pos": [
        11303,
        11365
      ]
    },
    {
      "content": "The namespace and service bus name.",
      "pos": [
        11369,
        11404
      ]
    },
    {
      "content": "Queue Name - Queues are messaging entities, similar to event hubs and topics.",
      "pos": [
        11408,
        11485
      ]
    },
    {
      "content": "They're designed to collect event streams from a number of different devices and services.",
      "pos": [
        11486,
        11576
      ]
    },
    {
      "content": "When a queue is created it is also given a specific name.",
      "pos": [
        11577,
        11634
      ]
    },
    {
      "content": "What serialization format is utilized for the data (Avro, CSV, JSON).",
      "pos": [
        11638,
        11707
      ]
    },
    {
      "content": "Adding Service Bus Queues as an output",
      "pos": [
        11713,
        11751
      ]
    },
    {
      "content": "graphic31",
      "pos": [
        11759,
        11768
      ]
    },
    {
      "content": "Then supply the details as shown below and select next.",
      "pos": [
        11782,
        11837
      ]
    },
    {
      "content": "graphic32",
      "pos": [
        11841,
        11850
      ]
    },
    {
      "content": "Verify your data format and serialization are correct.",
      "pos": [
        11864,
        11918
      ]
    },
    {
      "content": "graphic33",
      "pos": [
        11922,
        11931
      ]
    },
    {
      "content": "Service Bus Topics",
      "pos": [
        11948,
        11966
      ]
    },
    {
      "content": "Introduction to Service Bus Topics concepts",
      "pos": [
        11978,
        12021
      ]
    },
    {
      "content": "While Service Bus Queues provide a one to one communication method from sender to receiver, Service Bus Topics provide a one-to-many form of communication.",
      "pos": [
        12026,
        12181
      ]
    },
    {
      "pos": [
        12183,
        12565
      ],
      "content": "For further information on Service Bus Topics see <bpt id=\"p1\">[</bpt>Service Bus Queues, Topics, and Subscriptions<ept id=\"p1\">]</ept><bpt id=\"p2\">(https://msdn.microsoft.com/library/azure/hh367516.aspx \"</bpt>Service Bus Queues, Topics, and Subscriptions<ept id=\"p2\">\")</ept> and <bpt id=\"p3\">[</bpt>An Introduction to Service Bus Topics<ept id=\"p3\">]</ept><bpt id=\"p4\">(http://blogs.msdn.com/b/appfabric/archive/2011/05/25/an-introduction-to-service-bus-topics.aspx \"</bpt>An Introduction to Service Bus Topics<ept id=\"p4\">\")</ept>"
    },
    {
      "content": "Parameters",
      "pos": [
        12571,
        12581
      ]
    },
    {
      "content": "To start using a Service Bus Topics output, the following information will be needed:",
      "pos": [
        12587,
        12672
      ]
    },
    {
      "content": "Output Alias – Any friendly-named output alias that is easy to refer to.",
      "pos": [
        12677,
        12749
      ]
    },
    {
      "content": "This output alias is particularly helpful if it is decided to have multiple outputs for a job.",
      "pos": [
        12750,
        12844
      ]
    },
    {
      "content": "In that case, this alias will be referred to in your query.",
      "pos": [
        12845,
        12904
      ]
    },
    {
      "content": "The namespace and service bus name.",
      "pos": [
        12908,
        12943
      ]
    },
    {
      "content": "Topic Name - Topics are messaging entities, similar to event hubs and queues.",
      "pos": [
        12947,
        13024
      ]
    },
    {
      "content": "They're designed to collect event streams from a number of different devices and services.",
      "pos": [
        13025,
        13115
      ]
    },
    {
      "content": "When a topic is created, it is also given a specific name.",
      "pos": [
        13116,
        13174
      ]
    },
    {
      "content": "The messages sent to a Topic will not be available unless a subscription is created, so ensure there are one or more subscriptions under the topic.",
      "pos": [
        13175,
        13322
      ]
    },
    {
      "content": "What serialization format is utilized for the data (Avro, CSV, JSON).",
      "pos": [
        13326,
        13395
      ]
    },
    {
      "content": "Adding Service Bus Topics as an output",
      "pos": [
        13401,
        13439
      ]
    },
    {
      "content": "Select output to Service Bus Topics.",
      "pos": [
        13445,
        13481
      ]
    },
    {
      "content": "graphic34",
      "pos": [
        13485,
        13494
      ]
    },
    {
      "content": "Then supply the details as shown below and select next.",
      "pos": [
        13508,
        13563
      ]
    },
    {
      "content": "graphic35",
      "pos": [
        13567,
        13576
      ]
    },
    {
      "content": "Verify your data format and serialization are correct.",
      "pos": [
        13590,
        13644
      ]
    },
    {
      "content": "graphic33",
      "pos": [
        13648,
        13657
      ]
    },
    {
      "content": "Get help",
      "pos": [
        13675,
        13683
      ]
    },
    {
      "pos": [
        13684,
        13826
      ],
      "content": "For further assistance, try our <bpt id=\"p1\">[</bpt>Azure Stream Analytics forum<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept>"
    },
    {
      "content": "Next steps",
      "pos": [
        13831,
        13841
      ]
    },
    {
      "content": "Introduction to Azure Stream Analytics",
      "pos": [
        13846,
        13884
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics",
      "pos": [
        13923,
        13963
      ]
    },
    {
      "content": "Scale Azure Stream Analytics jobs",
      "pos": [
        14001,
        14034
      ]
    },
    {
      "content": "Azure Stream Analytics Query Language Reference",
      "pos": [
        14071,
        14118
      ]
    },
    {
      "content": "Azure Stream Analytics Management REST API Reference",
      "pos": [
        14179,
        14231
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Create Stream Analytics Outputs | Microsoft Azure\" \n    description=\"Learn how to connect to and configure the output targets for Stream Analytics solutions.\" \n    documentationCenter=\"\" \n    services=\"stream-analytics\"\n    authors=\"jeffstokes72\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"stream-analytics\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.tgt_pltfrm=\"na\" \n    ms.workload=\"data-services\" \n    ms.date=\"08/19/2015\" \n    ms.author=\"jeffstok\"/>\n\n# Create Stream Analytics outputs\n\n## Understanding Stream Analytics outputs ##\n---\nWhen creating a Stream Analytics job, one of the considerations is how the output of the job is consumed. How are the consumers of the data transformation viewing the results of the Stream Analytics job? What tool(s) will they be using to analyze the output? Is data retention or warehousing a requirement?\n\nAzure Stream Analytics provides seven different methods for storing and viewing job outputs. SQL Database, Blob storage, Event Hubs, Service Bus Queues, Service Bus Topics, Power BI and Table storage. This provides for both ease of viewing job output and flexibility in the consumption and storage of the job output for data warehousing and other purposes.\n\n## Using a SQL Database as an output ##\n---\nA SQL Database can be used as an output for data that is relational in nature or for applications that depend on content being hosted in a relational database. For more information on Azure SQL databases see [Azure SQL Databases](http://azure.microsoft.com/services/sql-database/).\n\n### Parameters ###\n\nTo start using a SQL Database, the following information will be needed about the SQL Database:\n\n1. Server Name\n2. Database Name\n3. A valid username/password combination\n4. Output table name. This table must already exist, the job will not create it.\n\n### Adding SQL Database as an output ###\n\n![graphic1][graphic1]\n\nGo to the outputs tab of the job, and click on the **ADD OUTPUT** command and click next.\n\n![graphic2][graphic2]\n\nChoose **SQL Database** as the output.\n\n![graphic3][graphic3]\n\nEnter the database information on the next page. The output alias is a friendly name used in queries to direct the query output to this database. The default alias if if only a single output is present is \"output\".\n\n![graphic4][graphic4]\n\nIf the database exists within the same subscription as the Stream Analytics job, the option to select \"Use SQL Database from Current Subscription\", is available. Then select the database from the drop down list.\n\n![graphic5][graphic5]\n\nClick next to add this output. Two operations should start, the first is to add the output.\n\n![graphic6][graphic6]\n\nThe second operation is to test the connection. Azure Stream Analytics will try to connect to the SQL Database and verify the credentials entered are correct and that the table is accessible.\n\n## Using Blob storage as an output ##\n---\nFor an introduction on Azure Blob storage and its usage, see the documentation at [How to use Blobs](./articles/storage/storage-dotnet-how-to-use-blobs.md).\n\n### Parameters ###\n\nTo start using a Blob storage output, the following information will be needed:\n\n1. If the storage account is in a different subscription than the streaming job then fields will appear to provide the Storage Account Name and Storage Account Key.\n2. The container name.\n3. The file name prefix.\n4. What serialization format is utilized for the data (Avro, CSV, JSON).\n\n### Adding Blob storage as an output ###\n\nSelect output to Blob storage.\n\n![graphic20][graphic20]\n\nThen supply the details as shown below:\n\n![graphic21][graphic21]\n\n## Using an Event Hub as an output ##\n---\n### Overview ###\n \nEvent Hubs are a highly scalable event ingestor, and typically are the most common way for Stream Analytics data ingress. Their robust handling of high numbers of events also make them perfect for job output.  One use of an Event Hub as output is when the output of an Stream Analytics job will be the input of another streaming job. For further details on Event Hubs visit the portal at [Event Hubs](https://azure.microsoft.com/services/event-hubs/ \"Event Hubs\").\n \n### Parameters ###\n\nThere are a few parameters that are needed to configure Event Hub data streams.\n\n1. Service Bus Namespace: Service Bus Namespace of the Event Hub. A Service Bus namespace is a container for a set of messaging entities. When creating a new Event Hub, a Service Bus namespace is also created. \n2. Event Hub Name: Name of the Event Hub.  It’s the name specified when creating a new Event Hub. \n3. Event Hub Policy Name: The name of the shared access policy for accessing the Event Hub.  Shared access policies can be configured for an Event Hub on the Configure tab. Each shared access policy will have a name, permissions set, and access keys generated.\n4. Event Hub Policy Key:  The primary or secondary key of the shared access policy for accessing the Event Hub.  \n5. Partition Key Column:  Optional parameter for Event Hub outputs. This column contains the partition key for Event Hub output.\n\n### Adding an Event Hub as an output ###\n\n1. Click **Output** from the top of the page, and then click **Add Output**. Select Event Hub as the output option and click the right button at the bottom of the window.\n\n    ![graphic38][graphic38]\n\n2. Provide the relevant information into the fields for the output and when finished click the right button at the bottom of the window to proceed.\n\n    ![graphic36][graphic36]\n\n3. Validate the Event Serialization Format, Encoding type and Format are set to the appropriate values and click the check box to complete the action.\n\n    ![graphic37][graphic37]\n\n## Using Power BI as an output ##\n---\n### Overview ###\nPower BI can be utilized as an output for a Stream Analytics job to provide for a rich visualization experience for Stream Analytics users. This capability can be utilized for operational dashboards, report generation and metric driven reporting. For more information on Power BI visit the [Power BI](https://powerbi.microsoft.com/) site.\n\n### Parameters ###\n\nThere are a few parameters that are needed to configure a Power BI output.\n\n1. Output Alias – Any friendly-named output alias that is easy to refer to. This output alias is particularly helpful if it is decided to have multiple outputs for a job. In that case, this alias will be referred to in your query. For example, use the output alias value = “OutPbi”.\n2. Dataset Name - Provide a dataset name that it is desired for the Power BI output to use. For example, use “pbidemo”.\n2. Table Name - Provide a table name under the dataset of the Power BI output. For example, use “pbidemo”. **Currently, Power BI output from Stream Analytics jobs may only have one table in a dataset.**\n\n### Adding Power BI as an output ###\n\n1.  Click **Output** from the top of the page, and then click **Add Output**. Select Power BI as the output option.\n\n    ![graphic22][graphic22]\n\n2.  A screen like the following is presented.\n\n    ![graphic23][graphic23]\n\n3.  In this step, provide the work or school account for authorizing the Power BI output. If you are not already signed up for Power BI, choose **Sign up now**.\n4.  Next a screen like the following will be presented.\n\n    ![graphic24][graphic24]\n\n\n>   [AZURE.NOTE] One should not explicitly create the dataset and table in the Power BI dashboard. The dataset and table will be automatically populated when the job is started and the job starts pumping output into Power BI. Note that if the job query doesn’t return any results, the dataset and table will not be created. Also be aware that if Power BI already had a dataset and table with the same name as the one provided in this Stream Analytics job, the existing data will be overwritten.\n\n*   Click **OK**, **Test Connection** and now the output configuration is complete.\n\n\n## Using Azure Table storage for an output ##\n---\nTable storage offers highly available, massively scalable storage, so that an application can automatically scale to meet user demand. Table storage is Microsoft’s NoSQL key/attribute store which one can leverage for structured data with less constraints on the schema. Azure Table storage can be used to store data for persistence and efficient retrieval. For further information on Azure Table storage visit [Azure Table storage](./articles/storage/storage-introduction.md).\n\n### Parameters ###\n\nTo start using an Azure Table storage, the following information is needed:\n\n1. Storage account name (if this storage is in a different subscription from the streaming job).\n2. Storage account key (if this storage is in a different subscription from the streaming job).\n3. Output table name (will be created if not exist).\n4. Partition Key (required).\n5. Row key\n\nFor a better design of Partition key and Row key, please refer article below\n[Designing a Scalable Partitioning Strategy for Azure Table Storage](https://msdn.microsoft.com/library/azure/hh508997.aspx).\n\n### Adding Azure Table storage as an output ###\n\n![graphic11][graphic11]\n\nGo to the outputs tab of the job, and click on the **ADD OUTPUT** command and click next.\n\n![graphic12][graphic12]\n\nChoose **Table storage** as the output.\n\n![graphic13][graphic13]\n\nEnter the Azure Table information on the next page. The output alias is the name that can be used in the query to direct the query output to this table.\n\n![graphic14][graphic14]\n\n![graphic15][graphic15]\n\nBatch Size is the number of records for a batch operation. Typically the default is sufficient for most jobs, refer to the [Table Batch Operation spec](https://msdn.microsoft.com/library/microsoft.windowsazure.storage.table.tablebatchoperation.aspx) for more details on modifying this setting.\n\nIf an Azure Storage account exists within the same subscription being used to create the job, select \"Use Storage Account from Current Subscription\", and select the Storage Account from the drop down.\n\nClick next to add this output. Two two operations should start, the first is to add the output.\n\n![graphic16][graphic16]\n\nThe second one is to test the connection. Azure Stream Analytics will try to connect to that Storage Account and verify the credentials entered are correct.\n\n## Service Bus Queues ##\n---\n### Introduction to Service Bus Queues concepts ###\nService Bus Queues offer a First In, First Out (FIFO) message delivery to one or more competing consumers. Typically, messages are expected to be received and processed by the receivers in the temporal order in which they were added to the queue, and each message is received and processed by only one message consumer.\n\nFor further information on Service Bus Queues see [Service Bus Queues, Topics, and Subscriptions](https://msdn.microsoft.com/library/azure/hh367516.aspx \"Service Bus Queues, Topics, and Subscriptions\") and [An Introduction to Service Bus Queues](http://blogs.msdn.com/b/appfabric/archive/2011/05/17/an-introduction-to-service-bus-queues.aspx \"An Introduction to Service Bus Queues\").\n\n### Parameters ###\n\nTo start using a Service Bus Queues output, the following information will be needed:\n\n1. Output Alias – Any friendly-named output alias that is easy to refer to. This output alias is particularly helpful if it is decided to have multiple outputs for a job. In that case, this alias will be referred to in the job query.\n2. The namespace and service bus name.\n3. Queue Name - Queues are messaging entities, similar to event hubs and topics. They're designed to collect event streams from a number of different devices and services. When a queue is created it is also given a specific name.\n4. What serialization format is utilized for the data (Avro, CSV, JSON).\n\n### Adding Service Bus Queues as an output ###\n\n![graphic31][graphic31]\n\nThen supply the details as shown below and select next.\n\n![graphic32][graphic32]\n\nVerify your data format and serialization are correct.\n\n![graphic33][graphic33]\n\n## Service Bus Topics ##\n---\n### Introduction to Service Bus Topics concepts ###\nWhile Service Bus Queues provide a one to one communication method from sender to receiver, Service Bus Topics provide a one-to-many form of communication.\n\nFor further information on Service Bus Topics see [Service Bus Queues, Topics, and Subscriptions](https://msdn.microsoft.com/library/azure/hh367516.aspx \"Service Bus Queues, Topics, and Subscriptions\") and [An Introduction to Service Bus Topics](http://blogs.msdn.com/b/appfabric/archive/2011/05/25/an-introduction-to-service-bus-topics.aspx \"An Introduction to Service Bus Topics\")\n\n### Parameters ###\n\nTo start using a Service Bus Topics output, the following information will be needed:\n\n1. Output Alias – Any friendly-named output alias that is easy to refer to. This output alias is particularly helpful if it is decided to have multiple outputs for a job. In that case, this alias will be referred to in your query.\n2. The namespace and service bus name.\n3. Topic Name - Topics are messaging entities, similar to event hubs and queues. They're designed to collect event streams from a number of different devices and services. When a topic is created, it is also given a specific name. The messages sent to a Topic will not be available unless a subscription is created, so ensure there are one or more subscriptions under the topic.\n4. What serialization format is utilized for the data (Avro, CSV, JSON).\n\n### Adding Service Bus Topics as an output ###\n\nSelect output to Service Bus Topics.\n\n![graphic34][graphic34]\n\nThen supply the details as shown below and select next.\n\n![graphic35][graphic35]\n\nVerify your data format and serialization are correct.\n\n![graphic33][graphic33]\n\n\n## Get help\nFor further assistance, try our [Azure Stream Analytics forum](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)\n\n## Next steps\n\n- [Introduction to Azure Stream Analytics](stream-analytics-introduction.md)\n- [Get started using Azure Stream Analytics](stream-analytics-get-started.md)\n- [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n- [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx)\n- [Azure Stream Analytics Management REST API Reference](https://msdn.microsoft.com/library/azure/dn835031.aspx)\n\n\n\n\n[graphic1]: ./media/stream-analytics-connect-data-event-outputs/1-stream-analytics-connect-data-event-input-output.png\n[graphic2]: ./media/stream-analytics-connect-data-event-outputs/2-stream-analytics-connect-data-event-input-output.png\n[graphic3]: ./media/stream-analytics-connect-data-event-outputs/3-stream-analytics-connect-data-event-input-output.png\n[graphic4]: ./media/stream-analytics-connect-data-event-outputs/4-stream-analytics-connect-data-event-input-output.png\n[graphic5]: ./media/stream-analytics-connect-data-event-outputs/5-stream-analytics-connect-data-event-input-output.png\n[graphic6]: ./media/stream-analytics-connect-data-event-outputs/6-stream-analytics-connect-data-event-input-output.png\n[graphic7]: ./media/stream-analytics-connect-data-event-outputs/7-stream-analytics-connect-data-event-input-output.png\n[graphic8]: ./media/stream-analytics-connect-data-event-outputs/8-stream-analytics-connect-data-event-input-output.png\n[graphic9]: ./media/stream-analytics-connect-data-event-outputs/9-stream-analytics-connect-data-event-input-output.png\n[graphic10]: ./media/stream-analytics-connect-data-event-outputs/10-stream-analytics-connect-data-event-input-output.png\n[graphic11]: ./media/stream-analytics-connect-data-event-outputs/11-stream-analytics-connect-data-event-input-output.png\n[graphic12]: ./media/stream-analytics-connect-data-event-outputs/12-stream-analytics-connect-data-event-input-output.png\n[graphic13]: ./media/stream-analytics-connect-data-event-outputs/13-stream-analytics-connect-data-event-input-output.png\n[graphic14]: ./media/stream-analytics-connect-data-event-outputs/14-stream-analytics-connect-data-event-input-output.png\n[graphic15]: ./media/stream-analytics-connect-data-event-outputs/15-stream-analytics-connect-data-event-input-output.png\n[graphic16]: ./media/stream-analytics-connect-data-event-outputs/16-stream-analytics-connect-data-event-input-output.png\n[graphic17]: ./media/stream-analytics-connect-data-event-outputs/17-stream-analytics-connect-data-event-input-output.png\n[graphic18]: ./media/stream-analytics-connect-data-event-outputs/18-stream-analytics-connect-data-event-input-output.png\n[graphic19]: ./media/stream-analytics-connect-data-event-outputs/19-stream-analytics-connect-data-event-input-output.png\n[graphic20]: ./media/stream-analytics-connect-data-event-outputs/20-stream-analytics-connect-data-event-input-output.png\n[graphic21]: ./media/stream-analytics-connect-data-event-outputs/21-stream-analytics-connect-data-event-input-output.png\n[graphic22]: ./media/stream-analytics-connect-data-event-outputs/22-stream-analytics-connect-data-event-input-output.png\n[graphic23]: ./media/stream-analytics-connect-data-event-outputs/23-stream-analytics-connect-data-event-input-output.png\n[graphic24]: ./media/stream-analytics-connect-data-event-outputs/24-stream-analytics-connect-data-event-input-output.png\n[graphic25]: ./media/stream-analytics-connect-data-event-outputs/25-stream-analytics-connect-data-event-input-output.png\n[graphic26]: ./media/stream-analytics-connect-data-event-outputs/26-stream-analytics-connect-data-event-input-output.png\n[graphic27]: ./media/stream-analytics-connect-data-event-outputs/27-stream-analytics-connect-data-event-input-output.png\n[graphic28]: ./media/stream-analytics-connect-data-event-outputs/28-stream-analytics-connect-data-event-input-output.png\n[graphic29]: ./media/stream-analytics-connect-data-event-outputs/29-stream-analytics-connect-data-event-input-output.png\n[graphic30]: ./media/stream-analytics-connect-data-event-outputs/30-stream-analytics-connect-data-event-input-output.png\n[graphic31]: ./media/stream-analytics-connect-data-event-outputs/31-stream-analytics-connect-data-event-input-output.png\n[graphic32]: ./media/stream-analytics-connect-data-event-outputs/32-stream-analytics-connect-data-event-input-output.png\n[graphic33]: ./media/stream-analytics-connect-data-event-outputs/33-stream-analytics-connect-data-event-input-output.png\n[graphic34]: ./media/stream-analytics-connect-data-event-outputs/34-stream-analytics-connect-data-event-input-output.png\n[graphic35]: ./media/stream-analytics-connect-data-event-outputs/35-stream-analytics-connect-data-event-input-output.png\n[graphic36]: ./media/stream-analytics-connect-data-event-outputs/36-stream-analytics-connect-data-event-input-output.png\n[graphic37]: ./media/stream-analytics-connect-data-event-outputs/37-stream-analytics-connect-data-event-input-output.png\n[graphic38]: ./media/stream-analytics-connect-data-event-outputs/38-stream-analytics-connect-data-event-input-output.png"
}