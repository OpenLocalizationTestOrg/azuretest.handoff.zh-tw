{
  "nodes": [
    {
      "content": "Move Data to and from  Azure Blob Storage | Microsoft Azure",
      "pos": [
        28,
        87
      ]
    },
    {
      "content": "Move Data to and from  Azure Blob Storage",
      "pos": [
        107,
        148
      ]
    },
    {
      "content": "Move Data to and from Azure Blob Storage",
      "pos": [
        521,
        561
      ]
    },
    {
      "content": "This topic describes three methods to move data to and from Azure Blob storage:",
      "pos": [
        563,
        642
      ]
    },
    {
      "content": "Using Azure Storage Explorer",
      "pos": [
        647,
        675
      ]
    },
    {
      "content": "Using AzCopy Command line utility",
      "pos": [
        691,
        724
      ]
    },
    {
      "content": "Using Azure SDK in Python",
      "pos": [
        738,
        763
      ]
    },
    {
      "content": "Which method is best for you will depend on your scenario.",
      "pos": [
        778,
        836
      ]
    },
    {
      "content": "The <bpt id=\"p1\">[</bpt>Scenarios for the Advanced Analytics Process and Technology (ADAPT) in Azure Machine Learning<ept id=\"p1\">](../machine-learning-data-science-plan-sample-scenarios.md)</ept> article helps you determine the resources you need for a variety of data science workflows used in the advanced analytics process.",
      "pos": [
        837,
        1126
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> As an alternative, you can use <bpt id=\"p1\">[</bpt>Azure Data Factory<ept id=\"p1\">](https://azure.microsoft.com/en-us/services/data-factory/)</ept> to create and schedule a pipeline that will download data from Azure blob storage, pass it to a published Azure Machine Learning web service, receive the predictive analytics results, and upload the results to storage.",
      "pos": [
        1130,
        1470
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Create predictive pipelines using Azure Data Factory and Azure Machine Learning<ept id=\"p1\">](../data-factory/data-factory-create-predictive-pipelines.md)</ept>.",
      "pos": [
        1471,
        1640
      ]
    },
    {
      "pos": [
        1645,
        1866
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> For a complete introduction to Azure blob storage, please refer to <bpt id=\"p1\">[</bpt>Azure Blob Basics<ept id=\"p1\">](../storage-dotnet-how-to-use-blobs.md)</ept> and  <bpt id=\"p2\">[</bpt>Azure Blob Service<ept id=\"p2\">](https://msdn.microsoft.com/library/azure/dd179376.aspx)</ept>."
    },
    {
      "content": "Before uploading/downloading data, you must know your Azure storage account name and account key.",
      "pos": [
        1869,
        1966
      ]
    },
    {
      "content": "For instructions on getting this information, see the \"How to: View, copy and regenerate storage access keys\" section of <bpt id=\"p1\">[</bpt>Manage storage accounts<ept id=\"p1\">](../storage-create-storage-account.md)</ept>.",
      "pos": [
        1967,
        2152
      ]
    },
    {
      "content": "This document assumes that you have an Azure storage account and the corresponding storage key(s).",
      "pos": [
        2153,
        2251
      ]
    },
    {
      "content": "Use Azure Storage Explorer",
      "pos": [
        2279,
        2305
      ]
    },
    {
      "content": "Azure Storage Explorer is a free windows based tool for inspecting and altering data in an Azure storage account.",
      "pos": [
        2308,
        2421
      ]
    },
    {
      "content": "It can be downloaded from <bpt id=\"p1\">[</bpt>Azure Storage Explorer<ept id=\"p1\">](http://azurestorageexplorer.codeplex.com/)</ept>.",
      "pos": [
        2422,
        2516
      ]
    },
    {
      "content": "The following steps document how to upload/download data using Azure Storage Explorer.",
      "pos": [
        2517,
        2603
      ]
    },
    {
      "content": "Launch Azure Storage Explorer",
      "pos": [
        2610,
        2639
      ]
    },
    {
      "content": "If the storage account you want to access has not been added to Azure Storage Explorer, click the \"Add Account\" button to add the account.",
      "pos": [
        2645,
        2783
      ]
    },
    {
      "content": "If it has already been added, select the account from the \"--Select a Storage Account--\" dropdown.",
      "pos": [
        2784,
        2882
      ]
    },
    {
      "content": "![Create workspace][1]",
      "pos": [
        2885,
        2907
      ]
    },
    {
      "content": "Enter storage account name and storage account key, and then click Add Storage Account.",
      "pos": [
        2916,
        3003
      ]
    },
    {
      "content": "You may add multiple storage accounts and each account will be displayed on a tab.",
      "pos": [
        3004,
        3086
      ]
    },
    {
      "content": "The containers under this storage account are shown in the left panel.",
      "pos": [
        3087,
        3157
      ]
    },
    {
      "content": "Select a container to see the blobs in the container in the right panel.",
      "pos": [
        3158,
        3230
      ]
    },
    {
      "content": "![Create workspace][2]",
      "pos": [
        3233,
        3255
      ]
    },
    {
      "content": "![Create workspace][3]",
      "pos": [
        3261,
        3283
      ]
    },
    {
      "content": "Upload data by clicking the \"Upload\" button.",
      "pos": [
        3292,
        3336
      ]
    },
    {
      "content": "Select one or multiple files to upload from the file system and click \"Open\" to begin uploading the file(s).",
      "pos": [
        3337,
        3445
      ]
    },
    {
      "content": "Download data by selecting the blob in the corresponding container and clicking the \"Download\" button .",
      "pos": [
        3449,
        3552
      ]
    },
    {
      "content": "Use AzCopy",
      "pos": [
        3577,
        3587
      ]
    },
    {
      "content": "AzCopy is a command line utility to upload and download data.",
      "pos": [
        3589,
        3650
      ]
    },
    {
      "pos": [
        3653,
        3906
      ],
      "content": "<bpt id=\"p1\">**</bpt>Warning<ept id=\"p1\">**</ept> If you are using a machine different from the VM that was set up earlier in the advanced analytics process, please install AzCopy using the following installation instructions: <bpt id=\"p2\">[</bpt>Download and install AzCopy<ept id=\"p2\">](../storage-use-azcopy.md#install)</ept>."
    },
    {
      "content": "Examples of uploading/downloading files to/from blobs:",
      "pos": [
        3911,
        3965
      ]
    },
    {
      "content": "When uploading files, /S will upload files recursively.",
      "pos": [
        5281,
        5336
      ]
    },
    {
      "content": "Without this parameter, any files in the subdirectory will not be uploaded.",
      "pos": [
        5337,
        5412
      ]
    },
    {
      "content": "When downloading file, /S will search the container recursively until all files in the specified directory and its subdirectories or all files that matching the specified pattern in the given directory and its subdirectories, are downloaded.",
      "pos": [
        5420,
        5661
      ]
    },
    {
      "content": "You cannot specify a specific blob file to download using the /Source parameter.",
      "pos": [
        5670,
        5750
      ]
    },
    {
      "content": "To download a specific file, specify the blob file name to download using the /Pattern parameter.",
      "pos": [
        5751,
        5848
      ]
    },
    {
      "content": "/S parameter can be used to have AzCopy look for a file name pattern recursively.",
      "pos": [
        5849,
        5930
      ]
    },
    {
      "content": "Without the pattern parameter, AzCopy will download all files in that directory.",
      "pos": [
        5931,
        6011
      ]
    },
    {
      "pos": [
        6014,
        6149
      ],
      "content": "For detailed usage of AzCopy, please refer to <bpt id=\"p1\">[</bpt>Getting Started with the AzCopy Command-Line Utility<ept id=\"p1\">](../storage-use-azcopy.md#install)</ept>."
    },
    {
      "content": "Use Python",
      "pos": [
        6179,
        6189
      ]
    },
    {
      "content": "With the Python API provided in Azure SDK, you can",
      "pos": [
        6191,
        6241
      ]
    },
    {
      "content": "Create a container",
      "pos": [
        6245,
        6263
      ]
    },
    {
      "content": "Upload a blob into a container",
      "pos": [
        6266,
        6296
      ]
    },
    {
      "content": "Download blobs",
      "pos": [
        6299,
        6313
      ]
    },
    {
      "content": "List the blobs in a container",
      "pos": [
        6316,
        6345
      ]
    },
    {
      "content": "Delete a blob",
      "pos": [
        6348,
        6361
      ]
    },
    {
      "content": "This section documents how to list, upload and download blobs.",
      "pos": [
        6363,
        6425
      ]
    },
    {
      "content": "For more details of the usage of the Python API, please refer <bpt id=\"p1\">[</bpt>How to Use the Blob Storage Service from Python<ept id=\"p1\">](../storage-python-how-to-use-blob-storage.md)</ept>.",
      "pos": [
        6426,
        6584
      ]
    },
    {
      "pos": [
        6589,
        6817
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you are using a machine different from the VM that was set up earlier in the advanced analytics process, you need to install the <bpt id=\"p1\">[</bpt>Python Azure SDK<ept id=\"p1\">](../python-how-to-install.md)</ept> before using the sample code below."
    },
    {
      "content": "Upload Data to Blob",
      "pos": [
        6823,
        6842
      ]
    },
    {
      "content": "Add the following snippet near the top of any Python code in which you wish to programmatically access Azure Storage:",
      "pos": [
        6844,
        6961
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>BlobService<ept id=\"p1\">**</ept> object lets you work with containers and blobs.",
      "pos": [
        7006,
        7073
      ]
    },
    {
      "content": "The following code creates a BlobService object using the storage account name and account key.",
      "pos": [
        7074,
        7169
      ]
    },
    {
      "content": "Replace account name and account key with your real account and key.",
      "pos": [
        7170,
        7238
      ]
    },
    {
      "content": "Use the following methods to upload data to a blob:",
      "pos": [
        7346,
        7397
      ]
    },
    {
      "content": "put\\_block\\_blob\\_from\\_path (uploads the contents of a file from the specified path)",
      "pos": [
        7403,
        7488
      ]
    },
    {
      "content": "put\\_block_blob\\_from\\_file (uploads the contents from an already opened file/stream)",
      "pos": [
        7492,
        7577
      ]
    },
    {
      "content": "put\\_block\\_blob\\_from\\_bytes (uploads an array of bytes)",
      "pos": [
        7581,
        7638
      ]
    },
    {
      "content": "put\\_block\\_blob\\_from\\_text (uploads the specified text value using the specified encoding)",
      "pos": [
        7642,
        7734
      ]
    },
    {
      "content": "The following sample code uploads a local file to a container:",
      "pos": [
        7737,
        7799
      ]
    },
    {
      "content": "The following sample code uploads all the files (excluding directories) in a local directory to blob storage:",
      "pos": [
        7919,
        8028
      ]
    },
    {
      "content": "Download Data from Blob",
      "pos": [
        8957,
        8980
      ]
    },
    {
      "content": "Use the following methods to download data from a blob:",
      "pos": [
        8982,
        9037
      ]
    },
    {
      "content": "get\\_blob\\_to\\_path",
      "pos": [
        9041,
        9060
      ]
    },
    {
      "content": "get\\_blob\\_to\\_file",
      "pos": [
        9064,
        9083
      ]
    },
    {
      "content": "get\\_blob\\_to\\_bytes",
      "pos": [
        9087,
        9107
      ]
    },
    {
      "content": "get\\_blob\\_to\\_text",
      "pos": [
        9111,
        9130
      ]
    },
    {
      "content": "These methods that perform the necessary chunking when the size of the data exceeds 64 MB.",
      "pos": [
        9133,
        9223
      ]
    },
    {
      "content": "The following sample code downloads the contents of a blob in a container to a local file:",
      "pos": [
        9226,
        9316
      ]
    },
    {
      "content": "The following sample code downloads all blobs from a container.",
      "pos": [
        9425,
        9488
      ]
    },
    {
      "content": "It uses list\\_blobs to get the list of available blobs in the container and downloads them to a local directory.",
      "pos": [
        9489,
        9601
      ]
    },
    {
      "content": "test",
      "pos": [
        10723,
        10727
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Move Data to and from  Azure Blob Storage | Microsoft Azure\" \n    description=\"Move Data to and from  Azure Blob Storage\" \n    services=\"machine-learning,storage\" \n    documentationCenter=\"\" \n    authors=\"msolhab\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"09/01/2015\" \n    ms.author=\"sunliangms;sachouks;mohabib;bradsev\" />\n\n# Move Data to and from Azure Blob Storage\n\nThis topic describes three methods to move data to and from Azure Blob storage:\n\n- [Using Azure Storage Explorer](#explorer)\n- [Using AzCopy Command line utility](#AzCopy)\n- [Using Azure SDK in Python](#PythonSDK)\n\nWhich method is best for you will depend on your scenario. The [Scenarios for the Advanced Analytics Process and Technology (ADAPT) in Azure Machine Learning](../machine-learning-data-science-plan-sample-scenarios.md) article helps you determine the resources you need for a variety of data science workflows used in the advanced analytics process.\n\n> [AZURE.TIP] As an alternative, you can use [Azure Data Factory](https://azure.microsoft.com/en-us/services/data-factory/) to create and schedule a pipeline that will download data from Azure blob storage, pass it to a published Azure Machine Learning web service, receive the predictive analytics results, and upload the results to storage. For more information, see [Create predictive pipelines using Azure Data Factory and Azure Machine Learning](../data-factory/data-factory-create-predictive-pipelines.md).\n\n\n> [AZURE.NOTE] For a complete introduction to Azure blob storage, please refer to [Azure Blob Basics](../storage-dotnet-how-to-use-blobs.md) and  [Azure Blob Service](https://msdn.microsoft.com/library/azure/dd179376.aspx). \n\nBefore uploading/downloading data, you must know your Azure storage account name and account key. For instructions on getting this information, see the \"How to: View, copy and regenerate storage access keys\" section of [Manage storage accounts](../storage-create-storage-account.md). This document assumes that you have an Azure storage account and the corresponding storage key(s).\n\n\n<a id=\"explorer\"></a>\n## Use Azure Storage Explorer \n\nAzure Storage Explorer is a free windows based tool for inspecting and altering data in an Azure storage account. It can be downloaded from [Azure Storage Explorer](http://azurestorageexplorer.codeplex.com/). The following steps document how to upload/download data using Azure Storage Explorer. \n\n1.  Launch Azure Storage Explorer \n2.  If the storage account you want to access has not been added to Azure Storage Explorer, click the \"Add Account\" button to add the account. If it has already been added, select the account from the \"--Select a Storage Account--\" dropdown.  \n![Create workspace][1]\n<br>\n3. Enter storage account name and storage account key, and then click Add Storage Account. You may add multiple storage accounts and each account will be displayed on a tab. The containers under this storage account are shown in the left panel. Select a container to see the blobs in the container in the right panel.  \n![Create workspace][2]\n<br>\n![Create workspace][3]\n<br>\n4. Upload data by clicking the \"Upload\" button. Select one or multiple files to upload from the file system and click \"Open\" to begin uploading the file(s).\n5. Download data by selecting the blob in the corresponding container and clicking the \"Download\" button .\n\n<a id=\"AzCopy\"></a>\n## Use AzCopy\n\nAzCopy is a command line utility to upload and download data. \n\n**Warning** If you are using a machine different from the VM that was set up earlier in the advanced analytics process, please install AzCopy using the following installation instructions: [Download and install AzCopy](../storage-use-azcopy.md#install).\n\n###Examples of uploading/downloading files to/from blobs:\n\n    # Uploading from local file system\n    AzCopy /Source:<your_local_directory> /Dest: https://<your_account_name>.blob.core.windows.net/<your_container_name> /DestKey:<your_account_key> /S \n\n    # Downloading blobs to local file system\n    AzCopy /Source:https://<your_account_name>.blob.core.windows.net/<your_container_name>/<your_sub_directory_at_blob>  /Dest:<your_local_directory> /SourceKey:<your_account_key> /Pattern:<file_pattern> /S\n\n    # Transferring blobs between Azure containers\n    AzCopy /Source:https://<your_account_name1>.blob.core.windows.net/<your_container_name1>/<your_sub_directory_at_blob1> /Dest:https://<your_account_name2>.blob.core.windows.net/<your_container_name2>/<your_sub_directory_at_blob2> /SourceKey:<your_account_key1> /DestKey:<your_account_key2> /Pattern:<file_pattern> /S\n    \n    <your_account_name>: your storage account name\n    <your_account_key>: your storage account key\n    <your_container_name>: your container name\n    <your_sub_directory_at_blob>: the sub directory in the container \n    <your_local_directory>: directory of local file system where files to be uploaded from or the directory of local file system files to be downloaded to\n    <file_pattern>: pattern of file names to be transferred. The standard wildcards are supported\n\n> [AZURE.TIP]   \n> 1. When uploading files, /S will upload files recursively. Without this parameter, any files in the subdirectory will not be uploaded.  \n> 2. When downloading file, /S will search the container recursively until all files in the specified directory and its subdirectories or all files that matching the specified pattern in the given directory and its subdirectories, are downloaded.  \n> 3.  You cannot specify a specific blob file to download using the /Source parameter. To download a specific file, specify the blob file name to download using the /Pattern parameter. /S parameter can be used to have AzCopy look for a file name pattern recursively. Without the pattern parameter, AzCopy will download all files in that directory. \n\nFor detailed usage of AzCopy, please refer to [Getting Started with the AzCopy Command-Line Utility](../storage-use-azcopy.md#install).\n\n\n<a id=\"PythonSDK\"></a> \n## Use Python\n\nWith the Python API provided in Azure SDK, you can\n\n- Create a container\n- Upload a blob into a container\n- Download blobs\n- List the blobs in a container\n- Delete a blob\n\nThis section documents how to list, upload and download blobs. For more details of the usage of the Python API, please refer [How to Use the Blob Storage Service from Python](../storage-python-how-to-use-blob-storage.md). \n\n> [AZURE.NOTE] If you are using a machine different from the VM that was set up earlier in the advanced analytics process, you need to install the [Python Azure SDK](../python-how-to-install.md) before using the sample code below.\n\n### Upload Data to Blob\n\nAdd the following snippet near the top of any Python code in which you wish to programmatically access Azure Storage:\n\n    from azure.storage import BlobService\n\nThe **BlobService** object lets you work with containers and blobs. The following code creates a BlobService object using the storage account name and account key. Replace account name and account key with your real account and key.\n    \n    blob_service = BlobService(account_name=\"<your_account_name>\", account_key=\"<your_account_key>\")\n\nUse the following methods to upload data to a blob:\n \n1. put\\_block\\_blob\\_from\\_path (uploads the contents of a file from the specified path)\n2. put\\_block_blob\\_from\\_file (uploads the contents from an already opened file/stream)\n3. put\\_block\\_blob\\_from\\_bytes (uploads an array of bytes)\n4. put\\_block\\_blob\\_from\\_text (uploads the specified text value using the specified encoding)\n \nThe following sample code uploads a local file to a container:\n    \n    blob_service.put_block_blob_from_path(\"<your_container_name>\", \"<your_blob_name>\", \"<your_local_file_name>\")\n\nThe following sample code uploads all the files (excluding directories) in a local directory to blob storage:\n\n    from azure.storage import BlobService\n    from os import listdir\n    from os.path import isfile, join\n    \n    # Set parameters here\n    ACCOUNT_NAME = \"<your_account_name>\"\n    ACCOUNT_KEY = \"<your_account_key>\"\n    CONTAINER_NAME = \"<your_container_name>\"\n    LOCAL_DIRECT = \"<your_local_directory>\"     \n    \n    blob_service = BlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n    # find all files in the LOCAL_DIRECT (excluding directory)\n    local_file_list = [f for f in listdir(LOCAL_DIRECT) if isfile(join(LOCAL_DIRECT, f))]\n    \n    file_num = len(local_file_list)\n    for i in range(file_num):\n        local_file = join(LOCAL_DIRECT, local_file_list[i])\n        blob_name = local_file_list[i]\n        try:\n            blob_service.put_block_blob_from_path(CONTAINER_NAME, blob_name, local_file)\n        except:\n            print \"something wrong happened when uploading the data %s\"%blob_name\n\n### Download Data from Blob\n\nUse the following methods to download data from a blob:\n1. get\\_blob\\_to\\_path\n2. get\\_blob\\_to\\_file\n3. get\\_blob\\_to\\_bytes\n4. get\\_blob\\_to\\_text \n\nThese methods that perform the necessary chunking when the size of the data exceeds 64 MB. \n\nThe following sample code downloads the contents of a blob in a container to a local file: \n\n    blob_service.get_blob_to_path(\"<your_container_name>\", \"<your_blob_name>\", \"<your_local_file_name>\")\n\nThe following sample code downloads all blobs from a container. It uses list\\_blobs to get the list of available blobs in the container and downloads them to a local directory. \n\n    from azure.storage import BlobService\n    from os.path import join\n    \n    # Set parameters here\n    ACCOUNT_NAME = \"<your_account_name>\"\n    ACCOUNT_KEY = \"<your_account_key>\"\n    CONTAINER_NAME = \"<your_container_name>\"\n    LOCAL_DIRECT = \"<your_local_directory>\"     \n    \n    blob_service = BlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n    \n    # List all blobs and download them one by one\n    blobs = blob_service.list_blobs(CONTAINER_NAME)\n    for blob in blobs:\n        local_file = join(LOCAL_DIRECT, blob.name)\n        try:\n            blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n        except:\n            print \"something wrong happened when downloading the data %s\"%blob.name\n\n<!-- Images -->\n\n[1]: ./media/machine-learning-data-science-move-azure-blob/data-science-process-uploading-data-to-blob-storage-img1.png\n[2]: ./media/machine-learning-data-science-move-azure-blob/data-science-process-uploading-data-to-blob-storage-img2.png\n[3]: ./media/machine-learning-data-science-move-azure-blob/data-science-process-uploading-data-to-blob-storage-img3.png\n \ntest\n"
}