<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Data Factory - Create Predictive Pipelines using Data Factory and Machine Learning | Microsoft Azure</source>
          <target state="new">Data Factory - Create Predictive Pipelines using Data Factory and Machine Learning | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning</source>
          <target state="new">Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning</source>
          <target state="new">Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Overview</source>
          <target state="new">Overview</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Azure Data Factory enables you to easily create pipelines that leverages a published <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">][azure-machine-learning]</ept> web service for predictive analytics.</source>
          <target state="new">Azure Data Factory enables you to easily create pipelines that leverages a published <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">][azure-machine-learning]</ept> web service for predictive analytics.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>This enables you to use Azure Data Factory to orchestrate  data movement and processing, and then perform batch scoring using Azure Machine Learning.</source>
          <target state="new">This enables you to use Azure Data Factory to orchestrate  data movement and processing, and then perform batch scoring using Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>To achieve this, you will need to do the following:</source>
          <target state="new">To achieve this, you will need to do the following:</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">**</bpt>AzureMLBatchScoring<ept id="p1">**</ept> activity.</source>
          <target state="new">Use the <bpt id="p1">**</bpt>AzureMLBatchScoring<ept id="p1">**</ept> activity.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Request URI<ept id="p1">**</ept> for the Batch Execution API.</source>
          <target state="new"><bpt id="p1">**</bpt>Request URI<ept id="p1">**</ept> for the Batch Execution API.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>You can find the Request URI by clicking on the <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> link in the web services page (shown below).</source>
          <target state="new">You can find the Request URI by clicking on the <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> link in the web services page (shown below).</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>API key<ept id="p1">**</ept> for the published Azure Machine Learning web service.</source>
          <target state="new"><bpt id="p1">**</bpt>API key<ept id="p1">**</ept> for the published Azure Machine Learning web service.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>You can find this information by clicking on the web service that you have published.</source>
          <target state="new">You can find this information by clicking on the web service that you have published.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Machine Learning Dashboard</source>
          <target state="new">Machine Learning Dashboard</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>predictive pipeline<ept id="p1">**</ept> has these parts:</source>
          <target state="new">A <bpt id="p1">**</bpt>predictive pipeline<ept id="p1">**</ept> has these parts:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Input and output tables</source>
          <target state="new">Input and output tables</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Azure Storage/Azure SQL and Azure ML linked services</source>
          <target state="new">Azure Storage/Azure SQL and Azure ML linked services</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>A pipeline with Azure ML Batch Scoring Activity</source>
          <target state="new">A pipeline with Azure ML Batch Scoring Activity</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> You can use Web service parameters that are exposed by a published Azure Machine Learning Web service in Azure Data Factory (ADF) pipelines.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> You can use Web service parameters that are exposed by a published Azure Machine Learning Web service in Azure Data Factory (ADF) pipelines.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>For more information, see the Web Service Parameters section in this article.</source>
          <target state="new">For more information, see the Web Service Parameters section in this article.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="new">Example</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>This example uses Azure Storage to hold both the input and output data.</source>
          <target state="new">This example uses Azure Storage to hold both the input and output data.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>You can also use Azure SQL Database instead of using Azure Storage.</source>
          <target state="new">You can also use Azure SQL Database instead of using Azure Storage.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>We recommend that you go through the <bpt id="p1">[</bpt>Build your first pipeline with Data Factory<ept id="p1">][adf-build-1st-pipeline]</ept> tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, tables, pipeline) in this example.</source>
          <target state="new">We recommend that you go through the <bpt id="p1">[</bpt>Build your first pipeline with Data Factory<ept id="p1">][adf-build-1st-pipeline]</ept> tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, tables, pipeline) in this example.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for your <bpt id="p2">**</bpt>Azure Storage<ept id="p2">**</ept>.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for your <bpt id="p2">**</bpt>Azure Storage<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>If the scoring input and output files will be in different storage accounts, you will need two linked services.</source>
          <target state="new">If the scoring input and output files will be in different storage accounts, you will need two linked services.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Here is a JSON example:</source>
          <target state="new">Here is a JSON example:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Create the <bpt id="p1">**</bpt>input<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>table<ept id="p2">**</ept>.</source>
          <target state="new">Create the <bpt id="p1">**</bpt>input<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>table<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Note that unlike some other Data Factory tables, these must both contain both <bpt id="p1">**</bpt>folderPath<ept id="p1">**</ept> and <bpt id="p2">**</bpt>fileName<ept id="p2">**</ept> values.</source>
          <target state="new">Note that unlike some other Data Factory tables, these must both contain both <bpt id="p1">**</bpt>folderPath<ept id="p1">**</ept> and <bpt id="p2">**</bpt>fileName<ept id="p2">**</ept> values.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files.</source>
          <target state="new">You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice.</source>
          <target state="new">You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>In that case, you would not include the <bpt id="p1">**</bpt>external<ept id="p1">**</ept> and <bpt id="p2">**</bpt>externalData<ept id="p2">**</ept> settings shown in the example below, and your ScoringInputBlob would be the output table of a different Activity.</source>
          <target state="new">In that case, you would not include the <bpt id="p1">**</bpt>external<ept id="p1">**</ept> and <bpt id="p2">**</bpt>externalData<ept id="p2">**</ept> settings shown in the example below, and your ScoringInputBlob would be the output table of a different Activity.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Your batch scoring csv file must have the column header row.</source>
          <target state="new">Your batch scoring csv file must have the column header row.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>If you are using the <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> to create/move the csv into the blob storage, you should set the sink property <bpt id="p2">**</bpt>blobWriterAddHeader<ept id="p2">**</ept> to <bpt id="p3">**</bpt>true<ept id="p3">**</ept>.</source>
          <target state="new">If you are using the <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> to create/move the csv into the blob storage, you should set the sink property <bpt id="p2">**</bpt>blobWriterAddHeader<ept id="p2">**</ept> to <bpt id="p3">**</bpt>true<ept id="p3">**</ept>.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>If the csv file does not have the header row, you may see the following error: <bpt id="p1">**</bpt>Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1<ept id="p1">**</ept>.</source>
          <target state="new">If the csv file does not have the header row, you may see the following error: <bpt id="p1">**</bpt>Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Create the <bpt id="p1">**</bpt>output<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>table<ept id="p2">**</ept>.</source>
          <target state="new">Create the <bpt id="p1">**</bpt>output<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>table<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>This example uses partitioning to create a unique output path for each slice execution.</source>
          <target state="new">This example uses partitioning to create a unique output path for each slice execution.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Without this, the activity would overwrite the file.</source>
          <target state="new">Without this, the activity would overwrite the file.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> of type: <bpt id="p2">**</bpt>AzureMLLinkedService<ept id="p2">**</ept>, providing the API key and model batch scoring URL.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> of type: <bpt id="p2">**</bpt>AzureMLLinkedService<ept id="p2">**</ept>, providing the API key and model batch scoring URL.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Finally, author a pipeline containing an <bpt id="p1">**</bpt>AzureMLBatchScoringActivity<ept id="p1">**</ept>.</source>
          <target state="new">Finally, author a pipeline containing an <bpt id="p1">**</bpt>AzureMLBatchScoringActivity<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>It will get the location of the input file from your input tables, call the AzureML batch scoring API, and copy the batch scoring output to the blob given in your output table.</source>
          <target state="new">It will get the location of the input file from your input tables, call the AzureML batch scoring API, and copy the batch scoring output to the blob given in your output table.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Unlike some other Data Factory activities, AzureMLBatchScoringActivity can have only one input and one output table.</source>
          <target state="new">Unlike some other Data Factory activities, AzureMLBatchScoringActivity can have only one input and one output table.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Web Service Parameters</source>
          <target state="new">Web Service Parameters</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>You can use Web service parameters that are exposed by a published Azure Machine Learning Web service in Azure Data Factory (ADF) pipelines.</source>
          <target state="new">You can use Web service parameters that are exposed by a published Azure Machine Learning Web service in Azure Data Factory (ADF) pipelines.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>You can create an experiment in Azure Machine Learning and publish it as a web service, and then use that web service in multiple ADF pipelines or activities, passing in different inputs via the Web Service Parameters.</source>
          <target state="new">You can create an experiment in Azure Machine Learning and publish it as a web service, and then use that web service in multiple ADF pipelines or activities, passing in different inputs via the Web Service Parameters.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Passing values for Web service parameters</source>
          <target state="new">Passing values for Web service parameters</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Add a <bpt id="p1">**</bpt>typeProperties<ept id="p1">**</ept> section to the <bpt id="p2">**</bpt>AzureMLBatchScoringActivty<ept id="p2">**</ept> section in the pipeline JSON to specify values for Web service parameters in that section as shown in the following example:</source>
          <target state="new">Add a <bpt id="p1">**</bpt>typeProperties<ept id="p1">**</ept> section to the <bpt id="p2">**</bpt>AzureMLBatchScoringActivty<ept id="p2">**</ept> section in the pipeline JSON to specify values for Web service parameters in that section as shown in the following example:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>You can also use <bpt id="p1">[</bpt>Data Factory Functions<ept id="p1">](https://msdn.microsoft.com/library/dn835056.aspx)</ept> in passing values for the Web service parameters as shown in the following example:</source>
          <target state="new">You can also use <bpt id="p1">[</bpt>Data Factory Functions<ept id="p1">](https://msdn.microsoft.com/library/dn835056.aspx)</ept> in passing values for the Web service parameters as shown in the following example:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Reader and Writer Modules</source>
          <target state="new">Reader and Writer Modules</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>A common scenario for using Web service parameters is the use of Azure SQL Readers and Writers.</source>
          <target state="new">A common scenario for using Web service parameters is the use of Azure SQL Readers and Writers.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>The reader module is used to load data into an experiment from data management services outside Azure Machine Learning Studio and the writer module is to save data from your experiments into data management services outside Azure Machine Learning Studio.</source>
          <target state="new">The reader module is used to load data into an experiment from data management services outside Azure Machine Learning Studio and the writer module is to save data from your experiments into data management services outside Azure Machine Learning Studio.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>For details about Azure Blob/Azure SQL reader/writer, see <bpt id="p1">[</bpt>Reader<ept id="p1">](https://msdn.microsoft.com/library/azure/dn905997.aspx)</ept> and <bpt id="p2">[</bpt>Writer<ept id="p2">](https://msdn.microsoft.com/library/azure/dn905984.aspx)</ept> topics on MSDN Library.</source>
          <target state="new">For details about Azure Blob/Azure SQL reader/writer, see <bpt id="p1">[</bpt>Reader<ept id="p1">](https://msdn.microsoft.com/library/azure/dn905997.aspx)</ept> and <bpt id="p2">[</bpt>Writer<ept id="p2">](https://msdn.microsoft.com/library/azure/dn905984.aspx)</ept> topics on MSDN Library.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>The example in the previous section used the Azure Blob reader and Azure Blob writer.</source>
          <target state="new">The example in the previous section used the Azure Blob reader and Azure Blob writer.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>This section discusses using Azure SQL reader and Azure SQL writer.</source>
          <target state="new">This section discusses using Azure SQL reader and Azure SQL writer.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Azure SQL as a data source</source>
          <target state="new">Azure SQL as a data source</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>In Azure ML Studio, you can build an experiment and publish a Web service with an Azure SQL Reader for the input.</source>
          <target state="new">In Azure ML Studio, you can build an experiment and publish a Web service with an Azure SQL Reader for the input.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The Azure SQL Reader has connection properties that can be exposed as Web service parameters, allowing values for the connection properties to be passed at runtime in the batch scoring request.</source>
          <target state="new">The Azure SQL Reader has connection properties that can be exposed as Web service parameters, allowing values for the connection properties to be passed at runtime in the batch scoring request.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>At runtime, the details from the input Data Factory table will be used by the Data Factory service to fill in the Web service parameters.</source>
          <target state="new">At runtime, the details from the input Data Factory table will be used by the Data Factory service to fill in the Web service parameters.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Note that you must use default names (Database server name, Database name, Server user account name, Server user account password) for the Web service parameters for this integration with the Data Factory service to work.</source>
          <target state="new">Note that you must use default names (Database server name, Database name, Server user account name, Server user account password) for the Web service parameters for this integration with the Data Factory service to work.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>If you have any additional Web service parameters, use the <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> section of the activity JSON.</source>
          <target state="new">If you have any additional Web service parameters, use the <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> section of the activity JSON.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>If you specify values for Azure SQL Reader parameters in this section, the values will override the values picked up from the input Azure SQL linked service.</source>
          <target state="new">If you specify values for Azure SQL Reader parameters in this section, the values will override the values picked up from the input Azure SQL linked service.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>We do not recommend you specify values for Azure SQL Reader directly in the webServiceParameters section.</source>
          <target state="new">We do not recommend you specify values for Azure SQL Reader directly in the webServiceParameters section.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Use the section to pass values for any additional parameters.</source>
          <target state="new">Use the section to pass values for any additional parameters.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>To use an Azure SQL Reader via an Azure Data Factory pipeline, do the following:</source>
          <target state="new">To use an Azure SQL Reader via an Azure Data Factory pipeline, do the following:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Create an <bpt id="p1">**</bpt>Azure SQL linked service<ept id="p1">**</ept>.</source>
          <target state="new">Create an <bpt id="p1">**</bpt>Azure SQL linked service<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Create a Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> that uses <bpt id="p2">**</bpt>AzureSqlTable<ept id="p2">**</ept>.</source>
          <target state="new">Create a Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> that uses <bpt id="p2">**</bpt>AzureSqlTable<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Set that Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> as the <bpt id="p2">**</bpt>input<ept id="p2">**</ept> for the <bpt id="p3">**</bpt>AzureMLBatchScoringActivity<ept id="p3">**</ept> in the pipeline JSON.</source>
          <target state="new">Set that Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> as the <bpt id="p2">**</bpt>input<ept id="p2">**</ept> for the <bpt id="p3">**</bpt>AzureMLBatchScoringActivity<ept id="p3">**</ept> in the pipeline JSON.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Azure SQL as a data sink</source>
          <target state="new">Azure SQL as a data sink</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>As with Azure SQL Reader, an Azure SQL Writer can also have its properties exposed as Web service parameters.</source>
          <target state="new">As with Azure SQL Reader, an Azure SQL Writer can also have its properties exposed as Web service parameters.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>An Azure SQL Writer uses settings from either the linked service associated with the input table or the output table.</source>
          <target state="new">An Azure SQL Writer uses settings from either the linked service associated with the input table or the output table.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>The following table describes when the input linked service is used vs. output linked service.</source>
          <target state="new">The following table describes when the input linked service is used vs. output linked service.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Output/Input</source>
          <target state="new">Output/Input</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Input is Azure SQL</source>
          <target state="new">Input is Azure SQL</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Input is Azure Blob</source>
          <target state="new">Input is Azure Blob</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Output is Azure SQL</source>
          <target state="new">Output is Azure SQL</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>The Data Factory service uses the connection string information from the INPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</source>
          <target state="new">The Data Factory service uses the connection string information from the INPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Note that you must use these default names for Web service parameters in Azure ML Studio.</source>
          <target state="new">Note that you must use these default names for Web service parameters in Azure ML Studio.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>If the Azure SQL Reader and Azure SQL Writer in your Azure ML model share the same Web service parameters mentioned above, you are fine.</source>
          <target state="new">If the Azure SQL Reader and Azure SQL Writer in your Azure ML model share the same Web service parameters mentioned above, you are fine.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>If they do not share same Web service paramers, for example, if the Azure SQL Writer uses parameters names: Database server name1, Database name1, Server user account name1, Server user account password1 (with '1' at the end), you must pass values for these OUTPUT web service parameters in the webServiceParameters section of activity JSON.</source>
          <target state="new">If they do not share same Web service paramers, for example, if the Azure SQL Writer uses parameters names: Database server name1, Database name1, Server user account name1, Server user account password1 (with '1' at the end), you must pass values for these OUTPUT web service parameters in the webServiceParameters section of activity JSON.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>You can pass values for any other Web service parameters using the webServiceParameters section of activity JSON.</source>
          <target state="new">You can pass values for any other Web service parameters using the webServiceParameters section of activity JSON.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>The Data Factory service uses the connection string information from the OUTPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</source>
          <target state="new">The Data Factory service uses the connection string information from the OUTPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Note that you must use these default names for Web service parameters in Azure ML Studio.</source>
          <target state="new">Note that you must use these default names for Web service parameters in Azure ML Studio.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>You can pass values for any other Web service parameters using the webServiceParameters section of activity JSON .</source>
          <target state="new">You can pass values for any other Web service parameters using the webServiceParameters section of activity JSON .</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>Input blob will be used as input location.</source>
          <target state="new">Input blob will be used as input location.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Output is Azure Blob</source>
          <target state="new">Output is Azure Blob</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>The Data Factory service uses the connection string information from the INPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</source>
          <target state="new">The Data Factory service uses the connection string information from the INPUT linked service to generate the web service parameters with names: "Database server name", "Database name", "Server user account name", "Server user account password".</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Note that you must use these default names for Web service parameters in Azure ML Studio.</source>
          <target state="new">Note that you must use these default names for Web service parameters in Azure ML Studio.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>You must pass values for any Web service parameters using the WebServiceParameters section of activity JSON.</source>
          <target state="new">You must pass values for any Web service parameters using the WebServiceParameters section of activity JSON.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Blobs will be used as input and output locations.</source>
          <target state="new">Blobs will be used as input and output locations.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Azure SQL Writer may encounter key violations if it is overwriting an identity column.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Azure SQL Writer may encounter key violations if it is overwriting an identity column.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>You should ensure that you structure your output table to avoid this situation.</source>
          <target state="new">You should ensure that you structure your output table to avoid this situation.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>You can use staging tables with a Stored Procedure Activity to merge rows, or to truncate the data before scoring.</source>
          <target state="new">You can use staging tables with a Stored Procedure Activity to merge rows, or to truncate the data before scoring.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>If you use this approach, set concurrency setting of the executionPolicy to 1.</source>
          <target state="new">If you use this approach, set concurrency setting of the executionPolicy to 1.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Azure Blob as a source</source>
          <target state="new">Azure Blob as a source</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>When using the Reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input.</source>
          <target state="new">When using the Reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight.</source>
          <target state="new">The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>The Reader module allows you to read files (with no extensions) by configuring the <bpt id="p1">**</bpt>Path to container, directory or blob<ept id="p1">**</ept> property of the reader module to point to the container/folder that contains the files as shown below.</source>
          <target state="new">The Reader module allows you to read files (with no extensions) by configuring the <bpt id="p1">**</bpt>Path to container, directory or blob<ept id="p1">**</ept> property of the reader module to point to the container/folder that contains the files as shown below.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>Note, the asterisk (i.e. \*) <bpt id="p1">**</bpt>specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\*)<ept id="p1">**</ept> will be read as part of the experiment.</source>
          <target state="new">Note, the asterisk (i.e. \*) <bpt id="p1">**</bpt>specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\*)<ept id="p1">**</ept> will be read as part of the experiment.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Azure Blob properties</source>
          <target state="new">Azure Blob properties</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Example of using Web service parameters</source>
          <target state="new">Example of using Web service parameters</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Pipeline with AzureMLBatchScoringActivity with Web Service Parameters</source>
          <target state="new">Pipeline with AzureMLBatchScoringActivity with Web Service Parameters</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>In the above JSON example:</source>
          <target state="new">In the above JSON example:</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>The Azure ML model uses both Azure SQL Reader and Azure SQL Writer</source>
          <target state="new">The Azure ML model uses both Azure SQL Reader and Azure SQL Writer</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>When exposed via Web service, the default names are used for the parameters</source>
          <target state="new">When exposed via Web service, the default names are used for the parameters</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>For the <bpt id="p1">**</bpt>reader<ept id="p1">**</ept>: Database server name, Database name, Server user account name, and Server user account password.</source>
          <target state="new">For the <bpt id="p1">**</bpt>reader<ept id="p1">**</ept>: Database server name, Database name, Server user account name, and Server user account password.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>For the <bpt id="p1">**</bpt>writer<ept id="p1">**</ept>: Database server name1, Database name1, Server user account name1, and Server user account password1.</source>
          <target state="new">For the <bpt id="p1">**</bpt>writer<ept id="p1">**</ept>: Database server name1, Database name1, Server user account name1, and Server user account password1.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Note that the reader and writer do not share parameters in this case.</source>
          <target state="new">Note that the reader and writer do not share parameters in this case.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>The Data Factory service automatically generates values for Web service parameters with the names <bpt id="p1">**</bpt>Database server name<ept id="p1">**</ept>, <bpt id="p2">**</bpt>Database name<ept id="p2">**</ept>, <bpt id="p3">**</bpt>Server user account name<ept id="p3">**</ept>, and <bpt id="p4">**</bpt>Server user account password<ept id="p4">**</ept>, which match the names of the input reader.</source>
          <target state="new">The Data Factory service automatically generates values for Web service parameters with the names <bpt id="p1">**</bpt>Database server name<ept id="p1">**</ept>, <bpt id="p2">**</bpt>Database name<ept id="p2">**</ept>, <bpt id="p3">**</bpt>Server user account name<ept id="p3">**</ept>, and <bpt id="p4">**</bpt>Server user account password<ept id="p4">**</ept>, which match the names of the input reader.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Therefore, you do not need to explicitly pass the values for these parameters via <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> in the activity JSON below.</source>
          <target state="new">Therefore, you do not need to explicitly pass the values for these parameters via <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> in the activity JSON below.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>The parameters for writer (the ones with '1' suffix) are not automatically filled in by the Data Factory service.</source>
          <target state="new">The parameters for writer (the ones with '1' suffix) are not automatically filled in by the Data Factory service.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Therefore, you need to specify values for these parameters in the <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> section of the activity JSON.</source>
          <target state="new">Therefore, you need to specify values for these parameters in the <bpt id="p1">**</bpt>webServiceParameters<ept id="p1">**</ept> section of the activity JSON.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Customer ID<ept id="p1">**</ept>, <bpt id="p2">**</bpt>scored labels<ept id="p2">**</ept>, and <bpt id="p3">**</bpt>scored probabilities<ept id="p3">**</ept> are saved as comma separated columns.</source>
          <target state="new"><bpt id="p1">**</bpt>Customer ID<ept id="p1">**</ept>, <bpt id="p2">**</bpt>scored labels<ept id="p2">**</ept>, and <bpt id="p3">**</bpt>scored probabilities<ept id="p3">**</ept> are saved as comma separated columns.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Data table name<ept id="p1">**</ept> in this example corresponds to a table in the output database.</source>
          <target state="new">The <bpt id="p1">**</bpt>Data table name<ept id="p1">**</ept> in this example corresponds to a table in the output database.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</source>
          <target state="new">Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>For example: 2014-10-14T16:32:41Z.</source>
          <target state="new">For example: 2014-10-14T16:32:41Z.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</source>
          <target state="new">The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</source>
          <target state="new">If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</source>
          <target state="new">To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</source>
          <target state="new">See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">b7ab04b1ec2154a30cce953f9eba7859a591f3d6</xliffext:olfilehash>
  </header>
</xliff>