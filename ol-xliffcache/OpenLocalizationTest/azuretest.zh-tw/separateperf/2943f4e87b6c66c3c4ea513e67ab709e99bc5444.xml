{
  "nodes": [
    {
      "content": "Run a Hadoop job using DocumentDB and HDInsight | Microsoft Azure",
      "pos": [
        28,
        93
      ]
    },
    {
      "content": "Learn how to run a simple Hive, Pig, and MapReduce job with DocumentDB and Azure HDInsight.",
      "pos": [
        113,
        204
      ]
    },
    {
      "pos": [
        524,
        606
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"DocumentDB-HDInsight\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run a Hadoop job using DocumentDB and HDInsight"
    },
    {
      "content": "This tutorial shows you how to run <bpt id=\"p1\">[</bpt>Apache Hive<ept id=\"p1\">][apache-hive]</ept>, <bpt id=\"p2\">[</bpt>Apache Pig<ept id=\"p2\">][apache-pig]</ept>, and <bpt id=\"p3\">[</bpt>Apache Hadoop<ept id=\"p3\">][apache-hadoop]</ept> MapReduce jobs on Azure HDInsight with DocumentDB's Hadoop connector.",
      "pos": [
        608,
        801
      ]
    },
    {
      "content": "DocumentDB's Hadoop connector allows DocumentDB to act as both a source and sink for Hive, Pig, and MapReduce jobs.",
      "pos": [
        802,
        917
      ]
    },
    {
      "content": "This tutorial will use DocumentDB as both the data source and destination for Hadoop jobs.",
      "pos": [
        918,
        1008
      ]
    },
    {
      "content": "After completing this tutorial, you'll be able to answer the following questions:",
      "pos": [
        1011,
        1092
      ]
    },
    {
      "content": "How do I load data from DocumentDB using a Hive, Pig, or MapReduce job?",
      "pos": [
        1096,
        1167
      ]
    },
    {
      "content": "How do I store data in DocumentDB using a Hive, Pig, or MapReduce job?",
      "pos": [
        1170,
        1240
      ]
    },
    {
      "content": "We recommend getting started by watching the following video, where we run through a Hive job using DocumentDB and HDInsight.",
      "pos": [
        1242,
        1367
      ]
    },
    {
      "content": "Then, return to this article, where you'll receive the full details on how you can run analytics jobs on your DocumentDB data.",
      "pos": [
        1445,
        1571
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> This tutorial assumes that you have prior experience using Apache Hadoop, Hive, and/or Pig.",
      "pos": [
        1575,
        1678
      ]
    },
    {
      "content": "If you are new to Apache Hadoop, Hive, and Pig, we recommend visiting the <bpt id=\"p1\">[</bpt>Apache Hadoop documentation<ept id=\"p1\">][apache-hadoop-doc]</ept>.",
      "pos": [
        1679,
        1802
      ]
    },
    {
      "content": "This tutorial also assumes that you have prior experience with DocumentDB and have a DocumentDB account.",
      "pos": [
        1803,
        1907
      ]
    },
    {
      "content": "If you are new to DocumentDB or you do not have a DocumentDB account, please check out our <bpt id=\"p1\">[</bpt>Getting Started<ept id=\"p1\">][getting-started]</ept> page.",
      "pos": [
        1908,
        2039
      ]
    },
    {
      "content": "Don't have time to complete the tutorial and just want to get the full sample PowerShell scripts for Hive, Pig, and MapReduce?",
      "pos": [
        2041,
        2167
      ]
    },
    {
      "content": "Not a problem, get them <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">][documentdb-hdinsight-samples]</ept>.",
      "pos": [
        2168,
        2229
      ]
    },
    {
      "content": "The download also contains the hql, pig, and java files for these samples.",
      "pos": [
        2230,
        2304
      ]
    },
    {
      "pos": [
        2309,
        2351
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"NewestVersion\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Newest Version"
    },
    {
      "content": "Hadoop Connector Version",
      "pos": [
        2384,
        2408
      ]
    },
    {
      "content": "1.1.0",
      "pos": [
        2426,
        2431
      ]
    },
    {
      "content": "Script Uri",
      "pos": [
        2454,
        2464
      ]
    },
    {
      "content": "https://portalcontent.blob.core.windows.net/scriptaction/documentdb-hadoop-installer-v03.ps1",
      "pos": [
        2482,
        2574
      ]
    },
    {
      "content": "Date Modified",
      "pos": [
        2597,
        2610
      ]
    },
    {
      "content": "07/20/2015",
      "pos": [
        2628,
        2638
      ]
    },
    {
      "content": "Supported HDInsight Versions",
      "pos": [
        2661,
        2689
      ]
    },
    {
      "content": "3.1, 3.2",
      "pos": [
        2707,
        2715
      ]
    },
    {
      "content": "Change Log",
      "pos": [
        2738,
        2748
      ]
    },
    {
      "content": "Updated DocumentDB Java SDK to 1.1.0",
      "pos": [
        2766,
        2802
      ]
    },
    {
      "content": "<ph id=\"ph1\">\n            Fixed connector's compatability with the &lt;a href=\"https://www.microsoft.com/download/details.aspx?id=40886\"&gt;</ph>Microsoft Hive ODBC driver",
      "pos": [
        2999,
        3146
      ]
    },
    {
      "pos": [
        3280,
        3321
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"Prerequisites\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "Before following the instructions in this tutorial, ensure that you have the following:",
      "pos": [
        3322,
        3409
      ]
    },
    {
      "content": "A DocumentDB account, a database, and a collection with documents inside.",
      "pos": [
        3413,
        3486
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Getting Started with DocumentDB<ept id=\"p1\">][getting-started]</ept>.",
      "pos": [
        3487,
        3564
      ]
    },
    {
      "content": "Import sample data into your DocumentDB account with the <bpt id=\"p1\">[</bpt>DocumentDB import tool<ept id=\"p1\">][documentdb-import-data]</ept>.",
      "pos": [
        3565,
        3671
      ]
    },
    {
      "content": "Throughput.",
      "pos": [
        3674,
        3685
      ]
    },
    {
      "content": "Reads and writes from HDInsight will be counted towards your allotted request units for your collections.",
      "pos": [
        3686,
        3791
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Provisioned throughput, request units, and database operations<ept id=\"p1\">][documentdb-manage-throughput]</ept>.",
      "pos": [
        3792,
        3913
      ]
    },
    {
      "content": "Capacity for an additional stored procedure within each output collection.",
      "pos": [
        3916,
        3990
      ]
    },
    {
      "content": "The stored procedures are used for transferring resulting documents.",
      "pos": [
        3991,
        4059
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Collections and provisioned throughput<ept id=\"p1\">][documentdb-manage-document-storage]</ept>.",
      "pos": [
        4060,
        4163
      ]
    },
    {
      "content": "Capacity for the resulting documents from the Hive, Pig, or MapReduce jobs.",
      "pos": [
        4166,
        4241
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Manage DocumentDB capacity and performance<ept id=\"p1\">][documentdb-manage-collections]</ept>.",
      "pos": [
        4242,
        4344
      ]
    },
    {
      "content": "[<bpt id=\"p1\">*</bpt>Optional<ept id=\"p1\">*</ept>] Capacity for an additional collection.",
      "pos": [
        4347,
        4398
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Provisioned document storage and index overhead<ept id=\"p1\">][documentdb-manage-document-storage]</ept>.",
      "pos": [
        4399,
        4511
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.WARNING]</ph> In order to avoid the creation of a new collection during any of the jobs, you can either print the results to stdout, save the output to your WASB container, or specify an already existing collection.",
      "pos": [
        4519,
        4736
      ]
    },
    {
      "content": "In the case of specifying an existing collection, new documents will be created inside the collection and already existing documents will only be affected if there is a conflict in <bpt id=\"p1\">*</bpt>ids<ept id=\"p1\">*</ept>.",
      "pos": [
        4737,
        4924
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>The connector will automatically overwrite existing documents with id conflicts<ept id=\"p1\">**</ept>.",
      "pos": [
        4925,
        5009
      ]
    },
    {
      "content": "You can turn off this feature by setting the upsert option to false.",
      "pos": [
        5010,
        5078
      ]
    },
    {
      "content": "If upsert is false and a conflict occurs, the Hadoop job will fail; reporting an id conflict error.",
      "pos": [
        5079,
        5178
      ]
    },
    {
      "pos": [
        5183,
        5250
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"CreateStorage\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 1: Create an Azure Storage account"
    },
    {
      "pos": [
        5254,
        5478
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> If you <bpt id=\"p1\">**</bpt>already<ept id=\"p1\">**</ept> have an Azure Storage Account and you are willing to create a new blob container within that Account, you can skip to <bpt id=\"p2\">[</bpt>Step 2: Create a customized HDInsight cluster<ept id=\"p2\">](#ProvisionHDInsight)</ept>."
    },
    {
      "content": "Azure HDInsight uses Azure Blob Storage for storing data.",
      "pos": [
        5480,
        5537
      ]
    },
    {
      "content": "It is called <bpt id=\"p1\">*</bpt>WASB<ept id=\"p1\">*</ept> or <bpt id=\"p2\">*</bpt>Azure Storage - Blob<ept id=\"p2\">*</ept>.",
      "pos": [
        5538,
        5584
      ]
    },
    {
      "content": "WASB is Microsoft's implementation of HDFS on Azure Blob storage.",
      "pos": [
        5585,
        5650
      ]
    },
    {
      "content": "For more information see <bpt id=\"p1\">[</bpt>Use Azure Blob storage with HDInsight<ept id=\"p1\">][hdinsight-storage]</ept>.",
      "pos": [
        5651,
        5735
      ]
    },
    {
      "content": "When you provision an HDInsight cluster, you specify an Azure Storage account.",
      "pos": [
        5737,
        5815
      ]
    },
    {
      "content": "A specific Blob storage container from that account is designated as the default file system, just like in HDFS.",
      "pos": [
        5816,
        5928
      ]
    },
    {
      "content": "The HDInsight cluster is by default provisioned in the same data center as the storage account you specify.",
      "pos": [
        5929,
        6036
      ]
    },
    {
      "content": "To create an Azure Storage account",
      "pos": [
        6040,
        6074
      ]
    },
    {
      "pos": [
        6081,
        6143
      ],
      "content": "Sign into the <bpt id=\"p1\">[</bpt>Azure management portal<ept id=\"p1\">][azure-classic-portal]</ept>."
    },
    {
      "pos": [
        6155,
        6305
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Azure HDInsight is currently supported in the Azure management portal, while Azure DocumentDB only exists in the Microsoft Azure  portal."
    },
    {
      "pos": [
        6310,
        6552
      ],
      "content": "Click **+ NEW** on the lower left corner, point to **DATA SERVICES**, point to **STORAGE**, and then click **QUICK CREATE**.\n ![Azure portal where you can use Quick Create to set up a new storage account.][image-storageaccount-quickcreate]",
      "leadings": [
        "",
        "   "
      ],
      "nodes": [
        {
          "content": "Click <bpt id=\"p1\">**</bpt>+ NEW<ept id=\"p1\">**</ept> on the lower left corner, point to <bpt id=\"p2\">**</bpt>DATA SERVICES<ept id=\"p2\">**</ept>, point to <bpt id=\"p3\">**</bpt>STORAGE<ept id=\"p3\">**</ept>, and then click <bpt id=\"p4\">**</bpt>QUICK CREATE<ept id=\"p4\">**</ept>.",
          "pos": [
            0,
            124
          ]
        },
        {
          "content": "<ph id=\"ph1\"> ![</ph>Azure portal where you can use Quick Create to set up a new storage account.<ph id=\"ph2\">][image-storageaccount-quickcreate]</ph>",
          "pos": [
            125,
            239
          ]
        }
      ]
    },
    {
      "content": "Enter the <bpt id=\"p1\">**</bpt>URL<ept id=\"p1\">**</ept>, select the <bpt id=\"p2\">**</bpt>LOCATION<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>REPLICATION<ept id=\"p3\">**</ept> values, and then click <bpt id=\"p4\">**</bpt>CREATE STORAGE ACCOUNT<ept id=\"p4\">**</ept>.",
      "pos": [
        6557,
        6670
      ]
    },
    {
      "content": "Affinity groups are not supported.",
      "pos": [
        6671,
        6705
      ]
    },
    {
      "content": "You will see the new storage account in the storage list.",
      "pos": [
        6716,
        6773
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> For best performance, make sure your storage account, HDInsight cluster, and DocumentDB account are located in the same Azure region.",
      "pos": [
        6781,
        6932
      ]
    },
    {
      "content": "Azure regions that support all three services are:  <bpt id=\"p1\">**</bpt>East Asia<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>Southeast Asia<ept id=\"p2\">**</ept>, <bpt id=\"p3\">**</bpt>North Europe<ept id=\"p3\">**</ept>, <bpt id=\"p4\">**</bpt>West Europe<ept id=\"p4\">**</ept>, <bpt id=\"p5\">**</bpt>East US<ept id=\"p5\">**</ept>, and <bpt id=\"p6\">**</bpt>West US<ept id=\"p6\">**</ept>.",
      "pos": [
        6933,
        7084
      ]
    },
    {
      "pos": [
        7089,
        7167
      ],
      "content": "Wait until the <bpt id=\"p1\">**</bpt>STATUS<ept id=\"p1\">**</ept> of the new storage account is changed to <bpt id=\"p2\">**</bpt>Online<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        7172,
        7250
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"ProvisionHDInsight\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 2: Create a customized HDInsight cluster"
    },
    {
      "content": "This tutorial uses Script Action from the Azure management portal to customize your HDInsight cluster.",
      "pos": [
        7251,
        7353
      ]
    },
    {
      "content": "In this tutorial we will use the Azure management portal to create your customized cluster.",
      "pos": [
        7354,
        7445
      ]
    },
    {
      "content": "For instructions on how to use PowerShell cmdlets or the HDInsight .NET SDK, check out the <bpt id=\"p1\">[</bpt>Customize HDInsight clusters using Script Action<ept id=\"p1\">][hdinsight-custom-provision]</ept> article.",
      "pos": [
        7446,
        7624
      ]
    },
    {
      "content": "Sign in to the <bpt id=\"p1\">[</bpt>Azure management portal<ept id=\"p1\">][azure-classic-portal]</ept>.",
      "pos": [
        7629,
        7692
      ]
    },
    {
      "content": "You may be already signed in from the previous step.",
      "pos": [
        7693,
        7745
      ]
    },
    {
      "pos": [
        7750,
        7872
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>+ NEW<ept id=\"p1\">**</ept> on the bottom of the page, click <bpt id=\"p2\">**</bpt>DATA SERVICES<ept id=\"p2\">**</ept>, click <bpt id=\"p3\">**</bpt>HDINSIGHT<ept id=\"p3\">**</ept>, and then click <bpt id=\"p4\">**</bpt>CUSTOM CREATE<ept id=\"p4\">**</ept>."
    },
    {
      "pos": [
        7877,
        7946
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Cluster Details<ept id=\"p1\">**</ept> page, type or choose the following values:"
    },
    {
      "content": "Provide Hadoop HDInsight initial cluster details",
      "pos": [
        7954,
        8002
      ]
    },
    {
      "pos": [
        8038,
        8957
      ],
      "content": "<table border='1'>\n     <tr><th>Property</th><th>Value</th></tr>\n     <tr><td>Cluster name</td><td>Name the cluster.<br/>\n         DNS name must start and end with an alpha numeric character, and may contain dashes.<br/>\n         The field must be a string between 3 and 63 characters.</td></tr>\n     <tr><td>Subscription Name</td>\n         <td>If you have more than one Azure Subscription, select the subscription corresponding to the storage account from <strong>Step 1</strong>. </td></tr>\n     <tr><td>Cluster Type</td>\n         <td>For cluster type, select <strong>Hadoop</strong>.</td></tr>\n     <tr><td>Operating System</td>\n         <td>For operating system, select <strong>Windows Server 2012 R2 Datacenter</strong>.</td></tr>\n     <tr><td>HDInsight version</td>\n         <td>Choose the version. </br>Select <Strong>HDInsight version 3.1</strong>.</td></tr>\n     </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            32,
            40
          ]
        },
        {
          "content": "Value",
          "pos": [
            49,
            54
          ]
        },
        {
          "content": "Cluster name",
          "pos": [
            78,
            90
          ]
        },
        {
          "content": "Name the cluster.",
          "pos": [
            99,
            116
          ]
        },
        {
          "content": "DNS name must start and end with an alpha numeric character, and may contain dashes.",
          "pos": [
            131,
            215
          ]
        },
        {
          "content": "The field must be a string between 3 and 63 characters.",
          "pos": [
            230,
            285
          ]
        },
        {
          "content": "Subscription Name",
          "pos": [
            309,
            326
          ]
        },
        {
          "content": "If you have more than one Azure Subscription, select the subscription corresponding to the storage account from <strong>Step 1</strong>. ",
          "pos": [
            345,
            482
          ],
          "nodes": [
            {
              "content": "If you have more than one Azure Subscription, select the subscription corresponding to the storage account from <ph id=\"ph1\">&lt;strong&gt;</ph>Step 1<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
              "pos": [
                0,
                136
              ]
            }
          ]
        },
        {
          "content": "Cluster Type",
          "pos": [
            506,
            518
          ]
        },
        {
          "content": "For cluster type, select <ph id=\"ph1\">&lt;strong&gt;</ph>Hadoop<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            537,
            586
          ]
        },
        {
          "content": "Operating System",
          "pos": [
            610,
            626
          ]
        },
        {
          "content": "For operating system, select <ph id=\"ph1\">&lt;strong&gt;</ph>Windows Server 2012 R2 Datacenter<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            645,
            725
          ]
        },
        {
          "content": "HDInsight version",
          "pos": [
            749,
            766
          ]
        },
        {
          "content": "Choose the version. ",
          "pos": [
            785,
            805
          ],
          "nodes": [
            {
              "content": "Choose the version.",
              "pos": [
                0,
                19
              ]
            }
          ]
        },
        {
          "content": "Select <ph id=\"ph1\">&lt;Strong&gt;</ph>HDInsight version 3.1<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            810,
            856
          ]
        }
      ]
    },
    {
      "content": "Enter or select the values as shown in the table and then click the right arrow.",
      "pos": [
        8966,
        9046
      ]
    },
    {
      "pos": [
        9055,
        9127
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Configure Cluster<ept id=\"p1\">**</ept> page, enter or select the following values:"
    },
    {
      "pos": [
        9133,
        9749
      ],
      "content": "<table border=\"1\">\n <tr><th>Name</th><th>Value</th></tr>\n <tr><td>Data nodes</td><td>Number of data nodes you want to deploy. </br>Take note that HDInsight's data nodes are associated with both performance and pricing.</td></tr>\n <tr><td>Region/Virtual network</td><td>Choose the same region as your newly created <strong>Storage account</strong> and your <strong>DocumentDB account</strong>. </br> HDInsight requires the storage account be located in the same region. Later in the configuration, you can only choose a storage account that is in the same region as you specified here.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Name",
          "pos": [
            28,
            32
          ]
        },
        {
          "content": "Value",
          "pos": [
            41,
            46
          ]
        },
        {
          "content": "Data nodes",
          "pos": [
            66,
            76
          ]
        },
        {
          "content": "Number of data nodes you want to deploy. ",
          "pos": [
            85,
            126
          ],
          "nodes": [
            {
              "content": "Number of data nodes you want to deploy.",
              "pos": [
                0,
                40
              ]
            }
          ]
        },
        {
          "content": "Take note that HDInsight's data nodes are associated with both performance and pricing.",
          "pos": [
            131,
            218
          ]
        },
        {
          "content": "Region/Virtual network",
          "pos": [
            238,
            260
          ]
        },
        {
          "content": "Choose the same region as your newly created <strong>Storage account</strong> and your <strong>DocumentDB account</strong>. ",
          "pos": [
            269,
            393
          ],
          "nodes": [
            {
              "content": "Choose the same region as your newly created <ph id=\"ph1\">&lt;strong&gt;</ph>Storage account<ph id=\"ph2\">&lt;/strong&gt;</ph> and your <ph id=\"ph3\">&lt;strong&gt;</ph>DocumentDB account<ph id=\"ph4\">&lt;/strong&gt;</ph>.",
              "pos": [
                0,
                123
              ]
            }
          ]
        },
        {
          "content": " HDInsight requires the storage account be located in the same region. Later in the configuration, you can only choose a storage account that is in the same region as you specified here.",
          "pos": [
            398,
            584
          ],
          "nodes": [
            {
              "content": "HDInsight requires the storage account be located in the same region.",
              "pos": [
                1,
                70
              ]
            },
            {
              "content": "Later in the configuration, you can only choose a storage account that is in the same region as you specified here.",
              "pos": [
                71,
                186
              ]
            }
          ]
        }
      ]
    },
    {
      "content": "Click the right arrow.",
      "pos": [
        9759,
        9781
      ]
    },
    {
      "pos": [
        9786,
        9855
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Configure Cluster User<ept id=\"p1\">**</ept> page, provide the following values:"
    },
    {
      "pos": [
        9861,
        10157
      ],
      "content": "<table border='1'>\n     <tr><th>Property</th><th>Value</th></tr>\n     <tr><td>User name</td>\n         <td>Specify the HDInsight cluster user name.</td></tr>\n     <tr><td>Password/Confirm Password</td>\n         <td>Specify the HDInsight cluster user password.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            32,
            40
          ]
        },
        {
          "content": "Value",
          "pos": [
            49,
            54
          ]
        },
        {
          "content": "User name",
          "pos": [
            78,
            87
          ]
        },
        {
          "content": "Specify the HDInsight cluster user name.",
          "pos": [
            106,
            146
          ]
        },
        {
          "content": "Password/Confirm Password",
          "pos": [
            170,
            195
          ]
        },
        {
          "content": "Specify the HDInsight cluster user password.",
          "pos": [
            214,
            258
          ]
        }
      ]
    },
    {
      "content": "Click the right arrow.",
      "pos": [
        10167,
        10189
      ]
    },
    {
      "pos": [
        10198,
        10260
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Storage Account<ept id=\"p1\">**</ept> page, provide the following values:"
    },
    {
      "content": "Provide storage account for Hadoop HDInsight cluster",
      "pos": [
        10268,
        10320
      ]
    },
    {
      "pos": [
        10356,
        12173
      ],
      "content": "<table border='1'>\n     <tr><th>Property</th><th>Value</th></tr>\n     <tr><td>Storage Account</td>\n         <td>Specify the Azure storage account that will be used as the default file system for the HDInsight cluster. You can choose one of the three options: Use Existing Storage, Create New Storage, or Use Storage from Another Subscription</br></br>\n         Please select <strong>Use Existing Storage</strong>.\n         </td>\n         </td></tr>\n     <tr><td>Account Name</td>\n         <td>\n         For <strong>Account name</strong>, select the account created in <strong>Step 1</strong>. The drop-down only lists the storage accounts under the same Azure subscription that are located in the same data center where you chose to provision the cluster.\n         </td></tr>\n     <tr><td>Default container</td>\n         <td>Specifies the default container on the storage account that is used as the default file system for the HDInsight cluster. If you choose <strong>Use Existing Storage</strong> for the <strong>Storage Account</strong> field, and there are no existing containers in that account, the container is created by default with the same name as the cluster name. If a container with the name of the cluster already exists, a sequence number will be appended to the container name.\n     </td></tr>\n     <tr><td>Additional Storage Accounts</td>\n         <td>HDInsight supports multiple storage accounts. There is no limit on the additional storage accounts that can be used by a cluster. However, if you create a cluster using the Azure portal, you have a limit of seven due to the UI constraints. Each additional storage account you specify adds an extra Storage Account page to the wizard where you can specify the account information.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            32,
            40
          ]
        },
        {
          "content": "Value",
          "pos": [
            49,
            54
          ]
        },
        {
          "content": "Storage Account",
          "pos": [
            78,
            93
          ]
        },
        {
          "content": "Specify the Azure storage account that will be used as the default file system for the HDInsight cluster. You can choose one of the three options: Use Existing Storage, Create New Storage, or Use Storage from Another Subscription",
          "pos": [
            112,
            341
          ],
          "nodes": [
            {
              "content": "Specify the Azure storage account that will be used as the default file system for the HDInsight cluster.",
              "pos": [
                0,
                105
              ]
            },
            {
              "content": "You can choose one of the three options: Use Existing Storage, Create New Storage, or Use Storage from Another Subscription",
              "pos": [
                106,
                229
              ]
            }
          ]
        },
        {
          "content": "Please select <ph id=\"ph1\">&lt;strong&gt;</ph>Use Existing Storage<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            361,
            413
          ]
        },
        {
          "content": "Account Name",
          "pos": [
            462,
            474
          ]
        },
        {
          "content": "         For <strong>Account name</strong>, select the account created in <strong>Step 1</strong>. The drop-down only lists the storage accounts under the same Azure subscription that are located in the same data center where you chose to provision the cluster.",
          "pos": [
            494,
            755
          ],
          "nodes": [
            {
              "content": "For <ph id=\"ph1\">&lt;strong&gt;</ph>Account name<ph id=\"ph2\">&lt;/strong&gt;</ph>, select the account created in <ph id=\"ph3\">&lt;strong&gt;</ph>Step 1<ph id=\"ph4\">&lt;/strong&gt;</ph>.",
              "pos": [
                9,
                98
              ]
            },
            {
              "content": "The drop-down only lists the storage accounts under the same Azure subscription that are located in the same data center where you chose to provision the cluster.",
              "pos": [
                99,
                261
              ]
            }
          ]
        },
        {
          "content": "Default container",
          "pos": [
            789,
            806
          ]
        },
        {
          "content": "Specifies the default container on the storage account that is used as the default file system for the HDInsight cluster. If you choose <strong>Use Existing Storage</strong> for the <strong>Storage Account</strong> field, and there are no existing containers in that account, the container is created by default with the same name as the cluster name. If a container with the name of the cluster already exists, a sequence number will be appended to the container name.",
          "pos": [
            825,
            1294
          ],
          "nodes": [
            {
              "content": "Specifies the default container on the storage account that is used as the default file system for the HDInsight cluster.",
              "pos": [
                0,
                121
              ]
            },
            {
              "content": "If you choose <ph id=\"ph1\">&lt;strong&gt;</ph>Use Existing Storage<ph id=\"ph2\">&lt;/strong&gt;</ph> for the <ph id=\"ph3\">&lt;strong&gt;</ph>Storage Account<ph id=\"ph4\">&lt;/strong&gt;</ph> field, and there are no existing containers in that account, the container is created by default with the same name as the cluster name.",
              "pos": [
                122,
                351
              ]
            },
            {
              "content": "If a container with the name of the cluster already exists, a sequence number will be appended to the container name.",
              "pos": [
                352,
                469
              ]
            }
          ]
        },
        {
          "content": "Additional Storage Accounts",
          "pos": [
            1324,
            1351
          ]
        },
        {
          "content": "HDInsight supports multiple storage accounts. There is no limit on the additional storage accounts that can be used by a cluster. However, if you create a cluster using the Azure portal, you have a limit of seven due to the UI constraints. Each additional storage account you specify adds an extra Storage Account page to the wizard where you can specify the account information.",
          "pos": [
            1370,
            1749
          ],
          "nodes": [
            {
              "content": "HDInsight supports multiple storage accounts.",
              "pos": [
                0,
                45
              ]
            },
            {
              "content": "There is no limit on the additional storage accounts that can be used by a cluster.",
              "pos": [
                46,
                129
              ]
            },
            {
              "content": "However, if you create a cluster using the Azure portal, you have a limit of seven due to the UI constraints.",
              "pos": [
                130,
                239
              ]
            },
            {
              "content": "Each additional storage account you specify adds an extra Storage Account page to the wizard where you can specify the account information.",
              "pos": [
                240,
                379
              ]
            }
          ]
        }
      ]
    },
    {
      "content": "Click the right arrow.",
      "pos": [
        12179,
        12201
      ]
    },
    {
      "content": "On the <bpt id=\"p1\">**</bpt>Script Actions<ept id=\"p1\">**</ept> page, click <bpt id=\"p2\">**</bpt>add script action<ept id=\"p2\">**</ept> to provide details about PowerShell script that you will run to customize your cluster, as the cluster is being created.",
      "pos": [
        12206,
        12386
      ]
    },
    {
      "content": "The PowerShell script will install the DocumentDB Hadoop Connector onto your HDInsight clusters during cluster creation.",
      "pos": [
        12387,
        12507
      ]
    },
    {
      "content": "Configure Script Action to customize an HDInsight cluster",
      "pos": [
        12519,
        12576
      ]
    },
    {
      "pos": [
        12612,
        13504
      ],
      "content": "<table border='1'>\n     <tr><th>Property</th><th>Value</th></tr>\n     <tr><td>Name</td>\n         <td>Specify a name for the script action.</td></tr>\n     <tr><td>Script URI</td>\n         <td>Specify the URI to the script that is invoked to customize the cluster.</br></br>\n         Please enter: </br> <strong>https://portalcontent.blob.core.windows.net/scriptaction/documentdb-hadoop-installer-v03.ps1</strong>.</td></tr>\n     <tr><td>Node Type</td>\n         <td>Specifies the nodes on which the customization script is run. You can choose <b>All Nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes</b> only.</br></br>\n         Please select <strong>All Nodes</strong>.</td></tr>\n     <tr><td>Parameters</td>\n         <td>Specify the parameters, if required by the script.</br></br>\n         <strong>No Parameters needed</strong>.</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Property",
          "pos": [
            32,
            40
          ]
        },
        {
          "content": "Value",
          "pos": [
            49,
            54
          ]
        },
        {
          "content": "Name",
          "pos": [
            78,
            82
          ]
        },
        {
          "content": "Specify a name for the script action.",
          "pos": [
            101,
            138
          ]
        },
        {
          "content": "Script URI",
          "pos": [
            162,
            172
          ]
        },
        {
          "content": "Specify the URI to the script that is invoked to customize the cluster.",
          "pos": [
            191,
            262
          ]
        },
        {
          "content": "Please enter:",
          "pos": [
            282,
            295
          ]
        },
        {
          "content": "<ph id=\"ph1\">&lt;strong&gt;</ph>https://portalcontent.blob.core.windows.net/scriptaction/documentdb-hadoop-installer-v03.ps1<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            302,
            412
          ]
        },
        {
          "content": "Node Type",
          "pos": [
            436,
            445
          ]
        },
        {
          "content": "Specifies the nodes on which the customization script is run. You can choose <b>All Nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes</b> only.",
          "pos": [
            464,
            611
          ],
          "nodes": [
            {
              "content": "Specifies the nodes on which the customization script is run.",
              "pos": [
                0,
                61
              ]
            },
            {
              "content": "You can choose <ph id=\"ph1\">&lt;b&gt;</ph>All Nodes<ph id=\"ph2\">&lt;/b&gt;</ph>, <ph id=\"ph3\">&lt;b&gt;</ph>Head nodes only<ph id=\"ph4\">&lt;/b&gt;</ph>, or <ph id=\"ph5\">&lt;b&gt;</ph>Worker nodes<ph id=\"ph6\">&lt;/b&gt;</ph> only.",
              "pos": [
                62,
                147
              ]
            }
          ]
        },
        {
          "content": "Please select <ph id=\"ph1\">&lt;strong&gt;</ph>All Nodes<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            631,
            672
          ]
        },
        {
          "content": "Parameters",
          "pos": [
            696,
            706
          ]
        },
        {
          "content": "Specify the parameters, if required by the script.",
          "pos": [
            725,
            775
          ]
        },
        {
          "content": "<ph id=\"ph1\">         &lt;strong&gt;</ph>No Parameters needed<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
          "pos": [
            786,
            833
          ]
        }
      ]
    },
    {
      "content": "Click the check mark to complete the cluster creation.",
      "pos": [
        13510,
        13564
      ]
    },
    {
      "pos": [
        13569,
        13644
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"InstallCmdlets\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 3: Install and configure Azure PowerShell"
    },
    {
      "content": "Install Azure PowerShell.",
      "pos": [
        13649,
        13674
      ]
    },
    {
      "content": "Instructions can be found <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">][powershell-install-configure]</ept>.",
      "pos": [
        13675,
        13738
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Alternatively, just for Hive queries, you can use HDInsight's online Hive Editor.",
      "pos": [
        13746,
        13840
      ]
    },
    {
      "content": "To do so, sign in to the <bpt id=\"p1\">[</bpt>Azure management portal<ept id=\"p1\">][azure-classic-portal]</ept>, click <bpt id=\"p2\">**</bpt>HDInsight<ept id=\"p2\">**</ept> on the left pane to view a list of your HDInsight clusters.",
      "pos": [
        13841,
        13994
      ]
    },
    {
      "content": "Click the cluster you want to run Hive queries on, and then click <bpt id=\"p1\">**</bpt>Query Console<ept id=\"p1\">**</ept>.",
      "pos": [
        13995,
        14079
      ]
    },
    {
      "content": "Open the Azure PowerShell Integrated Scripting Environment:",
      "pos": [
        14084,
        14143
      ]
    },
    {
      "content": "On a computer running Windows 8 or Windows Server 2012 or higher, you can use the built-in Search.",
      "pos": [
        14150,
        14248
      ]
    },
    {
      "content": "From the Start screen, type <bpt id=\"p1\">**</bpt>powershell ise<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept>.",
      "pos": [
        14249,
        14316
      ]
    },
    {
      "content": "On a computer running a version earlier than Windows 8 or Windows Server 2012, use the Start menu.",
      "pos": [
        14324,
        14422
      ]
    },
    {
      "content": "From the Start menu, type <bpt id=\"p1\">**</bpt>Command Prompt<ept id=\"p1\">**</ept> in the search box, then in the list of results, click <bpt id=\"p2\">**</bpt>Command Prompt<ept id=\"p2\">**</ept>.",
      "pos": [
        14423,
        14541
      ]
    },
    {
      "content": "In the Command Prompt, type <bpt id=\"p1\">**</bpt>powershell_ise<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept>.",
      "pos": [
        14542,
        14609
      ]
    },
    {
      "content": "Add your Azure Account.",
      "pos": [
        14614,
        14637
      ]
    },
    {
      "pos": [
        14645,
        14712
      ],
      "content": "In the Console Pane, type <bpt id=\"p1\">**</bpt>Add-AzureAccount<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        14721,
        14810
      ],
      "content": "Type in the email address associated with your Azure subscription and click <bpt id=\"p1\">**</bpt>Continue<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Type in the password for your Azure subscription.",
      "pos": [
        14819,
        14868
      ]
    },
    {
      "pos": [
        14877,
        14895
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Sign in<ept id=\"p1\">**</ept>."
    },
    {
      "content": "The following diagram identifies the important parts of your Azure PowerShell Scripting Environment.",
      "pos": [
        14900,
        15000
      ]
    },
    {
      "content": "Diagram for Azure PowerShell",
      "pos": [
        15009,
        15037
      ]
    },
    {
      "pos": [
        15069,
        15144
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"RunHive\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 4: Run a Hive job using DocumentDB and HDInsight"
    },
    {
      "pos": [
        15148,
        15249
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> All variables indicated by &lt; &gt; must be filled in using your configuration settings."
    },
    {
      "content": "Set the following variables in your PowerShell Script pane.",
      "pos": [
        15254,
        15313
      ]
    },
    {
      "content": "Let's begin constructing your query string.",
      "pos": [
        15754,
        15797
      ]
    },
    {
      "content": "We'll write a Hive query that takes all documents' system generated timestamps (_ts) and unique ids (_rid) from a DocumentDB collection, tallies all documents by the minute, and then stores the results back into a new DocumentDB collection.",
      "pos": [
        15798,
        16038
      ]
    },
    {
      "content": "First, let's create a Hive table from our DocumentDB collection.",
      "pos": [
        16052,
        16116
      ]
    },
    {
      "content": "Add the following code snippet to the PowerShell Script pane <ph id=\"ph1\">&lt;strong&gt;</ph>after<ph id=\"ph2\">&lt;/strong&gt;</ph> the code snippet from #1.",
      "pos": [
        16117,
        16226
      ]
    },
    {
      "content": "Make sure you include the optional DocumentDB.query parameter to trim our documents to just _ts and _rid.",
      "pos": [
        16227,
        16332
      ]
    },
    {
      "content": "Naming DocumentDB.inputCollections was not a mistake.",
      "pos": [
        16360,
        16413
      ]
    },
    {
      "content": "Yes, we allow adding multiple collections as an input:",
      "pos": [
        16416,
        16470
      ]
    },
    {
      "content": "'<bpt id=\"p1\">*</bpt>DocumentDB.inputCollections<ept id=\"p1\">*</ept>' = '<bpt id=\"p2\">*</bpt>\\&lt;DocumentDB Input Collection Name 1\\&gt;<ept id=\"p2\">*</ept>,<bpt id=\"p3\">*</bpt>\\&lt;DocumentDB Input Collection Name 2\\&gt;<ept id=\"p3\">*</ept>'",
      "pos": [
        16481,
        16598
      ]
    },
    {
      "content": "The collection names are separated without spaces, using only a single comma.",
      "pos": [
        16605,
        16682
      ]
    },
    {
      "content": "Next, let's create a Hive table for the output collection.",
      "pos": [
        17613,
        17671
      ]
    },
    {
      "content": "The output document properties will be the month, day, hour, minute, and the total number of occurrences.",
      "pos": [
        17672,
        17777
      ]
    },
    {
      "content": "Yet again, naming DocumentDB.outputCollections was not a mistake.",
      "pos": [
        17800,
        17865
      ]
    },
    {
      "content": "Yes, we allow adding multiple collections as an output:",
      "pos": [
        17868,
        17923
      ]
    },
    {
      "content": "'<bpt id=\"p1\">*</bpt>DocumentDB.outputCollections<ept id=\"p1\">*</ept>' = '<bpt id=\"p2\">*</bpt>\\&lt;DocumentDB Output Collection Name 1\\&gt;<ept id=\"p2\">*</ept>,<bpt id=\"p3\">*</bpt>\\&lt;DocumentDB Output Collection Name 2\\&gt;<ept id=\"p3\">*</ept>'",
      "pos": [
        17934,
        18054
      ]
    },
    {
      "content": "The collection names are separated without spaces, using only a single comma.",
      "pos": [
        18061,
        18138
      ]
    },
    {
      "content": "Documents will be distributed round-robin across multiple collections.",
      "pos": [
        18154,
        18224
      ]
    },
    {
      "content": "A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.",
      "pos": [
        18225,
        18365
      ]
    },
    {
      "content": "Finally, let's tally the documents by month, day, hour, and minute and insert the results back into the output Hive table.",
      "pos": [
        19168,
        19290
      ]
    },
    {
      "content": "Add the following script snippet to create a Hive job definition from the previous query.",
      "pos": [
        19958,
        20047
      ]
    },
    {
      "content": "You can also use the -File switch to specify a HiveQL script file on HDFS.",
      "pos": [
        20260,
        20334
      ]
    },
    {
      "content": "Add the following snippet to save the start time and submit the Hive job.",
      "pos": [
        20339,
        20412
      ]
    },
    {
      "content": "Add the following to wait for the Hive job to complete.",
      "pos": [
        20663,
        20718
      ]
    },
    {
      "content": "Add the following to print the standard output and the start and end times.",
      "pos": [
        20841,
        20916
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> your new script!",
      "pos": [
        21229,
        21253
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Click<ept id=\"p1\">**</ept> the green execute button.",
      "pos": [
        21254,
        21289
      ]
    },
    {
      "content": "Check the results.",
      "pos": [
        21295,
        21313
      ]
    },
    {
      "content": "Sign into the <bpt id=\"p1\">[</bpt>Azure Preview portal<ept id=\"p1\">][azure-portal]</ept>.",
      "pos": [
        21314,
        21365
      ]
    },
    {
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>Browse<ph id=\"ph2\">&lt;/strong&gt;</ph> on the left-side panel.",
      "pos": [
        21374,
        21427
      ]
    },
    {
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>everything<ph id=\"ph2\">&lt;/strong&gt;</ph> at the top-right of the browse panel.",
      "pos": [
        21441,
        21512
      ]
    },
    {
      "content": "Find and click <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Accounts<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
      "pos": [
        21526,
        21578
      ]
    },
    {
      "content": "Next, find your <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Account<ph id=\"ph2\">&lt;/strong&gt;</ph>, then <ph id=\"ph3\">&lt;strong&gt;</ph>DocumentDB Database<ph id=\"ph4\">&lt;/strong&gt;</ph> and your <ph id=\"ph5\">&lt;strong&gt;</ph>DocumentDB Collection<ph id=\"ph6\">&lt;/strong&gt;</ph> associated with the output collection specified in your Hive query.",
      "pos": [
        21592,
        21802
      ]
    },
    {
      "content": "Finally, click <ph id=\"ph1\">&lt;strong&gt;</ph>Document Explorer<ph id=\"ph2\">&lt;/strong&gt;</ph> underneath <ph id=\"ph3\">&lt;strong&gt;</ph>Developer Tools<ph id=\"ph4\">&lt;/strong&gt;</ph>.",
      "pos": [
        21815,
        21909
      ]
    },
    {
      "content": "You will see the results of your Hive query.",
      "pos": [
        21924,
        21968
      ]
    },
    {
      "content": "Hive query results",
      "pos": [
        21976,
        21994
      ]
    },
    {
      "pos": [
        22026,
        22099
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"RunPig\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 5: Run a Pig job using DocumentDB and HDInsight"
    },
    {
      "pos": [
        22103,
        22204
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> All variables indicated by &lt; &gt; must be filled in using your configuration settings."
    },
    {
      "content": "Set the following variables in your PowerShell Script pane.",
      "pos": [
        22209,
        22268
      ]
    },
    {
      "content": "Let's begin constructing your query string.",
      "pos": [
        22505,
        22548
      ]
    },
    {
      "content": "We'll write a Pig query that takes all documents' system generated timestamps (_ts) and unique ids (_rid) from a DocumentDB collection, tallies all documents by the minute, and then stores the results back into a new DocumentDB collection.",
      "pos": [
        22549,
        22788
      ]
    },
    {
      "content": "First, load documents from DocumentDB into HDInsight.",
      "pos": [
        22800,
        22853
      ]
    },
    {
      "content": "Add the following code snippet to the PowerShell Script pane <ph id=\"ph1\">&lt;strong&gt;</ph>after<ph id=\"ph2\">&lt;/strong&gt;</ph> the code snippet from #1.",
      "pos": [
        22854,
        22963
      ]
    },
    {
      "content": "Make sure to add a DocumentDB query to the optional DocumentDB query parameter to trim our documents to just _ts and _rid.",
      "pos": [
        22964,
        23086
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Yes, we allow adding multiple collections as an input:",
      "pos": [
        23098,
        23165
      ]
    },
    {
      "content": "'<bpt id=\"p1\">*</bpt>\\&lt;DocumentDB Input Collection Name 1\\&gt;<ept id=\"p1\">*</ept>,<bpt id=\"p2\">*</bpt>\\&lt;DocumentDB Input Collection Name 2\\&gt;<ept id=\"p2\">*</ept>'",
      "pos": [
        23176,
        23259
      ]
    },
    {
      "content": "The collection names are separated without spaces, using only a single comma.",
      "pos": [
        23265,
        23342
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;/b&gt;</ph>",
      "pos": [
        23343,
        23347
      ]
    },
    {
      "content": "Documents will be distributed round-robin across multiple collections.",
      "pos": [
        23353,
        23423
      ]
    },
    {
      "content": "A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.",
      "pos": [
        23424,
        23564
      ]
    },
    {
      "content": "Next, let's tally the documents by the month, day, hour, minute, and the total number of occurrences.",
      "pos": [
        24207,
        24308
      ]
    },
    {
      "content": "Finally, let's store the results into our new output collection.",
      "pos": [
        24895,
        24959
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Yes, we allow adding multiple collections as an output:",
      "pos": [
        24967,
        25035
      ]
    },
    {
      "content": "'\\&lt;DocumentDB Output Collection Name 1\\&gt;,\\&lt;DocumentDB Output Collection Name 2\\&gt;'",
      "pos": [
        25046,
        25127
      ]
    },
    {
      "content": "The collection names are separated without spaces, using only a single comma.",
      "pos": [
        25133,
        25210
      ]
    },
    {
      "content": "Documents will be distributed round-robin across the multiple collections.",
      "pos": [
        25220,
        25294
      ]
    },
    {
      "content": "A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.",
      "pos": [
        25295,
        25435
      ]
    },
    {
      "content": "Add the following script snippet to create a Pig job definition from the previous query.",
      "pos": [
        25868,
        25956
      ]
    },
    {
      "content": "You can also use the -File switch to specify a Pig script file on HDFS.",
      "pos": [
        26194,
        26265
      ]
    },
    {
      "content": "Add the following snippet to save the start time and submit the Pig job.",
      "pos": [
        26270,
        26342
      ]
    },
    {
      "content": "Add the following to wait for the Pig job to complete.",
      "pos": [
        26591,
        26645
      ]
    },
    {
      "content": "Add the following to print the standard output and the start and end times.",
      "pos": [
        26766,
        26841
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> your new script!",
      "pos": [
        27161,
        27185
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Click<ept id=\"p1\">**</ept> the green execute button.",
      "pos": [
        27186,
        27221
      ]
    },
    {
      "content": "Check the results.",
      "pos": [
        27227,
        27245
      ]
    },
    {
      "content": "Sign into the <bpt id=\"p1\">[</bpt>Azure Preview portal<ept id=\"p1\">][azure-portal]</ept>.",
      "pos": [
        27246,
        27297
      ]
    },
    {
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>Browse<ph id=\"ph2\">&lt;/strong&gt;</ph> on the left-side panel.",
      "pos": [
        27306,
        27359
      ]
    },
    {
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>everything<ph id=\"ph2\">&lt;/strong&gt;</ph> at the top-right of the browse panel.",
      "pos": [
        27373,
        27444
      ]
    },
    {
      "content": "Find and click <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Accounts<ph id=\"ph2\">&lt;/strong&gt;</ph>.",
      "pos": [
        27458,
        27510
      ]
    },
    {
      "content": "Next, find your <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Account<ph id=\"ph2\">&lt;/strong&gt;</ph>, then <ph id=\"ph3\">&lt;strong&gt;</ph>DocumentDB Database<ph id=\"ph4\">&lt;/strong&gt;</ph> and your <ph id=\"ph5\">&lt;strong&gt;</ph>DocumentDB Collection<ph id=\"ph6\">&lt;/strong&gt;</ph> associated with the output collection specified in your Pig query.",
      "pos": [
        27524,
        27733
      ]
    },
    {
      "content": "Finally, click <ph id=\"ph1\">&lt;strong&gt;</ph>Document Explorer<ph id=\"ph2\">&lt;/strong&gt;</ph> underneath <ph id=\"ph3\">&lt;strong&gt;</ph>Developer Tools<ph id=\"ph4\">&lt;/strong&gt;</ph>.",
      "pos": [
        27746,
        27840
      ]
    },
    {
      "content": "You will see the results of your Pig query.",
      "pos": [
        27855,
        27898
      ]
    },
    {
      "content": "Pig query results",
      "pos": [
        27906,
        27923
      ]
    },
    {
      "pos": [
        27954,
        28039
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"RunMapReduce\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Step 6: Run a MapReduce job using DocumentDB and HDInsight"
    },
    {
      "content": "Set the following variables in your PowerShell Script pane.",
      "pos": [
        28044,
        28103
      ]
    },
    {
      "content": "We'll execute a MapReduce job that tallies the number of occurrences for each Document property from your DocumentDB collection.",
      "pos": [
        28278,
        28406
      ]
    },
    {
      "content": "Add this script snippet <bpt id=\"p1\">**</bpt>after<ept id=\"p1\">**</ept> the snippet above.",
      "pos": [
        28407,
        28459
      ]
    },
    {
      "pos": [
        28869,
        28976
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> TallyProperties-v01.jar comes with the custom installation of the DocumentDB Hadoop Connector."
    },
    {
      "content": "Add the following command to submit the MapReduce job.",
      "pos": [
        28981,
        29035
      ]
    },
    {
      "content": "In addition to the MapReduce job definition, you also provide the HDInsight cluster name where you want to run the MapReduce job, and the credentials.",
      "pos": [
        29348,
        29498
      ]
    },
    {
      "content": "The Start-AzureHDInsightJob is an asynchronized call.",
      "pos": [
        29499,
        29552
      ]
    },
    {
      "content": "To check the completion of the job, use the <bpt id=\"p1\">*</bpt>Wait-AzureHDInsightJob<ept id=\"p1\">*</ept> cmdlet.",
      "pos": [
        29553,
        29629
      ]
    },
    {
      "content": "Add the following command to check any errors with running the MapReduce job.",
      "pos": [
        29634,
        29711
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> your new script!",
      "pos": [
        30005,
        30029
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Click<ept id=\"p1\">**</ept> the green execute button.",
      "pos": [
        30030,
        30065
      ]
    },
    {
      "content": "Check the results.",
      "pos": [
        30070,
        30088
      ]
    },
    {
      "content": "Sign into the <bpt id=\"p1\">[</bpt>Azure Preview portal<ept id=\"p1\">][azure-portal]</ept>.",
      "pos": [
        30089,
        30140
      ]
    },
    {
      "pos": [
        30149,
        30202
      ],
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>Browse<ph id=\"ph2\">&lt;/strong&gt;</ph> on the left-side panel."
    },
    {
      "pos": [
        30210,
        30281
      ],
      "content": "Click <ph id=\"ph1\">&lt;strong&gt;</ph>everything<ph id=\"ph2\">&lt;/strong&gt;</ph> at the top-right of the browse panel."
    },
    {
      "pos": [
        30289,
        30341
      ],
      "content": "Find and click <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Accounts<ph id=\"ph2\">&lt;/strong&gt;</ph>."
    },
    {
      "pos": [
        30349,
        30562
      ],
      "content": "Next, find your <ph id=\"ph1\">&lt;strong&gt;</ph>DocumentDB Account<ph id=\"ph2\">&lt;/strong&gt;</ph>, then <ph id=\"ph3\">&lt;strong&gt;</ph>DocumentDB Database<ph id=\"ph4\">&lt;/strong&gt;</ph> and your <ph id=\"ph5\">&lt;strong&gt;</ph>DocumentDB Collection<ph id=\"ph6\">&lt;/strong&gt;</ph> associated with the output collection specified in your MapReduce job."
    },
    {
      "pos": [
        30570,
        30664
      ],
      "content": "Finally, click <ph id=\"ph1\">&lt;strong&gt;</ph>Document Explorer<ph id=\"ph2\">&lt;/strong&gt;</ph> underneath <ph id=\"ph3\">&lt;strong&gt;</ph>Developer Tools<ph id=\"ph4\">&lt;/strong&gt;</ph>."
    },
    {
      "content": "You will see the results of your MapReduce job.",
      "pos": [
        30670,
        30717
      ]
    },
    {
      "content": "MapReduce query results",
      "pos": [
        30725,
        30748
      ]
    },
    {
      "pos": [
        30785,
        30819
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"NextSteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next Steps"
    },
    {
      "content": "Congratulations!",
      "pos": [
        30821,
        30837
      ]
    },
    {
      "content": "You just ran your first Hive, Pig, and MapReduce jobs using Azure DocumentDB and HDInsight.",
      "pos": [
        30838,
        30929
      ]
    },
    {
      "content": "We have open sourced our Hadoop Connector.",
      "pos": [
        30932,
        30974
      ]
    },
    {
      "content": "If you're interested, you can contribute on <bpt id=\"p1\">[</bpt>GitHub<ept id=\"p1\">][documentdb-github]</ept>.",
      "pos": [
        30975,
        31047
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        31049,
        31091
      ]
    },
    {
      "content": "Develop a Java application with Documentdb",
      "pos": [
        31096,
        31138
      ]
    },
    {
      "content": "Develop Java MapReduce programs for Hadoop in HDInsight",
      "pos": [
        31172,
        31227
      ]
    },
    {
      "content": "Get started using Hadoop with Hive in HDInsight to analyze mobile handset use",
      "pos": [
        31273,
        31350
      ]
    },
    {
      "content": "Use MapReduce with HDInsight",
      "pos": [
        31378,
        31406
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        31436,
        31459
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        31484,
        31506
      ]
    },
    {
      "content": "Customize HDInsight clusters using Script Action",
      "pos": [
        31530,
        31578
      ]
    },
    {
      "content": "test",
      "pos": [
        33888,
        33892
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Run a Hadoop job using DocumentDB and HDInsight | Microsoft Azure\" \n    description=\"Learn how to run a simple Hive, Pig, and MapReduce job with DocumentDB and Azure HDInsight.\"\n    services=\"documentdb\" \n    authors=\"AndrewHoh\" \n    manager=\"jhubbard\" \n    editor=\"mimig\"\n    documentationCenter=\"\"/>\n\n\n<tags \n    ms.service=\"documentdb\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"java\" \n    ms.topic=\"article\" \n    ms.date=\"07/20/2015\" \n    ms.author=\"anhoh\"/>\n\n#<a name=\"DocumentDB-HDInsight\"></a>Run a Hadoop job using DocumentDB and HDInsight\n\nThis tutorial shows you how to run [Apache Hive][apache-hive], [Apache Pig][apache-pig], and [Apache Hadoop][apache-hadoop] MapReduce jobs on Azure HDInsight with DocumentDB's Hadoop connector. DocumentDB's Hadoop connector allows DocumentDB to act as both a source and sink for Hive, Pig, and MapReduce jobs. This tutorial will use DocumentDB as both the data source and destination for Hadoop jobs. \n\nAfter completing this tutorial, you'll be able to answer the following questions:\n\n- How do I load data from DocumentDB using a Hive, Pig, or MapReduce job?\n- How do I store data in DocumentDB using a Hive, Pig, or MapReduce job?\n\nWe recommend getting started by watching the following video, where we run through a Hive job using DocumentDB and HDInsight.\n\n> [AZURE.VIDEO use-azure-documentdb-hadoop-connector-with-azure-hdinsight]\n\nThen, return to this article, where you'll receive the full details on how you can run analytics jobs on your DocumentDB data.\n\n> [AZURE.TIP] This tutorial assumes that you have prior experience using Apache Hadoop, Hive, and/or Pig. If you are new to Apache Hadoop, Hive, and Pig, we recommend visiting the [Apache Hadoop documentation][apache-hadoop-doc]. This tutorial also assumes that you have prior experience with DocumentDB and have a DocumentDB account. If you are new to DocumentDB or you do not have a DocumentDB account, please check out our [Getting Started][getting-started] page.\n\nDon't have time to complete the tutorial and just want to get the full sample PowerShell scripts for Hive, Pig, and MapReduce? Not a problem, get them [here][documentdb-hdinsight-samples]. The download also contains the hql, pig, and java files for these samples.\n\n## <a name=\"NewestVersion\"></a>Newest Version\n\n<table border='1'>\n    <tr><th>Hadoop Connector Version</th>\n        <td>1.1.0</td></tr>\n    <tr><th>Script Uri</th>\n        <td>https://portalcontent.blob.core.windows.net/scriptaction/documentdb-hadoop-installer-v03.ps1</td></tr>\n    <tr><th>Date Modified</th>\n        <td>07/20/2015</td></tr>\n    <tr><th>Supported HDInsight Versions</th>\n        <td>3.1, 3.2</td></tr>\n    <tr><th>Change Log</th>\n        <td>Updated DocumentDB Java SDK to 1.1.0</br>\n            Removed additional output parameter for custom indexing paths</br>\n            Added optional parameter for custom string precision (-1 by default)</br>\n            6/11/2015</br>\n            Fixed connector's compatability with the <a href=\"https://www.microsoft.com/download/details.aspx?id=40886\">Microsoft Hive ODBC driver</a></br>\n            Added ability to change output collection offer type (S3 offer by default)</br>\n        </td></tr>\n</table>\n\n## <a name=\"Prerequisites\"></a>Prerequisites\nBefore following the instructions in this tutorial, ensure that you have the following:\n\n- A DocumentDB account, a database, and a collection with documents inside. For more information, see [Getting Started with DocumentDB][getting-started]. Import sample data into your DocumentDB account with the [DocumentDB import tool][documentdb-import-data].\n- Throughput. Reads and writes from HDInsight will be counted towards your allotted request units for your collections. For more information, see [Provisioned throughput, request units, and database operations][documentdb-manage-throughput].\n- Capacity for an additional stored procedure within each output collection. The stored procedures are used for transferring resulting documents. For more information, see [Collections and provisioned throughput][documentdb-manage-document-storage].\n- Capacity for the resulting documents from the Hive, Pig, or MapReduce jobs. For more information, see [Manage DocumentDB capacity and performance][documentdb-manage-collections].\n- [*Optional*] Capacity for an additional collection. For more information, see [Provisioned document storage and index overhead][documentdb-manage-document-storage].\n    \n> [AZURE.WARNING] In order to avoid the creation of a new collection during any of the jobs, you can either print the results to stdout, save the output to your WASB container, or specify an already existing collection. In the case of specifying an existing collection, new documents will be created inside the collection and already existing documents will only be affected if there is a conflict in *ids*. **The connector will automatically overwrite existing documents with id conflicts**. You can turn off this feature by setting the upsert option to false. If upsert is false and a conflict occurs, the Hadoop job will fail; reporting an id conflict error.\n\n## <a name=\"CreateStorage\"></a>Step 1: Create an Azure Storage account\n\n> [AZURE.IMPORTANT] If you **already** have an Azure Storage Account and you are willing to create a new blob container within that Account, you can skip to [Step 2: Create a customized HDInsight cluster](#ProvisionHDInsight).\n\nAzure HDInsight uses Azure Blob Storage for storing data. It is called *WASB* or *Azure Storage - Blob*. WASB is Microsoft's implementation of HDFS on Azure Blob storage. For more information see [Use Azure Blob storage with HDInsight][hdinsight-storage].\n\nWhen you provision an HDInsight cluster, you specify an Azure Storage account. A specific Blob storage container from that account is designated as the default file system, just like in HDFS. The HDInsight cluster is by default provisioned in the same data center as the storage account you specify.\n\n**To create an Azure Storage account**\n\n1. Sign into the [Azure management portal][azure-classic-portal].\n    \n    > [AZURE.NOTE] Azure HDInsight is currently supported in the Azure management portal, while Azure DocumentDB only exists in the Microsoft Azure  portal.\n\n2. Click **+ NEW** on the lower left corner, point to **DATA SERVICES**, point to **STORAGE**, and then click **QUICK CREATE**.\n    ![Azure portal where you can use Quick Create to set up a new storage account.][image-storageaccount-quickcreate]\n\n3. Enter the **URL**, select the **LOCATION** and **REPLICATION** values, and then click **CREATE STORAGE ACCOUNT**. Affinity groups are not supported. \n    \n    You will see the new storage account in the storage list.\n\n    > [AZURE.IMPORTANT] For best performance, make sure your storage account, HDInsight cluster, and DocumentDB account are located in the same Azure region. Azure regions that support all three services are:  **East Asia**, **Southeast Asia**, **North Europe**, **West Europe**, **East US**, and **West US**.\n\n4. Wait until the **STATUS** of the new storage account is changed to **Online**.\n\n## <a name=\"ProvisionHDInsight\"></a>Step 2: Create a customized HDInsight cluster\nThis tutorial uses Script Action from the Azure management portal to customize your HDInsight cluster. In this tutorial we will use the Azure management portal to create your customized cluster. For instructions on how to use PowerShell cmdlets or the HDInsight .NET SDK, check out the [Customize HDInsight clusters using Script Action][hdinsight-custom-provision] article.\n\n1. Sign in to the [Azure management portal][azure-classic-portal]. You may be already signed in from the previous step.\n\n2. Click **+ NEW** on the bottom of the page, click **DATA SERVICES**, click **HDINSIGHT**, and then click **CUSTOM CREATE**.\n\n3. On the **Cluster Details** page, type or choose the following values:\n\n    ![Provide Hadoop HDInsight initial cluster details][image-customprovision-page1]\n\n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>Cluster name</td><td>Name the cluster.<br/>\n            DNS name must start and end with an alpha numeric character, and may contain dashes.<br/>\n            The field must be a string between 3 and 63 characters.</td></tr>\n        <tr><td>Subscription Name</td>\n            <td>If you have more than one Azure Subscription, select the subscription corresponding to the storage account from <strong>Step 1</strong>. </td></tr>\n        <tr><td>Cluster Type</td>\n            <td>For cluster type, select <strong>Hadoop</strong>.</td></tr>\n        <tr><td>Operating System</td>\n            <td>For operating system, select <strong>Windows Server 2012 R2 Datacenter</strong>.</td></tr>\n        <tr><td>HDInsight version</td>\n            <td>Choose the version. </br>Select <Strong>HDInsight version 3.1</strong>.</td></tr>\n        </table>\n\n    <p>Enter or select the values as shown in the table and then click the right arrow.</p>\n\n4. On the **Configure Cluster** page, enter or select the following values:\n\n    <table border=\"1\">\n    <tr><th>Name</th><th>Value</th></tr>\n    <tr><td>Data nodes</td><td>Number of data nodes you want to deploy. </br>Take note that HDInsight's data nodes are associated with both performance and pricing.</td></tr>\n    <tr><td>Region/Virtual network</td><td>Choose the same region as your newly created <strong>Storage account</strong> and your <strong>DocumentDB account</strong>. </br> HDInsight requires the storage account be located in the same region. Later in the configuration, you can only choose a storage account that is in the same region as you specified here.</td></tr>\n    </table>\n    \n    Click the right arrow.\n\n5. On the **Configure Cluster User** page, provide the following values:\n\n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>User name</td>\n            <td>Specify the HDInsight cluster user name.</td></tr>\n        <tr><td>Password/Confirm Password</td>\n            <td>Specify the HDInsight cluster user password.</td></tr>\n    </table>\n    \n    Click the right arrow.\n    \n6. On the **Storage Account** page, provide the following values:\n\n    ![Provide storage account for Hadoop HDInsight cluster][image-customprovision-page4]\n\n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>Storage Account</td>\n            <td>Specify the Azure storage account that will be used as the default file system for the HDInsight cluster. You can choose one of the three options: Use Existing Storage, Create New Storage, or Use Storage from Another Subscription</br></br>\n            Please select <strong>Use Existing Storage</strong>.\n            </td>\n            </td></tr>\n        <tr><td>Account Name</td>\n            <td>\n            For <strong>Account name</strong>, select the account created in <strong>Step 1</strong>. The drop-down only lists the storage accounts under the same Azure subscription that are located in the same data center where you chose to provision the cluster.\n            </td></tr>\n        <tr><td>Default container</td>\n            <td>Specifies the default container on the storage account that is used as the default file system for the HDInsight cluster. If you choose <strong>Use Existing Storage</strong> for the <strong>Storage Account</strong> field, and there are no existing containers in that account, the container is created by default with the same name as the cluster name. If a container with the name of the cluster already exists, a sequence number will be appended to the container name.\n        </td></tr>\n        <tr><td>Additional Storage Accounts</td>\n            <td>HDInsight supports multiple storage accounts. There is no limit on the additional storage accounts that can be used by a cluster. However, if you create a cluster using the Azure portal, you have a limit of seven due to the UI constraints. Each additional storage account you specify adds an extra Storage Account page to the wizard where you can specify the account information.</td></tr>\n    </table>\n\n    Click the right arrow.\n\n7. On the **Script Actions** page, click **add script action** to provide details about PowerShell script that you will run to customize your cluster, as the cluster is being created. The PowerShell script will install the DocumentDB Hadoop Connector onto your HDInsight clusters during cluster creation.\n    \n    ![Configure Script Action to customize an HDInsight cluster][image-customprovision-page5]\n\n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>Name</td>\n            <td>Specify a name for the script action.</td></tr>\n        <tr><td>Script URI</td>\n            <td>Specify the URI to the script that is invoked to customize the cluster.</br></br>\n            Please enter: </br> <strong>https://portalcontent.blob.core.windows.net/scriptaction/documentdb-hadoop-installer-v03.ps1</strong>.</td></tr>\n        <tr><td>Node Type</td>\n            <td>Specifies the nodes on which the customization script is run. You can choose <b>All Nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes</b> only.</br></br>\n            Please select <strong>All Nodes</strong>.</td></tr>\n        <tr><td>Parameters</td>\n            <td>Specify the parameters, if required by the script.</br></br>\n            <strong>No Parameters needed</strong>.</td></tr>\n    </table>\n\n    Click the check mark to complete the cluster creation.\n\n## <a name=\"InstallCmdlets\"></a>Step 3: Install and configure Azure PowerShell\n\n1. Install Azure PowerShell. Instructions can be found [here][powershell-install-configure].\n\n    > [AZURE.NOTE] Alternatively, just for Hive queries, you can use HDInsight's online Hive Editor. To do so, sign in to the [Azure management portal][azure-classic-portal], click **HDInsight** on the left pane to view a list of your HDInsight clusters. Click the cluster you want to run Hive queries on, and then click **Query Console**.\n\n2. Open the Azure PowerShell Integrated Scripting Environment:\n    - On a computer running Windows 8 or Windows Server 2012 or higher, you can use the built-in Search. From the Start screen, type **powershell ise** and click **Enter**. \n    - On a computer running a version earlier than Windows 8 or Windows Server 2012, use the Start menu. From the Start menu, type **Command Prompt** in the search box, then in the list of results, click **Command Prompt**. In the Command Prompt, type **powershell_ise** and click **Enter**.\n\n3. Add your Azure Account.\n    1. In the Console Pane, type **Add-AzureAccount** and click **Enter**. \n    2. Type in the email address associated with your Azure subscription and click **Continue**. \n    3. Type in the password for your Azure subscription. \n    4. Click **Sign in**.\n\n4. The following diagram identifies the important parts of your Azure PowerShell Scripting Environment. \n\n    ![Diagram for Azure PowerShell][azure-powershell-diagram]\n\n## <a name=\"RunHive\"></a>Step 4: Run a Hive job using DocumentDB and HDInsight\n\n> [AZURE.IMPORTANT] All variables indicated by < > must be filled in using your configuration settings.\n\n1. Set the following variables in your PowerShell Script pane.\n\n        # Provide Azure subscription name, the Azure Storage account and container that is used for the default HDInsight file system.\n        $subscriptionName = \"<SubscriptionName>\"\n        $storageAccountName = \"<AzureStorageAccountName>\"\n        $containerName = \"<AzureStorageContainerName>\"\n\n        # Provide the HDInsight cluster name where you want to run the Hive job.\n        $clusterName = \"<HDInsightClusterName>\"\n\n2. \n    <p>Let's begin constructing your query string. We'll write a Hive query that takes all documents' system generated timestamps (_ts) and unique ids (_rid) from a DocumentDB collection, tallies all documents by the minute, and then stores the results back into a new DocumentDB collection. </p>\n\n    <p>First, let's create a Hive table from our DocumentDB collection. Add the following code snippet to the PowerShell Script pane <strong>after</strong> the code snippet from #1. Make sure you include the optional DocumentDB.query parameter to trim our documents to just _ts and _rid. </p>\n\n    > [AZURE.NOTE] **Naming DocumentDB.inputCollections was not a mistake.** Yes, we allow adding multiple collections as an input: </br>\n    '*DocumentDB.inputCollections*' = '*\\<DocumentDB Input Collection Name 1\\>*,*\\<DocumentDB Input Collection Name 2\\>*' </br> The collection names are separated without spaces, using only a single comma.\n\n\n        # Create a Hive table using data from DocumentDB. Pass DocumentDB the query to filter transferred data to _rid and _ts.\n        $queryStringPart1 = \"drop table DocumentDB_timestamps; \"  + \n                            \"create external table DocumentDB_timestamps(id string, ts BIGINT) \"  +\n                            \"stored by 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler' \"  +\n                            \"tblproperties ( \" + \n                                \"'DocumentDB.endpoint' = '<DocumentDB Endpoint>', \" +\n                                \"'DocumentDB.key' = '<DocumentDB Primary Key>', \" +\n                                \"'DocumentDB.db' = '<DocumentDB Database Name>', \" +\n                                \"'DocumentDB.inputCollections' = '<DocumentDB Input Collection Name>', \" +\n                                \"'DocumentDB.query' = 'SELECT r._rid AS id, r._ts AS ts FROM root r' ); \"\n \n3.  Next, let's create a Hive table for the output collection. The output document properties will be the month, day, hour, minute, and the total number of occurrences.\n\n    > [AZURE.NOTE] **Yet again, naming DocumentDB.outputCollections was not a mistake.** Yes, we allow adding multiple collections as an output: </br>\n    '*DocumentDB.outputCollections*' = '*\\<DocumentDB Output Collection Name 1\\>*,*\\<DocumentDB Output Collection Name 2\\>*' </br> The collection names are separated without spaces, using only a single comma. </br></br>\n    Documents will be distributed round-robin across multiple collections. A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.\n\n        # Create a Hive table for the output data to DocumentDB.\n        $queryStringPart2 = \"drop table DocumentDB_analytics; \" +\n                              \"create external table DocumentDB_analytics(Month INT, Day INT, Hour INT, Minute INT, Total INT) \" +\n                              \"stored by 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler' \" + \n                              \"tblproperties ( \" + \n                                  \"'DocumentDB.endpoint' = '<DocumentDB Endpoint>', \" +\n                                  \"'DocumentDB.key' = '<DocumentDB Primary Key>', \" +  \n                                  \"'DocumentDB.db' = '<DocumentDB Database Name>', \" +\n                                  \"'DocumentDB.outputCollections' = '<DocumentDB Output Collection Name>' ); \"\n\n4. Finally, let's tally the documents by month, day, hour, and minute and insert the results back into the output Hive table.\n\n        # GROUP BY minute, COUNT entries for each, INSERT INTO output Hive table.\n        $queryStringPart3 = \"INSERT INTO table DocumentDB_analytics \" +\n                              \"SELECT month(from_unixtime(ts)) as Month, day(from_unixtime(ts)) as Day, \" +\n                              \"hour(from_unixtime(ts)) as Hour, minute(from_unixtime(ts)) as Minute, \" +\n                              \"COUNT(*) AS Total \" +\n                              \"FROM DocumentDB_timestamps \" +\n                              \"GROUP BY month(from_unixtime(ts)), day(from_unixtime(ts)), \" +\n                              \"hour(from_unixtime(ts)) , minute(from_unixtime(ts)); \"\n\n5. Add the following script snippet to create a Hive job definition from the previous query.\n\n        # Create a Hive job definition.\n        $queryString = $queryStringPart1 + $queryStringPart2 + $queryStringPart3\n        $hiveJobDefinition = New-AzureHDInsightHiveJobDefinition -Query $queryString\n\n    You can also use the -File switch to specify a HiveQL script file on HDFS.\n\n6. Add the following snippet to save the start time and submit the Hive job.\n\n        # Save the start time and submit the job to the cluster.\n        $startTime = Get-Date\n        Select-AzureSubscription $subscriptionName\n        $hiveJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $hiveJobDefinition\n\n7. Add the following to wait for the Hive job to complete.\n\n        # Wait for the Hive job to complete.\n        Wait-AzureHDInsightJob -Job $hiveJob -WaitTimeoutInSeconds 3600\n\n8. Add the following to print the standard output and the start and end times.\n\n        # Print the standard error, the standard output of the Hive job, and the start and end time.\n        $endTime = Get-Date\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $hiveJob.JobId -StandardOutput\n        Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green\n\n9. **Run** your new script! **Click** the green execute button.\n\n10. Check the results. Sign into the [Azure Preview portal][azure-portal]. \n    1. Click <strong>Browse</strong> on the left-side panel. </br>\n    2. Click <strong>everything</strong> at the top-right of the browse panel. </br>\n    3. Find and click <strong>DocumentDB Accounts</strong>. </br>\n    4. Next, find your <strong>DocumentDB Account</strong>, then <strong>DocumentDB Database</strong> and your <strong>DocumentDB Collection</strong> associated with the output collection specified in your Hive query.</br>\n    5. Finally, click <strong>Document Explorer</strong> underneath <strong>Developer Tools</strong>.</br></p>\n\n    You will see the results of your Hive query.\n\n    ![Hive query results][image-hive-query-results]\n\n## <a name=\"RunPig\"></a>Step 5: Run a Pig job using DocumentDB and HDInsight\n\n> [AZURE.IMPORTANT] All variables indicated by < > must be filled in using your configuration settings.\n\n1. Set the following variables in your PowerShell Script pane.\n\n        # Provide Azure subscription name.\n        $subscriptionName = \"Azure Subscription Name\"\n\n        # Provide HDInsight cluster name where you want to run the Pig job.\n        $clusterName = \"Azure HDInsight Cluster Name\"\n\n2. <p>Let's begin constructing your query string. We'll write a Pig query that takes all documents' system generated timestamps (_ts) and unique ids (_rid) from a DocumentDB collection, tallies all documents by the minute, and then stores the results back into a new DocumentDB collection.</p>\n    <p>First, load documents from DocumentDB into HDInsight. Add the following code snippet to the PowerShell Script pane <strong>after</strong> the code snippet from #1. Make sure to add a DocumentDB query to the optional DocumentDB query parameter to trim our documents to just _ts and _rid.</p>\n\n    > [AZURE.NOTE] Yes, we allow adding multiple collections as an input: </br>\n    '*\\<DocumentDB Input Collection Name 1\\>*,*\\<DocumentDB Input Collection Name 2\\>*'</br> The collection names are separated without spaces, using only a single comma. </b>\n\n    Documents will be distributed round-robin across multiple collections. A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.\n\n        # Load data from DocumentDB. Pass DocumentDB query to filter transferred data to _rid and _ts.\n        $queryStringPart1 = \"DocumentDB_timestamps = LOAD '<DocumentDB Endpoint>' USING com.microsoft.azure.documentdb.pig.DocumentDBLoader( \" +\n                                                        \"'<DocumentDB Primary Key>', \" +\n                                                        \"'<DocumentDB Database Name>', \" +\n                                                        \"'<DocumentDB Input Collection Name>', \" +\n                                                        \"'SELECT r._rid AS id, r._ts AS ts FROM root r' ); \"\n\n3.  Next, let's tally the documents by the month, day, hour, minute, and the total number of occurrences.\n\n        # GROUP BY minute and COUNT entries for each.\n        $queryStringPart2 = \"timestamp_record = FOREACH DocumentDB_timestamps GENERATE `$0#'id' as id:int, ToDate((long)(`$0#'ts') * 1000) as timestamp:datetime; \" +\n                            \"by_minute = GROUP timestamp_record BY (GetYear(timestamp), GetMonth(timestamp), GetDay(timestamp), GetHour(timestamp), GetMinute(timestamp)); \" +\n                            \"by_minute_count = FOREACH by_minute GENERATE FLATTEN(group) as (Year:int, Month:int, Day:int, Hour:int, Minute:int), COUNT(timestamp_record) as Total:int; \"\n\n4. Finally, let's store the results into our new output collection.\n\n    > [AZURE.NOTE] Yes, we allow adding multiple collections as an output: </br>\n    '\\<DocumentDB Output Collection Name 1\\>,\\<DocumentDB Output Collection Name 2\\>'</br> The collection names are separated without spaces, using only a single comma.</br>\n    Documents will be distributed round-robin across the multiple collections. A batch of documents will be stored in one collection, then a second batch of documents will be stored in the next collection, and so forth.\n\n        # Store output data to DocumentDB.\n        $queryStringPart3 = \"STORE by_minute_count INTO '<DocumentDB Endpoint>' \" +\n                            \"USING com.microsoft.azure.documentdb.pig.DocumentDBStorage( \" +\n                                \"'<DocumentDB Primary Key>', \" +\n                                \"'<DocumentDB Database Name>', \" +\n                                \"'<DocumentDB Output Collection Name>'); \"\n\n5. Add the following script snippet to create a Pig job definition from the previous query.\n\n        # Create a Pig job definition.\n        $queryString = $queryStringPart1 + $queryStringPart2 + $queryStringPart3\n        $pigJobDefinition = New-AzureHDInsightPigJobDefinition -Query $queryString -StatusFolder $statusFolder\n\n    You can also use the -File switch to specify a Pig script file on HDFS.\n\n6. Add the following snippet to save the start time and submit the Pig job.\n\n        # Save the start time and submit the job to the cluster.\n        $startTime = Get-Date\n        Select-AzureSubscription $subscriptionName\n        $pigJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $pigJobDefinition\n\n7. Add the following to wait for the Pig job to complete.\n\n        # Wait for the Pig job to complete.\n        Wait-AzureHDInsightJob -Job $pigJob -WaitTimeoutInSeconds 3600\n\n8. Add the following to print the standard output and the start and end times.\n\n        # Print the standard error, the standard output of the Hive job, and the start and end time.\n        $endTime = Get-Date\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $pigJob.JobId -StandardOutput\n        Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green\n        \n9. **Run** your new script! **Click** the green execute button.\n\n10. Check the results. Sign into the [Azure Preview portal][azure-portal]. \n    1. Click <strong>Browse</strong> on the left-side panel. </br>\n    2. Click <strong>everything</strong> at the top-right of the browse panel. </br>\n    3. Find and click <strong>DocumentDB Accounts</strong>. </br>\n    4. Next, find your <strong>DocumentDB Account</strong>, then <strong>DocumentDB Database</strong> and your <strong>DocumentDB Collection</strong> associated with the output collection specified in your Pig query.</br>\n    5. Finally, click <strong>Document Explorer</strong> underneath <strong>Developer Tools</strong>.</br></p>\n\n    You will see the results of your Pig query.\n\n    ![Pig query results][image-pig-query-results]\n\n## <a name=\"RunMapReduce\"></a>Step 6: Run a MapReduce job using DocumentDB and HDInsight\n\n1. Set the following variables in your PowerShell Script pane.\n        \n        $subscriptionName = \"<SubscriptionName>\"   # Azure subscription name\n        $clusterName = \"<ClusterName>\"             # HDInsight cluster name\n        \n2. We'll execute a MapReduce job that tallies the number of occurrences for each Document property from your DocumentDB collection. Add this script snippet **after** the snippet above.\n\n        # Define the MapReduce job.\n        $TallyPropertiesJobDefinition = New-AzureHDInsightMapReduceJobDefinition -JarFile \"wasb:///example/jars/TallyProperties-v01.jar\" -ClassName \"TallyProperties\" -Arguments \"<DocumentDB Endpoint>\",\"<DocumentDB Primary Key>\", \"<DocumentDB Database Name>\",\"<DocumentDB Input Collection Name>\",\"<DocumentDB Output Collection Name>\",\"<[Optional] DocumentDB Query>\"\n\n    > [AZURE.NOTE] TallyProperties-v01.jar comes with the custom installation of the DocumentDB Hadoop Connector.\n\n3. Add the following command to submit the MapReduce job.\n\n        # Save the start time and submit the job.\n        $startTime = Get-Date\n        Select-AzureSubscription $subscriptionName\n        $TallyPropertiesJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $TallyPropertiesJobDefinition | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600  \n\n    In addition to the MapReduce job definition, you also provide the HDInsight cluster name where you want to run the MapReduce job, and the credentials. The Start-AzureHDInsightJob is an asynchronized call. To check the completion of the job, use the *Wait-AzureHDInsightJob* cmdlet.\n\n4. Add the following command to check any errors with running the MapReduce job.    \n    \n        # Get the job output and print the start and end time.\n        $endTime = Get-Date\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $TallyPropertiesJob.JobId -StandardError\n        Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green \n\n5. **Run** your new script! **Click** the green execute button.\n\n6. Check the results. Sign into the [Azure Preview portal][azure-portal]. \n    1. Click <strong>Browse</strong> on the left-side panel.\n    2. Click <strong>everything</strong> at the top-right of the browse panel.\n    3. Find and click <strong>DocumentDB Accounts</strong>.\n    4. Next, find your <strong>DocumentDB Account</strong>, then <strong>DocumentDB Database</strong> and your <strong>DocumentDB Collection</strong> associated with the output collection specified in your MapReduce job.\n    5. Finally, click <strong>Document Explorer</strong> underneath <strong>Developer Tools</strong>.\n\n    You will see the results of your MapReduce job.\n\n    ![MapReduce query results][image-mapreduce-query-results]\n\n## <a name=\"NextSteps\"></a>Next Steps\n\nCongratulations! You just ran your first Hive, Pig, and MapReduce jobs using Azure DocumentDB and HDInsight. \n\nWe have open sourced our Hadoop Connector. If you're interested, you can contribute on [GitHub][documentdb-github].\n\nTo learn more, see the following articles:\n\n- [Develop a Java application with Documentdb][documentdb-java-application]\n- [Develop Java MapReduce programs for Hadoop in HDInsight][hdinsight-develop-deploy-java-mapreduce]\n- [Get started using Hadoop with Hive in HDInsight to analyze mobile handset use][hdinsight-get-started]\n- [Use MapReduce with HDInsight][hdinsight-use-mapreduce]\n- [Use Hive with HDInsight][hdinsight-use-hive]\n- [Use Pig with HDInsight][hdinsight-use-pig]\n- [Customize HDInsight clusters using Script Action][hdinsight-hadoop-customize-cluster]\n\n[apache-hadoop]: http://hadoop.apache.org/\n[apache-hadoop-doc]: http://hadoop.apache.org/docs/current/\n[apache-hive]: http://hive.apache.org/\n[apache-pig]: http://pig.apache.org/\n[getting-started]: documentdb-get-started.md\n\n[azure-classic-portal]: https://manage.windowsazure.com/\n[azure-powershell-diagram]: ./media/documentdb-run-hadoop-with-hdinsight/azurepowershell-diagram-med.png\n[azure-portal]: https://portal.azure.com/\n\n[documentdb-hdinsight-samples]: http://portalcontent.blob.core.windows.net/samples/documentdb-hdinsight-samples.zip\n[documentdb-github]: https://github.com/Azure/azure-documentdb-hadoop\n[documentdb-java-application]: documentdb-java-application.md\n[documentdb-manage-collections]: documentdb-manage.md#Collections\n[documentdb-manage-document-storage]: documentdb-manage.md#IndexOverhead\n[documentdb-manage-throughput]: documentdb-manage.md#ProvThroughput\n[documentdb-import-data]: documentdb-import-data.md\n\n[hdinsight-custom-provision]: ../hdinsight/hdinsight-provision-clusters.md#powershell\n[hdinsight-develop-deploy-java-mapreduce]: ../hdinsight/hdinsight-develop-deploy-java-mapreduce.md\n[hdinsight-hadoop-customize-cluster]: ../hdinsight/hdinsight-hadoop-customize-cluster.md\n[hdinsight-get-started]: ../hdinsight-get-started.md \n[hdinsight-storage]: ../hdinsight-use-blob-storage.md\n[hdinsight-use-hive]: ../hdinsight/hdinsight-use-hive.md\n[hdinsight-use-mapreduce]: ../hdinsight/hdinsight-use-mapreduce.md\n[hdinsight-use-pig]: ../hdinsight/hdinsight-use-pig.md\n\n[image-customprovision-page1]: ./media/documentdb-run-hadoop-with-hdinsight/customprovision-page1.png\n[image-customprovision-page4]: ./media/documentdb-run-hadoop-with-hdinsight/customprovision-page4.png\n[image-customprovision-page5]: ./media/documentdb-run-hadoop-with-hdinsight/customprovision-page5.png\n[image-storageaccount-quickcreate]: ./media/documentdb-run-hadoop-with-hdinsight/storagequickcreate.png\n[image-hive-query-results]: ./media/documentdb-run-hadoop-with-hdinsight/hivequeryresults.PNG\n[image-mapreduce-query-results]: ./media/documentdb-run-hadoop-with-hdinsight/mapreducequeryresults.PNG\n[image-pig-query-results]: ./media/documentdb-run-hadoop-with-hdinsight/pigqueryresults.PNG\n\n[powershell-install-configure]: ../install-configure-powershell.md\n \n\ntest\n"
}