{
  "nodes": [
    {
      "content": "Use Hadoop Pig with Remote Desktop in HDInsight | Microsoft Azure",
      "pos": [
        26,
        91
      ]
    },
    {
      "content": "Learn how to use the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based Hadoop cluster in HDInsight.",
      "pos": [
        109,
        250
      ]
    },
    {
      "content": "Run Pig jobs from a Remote Desktop connection",
      "pos": [
        567,
        612
      ]
    },
    {
      "content": "This document provides a walkthrough for using the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based HDInsight cluster.",
      "pos": [
        692,
        853
      ]
    },
    {
      "content": "Pig Latin allows you to create MapReduce applications by describing data transformations, rather than map and reduce functions.",
      "pos": [
        854,
        981
      ]
    },
    {
      "content": "In this document, learn how to",
      "pos": [
        983,
        1013
      ]
    },
    {
      "pos": [
        1017,
        1049
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"prereq\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "To complete the steps in this article, you will need the following.",
      "pos": [
        1051,
        1118
      ]
    },
    {
      "content": "A Windows-based HDInsight (Hadoop on HDInsight) cluster",
      "pos": [
        1122,
        1177
      ]
    },
    {
      "content": "A client computer running Windows 10, Windows 8, or Windows 7",
      "pos": [
        1181,
        1242
      ]
    },
    {
      "pos": [
        1246,
        1293
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"connect\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Connect with Remote Desktop"
    },
    {
      "pos": [
        1295,
        1489
      ],
      "content": "Enable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at <bpt id=\"p1\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p1\">](hdinsight-administer-use-management-portal.md#rdp)</ept>."
    },
    {
      "pos": [
        1493,
        1528
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"pig\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Use the Pig command"
    },
    {
      "pos": [
        1533,
        1644
      ],
      "content": "After you have a Remote Desktop connection, start the <bpt id=\"p1\">**</bpt>Hadoop Command Line<ept id=\"p1\">**</ept> by using the icon on the desktop."
    },
    {
      "content": "Use the following to start the Pig command:",
      "pos": [
        1649,
        1692
      ]
    },
    {
      "pos": [
        1726,
        1771
      ],
      "content": "You will be presented with a <ph id=\"ph1\">`grunt&gt;`</ph> prompt."
    },
    {
      "content": "Enter the following statement:",
      "pos": [
        1776,
        1806
      ]
    },
    {
      "content": "This command loads the contents of the sample.log file into the LOGS file.",
      "pos": [
        1868,
        1942
      ]
    },
    {
      "content": "You can view the contents of the file by using the following command:",
      "pos": [
        1943,
        2012
      ]
    },
    {
      "content": "Transform the data by applying a regular expression to extract only the logging level from each record:",
      "pos": [
        2037,
        2140
      ]
    },
    {
      "content": "You can use <bpt id=\"p1\">**</bpt>DUMP<ept id=\"p1\">**</ept> to view the data after the transformation.",
      "pos": [
        2260,
        2323
      ]
    },
    {
      "content": "In this case, <ph id=\"ph1\">`DUMP LEVELS;`</ph>.",
      "pos": [
        2324,
        2353
      ]
    },
    {
      "content": "Continue applying transformations by using the following statements.",
      "pos": [
        2358,
        2426
      ]
    },
    {
      "content": "Use <ph id=\"ph1\">`DUMP`</ph> to view the result of the transformation after each step.",
      "pos": [
        2427,
        2495
      ]
    },
    {
      "pos": [
        2501,
        3368
      ],
      "content": "<table>\n <tr>\n <th>Statement</th><th>What it does</th>\n </tr>\n <tr>\n <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n </tr>\n <tr>\n <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n </tr>\n <tr>\n <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES</td>\n </tr>\n <tr>\n <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending,) and stores into RESULT</td>\n </tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Statement",
          "pos": [
            19,
            28
          ]
        },
        {
          "content": "What it does",
          "pos": [
            37,
            49
          ]
        },
        {
          "content": "FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;",
          "pos": [
            73,
            128
          ]
        },
        {
          "content": "Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.",
          "pos": [
            137,
            237
          ]
        },
        {
          "content": "GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;",
          "pos": [
            261,
            310
          ]
        },
        {
          "content": "Groups the rows by log level and stores the results into GROUPEDLEVELS.",
          "pos": [
            319,
            390
          ]
        },
        {
          "content": "FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;",
          "pos": [
            414,
            518
          ]
        },
        {
          "content": "Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES",
          "pos": [
            527,
            656
          ],
          "nodes": [
            {
              "content": "Creates a new set of data that contains each unique log level value and how many times it occurs.",
              "pos": [
                0,
                97
              ]
            },
            {
              "content": "This is stored into FREQUENCIES",
              "pos": [
                98,
                129
              ]
            }
          ]
        },
        {
          "content": "RESULT = order FREQUENCIES by COUNT desc;",
          "pos": [
            680,
            721
          ]
        },
        {
          "content": "Orders the log levels by count (descending,) and stores into RESULT",
          "pos": [
            730,
            797
          ]
        }
      ]
    },
    {
      "content": "You can also save the results of a transformation by using the <ph id=\"ph1\">`STORE`</ph> statement.",
      "pos": [
        3373,
        3454
      ]
    },
    {
      "content": "For example, the following command saves the <ph id=\"ph1\">`RESULT`</ph> to the <bpt id=\"p1\">**</bpt>/example/data/pigout<ept id=\"p1\">**</ept> directory in the default storage container for your cluster:",
      "pos": [
        3455,
        3601
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The data is stored in the specified directory in files named <bpt id=\"p1\">**</bpt>part-nnnnn<ept id=\"p1\">**</ept>.",
      "pos": [
        3666,
        3755
      ]
    },
    {
      "content": "If the directory already exists, you will receive an error message.",
      "pos": [
        3756,
        3823
      ]
    },
    {
      "content": "To exit the grunt prompt, enter the following statement.",
      "pos": [
        3828,
        3884
      ]
    },
    {
      "content": "Pig Latin batch files",
      "pos": [
        3904,
        3925
      ]
    },
    {
      "content": "You can also use the Pig command to run Pig Latin that is contained in a file.",
      "pos": [
        3927,
        4005
      ]
    },
    {
      "pos": [
        4010,
        4136
      ],
      "content": "After exiting the grunt prompt, open <bpt id=\"p1\">**</bpt>Notepad<ept id=\"p1\">**</ept> and create a new file named <bpt id=\"p2\">**</bpt>pigbatch.pig<ept id=\"p2\">**</ept> in the <bpt id=\"p3\">**</bpt>%PIG_HOME%<ept id=\"p3\">**</ept> directory."
    },
    {
      "pos": [
        4141,
        4224
      ],
      "content": "Type or paste the following lines into the <bpt id=\"p1\">**</bpt>pigbatch.pig<ept id=\"p1\">**</ept> file, and then save it:"
    },
    {
      "pos": [
        4704,
        4777
      ],
      "content": "Use the following to run the <bpt id=\"p1\">**</bpt>pigbatch.pig<ept id=\"p1\">**</ept> file using the pig command."
    },
    {
      "pos": [
        4820,
        4966
      ],
      "content": "When the batch job completes, you should see the following output, which should be the same as when you used <ph id=\"ph1\">`DUMP RESULT;`</ph> in the previous steps:"
    },
    {
      "pos": [
        5083,
        5110
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"summary\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Summary"
    },
    {
      "content": "As you can see, the Pig command allows you to interactively run MapReduce operations, or run Pig Latin jobs that are stored in a batch file.",
      "pos": [
        5112,
        5252
      ]
    },
    {
      "pos": [
        5256,
        5288
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "For general information about Pig in HDInsight:",
      "pos": [
        5290,
        5337
      ]
    },
    {
      "content": "Use Pig with Hadoop on HDInsight",
      "pos": [
        5342,
        5374
      ]
    },
    {
      "content": "For information about other ways you can work with Hadoop on HDInsight:",
      "pos": [
        5399,
        5470
      ]
    },
    {
      "content": "Use Hive with Hadoop on HDInsight",
      "pos": [
        5475,
        5508
      ]
    },
    {
      "content": "Use MapReduce with Hadoop on HDInsight",
      "pos": [
        5537,
        5575
      ]
    },
    {
      "content": "test",
      "pos": [
        5606,
        5610
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Pig with Remote Desktop in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based Hadoop cluster in HDInsight.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/06/2015\"\n   ms.author=\"larryfr\"/>\n\n#Run Pig jobs from a Remote Desktop connection\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-selector-use-pig.md)]\n\nThis document provides a walkthrough for using the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based HDInsight cluster. Pig Latin allows you to create MapReduce applications by describing data transformations, rather than map and reduce functions.\n\nIn this document, learn how to\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following.\n\n* A Windows-based HDInsight (Hadoop on HDInsight) cluster\n\n* A client computer running Windows 10, Windows 8, or Windows 7\n\n##<a id=\"connect\"></a>Connect with Remote Desktop\n\nEnable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp).\n\n##<a id=\"pig\"></a>Use the Pig command\n\n2. After you have a Remote Desktop connection, start the **Hadoop Command Line** by using the icon on the desktop.\n\n2. Use the following to start the Pig command:\n\n        %pig_home%\\bin\\pig\n\n    You will be presented with a `grunt>` prompt.\n\n3. Enter the following statement:\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n\n    This command loads the contents of the sample.log file into the LOGS file. You can view the contents of the file by using the following command:\n\n        DUMP LOGS;\n\n4. Transform the data by applying a regular expression to extract only the logging level from each record:\n\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n\n    You can use **DUMP** to view the data after the transformation. In this case, `DUMP LEVELS;`.\n\n5. Continue applying transformations by using the following statements. Use `DUMP` to view the result of the transformation after each step.\n\n    <table>\n    <tr>\n    <th>Statement</th><th>What it does</th>\n    </tr>\n    <tr>\n    <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES</td>\n    </tr>\n    <tr>\n    <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending,) and stores into RESULT</td>\n    </tr>\n    </table>\n\n6. You can also save the results of a transformation by using the `STORE` statement. For example, the following command saves the `RESULT` to the **/example/data/pigout** directory in the default storage container for your cluster:\n\n        STORE RESULT into 'wasb:///example/data/pigout'\n\n    > [AZURE.NOTE] The data is stored in the specified directory in files named **part-nnnnn**. If the directory already exists, you will receive an error message.\n\n7. To exit the grunt prompt, enter the following statement.\n\n        QUIT;\n\n###Pig Latin batch files\n\nYou can also use the Pig command to run Pig Latin that is contained in a file.\n\n3. After exiting the grunt prompt, open **Notepad** and create a new file named **pigbatch.pig** in the **%PIG_HOME%** directory.\n\n4. Type or paste the following lines into the **pigbatch.pig** file, and then save it:\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n        FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n        GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n        FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n        RESULT = order FREQUENCIES by COUNT desc;\n        DUMP RESULT;\n\n5. Use the following to run the **pigbatch.pig** file using the pig command.\n\n        pig %PIG_HOME%\\pigbatch.pig\n\n    When the batch job completes, you should see the following output, which should be the same as when you used `DUMP RESULT;` in the previous steps:\n\n        (TRACE,816)\n        (DEBUG,434)\n        (INFO,96)\n        (WARN,11)\n        (ERROR,6)\n        (FATAL,2)\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Pig command allows you to interactively run MapReduce operations, or run Pig Latin jobs that are stored in a batch file.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information about Pig in HDInsight:\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\nFor information about other ways you can work with Hadoop on HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n\ntest\n"
}