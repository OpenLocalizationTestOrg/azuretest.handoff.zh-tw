{
  "nodes": [
    {
      "content": "Azure Media Services Fragmented MP4 Live Ingest Specification",
      "pos": [
        28,
        89
      ]
    },
    {
      "content": "This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.",
      "pos": [
        109,
        247
      ]
    },
    {
      "content": "Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.",
      "pos": [
        248,
        435
      ]
    },
    {
      "content": "At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.",
      "pos": [
        436,
        567
      ]
    },
    {
      "content": "This document also discusses best practices in building highly redundant and robust live ingest mechanisms.",
      "pos": [
        568,
        675
      ]
    },
    {
      "content": "Azure Media Services Fragmented MP4 Live Ingest Specification",
      "pos": [
        987,
        1048
      ]
    },
    {
      "content": "This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services.",
      "pos": [
        1050,
        1188
      ]
    },
    {
      "content": "Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform.",
      "pos": [
        1189,
        1376
      ]
    },
    {
      "content": "At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services.",
      "pos": [
        1377,
        1508
      ]
    },
    {
      "content": "This document also discusses best practices in building highly redundant and robust live ingest mechanisms.",
      "pos": [
        1509,
        1616
      ]
    },
    {
      "content": "Conformance Notation",
      "pos": [
        1620,
        1640
      ]
    },
    {
      "content": "The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.",
      "pos": [
        1642,
        1835
      ]
    },
    {
      "content": "Service Diagram",
      "pos": [
        1839,
        1854
      ]
    },
    {
      "content": "The diagram below shows the high level architecture of the live streaming service in Microsoft Azure Media Services:",
      "pos": [
        1857,
        1973
      ]
    },
    {
      "content": "Live Encoder pushes live feeds into Channels which are created and provisioned via the Microsoft Azure Media Services SDK.",
      "pos": [
        1979,
        2101
      ]
    },
    {
      "content": "Channels, Programs and Streaming endpoint in Microsoft Azure Media Services handle all the live streaming functionalities including ingest, formatting, cloud DVR, security, scalability and redundancy.",
      "pos": [
        2106,
        2306
      ]
    },
    {
      "content": "Optionally customers could choose to deploy a CDN layer between the Streaming endpoint and the client endpoints.",
      "pos": [
        2311,
        2423
      ]
    },
    {
      "content": "Client endpoints stream from the Streaming endpoint using HTTP Adaptive Streaming protocols (e.g. Smooth Streaming, DASH, HDS or HLS).",
      "pos": [
        2428,
        2562
      ]
    },
    {
      "content": "image1",
      "pos": [
        2566,
        2572
      ]
    },
    {
      "content": "Bit-stream Format – ISO 14496-12 Fragmented MP4",
      "pos": [
        2586,
        2633
      ]
    },
    {
      "content": "The wire format for live streaming ingest that is discussed in this document is based on [ISO-14496-12].",
      "pos": [
        2635,
        2739
      ]
    },
    {
      "content": "Please refer to <bpt id=\"p1\">[</bpt>[MS-SSTR]<ept id=\"p1\">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> for detailed explanation of Fragmented MP4 format and extensions for both video-on-demand files and live streaming ingestion.",
      "pos": [
        2740,
        2943
      ]
    },
    {
      "content": "Below is a list of special format definitions that apply to live ingest into Microsoft Azure Media Services:",
      "pos": [
        2946,
        3054
      ]
    },
    {
      "content": "The ‘ftyp’, LiveServerManifestBox, and ‘moov’ box MUST be sent with each request (HTTP POST).",
      "pos": [
        3059,
        3152
      ]
    },
    {
      "content": "It MUST be sent at the beginning of the stream and anytime the encoder must reconnect to resume stream ingest.",
      "pos": [
        3154,
        3264
      ]
    },
    {
      "content": "Please refer to Section 6 in [1] for more details.",
      "pos": [
        3266,
        3316
      ]
    },
    {
      "content": "Section 3.3.2 in [1] defines an optional box called StreamManifestBox for live ingest.",
      "pos": [
        3320,
        3406
      ]
    },
    {
      "content": "Due to the routing logic of Microsoft Azure’s load balancer, usage of this box is deprecated and SHOULD NOT be present when ingesting into Microsoft Azure Media Service.",
      "pos": [
        3407,
        3576
      ]
    },
    {
      "content": "If this box is present, Azure Media Services silently ignores it.",
      "pos": [
        3577,
        3642
      ]
    },
    {
      "content": "The TrackFragmentExtendedHeaderBox defined in 3.2.3.2 in [1] MUST be present for each fragment.",
      "pos": [
        3646,
        3741
      ]
    },
    {
      "content": "Version 2 of the TrackFragmentExtendedHeaderBox SHOULD be used in order to generate media segments with identical URLs in multiple datacenters.",
      "pos": [
        3745,
        3888
      ]
    },
    {
      "content": "The fragment index field is REQUIRED for cross-datacenter failover of index-based streaming formats such as Apple HTTP Live Streaming (HLS) and index-based MPEG-DASH.",
      "pos": [
        3889,
        4055
      ]
    },
    {
      "content": "To enable cross-datacenter failover, the fragment index MUST be synchronized across multiple encoders, and increase by 1 for each successive media fragment, even across encoder restarts or failures.",
      "pos": [
        4057,
        4255
      ]
    },
    {
      "content": "Section 3.3.6 in [1] defines box called MovieFragmentRandomAccessBox (‘mfra’) that MAY be sent at the end of live ingestion to indicate EOS (End-of-Stream) to the channel.",
      "pos": [
        4259,
        4430
      ]
    },
    {
      "content": "Due to the ingest logic of Azure Media Services, usage of EOS (End-of-Stream) is deprecated and the ‘mfra’ box for live ingestion SHOULD NOT be sent.",
      "pos": [
        4431,
        4580
      ]
    },
    {
      "content": "If sent, Azure Media Services silently ignores it.",
      "pos": [
        4581,
        4631
      ]
    },
    {
      "content": "It is recommended to use <bpt id=\"p1\">[</bpt>Channel Reset<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn783458.aspx#reset_channels)</ept> to reset the state of the ingest point and also it is recommended to use <bpt id=\"p2\">[</bpt>Program Stop<ept id=\"p2\">](https://msdn.microsoft.com/library/azure/dn783463.aspx#stop_programs)</ept> to end a presentation and stream.",
      "pos": [
        4632,
        4935
      ]
    },
    {
      "content": "The MP4 fragment duration SHOULD be constant, in order to reduce the size of the client manifests and improve client download heuristics through use of repeat tags.",
      "pos": [
        4939,
        5103
      ]
    },
    {
      "content": "The duration MAY fluctuate in order to compensate for non-integer frame rates.",
      "pos": [
        5105,
        5183
      ]
    },
    {
      "content": "The MP4 fragment duration SHOULD be between approximately 2 and 6 seconds.",
      "pos": [
        5187,
        5261
      ]
    },
    {
      "content": "MP4 fragment timestamps and indexes (TrackFragmentExtendedHeaderBox fragment_absolute_time and fragment_index) SHOULD arrive in increasing order.",
      "pos": [
        5265,
        5410
      ]
    },
    {
      "content": "Although Azure Media Services is resilient to duplicate fragments, it has very limited ability to reorder fragments according to the media timeline.",
      "pos": [
        5412,
        5560
      ]
    },
    {
      "content": "Protocol Format – HTTP",
      "pos": [
        5564,
        5586
      ]
    },
    {
      "content": "ISO Fragmented MP4 based live ingest for Microsoft Azure Media Services uses a standard long running HTTP POST request to transmit encoded media data packaged in Fragmented MP4 format to the service.",
      "pos": [
        5588,
        5787
      ]
    },
    {
      "content": "Each HTTP POST sends a complete Fragmented MP4 bit-stream (“Stream”) starting from beginning with header boxes ( ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) and continuing with a sequence of fragments (‘moof’ and ‘mdat’ boxes).",
      "pos": [
        5788,
        6023
      ]
    },
    {
      "content": "Please refer to section 9.2 in [1] for URL syntax for HTTP POST request.",
      "pos": [
        6024,
        6096
      ]
    },
    {
      "content": "An example of the POST URL is:",
      "pos": [
        6097,
        6127
      ]
    },
    {
      "content": "Here are the detailed requirements:",
      "pos": [
        6211,
        6246
      ]
    },
    {
      "content": "Encoder SHOULD start the broadcast by sending an HTTP POST request with an empty “body” (zero content length) using the same ingestion URL.",
      "pos": [
        6251,
        6390
      ]
    },
    {
      "content": "This can help quickly detect if the live ingestion endpoint is valid and if there is any authentication or other conditions required.",
      "pos": [
        6391,
        6524
      ]
    },
    {
      "content": "Per HTTP protocol, the server won’t be able to send back HTTP response until the entire request including POST body is received.",
      "pos": [
        6525,
        6653
      ]
    },
    {
      "content": "Given the long running nature of live event, without this step, the encoder may not be able to detect any error until it finishes sending all the data.",
      "pos": [
        6654,
        6805
      ]
    },
    {
      "content": "Encoder MUST handle any errors or authentication challenges as a result of (1).",
      "pos": [
        6809,
        6888
      ]
    },
    {
      "content": "If (1) succeeds with a 200 response, continue.",
      "pos": [
        6889,
        6935
      ]
    },
    {
      "content": "Encoder MUST start a new HTTP POST request with the fragmented MP4 stream.",
      "pos": [
        6939,
        7013
      ]
    },
    {
      "content": "The payload MUST start with the header boxes followed by fragments.",
      "pos": [
        7015,
        7082
      ]
    },
    {
      "content": "Note the ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box (in this order) MUST be sent with each request, even if the encoder must reconnect because the previous request was terminated prior to the end of the stream.",
      "pos": [
        7084,
        7302
      ]
    },
    {
      "content": "Encoder MUST use Chunked Transfer Encoding for uploading since it’s impossible to predict the entire content length of the live event.",
      "pos": [
        7307,
        7441
      ]
    },
    {
      "content": "When the event is over, after sending the last fragment, the encoder MUST gracefully end the Chunked Transfer Encoding message sequence (most HTTP client stacks handle it automatically).",
      "pos": [
        7445,
        7631
      ]
    },
    {
      "content": "Encoder MUST wait for the service to return the final response code and then terminate the connection.",
      "pos": [
        7632,
        7734
      ]
    },
    {
      "content": "Encoder MUST NOT use the Events() noun as described in 9.2 in [1] for live ingestion into Microsoft Azure Media Services.",
      "pos": [
        7739,
        7860
      ]
    },
    {
      "content": "If the HTTP POST request terminates or times out prior to the end of the stream with a TCP error, the encoder MUST issue a new POST request using a new connection and follow the requirements above with the additional requirement that the encoder MUST resend the previous two MP4 fragments for each track in the stream, and resume without introducing discontinuities in the media timeline.",
      "pos": [
        7864,
        8252
      ]
    },
    {
      "content": "Resending the last two MP4 fragments for each track ensures that there is no data loss.",
      "pos": [
        8254,
        8341
      ]
    },
    {
      "content": "In other words, if a stream contains both an audio and video track, and the current POST request fails, the encoder must reconnect and resend the last two fragments for the audio track, which were previously successfully sent, and the last two fragments for the video track, which were previously successfully sent, in order to ensure that there is no data loss.",
      "pos": [
        8343,
        8705
      ]
    },
    {
      "content": "The encoder MUST maintain a “forward” buffer of media fragments, which it resends when reconnecting.",
      "pos": [
        8707,
        8807
      ]
    },
    {
      "content": "Timescale",
      "pos": [
        8811,
        8820
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>[MS-SSTR]<ept id=\"p1\">](https://msdn.microsoft.com/library/ff469518.aspx)</ept> describes the usage of “Timescale” for SmoothStreamingMedia (Section 2.2.2.1), StreamElement (Section 2.2.2.3), StreamFragmentElement(2.2.2.6) and LiveSMIL (Section 2.2.7.3.1).",
      "pos": [
        8823,
        9061
      ]
    },
    {
      "content": "If timescale value is not present, the default value used is 10,000,000 (10 MHz).",
      "pos": [
        9062,
        9143
      ]
    },
    {
      "content": "Although Smooth Streaming Format Specification doesn’t block usage of other timescale values, most of the encoder implementations uses this default value (10 MHz) to generate Smooth Streaming ingest data.",
      "pos": [
        9144,
        9348
      ]
    },
    {
      "content": "Due to <bpt id=\"p1\">[</bpt>Azure Media Dynamic Packaging<ept id=\"p1\">](media-services-dynamic-packaging-overview.md)</ept> feature, it is recommend to use 90 kHz timescale for video streams and 44.1 or 48.1 kHz for audio streams.",
      "pos": [
        9349,
        9540
      ]
    },
    {
      "content": "If different timescale values are used for different streams, the stream level timescale MUST be sent.",
      "pos": [
        9541,
        9643
      ]
    },
    {
      "content": "Please refer to <bpt id=\"p1\">[</bpt>[MS-SSTR]<ept id=\"p1\">](https://msdn.microsoft.com/library/ff469518.aspx)</ept>.",
      "pos": [
        9644,
        9722
      ]
    },
    {
      "content": "Definition of “Stream”",
      "pos": [
        9731,
        9753
      ]
    },
    {
      "content": "“Stream” is the basic unit of operation in live ingestion for composing live presentation, handling streaming failover and redundancy scenarios.",
      "pos": [
        9757,
        9901
      ]
    },
    {
      "content": "“Stream” is defined as one unique Fragmented MP4 bit-stream which may contain a single track or multiple tracks.",
      "pos": [
        9902,
        10014
      ]
    },
    {
      "content": "A full live presentation could contain one or more streams depending on the configuration of the live encoder(s).",
      "pos": [
        10015,
        10128
      ]
    },
    {
      "content": "The examples below illustrate various options of using stream(s) to compose a full live presentation.",
      "pos": [
        10129,
        10230
      ]
    },
    {
      "content": "Example:",
      "pos": [
        10234,
        10242
      ]
    },
    {
      "content": "Customer wants to create a live streaming presentation which includes the following audio/video bitrates:",
      "pos": [
        10247,
        10352
      ]
    },
    {
      "content": "Video – 3000kbps, 1500kbps, 750kbps",
      "pos": [
        10354,
        10389
      ]
    },
    {
      "content": "Audio – 128kbps",
      "pos": [
        10391,
        10406
      ]
    },
    {
      "content": "Option 1: All tracks in one stream",
      "pos": [
        10411,
        10445
      ]
    },
    {
      "content": "In this option, a single encoder generates all audio/video tracks and bundle them into one Fragmented MP4 bit-stream which then gets sent via a single HTTP POST connection.",
      "pos": [
        10447,
        10619
      ]
    },
    {
      "content": "In this example, there is only one stream for this live presentation:",
      "pos": [
        10620,
        10689
      ]
    },
    {
      "content": "image2",
      "pos": [
        10693,
        10699
      ]
    },
    {
      "content": "Option 2: Each track in a separate stream",
      "pos": [
        10713,
        10754
      ]
    },
    {
      "content": "In this option, the encoder(s) only put one track into each Fragment MP4 bit-stream and post all the streams over multiple separate HTTP connections.",
      "pos": [
        10756,
        10905
      ]
    },
    {
      "content": "This could be done with one encoders or multiple encoders.",
      "pos": [
        10906,
        10964
      ]
    },
    {
      "content": "From live ingestion’s point of view, this live presentation is composed of four streams.",
      "pos": [
        10965,
        11053
      ]
    },
    {
      "content": "image3",
      "pos": [
        11057,
        11063
      ]
    },
    {
      "content": "Option 3: Bundle audio track with the lowest bitrate video track into one stream",
      "pos": [
        11077,
        11157
      ]
    },
    {
      "content": "In this option, the customer chooses to bundle the audio track with the lowest bitrate video track into one Fragment MP4 bit-stream and leave the other two video tracks each being its own stream.",
      "pos": [
        11159,
        11354
      ]
    },
    {
      "content": "image4",
      "pos": [
        11360,
        11366
      ]
    },
    {
      "content": "Summary",
      "pos": [
        11381,
        11388
      ]
    },
    {
      "content": "What’s shown above is NOT an exhaustive list of all the possible ingestion options for this example.",
      "pos": [
        11390,
        11490
      ]
    },
    {
      "content": "As a matter of fact, any grouping of tracks into streams is supported by the live ingestion.",
      "pos": [
        11491,
        11583
      ]
    },
    {
      "content": "Customers and encoder vendors can choose their own implementations based on engineering complexity, encoder capacity, and redundancy and failover considerations.",
      "pos": [
        11584,
        11745
      ]
    },
    {
      "content": "However it should be noted that in most cases there is only one audio track for the entire live presentation so it’s important to ensure the healthiness of the ingest stream that contains the audio track.",
      "pos": [
        11746,
        11950
      ]
    },
    {
      "content": "This consideration often results in putting audio track into its own stream (as in Option 2) or bundling it with the lowest bitrate video track (as in Option 3).",
      "pos": [
        11951,
        12112
      ]
    },
    {
      "content": "Also for better redundancy and fault-tolerance, sending the same audio track in two different streams (Option 2 with redundant audio tracks) or bundling the audio track at least two of the lowest bitrate video tracks (Option 3 with audio bundled in at least two video streams)  is highly recommended for live ingest into Microsoft Azure Media Services.",
      "pos": [
        12113,
        12465
      ]
    },
    {
      "content": "Service Failover",
      "pos": [
        12469,
        12485
      ]
    },
    {
      "content": "Given the nature of live streaming, good failover support is critical for ensuring the availability of the service.",
      "pos": [
        12488,
        12603
      ]
    },
    {
      "content": "Microsoft Azure Media Services is designed to handle various types of failures including network errors, server errors, storage problems, etc. When used in conjunction with proper failover logic from the live encoder side, customer can achieve a highly reliable live streaming service from the cloud.",
      "pos": [
        12604,
        12904
      ]
    },
    {
      "content": "In this section, we will discuss service failover scenarios.",
      "pos": [
        12906,
        12966
      ]
    },
    {
      "content": "In this case, the failure happens somewhere within the service and manifests itself as a network error.",
      "pos": [
        12967,
        13070
      ]
    },
    {
      "content": "Here are some recommendations for the encoder implementation for handling service failover:",
      "pos": [
        13071,
        13162
      ]
    },
    {
      "content": "Use a 10 second timeout for establishing the TCP connection.",
      "pos": [
        13168,
        13228
      ]
    },
    {
      "content": "If an attempt to establish the connection takes longer than 10 seconds, abort the operation and try again.",
      "pos": [
        13230,
        13336
      ]
    },
    {
      "content": "Use a short timeout for sending the HTTP request message chunks.",
      "pos": [
        13341,
        13405
      ]
    },
    {
      "content": "If the target MP4 fragment duration is N seconds, use a send timeout between N and 2N seconds; for example, use a timeout of 6 to 12 seconds if the MP4 fragment duration is 6 seconds.",
      "pos": [
        13407,
        13590
      ]
    },
    {
      "content": "If a timeout occurs, reset the connection, open a new connection, and resume stream ingest on the new connection.",
      "pos": [
        13592,
        13705
      ]
    },
    {
      "content": "Maintain a rolling buffer containing the last two fragments, for each track, that were successfully and completely sent to the service.",
      "pos": [
        13710,
        13845
      ]
    },
    {
      "content": "If the HTTP POST request for a stream is terminated or times out prior to the end of the stream, open a new connection and begin another HTTP POST request, resend the stream headers, resend the last two fragments for each track, and resume the stream without introducing a discontinuity in the media timeline.",
      "pos": [
        13847,
        14156
      ]
    },
    {
      "content": "This will reduce the chance of data loss.",
      "pos": [
        14158,
        14199
      ]
    },
    {
      "content": "It is recommended that the encoder does NOT limit the number of retries to establish a connection or resume streaming after a TCP error occurs.",
      "pos": [
        14203,
        14346
      ]
    },
    {
      "content": "After a TCP error:",
      "pos": [
        14350,
        14368
      ]
    },
    {
      "content": "The current connection MUST be closed, and a new connection MUST be created for a new HTTP POST request.",
      "pos": [
        14376,
        14480
      ]
    },
    {
      "content": "The new HTTP POST URL MUST be the same as the initial POST URL.",
      "pos": [
        14488,
        14551
      ]
    },
    {
      "content": "The new HTTP POST MUST include stream headers (‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) identical to the stream headers in the initial POST.",
      "pos": [
        14559,
        14710
      ]
    },
    {
      "content": "The last two fragments sent for each track MUST be resent, and streaming resumed without introducing a discontinuity in the media timeline.",
      "pos": [
        14718,
        14857
      ]
    },
    {
      "content": "The MP4 fragment timestamps must increase continuously, even across HTTP POST requests.",
      "pos": [
        14859,
        14946
      ]
    },
    {
      "content": "The encoder SHOULD terminate the HTTP POST request if data is not being sent at a rate commensurate with the MP4 fragment duration.",
      "pos": [
        14950,
        15081
      ]
    },
    {
      "content": "An HTTP POST request that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update.",
      "pos": [
        15083,
        15238
      ]
    },
    {
      "content": "For this reason, the HTTP POST for sparse (ad signal) tracks SHOULD be short lived, terminating as soon as the sparse fragment is sent.",
      "pos": [
        15240,
        15375
      ]
    },
    {
      "content": "Encoder Failover",
      "pos": [
        15379,
        15395
      ]
    },
    {
      "content": "Encoder failover is the second type of failover scenario that needs to be addressed for end-to-end live streaming delivery.",
      "pos": [
        15397,
        15520
      ]
    },
    {
      "content": "In this scenario, the error condition happened on the encoder side.",
      "pos": [
        15521,
        15588
      ]
    },
    {
      "content": "image5",
      "pos": [
        15593,
        15599
      ]
    },
    {
      "content": "Below are the expectations from the live ingestion endpoint when encoder failover happens:",
      "pos": [
        15611,
        15701
      ]
    },
    {
      "content": "A new encoder instance SHOULD be created in order to continue streaming, as illustrated in the diagram above (Stream for 3000k video with dashed line).",
      "pos": [
        15706,
        15857
      ]
    },
    {
      "content": "The new encoder MUST use the same URL for HTTP POST requests as the failed instance.",
      "pos": [
        15861,
        15945
      ]
    },
    {
      "content": "The new encoder’s POST request MUST include the same fragmented MP4 header boxes as the failed instance.",
      "pos": [
        15949,
        16053
      ]
    },
    {
      "content": "The new encoder MUST be properly synchronized with all other running encoders for the same live presentation to generate synchronized audio/video samples with aligned fragment boundaries.",
      "pos": [
        16057,
        16244
      ]
    },
    {
      "content": "The new stream MUST be semantically equivalent with the previous stream and interchangeable at header and fragment level.",
      "pos": [
        16248,
        16369
      ]
    },
    {
      "content": "The new encoder SHOULD try to minimize data loss.",
      "pos": [
        16373,
        16422
      ]
    },
    {
      "content": "The fragment_absolute_time and fragment_index of media fragments SHOULD increase from the point where the encoder last stopped.",
      "pos": [
        16424,
        16551
      ]
    },
    {
      "content": "The fragment_absolute_time and fragment_index SHOULD increase in a continuous fashion, but it is permissible to introduce a discontinuity if necessary.",
      "pos": [
        16553,
        16704
      ]
    },
    {
      "content": "Azure Media Services will ignore fragments that it has already received and processed, so it is better to err on the side of resending fragments than to introduce discontinuities in the media timeline.",
      "pos": [
        16706,
        16907
      ]
    },
    {
      "content": "Encoder Redundancy",
      "pos": [
        16912,
        16930
      ]
    },
    {
      "content": "For certain critical live events that demand even higher availability and quality of experience, it is recommended to employ active-active redundant encoders to achieve seamless failover with no data loss.",
      "pos": [
        16933,
        17138
      ]
    },
    {
      "content": "image6",
      "pos": [
        17142,
        17148
      ]
    },
    {
      "content": "As illustrated in the diagram above, there are two group of encoders pushing two copies of each stream simultaneously into the live service.",
      "pos": [
        17159,
        17299
      ]
    },
    {
      "content": "This setup is supported because Microsoft Azure Media Services has the ability to filter out duplicate fragments based on stream ID and fragment timestamp.",
      "pos": [
        17300,
        17455
      ]
    },
    {
      "content": "The resulting live stream and archive will be a single of copy of all the streams that is the best possible aggregation from the two sources.",
      "pos": [
        17456,
        17597
      ]
    },
    {
      "content": "For example, in a hypothetical extreme case, as long as there is one encoder (doesn’t have to be the same one) running at any given point in time for each stream, the resulting live stream from the service will be continuous without data loss.",
      "pos": [
        17598,
        17841
      ]
    },
    {
      "content": "The requirement for this scenario is almost the same as the requirements in Encoder Failover case with the exception that the second set of encoders are running at the same time as the primary encoders.",
      "pos": [
        17844,
        18046
      ]
    },
    {
      "content": "Service Redundancy",
      "pos": [
        18050,
        18068
      ]
    },
    {
      "content": "For highly redundant global distribution, it is sometimes required to have cross-region backup to handle regional disasters.",
      "pos": [
        18072,
        18196
      ]
    },
    {
      "content": "Expanding on the “Encoder Redundancy” topology, customers can choose to have a redundant service deployment in a different region which is connected with the 2nd set of encoders.",
      "pos": [
        18197,
        18375
      ]
    },
    {
      "content": "Customers could also work with a CDN provider to deploy a GTM (Global Traffic Manager) in front of the two service deployments to seamlessly route client traffic.",
      "pos": [
        18376,
        18538
      ]
    },
    {
      "content": "The requirements for the encoders are the same as “Encoder Redundancy” case with the only exception that the second set of encoders need to be pointed to a different live ingest end point.",
      "pos": [
        18539,
        18727
      ]
    },
    {
      "content": "The diagram below shows this setup:",
      "pos": [
        18728,
        18763
      ]
    },
    {
      "content": "image7",
      "pos": [
        18767,
        18773
      ]
    },
    {
      "content": "Special Types of Ingestion Formats",
      "pos": [
        18786,
        18820
      ]
    },
    {
      "content": "This section discusses some special type of live ingestion formats that are designed to handle some specific scenarios.",
      "pos": [
        18823,
        18942
      ]
    },
    {
      "content": "Sparse Track",
      "pos": [
        18947,
        18959
      ]
    },
    {
      "content": "When delivering a live streaming presentation with rich client experience, it is often necessary to transmit time-synchronized events or signals in-band with the main media data.",
      "pos": [
        18961,
        19139
      ]
    },
    {
      "content": "One example of this is dynamic live Ads insertion.",
      "pos": [
        19140,
        19190
      ]
    },
    {
      "content": "This type of event signaling is different from regular audio/video streaming because of its sparse nature.",
      "pos": [
        19191,
        19297
      ]
    },
    {
      "content": "In another words, the signaling data usually does not happen continuously and the interval can be hard to predict.",
      "pos": [
        19298,
        19412
      ]
    },
    {
      "content": "The concept of Sparse Track was specifically designed to ingest and broadcast in-band signaling data.",
      "pos": [
        19413,
        19514
      ]
    },
    {
      "content": "Below is a recommended implementation for ingesting sparse track:",
      "pos": [
        19516,
        19581
      ]
    },
    {
      "content": "Create a separate Fragmented MP4 bit-stream which just contains sparse track(s) without audio/video tracks.",
      "pos": [
        19586,
        19693
      ]
    },
    {
      "content": "In the “Live Server Manifest Box” as defined in Section 6 in [1], use “parentTrackName” parameter to specify the name of the parent track.",
      "pos": [
        19697,
        19835
      ]
    },
    {
      "content": "Please refer to section 4.2.1.2.1.2 in [1] for more details.",
      "pos": [
        19836,
        19896
      ]
    },
    {
      "content": "In the “Live Server Manifest Box”, manifestOutput MUST be set to “true”.",
      "pos": [
        19900,
        19972
      ]
    },
    {
      "content": "Given the sparse nature of the signaling event, it is recommended that:",
      "pos": [
        19976,
        20047
      ]
    },
    {
      "content": "At the beginning of the live event, encoder sends the initial header boxes to the service which would allow the service to register the sparse track in the client manifest.",
      "pos": [
        20055,
        20227
      ]
    },
    {
      "content": "The encoder SHOULD terminate the HTTP POST request when data is not being sent.",
      "pos": [
        20235,
        20314
      ]
    },
    {
      "content": "A long running HTTP POST that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update or server reboot, as the media server will be temporarily blocked in a receive operation on the socket.",
      "pos": [
        20316,
        20578
      ]
    },
    {
      "content": "During the time when signaling data is not available, the encoder SHOULD close the HTTP POST request.",
      "pos": [
        20587,
        20688
      ]
    },
    {
      "content": "While the POST request is active, the encoder SHOULD send data",
      "pos": [
        20690,
        20752
      ]
    },
    {
      "content": "When sending sparse fragments, encoder can set explicit Content-Length header if it’s available.",
      "pos": [
        20761,
        20857
      ]
    },
    {
      "content": "When sending sparse fragment with a new connection, encoder SHOULD start sending from the header boxes followed by the new fragments.",
      "pos": [
        20865,
        20998
      ]
    },
    {
      "content": "This is to handle the case where failover happened in between and the new sparse connection is being established to a new server which has not seen the sparse track before.",
      "pos": [
        20999,
        21171
      ]
    },
    {
      "content": "The sparse track fragment will be made available to the client when the corresponding parent track fragment that has equal or bigger timestamp value is made available to the client.",
      "pos": [
        21179,
        21360
      ]
    },
    {
      "content": "For example, if the sparse fragment has a timestamp of t=1000, it is expected after the client sees video (assuming the parent track name is video) fragment timestamp 1000 or beyond, it can download the sparse fragment t=1000.",
      "pos": [
        21361,
        21587
      ]
    },
    {
      "content": "Please note that the actual signal could very well be used for a different position in the presentation timeline for its designated purpose.",
      "pos": [
        21588,
        21728
      ]
    },
    {
      "content": "In the example above, it’s possible that the sparse fragment of t=1000 has a XML payload which is for inserting an Ad in a position that’s a few seconds later.",
      "pos": [
        21729,
        21888
      ]
    },
    {
      "content": "The payload of sparse track fragment can be in various different formats (e.g. XML or text or binary, etc.) depending on different scenarios.",
      "pos": [
        21896,
        22037
      ]
    },
    {
      "content": "Redundant Audio Track",
      "pos": [
        22044,
        22065
      ]
    },
    {
      "content": "In a typical HTTP Adaptive Streaming scenario (e.g. Smooth Streaming or DASH), there is often only one audio track in the entire presentation.",
      "pos": [
        22067,
        22209
      ]
    },
    {
      "content": "Unlike video tracks which have multiple quality levels for the client to choose from in error conditions, the audio track can be a single point of failure if the ingestion of the stream that contains the audio track is broken.",
      "pos": [
        22210,
        22436
      ]
    },
    {
      "content": "To solve this problem, Microsoft Azure Media Services supports live ingestion of redundant audio tracks.",
      "pos": [
        22439,
        22543
      ]
    },
    {
      "content": "The idea is that the same audio track can be sent multiple times in different streams.",
      "pos": [
        22544,
        22630
      ]
    },
    {
      "content": "While the service will only register the audio track once in the client manifest, it is able to use redundant audio tracks as backups for retrieving audio fragments if the primary audio track is having issues.",
      "pos": [
        22631,
        22840
      ]
    },
    {
      "content": "In order to ingest redundant audio tracks, the encoder needs to:",
      "pos": [
        22841,
        22905
      ]
    },
    {
      "content": "Create the same audio track in multiple Fragment MP4 bit-streams.",
      "pos": [
        22910,
        22975
      ]
    },
    {
      "content": "The redundant audio tracks MUST be semantically equivalent with exactly the same fragment timestamps and interchangeable at header and fragment level.",
      "pos": [
        22976,
        23126
      ]
    },
    {
      "content": "Ensure that the “audio” entry in the Live Server Manifest (Section 6 in [1]) be the same for all redundant audio tracks.",
      "pos": [
        23130,
        23250
      ]
    },
    {
      "content": "Below is a recommended implementation for redundant audio tracks:",
      "pos": [
        23252,
        23317
      ]
    },
    {
      "content": "Send each unique audio track in a stream by itself.",
      "pos": [
        23322,
        23373
      ]
    },
    {
      "content": "Also send a redundant stream for each of these audio track streams, where the 2nd stream differs from the 1st only by the identifier in the HTTP POST URL:  {protocol}://{server address}/{publishing point path}/Streams({identifier}).",
      "pos": [
        23375,
        23607
      ]
    },
    {
      "content": "Use separate streams to send the two lowest video bitrates.",
      "pos": [
        23611,
        23670
      ]
    },
    {
      "content": "Each of these streams SHOULD also contain a copy of each unique audio track.",
      "pos": [
        23671,
        23747
      ]
    },
    {
      "content": "For example, when multiple languages are supported, these streams SHOULD contain audio tracks for each language.",
      "pos": [
        23749,
        23861
      ]
    },
    {
      "content": "Use separate server (encoder) instances to encode and send the redundant streams mentioned in (1) and (2).",
      "pos": [
        23865,
        23971
      ]
    },
    {
      "content": "test",
      "pos": [
        24573,
        24577
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Azure Media Services Fragmented MP4 Live Ingest Specification\" \n    description=\"This specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services. Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform. At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services. This document also discusses best practices in building highly redundant and robust live ingest mechanisms.\" \n    services=\"media-services\" \n    documentationCenter=\"\" \n    authors=\"juliako\" \n    manager=\"dwrede\" \n    editor=\"\"/>\n\n<tags \n    ms.service=\"media-services\" \n    ms.workload=\"media\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/11/2015\" \n    ms.author=\"juliako\"/>\n\n#Azure Media Services Fragmented MP4 Live Ingest Specification\n\nThis specification describes the protocol and format for Fragmented MP4 based live streaming ingestion for Microsoft Azure Media Services. Microsoft Azure Media Services provides live streaming service which allows customers to stream live events and broadcast content in real-time using Microsoft Azure as the cloud platform. At the time of writing, pre-encoded Fragment MP4 is the only ingest mechanism for live streaming in Microsoft Azure Media Services. This document also discusses best practices in building highly redundant and robust live ingest mechanisms.\n\n##Conformance Notation\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\n\n##Service Diagram \n\nThe diagram below shows the high level architecture of the live streaming service in Microsoft Azure Media Services:\n\n1.  Live Encoder pushes live feeds into Channels which are created and provisioned via the Microsoft Azure Media Services SDK.\n2.  Channels, Programs and Streaming endpoint in Microsoft Azure Media Services handle all the live streaming functionalities including ingest, formatting, cloud DVR, security, scalability and redundancy.\n3.  Optionally customers could choose to deploy a CDN layer between the Streaming endpoint and the client endpoints.\n4.  Client endpoints stream from the Streaming endpoint using HTTP Adaptive Streaming protocols (e.g. Smooth Streaming, DASH, HDS or HLS).\n\n![image1][image1]\n\n\n##Bit-stream Format – ISO 14496-12 Fragmented MP4\n\nThe wire format for live streaming ingest that is discussed in this document is based on [ISO-14496-12]. Please refer to [[MS-SSTR]](https://msdn.microsoft.com/library/ff469518.aspx) for detailed explanation of Fragmented MP4 format and extensions for both video-on-demand files and live streaming ingestion. \n\nBelow is a list of special format definitions that apply to live ingest into Microsoft Azure Media Services:\n\n1. The ‘ftyp’, LiveServerManifestBox, and ‘moov’ box MUST be sent with each request (HTTP POST).  It MUST be sent at the beginning of the stream and anytime the encoder must reconnect to resume stream ingest.  Please refer to Section 6 in [1] for more details.\n2. Section 3.3.2 in [1] defines an optional box called StreamManifestBox for live ingest. Due to the routing logic of Microsoft Azure’s load balancer, usage of this box is deprecated and SHOULD NOT be present when ingesting into Microsoft Azure Media Service. If this box is present, Azure Media Services silently ignores it.\n3. The TrackFragmentExtendedHeaderBox defined in 3.2.3.2 in [1] MUST be present for each fragment.\n4. Version 2 of the TrackFragmentExtendedHeaderBox SHOULD be used in order to generate media segments with identical URLs in multiple datacenters. The fragment index field is REQUIRED for cross-datacenter failover of index-based streaming formats such as Apple HTTP Live Streaming (HLS) and index-based MPEG-DASH.  To enable cross-datacenter failover, the fragment index MUST be synchronized across multiple encoders, and increase by 1 for each successive media fragment, even across encoder restarts or failures.\n5. Section 3.3.6 in [1] defines box called MovieFragmentRandomAccessBox (‘mfra’) that MAY be sent at the end of live ingestion to indicate EOS (End-of-Stream) to the channel. Due to the ingest logic of Azure Media Services, usage of EOS (End-of-Stream) is deprecated and the ‘mfra’ box for live ingestion SHOULD NOT be sent. If sent, Azure Media Services silently ignores it. It is recommended to use [Channel Reset](https://msdn.microsoft.com/library/azure/dn783458.aspx#reset_channels) to reset the state of the ingest point and also it is recommended to use [Program Stop](https://msdn.microsoft.com/library/azure/dn783463.aspx#stop_programs) to end a presentation and stream.\n6. The MP4 fragment duration SHOULD be constant, in order to reduce the size of the client manifests and improve client download heuristics through use of repeat tags.  The duration MAY fluctuate in order to compensate for non-integer frame rates.\n7. The MP4 fragment duration SHOULD be between approximately 2 and 6 seconds.\n8. MP4 fragment timestamps and indexes (TrackFragmentExtendedHeaderBox fragment_absolute_time and fragment_index) SHOULD arrive in increasing order.  Although Azure Media Services is resilient to duplicate fragments, it has very limited ability to reorder fragments according to the media timeline.\n\n##Protocol Format – HTTP\n\nISO Fragmented MP4 based live ingest for Microsoft Azure Media Services uses a standard long running HTTP POST request to transmit encoded media data packaged in Fragmented MP4 format to the service. Each HTTP POST sends a complete Fragmented MP4 bit-stream (“Stream”) starting from beginning with header boxes ( ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) and continuing with a sequence of fragments (‘moof’ and ‘mdat’ boxes). Please refer to section 9.2 in [1] for URL syntax for HTTP POST request. An example of the POST URL is: \n\n    http://customer.channel.mediaservices.windows.net/ingest.isml/streams(720p)\n\nHere are the detailed requirements:\n\n1. Encoder SHOULD start the broadcast by sending an HTTP POST request with an empty “body” (zero content length) using the same ingestion URL. This can help quickly detect if the live ingestion endpoint is valid and if there is any authentication or other conditions required. Per HTTP protocol, the server won’t be able to send back HTTP response until the entire request including POST body is received. Given the long running nature of live event, without this step, the encoder may not be able to detect any error until it finishes sending all the data.\n2. Encoder MUST handle any errors or authentication challenges as a result of (1). If (1) succeeds with a 200 response, continue.\n3. Encoder MUST start a new HTTP POST request with the fragmented MP4 stream.  The payload MUST start with the header boxes followed by fragments.  Note the ‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box (in this order) MUST be sent with each request, even if the encoder must reconnect because the previous request was terminated prior to the end of the stream. \n4. Encoder MUST use Chunked Transfer Encoding for uploading since it’s impossible to predict the entire content length of the live event.\n5. When the event is over, after sending the last fragment, the encoder MUST gracefully end the Chunked Transfer Encoding message sequence (most HTTP client stacks handle it automatically). Encoder MUST wait for the service to return the final response code and then terminate the connection. \n6. Encoder MUST NOT use the Events() noun as described in 9.2 in [1] for live ingestion into Microsoft Azure Media Services.\n7. If the HTTP POST request terminates or times out prior to the end of the stream with a TCP error, the encoder MUST issue a new POST request using a new connection and follow the requirements above with the additional requirement that the encoder MUST resend the previous two MP4 fragments for each track in the stream, and resume without introducing discontinuities in the media timeline.  Resending the last two MP4 fragments for each track ensures that there is no data loss.  In other words, if a stream contains both an audio and video track, and the current POST request fails, the encoder must reconnect and resend the last two fragments for the audio track, which were previously successfully sent, and the last two fragments for the video track, which were previously successfully sent, in order to ensure that there is no data loss.  The encoder MUST maintain a “forward” buffer of media fragments, which it resends when reconnecting.\n\n##Timescale \n\n[[MS-SSTR]](https://msdn.microsoft.com/library/ff469518.aspx) describes the usage of “Timescale” for SmoothStreamingMedia (Section 2.2.2.1), StreamElement (Section 2.2.2.3), StreamFragmentElement(2.2.2.6) and LiveSMIL (Section 2.2.7.3.1). If timescale value is not present, the default value used is 10,000,000 (10 MHz). Although Smooth Streaming Format Specification doesn’t block usage of other timescale values, most of the encoder implementations uses this default value (10 MHz) to generate Smooth Streaming ingest data. Due to [Azure Media Dynamic Packaging](media-services-dynamic-packaging-overview.md) feature, it is recommend to use 90 kHz timescale for video streams and 44.1 or 48.1 kHz for audio streams. If different timescale values are used for different streams, the stream level timescale MUST be sent. Please refer to [[MS-SSTR]](https://msdn.microsoft.com/library/ff469518.aspx).     \n\n##Definition of “Stream”  \n\n“Stream” is the basic unit of operation in live ingestion for composing live presentation, handling streaming failover and redundancy scenarios. “Stream” is defined as one unique Fragmented MP4 bit-stream which may contain a single track or multiple tracks. A full live presentation could contain one or more streams depending on the configuration of the live encoder(s). The examples below illustrate various options of using stream(s) to compose a full live presentation.\n\n**Example:** \n\nCustomer wants to create a live streaming presentation which includes the following audio/video bitrates:\n\nVideo – 3000kbps, 1500kbps, 750kbps\n\nAudio – 128kbps\n\n###Option 1: All tracks in one stream\n\nIn this option, a single encoder generates all audio/video tracks and bundle them into one Fragmented MP4 bit-stream which then gets sent via a single HTTP POST connection. In this example, there is only one stream for this live presentation:\n\n![image2][image2]\n\n###Option 2: Each track in a separate stream\n\nIn this option, the encoder(s) only put one track into each Fragment MP4 bit-stream and post all the streams over multiple separate HTTP connections. This could be done with one encoders or multiple encoders. From live ingestion’s point of view, this live presentation is composed of four streams.\n\n![image3][image3]\n\n###Option 3: Bundle audio track with the lowest bitrate video track into one stream\n\nIn this option, the customer chooses to bundle the audio track with the lowest bitrate video track into one Fragment MP4 bit-stream and leave the other two video tracks each being its own stream. \n\n\n![image4][image4]\n\n\n###Summary\n\nWhat’s shown above is NOT an exhaustive list of all the possible ingestion options for this example. As a matter of fact, any grouping of tracks into streams is supported by the live ingestion. Customers and encoder vendors can choose their own implementations based on engineering complexity, encoder capacity, and redundancy and failover considerations. However it should be noted that in most cases there is only one audio track for the entire live presentation so it’s important to ensure the healthiness of the ingest stream that contains the audio track. This consideration often results in putting audio track into its own stream (as in Option 2) or bundling it with the lowest bitrate video track (as in Option 3). Also for better redundancy and fault-tolerance, sending the same audio track in two different streams (Option 2 with redundant audio tracks) or bundling the audio track at least two of the lowest bitrate video tracks (Option 3 with audio bundled in at least two video streams)  is highly recommended for live ingest into Microsoft Azure Media Services.\n\n##Service Failover \n\nGiven the nature of live streaming, good failover support is critical for ensuring the availability of the service. Microsoft Azure Media Services is designed to handle various types of failures including network errors, server errors, storage problems, etc. When used in conjunction with proper failover logic from the live encoder side, customer can achieve a highly reliable live streaming service from the cloud.\n\nIn this section, we will discuss service failover scenarios. In this case, the failure happens somewhere within the service and manifests itself as a network error. Here are some recommendations for the encoder implementation for handling service failover:\n\n\n1. Use a 10 second timeout for establishing the TCP connection.  If an attempt to establish the connection takes longer than 10 seconds, abort the operation and try again. \n2. Use a short timeout for sending the HTTP request message chunks.  If the target MP4 fragment duration is N seconds, use a send timeout between N and 2N seconds; for example, use a timeout of 6 to 12 seconds if the MP4 fragment duration is 6 seconds.  If a timeout occurs, reset the connection, open a new connection, and resume stream ingest on the new connection. \n3. Maintain a rolling buffer containing the last two fragments, for each track, that were successfully and completely sent to the service.  If the HTTP POST request for a stream is terminated or times out prior to the end of the stream, open a new connection and begin another HTTP POST request, resend the stream headers, resend the last two fragments for each track, and resume the stream without introducing a discontinuity in the media timeline.  This will reduce the chance of data loss.\n4. It is recommended that the encoder does NOT limit the number of retries to establish a connection or resume streaming after a TCP error occurs.\n5. After a TCP error:\n    1. The current connection MUST be closed, and a new connection MUST be created for a new HTTP POST request.\n    2. The new HTTP POST URL MUST be the same as the initial POST URL.\n    3. The new HTTP POST MUST include stream headers (‘ftyp’, “Live Server Manifest Box”, and ‘moov’ box) identical to the stream headers in the initial POST.\n    4. The last two fragments sent for each track MUST be resent, and streaming resumed without introducing a discontinuity in the media timeline.  The MP4 fragment timestamps must increase continuously, even across HTTP POST requests.\n6. The encoder SHOULD terminate the HTTP POST request if data is not being sent at a rate commensurate with the MP4 fragment duration.  An HTTP POST request that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update.  For this reason, the HTTP POST for sparse (ad signal) tracks SHOULD be short lived, terminating as soon as the sparse fragment is sent.\n\n##Encoder Failover\n\nEncoder failover is the second type of failover scenario that needs to be addressed for end-to-end live streaming delivery. In this scenario, the error condition happened on the encoder side. \n\n![image5][image5]\n\n\nBelow are the expectations from the live ingestion endpoint when encoder failover happens:\n\n1. A new encoder instance SHOULD be created in order to continue streaming, as illustrated in the diagram above (Stream for 3000k video with dashed line).\n2. The new encoder MUST use the same URL for HTTP POST requests as the failed instance.\n3. The new encoder’s POST request MUST include the same fragmented MP4 header boxes as the failed instance.\n4. The new encoder MUST be properly synchronized with all other running encoders for the same live presentation to generate synchronized audio/video samples with aligned fragment boundaries.\n5. The new stream MUST be semantically equivalent with the previous stream and interchangeable at header and fragment level.\n6. The new encoder SHOULD try to minimize data loss.  The fragment_absolute_time and fragment_index of media fragments SHOULD increase from the point where the encoder last stopped.  The fragment_absolute_time and fragment_index SHOULD increase in a continuous fashion, but it is permissible to introduce a discontinuity if necessary.  Azure Media Services will ignore fragments that it has already received and processed, so it is better to err on the side of resending fragments than to introduce discontinuities in the media timeline. \n\n##Encoder Redundancy \n\nFor certain critical live events that demand even higher availability and quality of experience, it is recommended to employ active-active redundant encoders to achieve seamless failover with no data loss.\n\n![image6][image6]\n\nAs illustrated in the diagram above, there are two group of encoders pushing two copies of each stream simultaneously into the live service. This setup is supported because Microsoft Azure Media Services has the ability to filter out duplicate fragments based on stream ID and fragment timestamp. The resulting live stream and archive will be a single of copy of all the streams that is the best possible aggregation from the two sources. For example, in a hypothetical extreme case, as long as there is one encoder (doesn’t have to be the same one) running at any given point in time for each stream, the resulting live stream from the service will be continuous without data loss. \n\nThe requirement for this scenario is almost the same as the requirements in Encoder Failover case with the exception that the second set of encoders are running at the same time as the primary encoders.\n\n##Service Redundancy  \n\nFor highly redundant global distribution, it is sometimes required to have cross-region backup to handle regional disasters. Expanding on the “Encoder Redundancy” topology, customers can choose to have a redundant service deployment in a different region which is connected with the 2nd set of encoders. Customers could also work with a CDN provider to deploy a GTM (Global Traffic Manager) in front of the two service deployments to seamlessly route client traffic. The requirements for the encoders are the same as “Encoder Redundancy” case with the only exception that the second set of encoders need to be pointed to a different live ingest end point. The diagram below shows this setup:\n\n![image7][image7]\n\n##Special Types of Ingestion Formats \n\nThis section discusses some special type of live ingestion formats that are designed to handle some specific scenarios.\n\n###Sparse Track\n\nWhen delivering a live streaming presentation with rich client experience, it is often necessary to transmit time-synchronized events or signals in-band with the main media data. One example of this is dynamic live Ads insertion. This type of event signaling is different from regular audio/video streaming because of its sparse nature. In another words, the signaling data usually does not happen continuously and the interval can be hard to predict. The concept of Sparse Track was specifically designed to ingest and broadcast in-band signaling data.\n\nBelow is a recommended implementation for ingesting sparse track:\n\n1. Create a separate Fragmented MP4 bit-stream which just contains sparse track(s) without audio/video tracks.\n2. In the “Live Server Manifest Box” as defined in Section 6 in [1], use “parentTrackName” parameter to specify the name of the parent track. Please refer to section 4.2.1.2.1.2 in [1] for more details.\n3. In the “Live Server Manifest Box”, manifestOutput MUST be set to “true”.\n4. Given the sparse nature of the signaling event, it is recommended that:\n    1. At the beginning of the live event, encoder sends the initial header boxes to the service which would allow the service to register the sparse track in the client manifest.\n    2. The encoder SHOULD terminate the HTTP POST request when data is not being sent.  A long running HTTP POST that does not send data can prevent Azure Media Services from quickly disconnecting from the encoder in the event of a service update or server reboot, as the media server will be temporarily blocked in a receive operation on the socket. \n    3. During the time when signaling data is not available, the encoder SHOULD close the HTTP POST request.  While the POST request is active, the encoder SHOULD send data \n    4. When sending sparse fragments, encoder can set explicit Content-Length header if it’s available.\n    5. When sending sparse fragment with a new connection, encoder SHOULD start sending from the header boxes followed by the new fragments. This is to handle the case where failover happened in between and the new sparse connection is being established to a new server which has not seen the sparse track before.\n    6. The sparse track fragment will be made available to the client when the corresponding parent track fragment that has equal or bigger timestamp value is made available to the client. For example, if the sparse fragment has a timestamp of t=1000, it is expected after the client sees video (assuming the parent track name is video) fragment timestamp 1000 or beyond, it can download the sparse fragment t=1000. Please note that the actual signal could very well be used for a different position in the presentation timeline for its designated purpose. In the example above, it’s possible that the sparse fragment of t=1000 has a XML payload which is for inserting an Ad in a position that’s a few seconds later.\n    7. The payload of sparse track fragment can be in various different formats (e.g. XML or text or binary, etc.) depending on different scenarios. \n\n\n###Redundant Audio Track\n\nIn a typical HTTP Adaptive Streaming scenario (e.g. Smooth Streaming or DASH), there is often only one audio track in the entire presentation. Unlike video tracks which have multiple quality levels for the client to choose from in error conditions, the audio track can be a single point of failure if the ingestion of the stream that contains the audio track is broken. \n\nTo solve this problem, Microsoft Azure Media Services supports live ingestion of redundant audio tracks. The idea is that the same audio track can be sent multiple times in different streams. While the service will only register the audio track once in the client manifest, it is able to use redundant audio tracks as backups for retrieving audio fragments if the primary audio track is having issues. In order to ingest redundant audio tracks, the encoder needs to:\n\n1. Create the same audio track in multiple Fragment MP4 bit-streams. The redundant audio tracks MUST be semantically equivalent with exactly the same fragment timestamps and interchangeable at header and fragment level.\n2. Ensure that the “audio” entry in the Live Server Manifest (Section 6 in [1]) be the same for all redundant audio tracks.\n\nBelow is a recommended implementation for redundant audio tracks:\n\n1. Send each unique audio track in a stream by itself.  Also send a redundant stream for each of these audio track streams, where the 2nd stream differs from the 1st only by the identifier in the HTTP POST URL:  {protocol}://{server address}/{publishing point path}/Streams({identifier}).\n2. Use separate streams to send the two lowest video bitrates. Each of these streams SHOULD also contain a copy of each unique audio track.  For example, when multiple languages are supported, these streams SHOULD contain audio tracks for each language.\n3. Use separate server (encoder) instances to encode and send the redundant streams mentioned in (1) and (2). \n\n\n[image1]: ./media/media-services-fmp4-live-ingest-overview/media-services-image1.png\n[image2]: ./media/media-services-fmp4-live-ingest-overview/media-services-image2.png\n[image3]: ./media/media-services-fmp4-live-ingest-overview/media-services-image3.png\n[image4]: ./media/media-services-fmp4-live-ingest-overview/media-services-image4.png\n[image5]: ./media/media-services-fmp4-live-ingest-overview/media-services-image5.png\n[image6]: ./media/media-services-fmp4-live-ingest-overview/media-services-image6.png\n[image7]: ./media/media-services-fmp4-live-ingest-overview/media-services-image7.png\n\n \ntest\n"
}