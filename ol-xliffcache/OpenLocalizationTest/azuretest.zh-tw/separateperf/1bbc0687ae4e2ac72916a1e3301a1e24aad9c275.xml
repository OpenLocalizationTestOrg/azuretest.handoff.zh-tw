{
  "nodes": [
    {
      "content": "Tips for using Hadoop on Linux-based HDInsight | Microsoft Azure",
      "pos": [
        26,
        90
      ]
    },
    {
      "content": "Get implementation tips for using Linux-based HDInsight (Hadoop) clusters on a familiar Linux environment running in the Azure cloud.",
      "pos": [
        108,
        241
      ]
    },
    {
      "content": "Information about using HDInsight on Linux (preview)",
      "pos": [
        559,
        611
      ]
    },
    {
      "content": "Linux-based Azure HDInsight clusters provide Hadoop on a familiar Linux environment, running in the Azure cloud.",
      "pos": [
        613,
        725
      ]
    },
    {
      "content": "For most things, it should work exactly as any other Hadoop-on-Linux installation.",
      "pos": [
        726,
        808
      ]
    },
    {
      "content": "This document calls out specific differences that you should be aware of.",
      "pos": [
        809,
        882
      ]
    },
    {
      "content": "Domain names",
      "pos": [
        887,
        899
      ]
    },
    {
      "pos": [
        901,
        1083
      ],
      "content": "The fully qualified domain name (FQDN) to use when connecting to the cluster is <bpt id=\"p1\">**</bpt>&amp;lt;clustername&gt;.azurehdinsight.net<ept id=\"p1\">**</ept> or (for SSH only) <bpt id=\"p2\">**</bpt>&amp;lt;clustername-ssh&gt;.azurehdinsight.net<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Remote access to services",
      "pos": [
        1088,
        1113
      ]
    },
    {
      "pos": [
        1117,
        1179
      ],
      "content": "<bpt id=\"p1\">**</bpt>Ambari (web)<ept id=\"p1\">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net"
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password, and then log in to Ambari.",
      "pos": [
        1187,
        1293
      ]
    },
    {
      "content": "This also uses the cluster administrator user and password.",
      "pos": [
        1294,
        1353
      ]
    },
    {
      "content": "Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.",
      "pos": [
        1366,
        1458
      ]
    },
    {
      "content": "While Ambari for your cluster is accessible directly over the Internet, some functionality relies on accessing nodes by the internal domain name used by the cluster.",
      "pos": [
        1464,
        1629
      ]
    },
    {
      "content": "Since this is an internal domain name, and not public, you will receive \"server not found\" errors when trying to access some features over the Internet.",
      "pos": [
        1630,
        1782
      ]
    },
    {
      "content": "To work around this problem, use an SSH tunnel to proxy web traffic to the cluster head node.",
      "pos": [
        1788,
        1881
      ]
    },
    {
      "content": "Use the <bpt id=\"p1\">**</bpt>SSH tunneling<ept id=\"p1\">**</ept> section of the following articles to create an SSH tunnel from a port on your local machine to the cluster:",
      "pos": [
        1882,
        2015
      ]
    },
    {
      "pos": [
        2023,
        2196
      ],
      "content": "<bpt id=\"p1\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>: Steps on creating an SSH tunnel by using the <ph id=\"ph1\">`ssh`</ph> command."
    },
    {
      "pos": [
        2204,
        2350
      ],
      "content": "<bpt id=\"p1\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-windows)</ept>: Steps on using PuTTY to create an SSH tunnel."
    },
    {
      "pos": [
        2354,
        2424
      ],
      "content": "<bpt id=\"p1\">**</bpt>Ambari (REST)<ept id=\"p1\">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/ambari"
    },
    {
      "pos": [
        2432,
        2511
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password."
    },
    {
      "content": "Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.",
      "pos": [
        2524,
        2616
      ]
    },
    {
      "pos": [
        2620,
        2699
      ],
      "content": "<bpt id=\"p1\">**</bpt>WebHCat (Templeton)<ept id=\"p1\">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/templeton"
    },
    {
      "pos": [
        2707,
        2786
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password."
    },
    {
      "content": "Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.",
      "pos": [
        2799,
        2891
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>SSH<ept id=\"p1\">**</ept> - &amp;lt;clustername&gt;-ssh.azurehdinsight.net on port 22 or 23.",
      "pos": [
        2895,
        2962
      ]
    },
    {
      "content": "Port 22 is used to connect to headnode0, while 23 is used to connect to headnode1.",
      "pos": [
        2963,
        3045
      ]
    },
    {
      "content": "For more information on the head nodes, see <bpt id=\"p1\">[</bpt>Availability and reliability of Hadoop clusters in HDInsight<ept id=\"p1\">](hdinsight-high-availability-linux.md)</ept>.",
      "pos": [
        3046,
        3191
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You can only access the cluster head nodes through SSH from a client machine.",
      "pos": [
        3199,
        3289
      ]
    },
    {
      "content": "Once connected, you can then access the worker nodes by using SSH from the head node.",
      "pos": [
        3290,
        3375
      ]
    },
    {
      "content": "File locations",
      "pos": [
        3380,
        3394
      ]
    },
    {
      "content": "Hadoop-related files can be found on the cluster nodes at <ph id=\"ph1\">`/usr/hdp`</ph>.",
      "pos": [
        3396,
        3465
      ]
    },
    {
      "content": "This directory contains the following subdirectories:",
      "pos": [
        3466,
        3519
      ]
    },
    {
      "pos": [
        3523,
        3702
      ],
      "content": "<bpt id=\"p1\">__</bpt>2.2.4.9-1<ept id=\"p1\">__</ept>: This directory is named for the version of the Hortonworks Data Platform used by HDInsight, so the number on your cluster may be different than the one listed here."
    },
    {
      "pos": [
        3705,
        3915
      ],
      "content": "<bpt id=\"p1\">__</bpt>current<ept id=\"p1\">__</ept>: This directory contains links to directories under the <bpt id=\"p2\">__</bpt>2.2.4.9-1<ept id=\"p2\">__</ept> directory, and exists so that you don't have to type a version number (that might change,) every time you want to access a file."
    },
    {
      "content": "Example data and JAR files can be found on Hadoop Distributed File System (HDFS) or Azure Blob storage at '/example' or 'wasb:///example'.",
      "pos": [
        3917,
        4055
      ]
    },
    {
      "content": "HDFS, Azure Blob storage, and storage best practices",
      "pos": [
        4060,
        4112
      ]
    },
    {
      "content": "In most Hadoop distributions, HDFS is backed by local storage on the machines in the cluster.",
      "pos": [
        4114,
        4207
      ]
    },
    {
      "content": "While this is efficient, it can be costly for a cloud-based solution where you are charged hourly for compute resources.",
      "pos": [
        4208,
        4328
      ]
    },
    {
      "content": "HDInsight uses Azure Blob storage as the default store, which provides the following benefits:",
      "pos": [
        4330,
        4424
      ]
    },
    {
      "content": "Cheap long-term storage",
      "pos": [
        4428,
        4451
      ]
    },
    {
      "content": "Accessibility from external services such as websites, file upload/download utilities, various language SDKs, and web browsers",
      "pos": [
        4455,
        4581
      ]
    },
    {
      "content": "Since it is the default store for HDInsight, you normally don't have to do anything to use it.",
      "pos": [
        4583,
        4677
      ]
    },
    {
      "content": "For example, the following command will list files in the <bpt id=\"p1\">**</bpt>/example/data<ept id=\"p1\">**</ept> folder, which is stored on Azure Blob storage:",
      "pos": [
        4678,
        4800
      ]
    },
    {
      "content": "Some commands may require you to specify that you are using Blob storage.",
      "pos": [
        4835,
        4908
      ]
    },
    {
      "content": "For these, you can prefix the command with <bpt id=\"p1\">**</bpt>WASB://<ept id=\"p1\">**</ept>.",
      "pos": [
        4909,
        4964
      ]
    },
    {
      "content": "HDInsight also allows you to associate multiple Blob storage accounts with a cluster.",
      "pos": [
        4966,
        5051
      ]
    },
    {
      "content": "To access data on a non-default Blob storage account, you can use the format <bpt id=\"p1\">**</bpt>WASB://&amp;lt;container-name&gt;@&amp;lt;account-name&gt;.blob.core.windows.net/<ept id=\"p1\">**</ept>.",
      "pos": [
        5052,
        5201
      ]
    },
    {
      "content": "For example, the following will list the contents of the <bpt id=\"p1\">**</bpt>/example/data<ept id=\"p1\">**</ept> directory for the specified container and Blob storage account:",
      "pos": [
        5202,
        5340
      ]
    },
    {
      "content": "What Blob storage is the cluster using?",
      "pos": [
        5429,
        5468
      ]
    },
    {
      "content": "During cluster creation, you selected to either use an existing Azure Storage account and container, or create a new one.",
      "pos": [
        5470,
        5591
      ]
    },
    {
      "content": "Then, you probably forgot about it.",
      "pos": [
        5592,
        5627
      ]
    },
    {
      "content": "You can find the default storage account and container by using the Ambari REST API.",
      "pos": [
        5628,
        5712
      ]
    },
    {
      "content": "Use the following command to retrieve HDFS configuration information:",
      "pos": [
        5717,
        5786
      ]
    },
    {
      "content": "In the JSON data returned, find the <ph id=\"ph1\">`fs.defaultFS`</ph> entry.",
      "pos": [
        5977,
        6034
      ]
    },
    {
      "content": "This will contain default container and storage account name in a format similar to the following:",
      "pos": [
        6035,
        6133
      ]
    },
    {
      "pos": [
        6213,
        6350
      ],
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> If you have installed <bpt id=\"p1\">[</bpt>jq<ept id=\"p1\">](http://stedolan.github.io/jq/)</ept>, you can use the following to return just the <ph id=\"ph2\">`fs.defaultFS`</ph> entry:"
    },
    {
      "content": "To find the key used to authenticate to the storage account, or to find any secondary storage accounts associated with the cluster, use the following:",
      "pos": [
        6630,
        6780
      ]
    },
    {
      "content": "In the JSON data returned, find the entries that begin with <ph id=\"ph1\">`fs.azure.account.key`</ph>.",
      "pos": [
        6979,
        7062
      ]
    },
    {
      "content": "The remainder of the entry name is the storage account name.",
      "pos": [
        7063,
        7123
      ]
    },
    {
      "content": "For example, <ph id=\"ph1\">`fs.azure.account.key.mystorage.blob.core.windows.net`</ph>.",
      "pos": [
        7124,
        7192
      ]
    },
    {
      "content": "The value stored in this entry is the key used to authenticate to the storage account.",
      "pos": [
        7193,
        7279
      ]
    },
    {
      "pos": [
        7287,
        7424
      ],
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> If you have installed <bpt id=\"p1\">[</bpt>jq<ept id=\"p1\">](http://stedolan.github.io/jq/)</ept>, you can use the following to return a list of the keys and values:"
    },
    {
      "content": "How do I access Blob storage?",
      "pos": [
        7847,
        7876
      ]
    },
    {
      "content": "Other than through the Hadoop command from the cluster, there are a variety of ways to access blobs:",
      "pos": [
        7878,
        7978
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Azure CLI for Mac, Linux and Windows<ept id=\"p1\">](../xplat-cli.md)</ept>: Command-Line interface commands for working with Azure.",
      "pos": [
        7982,
        8094
      ]
    },
    {
      "content": "After installing, use the <ph id=\"ph1\">`azure storage`</ph> command for help on using storage, or <ph id=\"ph2\">`azure blob`</ph> for blob-specific commands.",
      "pos": [
        8095,
        8215
      ]
    },
    {
      "pos": [
        8219,
        8363
      ],
      "content": "<bpt id=\"p1\">[</bpt>blobxfer.py<ept id=\"p1\">](https://github.com/Azure/azure-batch-samples/tree/master/Python/Storage)</ept>: A python script for working with blobs in Azure Storage."
    },
    {
      "content": "A variety of SDKs:",
      "pos": [
        8367,
        8385
      ]
    },
    {
      "content": "Java",
      "pos": [
        8394,
        8398
      ]
    },
    {
      "content": "Node.js",
      "pos": [
        8453,
        8460
      ]
    },
    {
      "content": "PHP",
      "pos": [
        8515,
        8518
      ]
    },
    {
      "content": "Python",
      "pos": [
        8572,
        8578
      ]
    },
    {
      "content": "Ruby",
      "pos": [
        8635,
        8639
      ]
    },
    {
      "content": ".NET",
      "pos": [
        8694,
        8698
      ]
    },
    {
      "content": "Storage REST API",
      "pos": [
        8748,
        8764
      ]
    },
    {
      "content": "Next steps",
      "pos": [
        8826,
        8836
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        8841,
        8864
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        8892,
        8914
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Use MapReduce jobs with HDInsight<ept id=\"p1\">](hdinsight-use-mapreduce.md)</ept>",
      "pos": [
        8940,
        9003
      ]
    },
    {
      "content": "test",
      "pos": [
        9004,
        9008
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Tips for using Hadoop on Linux-based HDInsight | Microsoft Azure\"\n   description=\"Get implementation tips for using Linux-based HDInsight (Hadoop) clusters on a familiar Linux environment running in the Azure cloud.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/24/2015\"\n   ms.author=\"larryfr\"/>\n\n# Information about using HDInsight on Linux (preview)\n\nLinux-based Azure HDInsight clusters provide Hadoop on a familiar Linux environment, running in the Azure cloud. For most things, it should work exactly as any other Hadoop-on-Linux installation. This document calls out specific differences that you should be aware of.\n\n## Domain names\n\nThe fully qualified domain name (FQDN) to use when connecting to the cluster is **&lt;clustername>.azurehdinsight.net** or (for SSH only) **&lt;clustername-ssh>.azurehdinsight.net**.\n\n## Remote access to services\n\n* **Ambari (web)** - https://&lt;clustername>.azurehdinsight.net\n\n    > [AZURE.NOTE] Authenticate by using the cluster administrator user and password, and then log in to Ambari. This also uses the cluster administrator user and password.\n    >\n    > Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.\n\n    While Ambari for your cluster is accessible directly over the Internet, some functionality relies on accessing nodes by the internal domain name used by the cluster. Since this is an internal domain name, and not public, you will receive \"server not found\" errors when trying to access some features over the Internet.\n\n    To work around this problem, use an SSH tunnel to proxy web traffic to the cluster head node. Use the **SSH tunneling** section of the following articles to create an SSH tunnel from a port on your local machine to the cluster:\n\n    * [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md): Steps on creating an SSH tunnel by using the `ssh` command.\n\n    * [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows): Steps on using PuTTY to create an SSH tunnel.\n\n* **Ambari (REST)** - https://&lt;clustername>.azurehdinsight.net/ambari\n\n    > [AZURE.NOTE] Authenticate by using the cluster administrator user and password.\n    >\n    > Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.\n\n* **WebHCat (Templeton)** - https://&lt;clustername>.azurehdinsight.net/templeton\n\n    > [AZURE.NOTE] Authenticate by using the cluster administrator user and password.\n    >\n    > Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.\n\n* **SSH** - &lt;clustername>-ssh.azurehdinsight.net on port 22 or 23. Port 22 is used to connect to headnode0, while 23 is used to connect to headnode1. For more information on the head nodes, see [Availability and reliability of Hadoop clusters in HDInsight](hdinsight-high-availability-linux.md).\n\n    > [AZURE.NOTE] You can only access the cluster head nodes through SSH from a client machine. Once connected, you can then access the worker nodes by using SSH from the head node.\n\n## File locations\n\nHadoop-related files can be found on the cluster nodes at `/usr/hdp`. This directory contains the following subdirectories:\n\n* __2.2.4.9-1__: This directory is named for the version of the Hortonworks Data Platform used by HDInsight, so the number on your cluster may be different than the one listed here.\n* __current__: This directory contains links to directories under the __2.2.4.9-1__ directory, and exists so that you don't have to type a version number (that might change,) every time you want to access a file.\n\nExample data and JAR files can be found on Hadoop Distributed File System (HDFS) or Azure Blob storage at '/example' or 'wasb:///example'.\n\n## HDFS, Azure Blob storage, and storage best practices\n\nIn most Hadoop distributions, HDFS is backed by local storage on the machines in the cluster. While this is efficient, it can be costly for a cloud-based solution where you are charged hourly for compute resources.\n\nHDInsight uses Azure Blob storage as the default store, which provides the following benefits:\n\n* Cheap long-term storage\n\n* Accessibility from external services such as websites, file upload/download utilities, various language SDKs, and web browsers\n\nSince it is the default store for HDInsight, you normally don't have to do anything to use it. For example, the following command will list files in the **/example/data** folder, which is stored on Azure Blob storage:\n\n    hadoop fs -ls /example/data\n\nSome commands may require you to specify that you are using Blob storage. For these, you can prefix the command with **WASB://**.\n\nHDInsight also allows you to associate multiple Blob storage accounts with a cluster. To access data on a non-default Blob storage account, you can use the format **WASB://&lt;container-name>@&lt;account-name>.blob.core.windows.net/**. For example, the following will list the contents of the **/example/data** directory for the specified container and Blob storage account:\n\n    hadoop fs -ls wasb://mycontainer@mystorage.blob.core.windows.net/example/data\n\n### What Blob storage is the cluster using?\n\nDuring cluster creation, you selected to either use an existing Azure Storage account and container, or create a new one. Then, you probably forgot about it. You can find the default storage account and container by using the Ambari REST API.\n\n1. Use the following command to retrieve HDFS configuration information:\n\n        curl -u admin:PASSWORD -G \"https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/configurations/service_config_versions?service_name=HDFS&service_config_version=1\"\n\n2. In the JSON data returned, find the `fs.defaultFS` entry. This will contain default container and storage account name in a format similar to the following:\n\n        wasb://CONTAINTERNAME@STORAGEACCOUNTNAME.blob.core.windows.net\n\n    > [AZURE.TIP] If you have installed [jq](http://stedolan.github.io/jq/), you can use the following to return just the `fs.defaultFS` entry:\n    >\n    > `curl -u admin:PASSWORD -G \"https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/configurations/service_config_versions?service_name=HDFS&service_config_version=1\" | jq '.items[].configurations[].properties[\"fs.defaultFS\"] | select(. != null)'`\n    \n3. To find the key used to authenticate to the storage account, or to find any secondary storage accounts associated with the cluster, use the following:\n\n        curl -u admin:PASSWORD -G \"https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/configurations/service_config_versions?service_name=HDFS&service_config_version=1\"\n        \n4. In the JSON data returned, find the entries that begin with `fs.azure.account.key`. The remainder of the entry name is the storage account name. For example, `fs.azure.account.key.mystorage.blob.core.windows.net`. The value stored in this entry is the key used to authenticate to the storage account.\n\n    > [AZURE.TIP] If you have installed [jq](http://stedolan.github.io/jq/), you can use the following to return a list of the keys and values:\n    >\n    > `curl -u admin:PASSWORD -G \"https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/configurations/service_config_versions?service_name=HDFS&service_config_version=1\" | jq '.items[].configurations[].properties as $in | $in | keys[] | select(. | contains(\"fs.azure.account.key.\")) as $item | $item | ltrimstr(\"fs.azure.account.key.\") | { storage_account: ., storage_account_key: $in[$item] }'`\n\n\n### How do I access Blob storage?\n\nOther than through the Hadoop command from the cluster, there are a variety of ways to access blobs:\n\n* [Azure CLI for Mac, Linux and Windows](../xplat-cli.md): Command-Line interface commands for working with Azure. After installing, use the `azure storage` command for help on using storage, or `azure blob` for blob-specific commands.\n\n* [blobxfer.py](https://github.com/Azure/azure-batch-samples/tree/master/Python/Storage): A python script for working with blobs in Azure Storage.\n\n* A variety of SDKs:\n\n    * [Java](https://github.com/Azure/azure-sdk-for-java)\n\n    * [Node.js](https://github.com/Azure/azure-sdk-for-node)\n\n    * [PHP](https://github.com/Azure/azure-sdk-for-php)\n\n    * [Python](https://github.com/Azure/azure-sdk-for-python)\n\n    * [Ruby](https://github.com/Azure/azure-sdk-for-ruby)\n\n    * [.NET](https://github.com/Azure/azure-sdk-for-net)\n\n* [Storage REST API](https://msdn.microsoft.com/library/azure/dd135733.aspx)\n\n## Next steps\n\n* [Use Hive with HDInsight](hdinsight-use-hive.md)\n* [Use Pig with HDInsight](hdinsight-use-pig.md)\n* [Use MapReduce jobs with HDInsight](hdinsight-use-mapreduce.md)\ntest\n"
}