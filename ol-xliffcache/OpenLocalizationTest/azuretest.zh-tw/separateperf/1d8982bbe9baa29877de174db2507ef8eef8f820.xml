{
  "nodes": [
    {
      "content": "Build your first Azure Data Factory pipeline using Azure PowerShell",
      "pos": [
        27,
        94
      ]
    },
    {
      "content": "In this tutorial, you will create a sample Azure Data Factory pipeline using Azure PowerShell.",
      "pos": [
        113,
        207
      ]
    },
    {
      "content": "Build your first Azure Data Factory pipeline using Azure PowerShell",
      "pos": [
        523,
        590
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        593,
        609
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        613,
        630
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        678,
        703
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        764,
        780
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        845,
        864
      ]
    },
    {
      "content": "In this article, you will learn how to use Azure PowerShell to create your first pipeline.",
      "pos": [
        920,
        1010
      ]
    },
    {
      "content": "This tutorial consists of the following steps:",
      "pos": [
        1011,
        1057
      ]
    },
    {
      "content": "Creating the data factory.",
      "pos": [
        1063,
        1089
      ]
    },
    {
      "content": "Creating the linked services (data stores, computes) and datasets.",
      "pos": [
        1094,
        1160
      ]
    },
    {
      "content": "Creating the pipeline.",
      "pos": [
        1165,
        1187
      ]
    },
    {
      "content": "This article does not provide a conceptual overview of the Azure Data Factory service.",
      "pos": [
        1189,
        1275
      ]
    },
    {
      "content": "For a detailed overview of the service, see the <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">](data-factory-introduction.md)</ept> article.",
      "pos": [
        1276,
        1399
      ]
    },
    {
      "content": "Step 1: Creating the data factory",
      "pos": [
        1404,
        1437
      ]
    },
    {
      "content": "In this step, you use Azure PowerShell to create an Azure Data Factory named ADFTutorialDataFactoryPSH.",
      "pos": [
        1439,
        1542
      ]
    },
    {
      "content": "Start Azure PowerShell and run the following commands.",
      "pos": [
        1547,
        1601
      ]
    },
    {
      "content": "Keep Azure PowerShell open until the end of this tutorial.",
      "pos": [
        1602,
        1660
      ]
    },
    {
      "content": "If you close and reopen, you need to run these commands again.",
      "pos": [
        1661,
        1723
      ]
    },
    {
      "pos": [
        1730,
        1845
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Add-AzureAccount<ept id=\"p1\">**</ept> and enter the  user name and password that you use to sign in to the Azure preview portal."
    },
    {
      "pos": [
        1854,
        1931
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureSubscription<ept id=\"p1\">**</ept> to view all the subscriptions for this account."
    },
    {
      "content": "Run <bpt id=\"p1\">**</bpt>Select-AzureSubscription<ept id=\"p1\">**</ept> to select the subscription that you want to work with.",
      "pos": [
        1938,
        2025
      ]
    },
    {
      "content": "This subscription should be the same as the one you used in the preview portal.",
      "pos": [
        2026,
        2105
      ]
    },
    {
      "content": "Switch to AzureResourceManager mode as the Azure Data Factory cmdlets are available in this mode.",
      "pos": [
        2109,
        2206
      ]
    },
    {
      "pos": [
        2257,
        2354
      ],
      "content": "Create an Azure resource group named <bpt id=\"p1\">*</bpt>ADFTutorialResourceGroup<ept id=\"p1\">*</ept> by running the following command."
    },
    {
      "content": "Some of the steps in this tutorial assume that you use the resource group named ADFTutorialResourceGroup.",
      "pos": [
        2444,
        2549
      ]
    },
    {
      "content": "If you use a different resource group, you will need to use it in place of ADFTutorialResourceGroup in this tutorial.",
      "pos": [
        2550,
        2667
      ]
    },
    {
      "pos": [
        2671,
        2772
      ],
      "content": "Run the <bpt id=\"p1\">**</bpt>New-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet to create a data factory named DataFactoryMyFirstPipelinePSH."
    },
    {
      "content": "The name of the Azure Data Factory must be globally unique.",
      "pos": [
        2910,
        2969
      ]
    },
    {
      "content": "If you receive the error <bpt id=\"p1\">**</bpt>Data factory name “DataFactoryMyFirstPipelinePSH” is not available<ept id=\"p1\">**</ept>, change the name (for example, yournameADFTutorialDataFactoryPSH).",
      "pos": [
        2970,
        3132
      ]
    },
    {
      "content": "Use this name in place of ADFTutorialFactoryPSH while performing steps in this tutorial.",
      "pos": [
        3133,
        3221
      ]
    },
    {
      "content": "In the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial.",
      "pos": [
        3223,
        3355
      ]
    },
    {
      "content": "Step 2: Create linked services and datasets",
      "pos": [
        3360,
        3403
      ]
    },
    {
      "content": "In this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then create a dataset to represent the output data from Hive processing.",
      "pos": [
        3404,
        3597
      ]
    },
    {
      "content": "Create Azure Storage linked service",
      "pos": [
        3603,
        3638
      ]
    },
    {
      "content": "Create a JSON file named StorageLinkedService.json in the C:\\ADFGetStartedPSH folder with the following content.",
      "pos": [
        3643,
        3755
      ]
    },
    {
      "content": "Create the folder ADFGetStartedPSH if it does not already exist.",
      "pos": [
        3756,
        3820
      ]
    },
    {
      "content": "Replace <bpt id=\"p1\">**</bpt>account name<ept id=\"p1\">**</ept> with the name of your Azure storage account and <bpt id=\"p2\">**</bpt>account key<ept id=\"p2\">**</ept> with the access key of the Azure storage account.",
      "pos": [
        4185,
        4323
      ]
    },
    {
      "content": "To learn how to get your storage access key, see <bpt id=\"p1\">[</bpt>View, copy and regenerate storage access keys<ept id=\"p1\">](http://azure.microsoft.com/documentation/articles/storage-create-storage-account/#view-copy-and-regenerate-storage-access-keys)</ept>.",
      "pos": [
        4324,
        4549
      ]
    },
    {
      "content": "In Azure PowerShell, switch to the ADFGetStartedPSH folder.",
      "pos": [
        4555,
        4614
      ]
    },
    {
      "content": "You can use the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">**</ept> cmdlet to create a linked service.",
      "pos": [
        4619,
        4707
      ]
    },
    {
      "content": "This cmdlet and other Data Factory cmdlets you use in this tutorial require you to pass values for the <bpt id=\"p1\">*</bpt>ResourceGroupName<ept id=\"p1\">*</ept> and <bpt id=\"p2\">*</bpt>DataFactoryName<ept id=\"p2\">*</ept> parameters.",
      "pos": [
        4708,
        4864
      ]
    },
    {
      "content": "Alternatively, you can use <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> to get a <bpt id=\"p2\">**</bpt>DataFactory<ept id=\"p2\">**</ept> object and pass the object without typing <bpt id=\"p3\">*</bpt>ResourceGroupName<ept id=\"p3\">*</ept> and <bpt id=\"p4\">*</bpt>DataFactoryName<ept id=\"p4\">*</ept> each time you run a cmdlet.",
      "pos": [
        4865,
        5053
      ]
    },
    {
      "content": "Run the following command to assign the output of the <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet to a <bpt id=\"p2\">**</bpt>$df<ept id=\"p2\">**</ept> variable.",
      "pos": [
        5054,
        5162
      ]
    },
    {
      "pos": [
        5282,
        5394
      ],
      "content": "Now, run the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">**</ept> cmdlet to create the linked <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> service."
    },
    {
      "pos": [
        5481,
        5689
      ],
      "content": "If you hadn't run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet and assigned the output to the <bpt id=\"p2\">**</bpt>$df<ept id=\"p2\">**</ept> variable, you would have to specify values for the <bpt id=\"p3\">*</bpt>ResourceGroupName<ept id=\"p3\">*</ept> and <bpt id=\"p4\">*</bpt>DataFactoryName<ept id=\"p4\">*</ept> parameters as follows."
    },
    {
      "pos": [
        5859,
        6036
      ],
      "content": "If you close Azure PowerShell in the middle of the tutorial, you will have run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> cmdlet next time you start Azure PowerShell to complete the tutorial."
    },
    {
      "content": "Create Azure HDInsight linked service",
      "pos": [
        6042,
        6079
      ]
    },
    {
      "content": "Now, you will create a linked service for an on-demand Azure HDInsight cluster that will be used to run the Hive script.",
      "pos": [
        6080,
        6200
      ]
    },
    {
      "content": "Create a JSON file named HDInsightOnDemandLinkedService.json in the C:\\ADFGetStartedPSH folder with the following content.",
      "pos": [
        6206,
        6328
      ]
    },
    {
      "pos": [
        7388,
        7508
      ],
      "content": "Run the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">**</ept> cmdlet to create the linked service called HDInsightOnDemandLinkedService."
    },
    {
      "content": "Create the output dataset",
      "pos": [
        7606,
        7631
      ]
    },
    {
      "content": "Now, you will create the output dataset to represent the data stored in the Azure Blob storage.",
      "pos": [
        7632,
        7727
      ]
    },
    {
      "content": "Create a JSON file named OutputTable.json in the C:\\ADFGetStartedPSH folder with the following content:",
      "pos": [
        7733,
        7836
      ]
    },
    {
      "content": "In the previous example, you are creating a dataset called <bpt id=\"p1\">**</bpt>AzureBlobOutput<ept id=\"p1\">**</ept>, and specifying the structure of the data that will be produced by the Hive script.",
      "pos": [
        8354,
        8516
      ]
    },
    {
      "content": "In addition, you specify that the results are stored in the blob container called <bpt id=\"p1\">**</bpt>data<ept id=\"p1\">**</ept> and the folder called <bpt id=\"p2\">**</bpt>partitioneddata<ept id=\"p2\">**</ept>.",
      "pos": [
        8517,
        8650
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> section specifies that the output dataset is produced on a monthly basis.",
      "pos": [
        8651,
        8745
      ]
    },
    {
      "content": "Run the following command in Azure PowerShell to create the Data Factory table.",
      "pos": [
        8750,
        8829
      ]
    },
    {
      "content": "Step 3: Creating your first pipeline",
      "pos": [
        8898,
        8934
      ]
    },
    {
      "content": "In this step, you will create your first pipeline.",
      "pos": [
        8935,
        8985
      ]
    },
    {
      "content": "Create a JSON file named MyFirstPipelinePSH.json in the C:\\ADFGetStartedPSH folder with the following content:",
      "pos": [
        8991,
        9101
      ]
    },
    {
      "pos": [
        9109,
        9209
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Replace <bpt id=\"p1\">**</bpt>storageaccountname<ept id=\"p1\">**</ept> with the name of your storage account in the  JSON."
    },
    {
      "content": "In the previous example, you are creating a pipeline that consists of a single activity that uses Hive to process data on an HDInsight cluster.",
      "pos": [
        10273,
        10416
      ]
    },
    {
      "pos": [
        10422,
        10611
      ],
      "content": "The Hive script file, partitionweblogs.hql, is stored in the Azure storage account (specified by the scriptLinkedService, called StorageLinkedService), and in a container called <bpt id=\"p1\">**</bpt>script<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        10617,
        10803
      ],
      "content": "The <bpt id=\"p1\">**</bpt>extendedProperties<ept id=\"p1\">**</ept> section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (for example, ${hiveconf:PartitionedData})."
    },
    {
      "pos": [
        10809,
        10906
      ],
      "content": "The <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> properties of the pipeline specifies the active period of the pipeline."
    },
    {
      "pos": [
        10912,
        11057
      ],
      "content": "In the activity JSON, you specify that the Hive script runs on the computer specified by the linked service – <bpt id=\"p1\">**</bpt>HDInsightOnDemandLinkedService<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Run the following command to create the Data Factory table.",
      "pos": [
        11061,
        11120
      ]
    },
    {
      "content": "Congratulations, you have successfully created your first pipeline using Azure PowerShell!",
      "pos": [
        11198,
        11288
      ]
    },
    {
      "pos": [
        11294,
        11369
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"MonitorDataSetsAndPipeline\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph> Monitor the datasets and pipeline"
    },
    {
      "content": "In this step, you will use Azure PowerShell to monitor what’s going on in an Azure data factory.",
      "pos": [
        11370,
        11466
      ]
    },
    {
      "pos": [
        11472,
        11545
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactory<ept id=\"p1\">**</ept> and assign the output to a <bpt id=\"p2\">**</bpt>$df<ept id=\"p2\">**</ept> variable."
    },
    {
      "pos": [
        11665,
        11797
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactorySlice<ept id=\"p1\">**</ept> to get details about all slices of the <bpt id=\"p2\">**</bpt>EmpSQLTable<ept id=\"p2\">**</ept>, which is the output table of the pipeline."
    },
    {
      "content": "Notice that the StartDateTime you specify here is the same start time specified in the pipeline JSON.",
      "pos": [
        11897,
        11998
      ]
    },
    {
      "content": "You should see output similar to the following.",
      "pos": [
        11999,
        12046
      ]
    },
    {
      "pos": [
        12433,
        12522
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryRun<ept id=\"p1\">**</ept> to get the details of activity runs for a specific slice."
    },
    {
      "content": "You should see output similar to the following.",
      "pos": [
        12618,
        12665
      ]
    },
    {
      "content": "You can keep running this cmdlet until you see the slice in Ready state or Failed state.",
      "pos": [
        13521,
        13609
      ]
    },
    {
      "content": "When the slice is in Ready state, check the partitioneddata folder in the data container in your blob storage for the output data.",
      "pos": [
        13610,
        13740
      ]
    },
    {
      "content": "Note that the creation of an on-demand HDInsight cluster usually takes some time.",
      "pos": [
        13742,
        13823
      ]
    },
    {
      "pos": [
        13825,
        13973
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Cmdlet Reference<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn820234.aspx)</ept> for comprehensive documentation on Data Factory cmdlets."
    },
    {
      "content": "Next steps",
      "pos": [
        13980,
        13990
      ]
    },
    {
      "content": "In this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand Azure HDInsight cluster.",
      "pos": [
        13991,
        14152
      ]
    },
    {
      "content": "To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see <bpt id=\"p1\">[</bpt>Tutorial: Copy data from an Azure Blob to Azure SQL<ept id=\"p1\">](./data-factory-get-started.md)</ept>.",
      "pos": [
        14153,
        14322
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        14327,
        14340
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        14341,
        14398
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-powershell.md)</ept>.",
      "pos": [
        14399,
        14565
      ]
    },
    {
      "content": "test",
      "pos": [
        14568,
        14572
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Build your first Azure Data Factory pipeline using Azure PowerShell\"\n    description=\"In this tutorial, you will create a sample Azure Data Factory pipeline using Azure PowerShell.\"\n    services=\"data-factory\"\n    documentationCenter=\"\"\n    authors=\"spelluru\"\n    manager=\"jhubbard\"\n    editor=\"monicar\"/>\n\n<tags\n    ms.service=\"data-factory\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"07/27/2015\"\n    ms.author=\"spelluru\"/>\n\n# Build your first Azure Data Factory pipeline using Azure PowerShell\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-build-your-first-pipeline.md)\n- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)\n- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)\n- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)\n\n\nIn this article, you will learn how to use Azure PowerShell to create your first pipeline. This tutorial consists of the following steps:\n\n1.  Creating the data factory.\n2.  Creating the linked services (data stores, computes) and datasets.\n3.  Creating the pipeline.\n\nThis article does not provide a conceptual overview of the Azure Data Factory service. For a detailed overview of the service, see the [Introduction to Azure Data Factory](data-factory-introduction.md) article.\n\n## Step 1: Creating the data factory\n\nIn this step, you use Azure PowerShell to create an Azure Data Factory named ADFTutorialDataFactoryPSH.\n\n1. Start Azure PowerShell and run the following commands. Keep Azure PowerShell open until the end of this tutorial. If you close and reopen, you need to run these commands again.\n    - Run **Add-AzureAccount** and enter the  user name and password that you use to sign in to the Azure preview portal.  \n    - Run **Get-AzureSubscription** to view all the subscriptions for this account.\n    - Run **Select-AzureSubscription** to select the subscription that you want to work with. This subscription should be the same as the one you used in the preview portal.\n2. Switch to AzureResourceManager mode as the Azure Data Factory cmdlets are available in this mode.\n\n        Switch-AzureMode AzureResourceManager\n3. Create an Azure resource group named *ADFTutorialResourceGroup* by running the following command.\n\n        New-AzureResourceGroup -Name ADFTutorialResourceGroup  -Location \"West US\"\n\n    Some of the steps in this tutorial assume that you use the resource group named ADFTutorialResourceGroup. If you use a different resource group, you will need to use it in place of ADFTutorialResourceGroup in this tutorial.\n4. Run the **New-AzureDataFactory** cmdlet to create a data factory named DataFactoryMyFirstPipelinePSH.  \n\n        New-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name DataFactoryMyFirstPipelinePSH –Location \"West US\"\n\n    The name of the Azure Data Factory must be globally unique. If you receive the error **Data factory name “DataFactoryMyFirstPipelinePSH” is not available**, change the name (for example, yournameADFTutorialDataFactoryPSH). Use this name in place of ADFTutorialFactoryPSH while performing steps in this tutorial.\n\nIn the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial.\n\n## Step 2: Create linked services and datasets\nIn this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then create a dataset to represent the output data from Hive processing.\n\n### Create Azure Storage linked service\n1.  Create a JSON file named StorageLinkedService.json in the C:\\ADFGetStartedPSH folder with the following content. Create the folder ADFGetStartedPSH if it does not already exist.\n\n        {\n            \"name\": \"StorageLinkedService\",\n            \"properties\": {\n                \"type\": \"AzureStorage\",\n                \"description\": \"\",\n                \"typeProperties\": {\n                    \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n                }\n            }\n        }\n\n    Replace **account name** with the name of your Azure storage account and **account key** with the access key of the Azure storage account. To learn how to get your storage access key, see [View, copy and regenerate storage access keys](http://azure.microsoft.com/documentation/articles/storage-create-storage-account/#view-copy-and-regenerate-storage-access-keys).\n\n2.  In Azure PowerShell, switch to the ADFGetStartedPSH folder.\n3.  You can use the **New-AzureDataFactoryLinkedService** cmdlet to create a linked service. This cmdlet and other Data Factory cmdlets you use in this tutorial require you to pass values for the *ResourceGroupName* and *DataFactoryName* parameters. Alternatively, you can use **Get-AzureDataFactory** to get a **DataFactory** object and pass the object without typing *ResourceGroupName* and *DataFactoryName* each time you run a cmdlet. Run the following command to assign the output of the **Get-AzureDataFactory** cmdlet to a **$df** variable.\n\n        $df=Get-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name DataFactoryMyFirstPipelinePSH\n\n4.  Now, run the **New-AzureDataFactoryLinkedService** cmdlet to create the linked **StorageLinkedService** service.\n\n        New-AzureDataFactoryLinkedService $df -File .\\StorageLinkedService.json\n\n    If you hadn't run the **Get-AzureDataFactory** cmdlet and assigned the output to the **$df** variable, you would have to specify values for the *ResourceGroupName* and *DataFactoryName* parameters as follows.\n\n        New-AzureDataFactoryLinkedService -ResourceGroupName ADFTutorialResourceGroup -DataFactoryName ADFTutorialDataFactoryPSH -File .\\StorageLinkedService.json\n\n    If you close Azure PowerShell in the middle of the tutorial, you will have run the **Get-AzureDataFactory** cmdlet next time you start Azure PowerShell to complete the tutorial.\n\n### Create Azure HDInsight linked service\nNow, you will create a linked service for an on-demand Azure HDInsight cluster that will be used to run the Hive script.\n\n1.  Create a JSON file named HDInsightOnDemandLinkedService.json in the C:\\ADFGetStartedPSH folder with the following content.\n\n\n        {\n          \"name\": \"HDInsightOnDemandLinkedService\",\n          \"properties\": {\n            \"type\": \"HDInsightOnDemand\",\n            \"typeProperties\": {\n              \"version\": \"3.1\",\n              \"clusterSize\": 1,\n              \"timeToLive\": \"00:05:00\",\n              \"jobsContainer\": \"adfjobs\",\n              \"linkedServiceName\": \"StorageLinkedService\"\n            }\n          }\n        }\n\n    The following table provides descriptions for the JSON properties used in the snippet:\n\n    Property | Description\n    -------- | -----------\n    Version | This specifies that the version of the HDInsight created to be 3.1.\n    ClusterSize | This creates a one node HDInsight cluster.\n    TimeToLive | This specifies that the idle time for the HDInsight cluster, before it is deleted.\n    JobsContainer | This specifies the name of the job container that will be created to store the logs that are generated by HDInsight\n    linkedServiceName | This specifies the storage account that will be used to store the logs that are generated by HDInsight\n2. Run the **New-AzureDataFactoryLinkedService** cmdlet to create the linked service called HDInsightOnDemandLinkedService.\n\n        New-AzureDataFactoryLinkedService $df -File .\\HDInsightOnDemandLinkedService.json\n\n\n### Create the output dataset\nNow, you will create the output dataset to represent the data stored in the Azure Blob storage.\n\n1.  Create a JSON file named OutputTable.json in the C:\\ADFGetStartedPSH folder with the following content:\n\n        {\n          \"name\": \"AzureBlobOutput\",\n          \"properties\": {\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"data/partitioneddata\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"availability\": {\n              \"frequency\": \"Month\",\n              \"interval\": 1\n            }\n          }\n        }\n\n    In the previous example, you are creating a dataset called **AzureBlobOutput**, and specifying the structure of the data that will be produced by the Hive script. In addition, you specify that the results are stored in the blob container called **data** and the folder called **partitioneddata**. The **availability** section specifies that the output dataset is produced on a monthly basis.\n\n2. Run the following command in Azure PowerShell to create the Data Factory table.\n\n        New-AzureDataFactoryTable $df -File .\\OutputTable.json\n\n## Step 3: Creating your first pipeline\nIn this step, you will create your first pipeline.\n\n1.  Create a JSON file named MyFirstPipelinePSH.json in the C:\\ADFGetStartedPSH folder with the following content:\n\n    > [AZURE.IMPORTANT] Replace **storageaccountname** with the name of your storage account in the  JSON.\n\n        {\n          \"name\": \"MyFirstPipeline\",\n          \"properties\": {\n            \"description\": \"My first Azure Data Factory pipeline\",\n            \"activities\": [\n              {\n                \"type\": \"HDInsightHive\",\n                \"typeProperties\": {\n                  \"scriptPath\": \"script/partitionweblogs.hql\",\n                  \"scriptLinkedService\": \"StorageLinkedService\",\n                  \"defines\": {\n                    \"partitionedtable\": \"wasb://data@<storageaccountname>.blob.core.windows.net/partitioneddata\"\n                  }\n                },\n                \"outputs\": [\n                  {\n                    \"name\": \"AzureBlobOutput\"\n                  }\n                ],\n                \"policy\": {\n                  \"concurrency\": 1,\n                  \"retry\": 3\n                },\n                \"name\": \"RunSampleHiveActivity\",\n                \"linkedServiceName\": \"HDInsightOnDemandLinkedService\"\n              }\n            ],\n            \"start\": \"2014-01-01\",\n            \"end\": \"2014-01-02\"\n          }\n        }\n\n    In the previous example, you are creating a pipeline that consists of a single activity that uses Hive to process data on an HDInsight cluster.\n\n    The Hive script file, partitionweblogs.hql, is stored in the Azure storage account (specified by the scriptLinkedService, called StorageLinkedService), and in a container called **script**.\n\n    The **extendedProperties** section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (for example, ${hiveconf:PartitionedData}).\n\n    The **start** and **end** properties of the pipeline specifies the active period of the pipeline.\n\n    In the activity JSON, you specify that the Hive script runs on the computer specified by the linked service – **HDInsightOnDemandLinkedService**.\n2. Run the following command to create the Data Factory table.\n\n        New-AzureDataFactoryPipeline $df -File .\\MyFirstPipelinePSH.json\n5. Congratulations, you have successfully created your first pipeline using Azure PowerShell!\n\n### <a name=\"MonitorDataSetsAndPipeline\"></a> Monitor the datasets and pipeline\nIn this step, you will use Azure PowerShell to monitor what’s going on in an Azure data factory.\n\n1.  Run **Get-AzureDataFactory** and assign the output to a **$df** variable.\n\n        $df=Get-AzureDataFactory -ResourceGroupName ADFTutorialResourceGroup -Name DataFactoryMyFirstPipelinePSH\n\n2.  Run **Get-AzureDataFactorySlice** to get details about all slices of the **EmpSQLTable**, which is the output table of the pipeline.  \n\n        Get-AzureDataFactorySlice $df -TableName AzureBlobOutput -StartDateTime 2014-01-01\n\n    Notice that the StartDateTime you specify here is the same start time specified in the pipeline JSON. You should see output similar to the following.\n\n        ResourceGroupName : ADFTutorialResourceGroup\n        DataFactoryName   : DataFactoryMyFirstPipelinePSH\n        TableName         : AzureBlobOutput\n        Start             : 1/1/2014 12:00:00 AM\n        End               : 2/1/2014 12:00:00 AM\n        RetryCount        : 0\n        Status            : InProgress\n        LatencyStatus     :\n        LongRetryCount    : 0\n\n3.  Run **Get-AzureDataFactoryRun** to get the details of activity runs for a specific slice.\n\n        Get-AzureDataFactoryRun $df -TableName AzureBlobOutput -StartDateTime 2014-01-01\n\n    You should see output similar to the following.\n\n        Id                  : 4dbc6a07-537d-4005-a53e-6b9a4b844089_635241312000000000_635268096000000000_AzureBlobOutput\n        ResourceGroupName   : ADFTutorialResourceGroup\n        DataFactoryName     : DataFactoryMyFirstPipelinePSH\n        TableName           : AzureBlobOutput\n        ProcessingStartTime : 7/7/2015 1:14:18 AM\n        ProcessingEndTime   : 12/31/9999 11:59:59 PM\n        PercentComplete     : 0\n        DataSliceStart      : 1/1/2014 12:00:00 AM\n        DataSliceEnd        : 2/1/2014 12:00:00 AM\n        Status              : AllocatingResources\n        Timestamp           : 7/7/2015 1:14:18 AM\n        RetryAttempt        : 0\n        Properties          : {}\n        ErrorMessage        :\n        ActivityName        : RunSampleHiveActivity\n        PipelineName        : MyFirstPipeline\n        Type                : Script\n\n    You can keep running this cmdlet until you see the slice in Ready state or Failed state. When the slice is in Ready state, check the partitioneddata folder in the data container in your blob storage for the output data.  Note that the creation of an on-demand HDInsight cluster usually takes some time.\n\nSee [Data Factory Cmdlet Reference](https://msdn.microsoft.com/library/azure/dn820234.aspx) for comprehensive documentation on Data Factory cmdlets.\n\n\n\n## Next steps\nIn this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand Azure HDInsight cluster. To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see [Tutorial: Copy data from an Azure Blob to Azure SQL](./data-factory-get-started.md).\n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-powershell.md). \n\ntest\n"
}