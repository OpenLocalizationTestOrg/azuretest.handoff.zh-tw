{
  "nodes": [
    {
      "content": "Build your first Azure Data Factory pipeline using Visual Studio",
      "pos": [
        27,
        91
      ]
    },
    {
      "content": "In this tutorial, you will create a sample Azure Data Factory pipeline using Visual Studio.",
      "pos": [
        110,
        201
      ]
    },
    {
      "content": "Build your first Azure Data Factory pipeline using Visual Studio",
      "pos": [
        518,
        582
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        585,
        601
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        605,
        622
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        670,
        695
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        756,
        772
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        837,
        856
      ]
    },
    {
      "content": "In this article, you will learn how to use the Visual Studio to create your first pipeline.",
      "pos": [
        912,
        1003
      ]
    },
    {
      "content": "This tutorial consists of the following steps:",
      "pos": [
        1004,
        1050
      ]
    },
    {
      "content": "Creating the data factory",
      "pos": [
        1056,
        1081
      ]
    },
    {
      "content": "Creating the linked services (data stores, computes) and datasets",
      "pos": [
        1086,
        1151
      ]
    },
    {
      "content": "Creating the pipeline",
      "pos": [
        1156,
        1177
      ]
    },
    {
      "content": "This article does not provide a conceptual overview of the Azure Data Factory service.",
      "pos": [
        1179,
        1265
      ]
    },
    {
      "content": "For a detailed overview of the service, see the <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">](data-factory-introduction.md)</ept> article.",
      "pos": [
        1266,
        1389
      ]
    },
    {
      "content": "Step 1: Creating the data factory",
      "pos": [
        1394,
        1427
      ]
    },
    {
      "pos": [
        1433,
        1523
      ],
      "content": "After logging into the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](http://portal.azure.com/)</ept>, do the following:"
    },
    {
      "pos": [
        1532,
        1574
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept> from the bottom-left corner."
    },
    {
      "pos": [
        1584,
        1633
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Data analytics<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        1642,
        1697
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Data Factory<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>Data analytics<ept id=\"p2\">**</ept> blade."
    },
    {
      "content": "Create blade",
      "pos": [
        1709,
        1721
      ]
    },
    {
      "pos": [
        1802,
        1887
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade, enter <bpt id=\"p2\">**</bpt>DataFactoryMyFirstPipeline<ept id=\"p2\">**</ept> for the Name."
    },
    {
      "content": "New data factory blade",
      "pos": [
        1895,
        1917
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Azure Data Factory names are globally unique.",
      "pos": [
        2010,
        2073
      ]
    },
    {
      "content": "You will need to prefix the name of the data factory with your name, to enable the successful creation of the factory.",
      "pos": [
        2074,
        2192
      ]
    },
    {
      "content": "If you have not created any resource group,  you will need to create a resource group.",
      "pos": [
        2198,
        2284
      ]
    },
    {
      "content": "To do this:",
      "pos": [
        2285,
        2296
      ]
    },
    {
      "pos": [
        2305,
        2338
      ],
      "content": "Click on <bpt id=\"p1\">**</bpt>RESOURCE GROUP NAME<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        2347,
        2418
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Create a new resource group<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>Resource group<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        2427,
        2497
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>ADF<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>Name<ept id=\"p2\">**</ept> in the <bpt id=\"p3\">**</bpt>Create resource group<ept id=\"p3\">**</ept> blade."
    },
    {
      "pos": [
        2506,
        2519
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Create resource group",
      "pos": [
        2535,
        2556
      ]
    },
    {
      "content": "After you have selected the resource group, verify that you are using the correct subscription where you want the data factory to be created.",
      "pos": [
        2645,
        2786
      ]
    },
    {
      "pos": [
        2791,
        2842
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Create<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>New data factory<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        2847,
        2952
      ],
      "content": "You will see the data factory being created in the <bpt id=\"p1\">**</bpt>Startboard<ept id=\"p1\">**</ept> of the Azure Preview Portal as follows:"
    },
    {
      "content": "Creating data factory status",
      "pos": [
        2963,
        2991
      ]
    },
    {
      "content": "Congratulations!",
      "pos": [
        3085,
        3101
      ]
    },
    {
      "content": "You have successfully created your first data factory.",
      "pos": [
        3102,
        3156
      ]
    },
    {
      "content": "After the data factory has been created successfully, you will see the data factory page, which shows you the contents of the data factory.",
      "pos": [
        3157,
        3296
      ]
    },
    {
      "content": "Data Factory blade",
      "pos": [
        3306,
        3324
      ]
    },
    {
      "content": "In the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial.",
      "pos": [
        3407,
        3539
      ]
    },
    {
      "content": "Walkthrough: Create and deploy Data Factory entities using Visual Studio",
      "pos": [
        3545,
        3617
      ]
    },
    {
      "content": "Pre-requisites",
      "pos": [
        3624,
        3638
      ]
    },
    {
      "content": "You must have the following installed on your computer:",
      "pos": [
        3640,
        3695
      ]
    },
    {
      "content": "Visual Studio 2013",
      "pos": [
        3699,
        3717
      ]
    },
    {
      "content": "Download Azure SDK for Visual Studio 2013.",
      "pos": [
        3720,
        3762
      ]
    },
    {
      "content": "Navigate to <bpt id=\"p1\">[</bpt>Azure Download Page<ept id=\"p1\">](http://azure.microsoft.com/downloads/)</ept> and click <bpt id=\"p2\">**</bpt>VS 2013 install<ept id=\"p2\">**</ept> in the <bpt id=\"p3\">**</bpt>.NET<ept id=\"p3\">**</ept> section.",
      "pos": [
        3763,
        3890
      ]
    },
    {
      "content": "Create the Visual Studio project",
      "pos": [
        3897,
        3929
      ]
    },
    {
      "content": "Launch <bpt id=\"p1\">**</bpt>Visual Studio 2013<ept id=\"p1\">**</ept>.",
      "pos": [
        3934,
        3964
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>File<ept id=\"p1\">**</ept>, point to <bpt id=\"p2\">**</bpt>New<ept id=\"p2\">**</ept>, and click <bpt id=\"p3\">**</bpt>Project<ept id=\"p3\">**</ept>.",
      "pos": [
        3965,
        4021
      ]
    },
    {
      "content": "You should see the <bpt id=\"p1\">**</bpt>New Project<ept id=\"p1\">**</ept> dialog box.",
      "pos": [
        4022,
        4068
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>New Project<ept id=\"p1\">**</ept> dialog, select the <bpt id=\"p2\">**</bpt>DataFactory<ept id=\"p2\">**</ept> template, and click <bpt id=\"p3\">**</bpt>Empty Data Factory Project<ept id=\"p3\">**</ept>.",
      "pos": [
        4074,
        4183
      ]
    },
    {
      "content": "If you don't see the DataFactory template, close Visual Studio, install Azure SDK for Visual Studio 2013, and reopen Visual Studio.",
      "pos": [
        4184,
        4315
      ]
    },
    {
      "content": "New project dialog box",
      "pos": [
        4325,
        4347
      ]
    },
    {
      "pos": [
        4433,
        4531
      ],
      "content": "Enter a <bpt id=\"p1\">**</bpt>name<ept id=\"p1\">**</ept> for the project, <bpt id=\"p2\">**</bpt>location<ept id=\"p2\">**</ept>, and a name for the <bpt id=\"p3\">**</bpt>solution<ept id=\"p3\">**</ept>, and click <bpt id=\"p4\">**</bpt>OK<ept id=\"p4\">**</ept>."
    },
    {
      "content": "Solution Explorer",
      "pos": [
        4539,
        4556
      ]
    },
    {
      "content": "Create linked services",
      "pos": [
        4642,
        4664
      ]
    },
    {
      "content": "In this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then crate a dataset to represent the output data from Hive processing.",
      "pos": [
        4665,
        4857
      ]
    },
    {
      "content": "Create Azure Storage linked service",
      "pos": [
        4865,
        4900
      ]
    },
    {
      "pos": [
        4906,
        5005
      ],
      "content": "Right-click <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> in the solution explorer, point to <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>, and click <bpt id=\"p3\">**</bpt>New Item<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        5015,
        5124
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Add New Item<ept id=\"p1\">**</ept> dialog box, select <bpt id=\"p2\">**</bpt>Azure Storage Linked Service<ept id=\"p2\">**</ept> from the list, and click <bpt id=\"p3\">**</bpt>Add<ept id=\"p3\">**</ept>."
    },
    {
      "content": "New Linked Service",
      "pos": [
        5133,
        5151
      ]
    },
    {
      "pos": [
        5245,
        5344
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>accountname<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>accountkey<ept id=\"p2\">**</ept> with the name of your Azure storage account and its key."
    },
    {
      "content": "Azure Storage Linked Service",
      "pos": [
        5353,
        5381
      ]
    },
    {
      "pos": [
        5477,
        5527
      ],
      "content": "Save the <bpt id=\"p1\">**</bpt>AzureStorageLinkedService1.json<ept id=\"p1\">**</ept> file."
    },
    {
      "content": "Create Azure HDInsight linked service",
      "pos": [
        5534,
        5571
      ]
    },
    {
      "content": "Now, you will create a linked service for an on-demand HDInsight cluster that will be used to run the Hive script.",
      "pos": [
        5572,
        5686
      ]
    },
    {
      "pos": [
        5692,
        5796
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Solution Explorer<ept id=\"p1\">**</ept>, right-click <bpt id=\"p2\">**</bpt>Linked Services<ept id=\"p2\">**</ept>, point to <bpt id=\"p3\">**</bpt>Add<ept id=\"p3\">**</ept>, and click <bpt id=\"p4\">**</bpt>New Item<ept id=\"p4\">**</ept>."
    },
    {
      "pos": [
        5800,
        5865
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>HDInsight On Demand Linked Service<ept id=\"p1\">**</ept>, and click <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        5870,
        5910
      ],
      "content": "Replace the <bpt id=\"p1\">**</bpt>JSON<ept id=\"p1\">**</ept> with the following:"
    },
    {
      "content": "The following table provides descriptions for the JSON properties used in the snippet:",
      "pos": [
        6245,
        6331
      ]
    },
    {
      "content": "Property",
      "pos": [
        6341,
        6349
      ]
    },
    {
      "content": "Description",
      "pos": [
        6352,
        6363
      ]
    },
    {
      "content": "Version",
      "pos": [
        6395,
        6402
      ]
    },
    {
      "content": "This specifies that the version of the HDInsight created to be 3.1.",
      "pos": [
        6405,
        6472
      ]
    },
    {
      "content": "ClusterSize",
      "pos": [
        6478,
        6489
      ]
    },
    {
      "content": "This creates a one node HDInsight cluster.",
      "pos": [
        6492,
        6534
      ]
    },
    {
      "content": "TimeToLive",
      "pos": [
        6540,
        6550
      ]
    },
    {
      "content": "This specifies that the idle time for the HDInsight cluster, before it is deleted.",
      "pos": [
        6553,
        6635
      ]
    },
    {
      "content": "JobsContainer",
      "pos": [
        6640,
        6653
      ]
    },
    {
      "content": "This specifies the name of the job container that will be created to store the logs that are generated by HDInsight",
      "pos": [
        6656,
        6771
      ]
    },
    {
      "content": "linkedServiceName",
      "pos": [
        6776,
        6793
      ]
    },
    {
      "content": "This specifies the storage account that will be used to store the logs that are generated by HDInsight",
      "pos": [
        6796,
        6898
      ]
    },
    {
      "pos": [
        6903,
        6958
      ],
      "content": "Save the <bpt id=\"p1\">**</bpt>HDInsightOnDemandLinkedService1.json<ept id=\"p1\">**</ept> file."
    },
    {
      "content": "Create the output dataset",
      "pos": [
        6965,
        6990
      ]
    },
    {
      "content": "Now, you will create the output dataset to represent the data stored in the Azure Blob storage.",
      "pos": [
        6991,
        7086
      ]
    },
    {
      "pos": [
        7092,
        7175
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Solution Explorer<ept id=\"p1\">**</ept>, right-click point to <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>, and click <bpt id=\"p3\">**</bpt>New Item<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        7180,
        7235
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Azure Blob<ept id=\"p1\">**</ept> from the list, and click <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Replace the <bpt id=\"p1\">**</bpt>JSON<ept id=\"p1\">**</ept> in the editor with the following: In the JSON snippet, you are creating a dataset called <bpt id=\"p2\">**</bpt>AzureBlobOutput<ept id=\"p2\">**</ept>, and specifying the structure of the data that will be produced by the Hive script.",
      "pos": [
        7240,
        7453
      ]
    },
    {
      "content": "In addition, you specify that the results are stored in the blob container called <bpt id=\"p1\">**</bpt>data<ept id=\"p1\">**</ept> and the folder called <bpt id=\"p2\">**</bpt>partitioneddata<ept id=\"p2\">**</ept>.",
      "pos": [
        7454,
        7587
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> section specifies that the output dataset is produced on a monthly basis.",
      "pos": [
        7588,
        7682
      ]
    },
    {
      "pos": [
        8295,
        8337
      ],
      "content": "Save the <bpt id=\"p1\">**</bpt>AzureBlobLocation1.json<ept id=\"p1\">**</ept> file."
    },
    {
      "content": "Creating your first pipeline",
      "pos": [
        8344,
        8372
      ]
    },
    {
      "content": "In this step, you will create your first pipeline.",
      "pos": [
        8373,
        8423
      ]
    },
    {
      "pos": [
        8428,
        8526
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Solution Explorer<ept id=\"p1\">**</ept>, right-click <bpt id=\"p2\">**</bpt>Pipelines<ept id=\"p2\">**</ept>, point to <bpt id=\"p3\">**</bpt>Add<ept id=\"p3\">**</ept>, and click <bpt id=\"p4\">**</bpt>New Item.<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        8531,
        8604
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Hive Transformation Pipeline<ept id=\"p1\">**</ept> from the list, and click <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        8609,
        8730
      ],
      "content": "Replace the <bpt id=\"p1\">**</bpt>JSON<ept id=\"p1\">**</ept> with the following snippet and replace <bpt id=\"p2\">**</bpt>storageaccountname<ept id=\"p2\">**</ept> with the name of your storage account."
    },
    {
      "content": "In the JSON snippet, you are creating a pipeline that consists of a single activity that uses Hive to process Data on an HDInsight cluster.",
      "pos": [
        9950,
        10089
      ]
    },
    {
      "pos": [
        10099,
        10302
      ],
      "content": "The Hive script file, <bpt id=\"p1\">**</bpt>partitionweblogs.hql<ept id=\"p1\">**</ept>, is stored in the Azure storage account (specified by the scriptLinkedService, called <bpt id=\"p2\">**</bpt>AzureStorageLinkedService1<ept id=\"p2\">**</ept>), and in a container called <bpt id=\"p3\">**</bpt>script<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        10308,
        10485
      ],
      "content": "The <bpt id=\"p1\">**</bpt>extendedProperties<ept id=\"p1\">**</ept> section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (e.g ${hiveconf:PartitionedData})."
    },
    {
      "pos": [
        10491,
        10588
      ],
      "content": "The <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> properties of the pipeline specifies the active period of the pipeline."
    },
    {
      "pos": [
        10594,
        10738
      ],
      "content": "In the activity JSON, you specify that the Hive script runs on the compute specified by the linked service – <bpt id=\"p1\">**</bpt>HDInsightOnDemandLinkedService<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        10742,
        10779
      ],
      "content": "Save the <bpt id=\"p1\">**</bpt>HiveActivity1.json<ept id=\"p1\">**</ept> file."
    },
    {
      "content": "Add partitionweblogs.hql as a dependency",
      "pos": [
        10785,
        10825
      ]
    },
    {
      "pos": [
        10831,
        10939
      ],
      "content": "Right-click Dependencies in the <bpt id=\"p1\">**</bpt>Solution Explorer<ept id=\"p1\">**</ept> window, point to <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>, and click <bpt id=\"p3\">**</bpt>Existing Item<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        10945,
        11046
      ],
      "content": "Navigate to the <bpt id=\"p1\">**</bpt>C:\\ADFGettingStarted<ept id=\"p1\">**</ept> and select <bpt id=\"p2\">**</bpt>partitionweblogs.hql<ept id=\"p2\">**</ept> file, and click <bpt id=\"p3\">**</bpt>Add<ept id=\"p3\">**</ept>."
    },
    {
      "content": "When you publish the solution in the next step, the HQL file is uploaded to the scripts container in your blob storage.",
      "pos": [
        11049,
        11168
      ]
    },
    {
      "content": "Publish/deploy Data Factory entities",
      "pos": [
        11176,
        11212
      ]
    },
    {
      "pos": [
        11219,
        11344
      ],
      "content": "In the toolbar area, right-click and select <bpt id=\"p1\">**</bpt>Data Factory<ept id=\"p1\">**</ept> to enable the Data Factory toolbar if it is not already enabled."
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory toolbar<ept id=\"p1\">**</ept>, click the <bpt id=\"p2\">**</bpt>drop-down box<ept id=\"p2\">**</ept> to see all the data factories in your Azure subscription.",
      "pos": [
        11350,
        11468
      ]
    },
    {
      "content": "If you see the <bpt id=\"p1\">**</bpt>Sign-in to Visual Studio<ept id=\"p1\">**</ept> dialog box:",
      "pos": [
        11469,
        11524
      ]
    },
    {
      "pos": [
        11534,
        11689
      ],
      "content": "Enter the <bpt id=\"p1\">**</bpt>email account<ept id=\"p1\">**</ept> associated with the Azure subscription in which you want to create the data factory, enter <bpt id=\"p2\">**</bpt>Password<ept id=\"p2\">**</ept>, and click <bpt id=\"p3\">**</bpt>Sign-in<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Once the sign-in is successful, you should see all the data factories in the Azure subscription.",
      "pos": [
        11698,
        11794
      ]
    },
    {
      "content": "In this tutorial, you will create a new data facotry.",
      "pos": [
        11795,
        11848
      ]
    },
    {
      "pos": [
        11860,
        12017
      ],
      "content": "In the drop-down list, select <bpt id=\"p1\">**</bpt>DataFactoryMyFirstPipeline<ept id=\"p1\">**</ept>, and click <bpt id=\"p2\">**</bpt>Publish<ept id=\"p2\">**</ept> button to deploy/publish the linked services, datasets, and the pipeline."
    },
    {
      "content": "Publish button",
      "pos": [
        12029,
        12043
      ]
    },
    {
      "content": "You should see the status of publishing in the <bpt id=\"p1\">**</bpt>Data Factory Task List<ept id=\"p1\">**</ept> window that is shown in the picture above.",
      "pos": [
        12119,
        12235
      ]
    },
    {
      "content": "Confirm that publishing has succeeded.",
      "pos": [
        12236,
        12274
      ]
    },
    {
      "content": "Use Server Explorer to review Data Factory entities",
      "pos": [
        12280,
        12331
      ]
    },
    {
      "pos": [
        12336,
        12416
      ],
      "content": "In <bpt id=\"p1\">**</bpt>Visual Studio<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>View<ept id=\"p2\">**</ept> on the menu, and click <bpt id=\"p3\">**</bpt>Server Explorer<ept id=\"p3\">**</ept>."
    },
    {
      "content": "In the Server Explorer window, expand <bpt id=\"p1\">**</bpt>Azure<ept id=\"p1\">**</ept> and expand <bpt id=\"p2\">**</bpt>Data Factory<ept id=\"p2\">**</ept>.",
      "pos": [
        12420,
        12496
      ]
    },
    {
      "content": "If you see <bpt id=\"p1\">**</bpt>Sign in to Visual Studio<ept id=\"p1\">**</ept>, enter the <bpt id=\"p2\">**</bpt>account<ept id=\"p2\">**</ept> associated with your Azure subscription and click <bpt id=\"p3\">**</bpt>Continue<ept id=\"p3\">**</ept>.",
      "pos": [
        12497,
        12623
      ]
    },
    {
      "content": "Enter <bpt id=\"p1\">**</bpt>password<ept id=\"p1\">**</ept>, and click <bpt id=\"p2\">**</bpt>Sign in<ept id=\"p2\">**</ept>.",
      "pos": [
        12624,
        12666
      ]
    },
    {
      "content": "Visual Studio tries to get information about all Azure data factories in your subscription.",
      "pos": [
        12667,
        12758
      ]
    },
    {
      "content": "You will see the status of this operation in the <bpt id=\"p1\">**</bpt>Data Factory Task List<ept id=\"p1\">**</ept> window.",
      "pos": [
        12759,
        12842
      ]
    },
    {
      "content": "Server Explorer",
      "pos": [
        12850,
        12865
      ]
    },
    {
      "pos": [
        12947,
        13104
      ],
      "content": "You can right-click on a data factory, and select <bpt id=\"p1\">**</bpt>Export Data Factory to New Project<ept id=\"p1\">**</ept> to create a Visual Studio project based on an existing data factory."
    },
    {
      "content": "Export data factory",
      "pos": [
        13112,
        13131
      ]
    },
    {
      "content": "Update Data Factory tools for Visual Studio",
      "pos": [
        13223,
        13266
      ]
    },
    {
      "content": "To update Azure Data Factory tools for Visual Studio, do the following:",
      "pos": [
        13268,
        13339
      ]
    },
    {
      "pos": [
        13344,
        13410
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Tools<ept id=\"p1\">**</ept> on the menu and select <bpt id=\"p2\">**</bpt>Extensions and Updates<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13414,
        13492
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Updates<ept id=\"p1\">**</ept> in the left pane and then select <bpt id=\"p2\">**</bpt>Visual Studio Gallery<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Select <bpt id=\"p1\">**</bpt>Azure Data Factory tools for Visual Studio<ept id=\"p1\">**</ept> and click <bpt id=\"p2\">**</bpt>Update<ept id=\"p2\">**</ept>.",
      "pos": [
        13496,
        13571
      ]
    },
    {
      "content": "If you do not see this entry, you already have the latest version of the tools.",
      "pos": [
        13572,
        13651
      ]
    },
    {
      "pos": [
        13654,
        13859
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Monitor datasets and pipeline<ept id=\"p1\">](data-factory-monitor-manage-pipelines.md)</ept> for instructions on how to use the Azure Preview Portal to monitor the pipeline and datasets you have created in this tutorial."
    },
    {
      "content": "Next Steps",
      "pos": [
        13866,
        13876
      ]
    },
    {
      "content": "In this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand HDInsight cluster.",
      "pos": [
        13877,
        14032
      ]
    },
    {
      "content": "To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see <bpt id=\"p1\">[</bpt>Tutorial: Copy data from an Azure blob to Azure SQL<ept id=\"p1\">](data-factory-get-started.md)</ept>.",
      "pos": [
        14033,
        14200
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        14207,
        14220
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        14221,
        14278
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-vs.md)</ept>.",
      "pos": [
        14279,
        14437
      ]
    },
    {
      "content": "test",
      "pos": [
        14439,
        14443
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Build your first Azure Data Factory pipeline using Visual Studio\"\n    description=\"In this tutorial, you will create a sample Azure Data Factory pipeline using Visual Studio.\"\n    services=\"data-factory\"\n    documentationCenter=\"\"\n    authors=\"spelluru\"\n    manager=\"jhubbard\"\n    editor=\"monicar\"/>\n\n<tags\n    ms.service=\"data-factory\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\" \n    ms.date=\"08/18/2015\"\n    ms.author=\"spelluru\"/>\n\n# Build your first Azure Data Factory pipeline using Visual Studio\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-build-your-first-pipeline.md)\n- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)\n- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)\n- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)\n\n\nIn this article, you will learn how to use the Visual Studio to create your first pipeline. This tutorial consists of the following steps:\n\n1.  Creating the data factory\n2.  Creating the linked services (data stores, computes) and datasets\n3.  Creating the pipeline\n\nThis article does not provide a conceptual overview of the Azure Data Factory service. For a detailed overview of the service, see the [Introduction to Azure Data Factory](data-factory-introduction.md) article.\n\n## Step 1: Creating the data factory\n\n1.  After logging into the [Azure Preview Portal](http://portal.azure.com/), do the following:\n    1.  Click **NEW** from the bottom-left corner. \n    2.  Click **Data analytics** in the **Create** blade.\n    3.  Click **Data Factory** on the **Data analytics** blade.\n\n        ![Create blade](./media/data-factory-build-your-first-pipeline-using-vs/create-blade.png)\n\n2.  In the **New data factory** blade, enter **DataFactoryMyFirstPipeline** for the Name.\n\n    ![New data factory blade](./media/data-factory-build-your-first-pipeline-using-vs/new-data-factory-blade.png)\n\n    > [AZURE.IMPORTANT] Azure Data Factory names are globally unique. You will need to prefix the name of the data factory with your name, to enable the successful creation of the factory. \n3.  If you have not created any resource group,  you will need to create a resource group. To do this:\n    1.  Click on **RESOURCE GROUP NAME**.\n    2.  Select **Create a new resource group** in the **Resource group** blade.\n    3.  Enter **ADF** for the **Name** in the **Create resource group** blade.\n    4.  Click **OK**.\n    \n        ![Create resource group](./media/data-factory-build-your-first-pipeline-using-vs/create-resource-group.png)\n4.  After you have selected the resource group, verify that you are using the correct subscription where you want the data factory to be created.\n5.  Click **Create** on the **New data factory** blade.\n6.  You will see the data factory being created in the **Startboard** of the Azure Preview Portal as follows:   \n\n    ![Creating data factory status](./media/data-factory-build-your-first-pipeline-using-vs/creating-data-factory-image.png)\n7. Congratulations! You have successfully created your first data factory. After the data factory has been created successfully, you will see the data factory page, which shows you the contents of the data factory.  \n\n    ![Data Factory blade](./media/data-factory-build-your-first-pipeline-using-vs/data-factory-blade.png)\n\nIn the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial. \n\n## Walkthrough: Create and deploy Data Factory entities using Visual Studio \n\n### Pre-requisites\n\nYou must have the following installed on your computer: \n- Visual Studio 2013\n- Download Azure SDK for Visual Studio 2013. Navigate to [Azure Download Page](http://azure.microsoft.com/downloads/) and click **VS 2013 install** in the **.NET** section.\n\n\n### Create the Visual Studio project \n1. Launch **Visual Studio 2013**. Click **File**, point to **New**, and click **Project**. You should see the **New Project** dialog box.  \n2. In the **New Project** dialog, select the **DataFactory** template, and click **Empty Data Factory Project**. If you don't see the DataFactory template, close Visual Studio, install Azure SDK for Visual Studio 2013, and reopen Visual Studio.  \n\n    ![New project dialog box](./media/data-factory-build-your-first-pipeline-using-vs/new-project-dialog.png)\n\n3. Enter a **name** for the project, **location**, and a name for the **solution**, and click **OK**.\n\n    ![Solution Explorer](./media/data-factory-build-your-first-pipeline-using-vs/solution-explorer.png)\n\n### Create linked services\nIn this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then crate a dataset to represent the output data from Hive processing.\n\n\n#### Create Azure Storage linked service\n\n\n4. Right-click **Linked Services** in the solution explorer, point to **Add**, and click **New Item**.      \n5. In the **Add New Item** dialog box, select **Azure Storage Linked Service** from the list, and click **Add**. \n\n    ![New Linked Service](./media/data-factory-build-your-first-pipeline-using-vs/new-linked-service-dialog.png)\n \n3. Replace **accountname** and **accountkey** with the name of your Azure storage account and its key. \n\n    ![Azure Storage Linked Service](./media/data-factory-build-your-first-pipeline-using-vs/azure-storage-linked-service.png)\n\n4. Save the **AzureStorageLinkedService1.json** file.\n\n#### Create Azure HDInsight linked service\nNow, you will create a linked service for an on-demand HDInsight cluster that will be used to run the Hive script. \n\n1. In the **Solution Explorer**, right-click **Linked Services**, point to **Add**, and click **New Item**.\n2. Select **HDInsight On Demand Linked Service**, and click **Add**. \n3. Replace the **JSON** with the following:\n\n        {\n          \"name\": \"HDInsightOnDemandLinkedService\",\n          \"properties\": {\n            \"type\": \"HDInsightOnDemandLinkedService\",\n            \"version\": \"3.1\",\n            \"clusterSize\": 1,\n            \"timeToLive\": \"00:05:00\",\n            \"linkedServiceName\": \"AzureStorageLinkedService1\"\n          }\n        }\n    \n    The following table provides descriptions for the JSON properties used in the snippet:\n    \n    Property | Description\n    -------- | -----------\n    Version | This specifies that the version of the HDInsight created to be 3.1. \n    ClusterSize | This creates a one node HDInsight cluster. \n    TimeToLive | This specifies that the idle time for the HDInsight cluster, before it is deleted.\n    JobsContainer | This specifies the name of the job container that will be created to store the logs that are generated by HDInsight\n    linkedServiceName | This specifies the storage account that will be used to store the logs that are generated by HDInsight\n\n4. Save the **HDInsightOnDemandLinkedService1.json** file.\n \n### Create the output dataset\nNow, you will create the output dataset to represent the data stored in the Azure Blob storage. \n\n1. In the **Solution Explorer**, right-click point to **Add**, and click **New Item**. \n2. Select **Azure Blob** from the list, and click **Add**. \n3. Replace the **JSON** in the editor with the following: In the JSON snippet, you are creating a dataset called **AzureBlobOutput**, and specifying the structure of the data that will be produced by the Hive script. In addition, you specify that the results are stored in the blob container called **data** and the folder called **partitioneddata**. The **availability** section specifies that the output dataset is produced on a monthly basis.\n    \n        {\n            \"name\": \"AzureBlobOutput\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"AzureBlobLocation\",\n                    \"folderPath\": \"data/partitioneddata\",\n                    \"format\": {\n                        \"type\": \"TextFormat\",\n                        \"columnDelimiter\": \",\"\n                    },\n                    \"linkedServiceName\": \"AzureStorageLinkedService1\"\n                },\n                \"availability\": {\n                    \"frequency\": \"Month\",\n                    \"interval\": 1\n                }\n            }\n        }\n\n4. Save the **AzureBlobLocation1.json** file.\n\n\n### Creating your first pipeline\nIn this step, you will create your first pipeline.\n\n1. In the **Solution Explorer**, right-click **Pipelines**, point to **Add**, and click **New Item.** \n2. Select **Hive Transformation Pipeline** from the list, and click **Add**. \n3. Replace the **JSON** with the following snippet and replace **storageaccountname** with the name of your storage account.\n\n        {\n            \"name\": \"MyFirstPipeline\",\n            \"properties\": {\n            \"description\": \"My first Azure Data Factory pipeline\",\n            \"activities\": [\n              {\n                    \"type\": \"HDInsightActivity\",\n                    \"transformation\": {\n                            \"scriptPath\": \"script/partitionweblogs.hql\",\n                            \"scriptLinkedService\": \"AzureStorageLinkedService1\",\n                            \"type\": \"Hive\",\n                            \"extendedProperties\": {\n                                \"partitionedtable\": \"wasb://data@<storageaccountname>.blob.core.windows.net/partitioneddata\"\n                            }\n                        },\n                        \"outputs\": [   {  \"name\": \"AzureBlobOutput\"    }   ],\n                        \"policy\": {  \n                            \"concurrency\": 1,\n                            \"retry\": 3\n                        },\n                        \"name\": \"RunSampleHiveActivity\",\n                        \"linkedServiceName\": \"HDInsightOnDemandLinkedService\"\n                    }\n                ],\n                \"start\": \"2014-01-01\",\n                \"end\": \"2014-01-02\"\n            }\n        }\n \n    In the JSON snippet, you are creating a pipeline that consists of a single activity that uses Hive to process Data on an HDInsight cluster.\n    \n    The Hive script file, **partitionweblogs.hql**, is stored in the Azure storage account (specified by the scriptLinkedService, called **AzureStorageLinkedService1**), and in a container called **script**.\n\n    The **extendedProperties** section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (e.g ${hiveconf:PartitionedData}).\n\n    The **start** and **end** properties of the pipeline specifies the active period of the pipeline.\n\n    In the activity JSON, you specify that the Hive script runs on the compute specified by the linked service – **HDInsightOnDemandLinkedService**.\n3. Save the **HiveActivity1.json** file.\n\n### Add partitionweblogs.hql as a dependency \n\n1. Right-click Dependencies in the **Solution Explorer** window, point to **Add**, and click **Existing Item**.  \n2. Navigate to the **C:\\ADFGettingStarted** and select **partitionweblogs.hql** file, and click **Add**. \n\nWhen you publish the solution in the next step, the HQL file is uploaded to the scripts container in your blob storage.  \n\n### Publish/deploy Data Factory entities\n  \n1. In the toolbar area, right-click and select **Data Factory** to enable the Data Factory toolbar if it is not already enabled. \n19. In the **Data Factory toolbar**, click the **drop-down box** to see all the data factories in your Azure subscription. If you see the **Sign-in to Visual Studio** dialog box: \n    20. Enter the **email account** associated with the Azure subscription in which you want to create the data factory, enter **Password**, and click **Sign-in**.\n    21. Once the sign-in is successful, you should see all the data factories in the Azure subscription. In this tutorial, you will create a new data facotry.       \n22. In the drop-down list, select **DataFactoryMyFirstPipeline**, and click **Publish** button to deploy/publish the linked services, datasets, and the pipeline.    \n\n    ![Publish button](./media/data-factory-build-your-first-pipeline-using-vs/publish.png)\n\n23. You should see the status of publishing in the **Data Factory Task List** window that is shown in the picture above. Confirm that publishing has succeeded.\n\n\n## Use Server Explorer to review Data Factory entities\n\n1. In **Visual Studio**, click **View** on the menu, and click **Server Explorer**.\n2. In the Server Explorer window, expand **Azure** and expand **Data Factory**. If you see **Sign in to Visual Studio**, enter the **account** associated with your Azure subscription and click **Continue**. Enter **password**, and click **Sign in**. Visual Studio tries to get information about all Azure data factories in your subscription. You will see the status of this operation in the **Data Factory Task List** window.\n\n    ![Server Explorer](./media/data-factory-build-your-first-pipeline-using-vs/server-explorer.png)\n3. You can right-click on a data factory, and select **Export Data Factory to New Project** to create a Visual Studio project based on an existing data factory.\n\n    ![Export data factory](./media/data-factory-build-your-first-pipeline-using-vs/export-data-factory-menu.png)\n\n## Update Data Factory tools for Visual Studio\n\nTo update Azure Data Factory tools for Visual Studio, do the following:\n\n1. Click **Tools** on the menu and select **Extensions and Updates**.\n2. Select **Updates** in the left pane and then select **Visual Studio Gallery**.\n3. Select **Azure Data Factory tools for Visual Studio** and click **Update**. If you do not see this entry, you already have the latest version of the tools. \n\nSee [Monitor datasets and pipeline](data-factory-monitor-manage-pipelines.md) for instructions on how to use the Azure Preview Portal to monitor the pipeline and datasets you have created in this tutorial.\n \n\n## Next Steps\nIn this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand HDInsight cluster. To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see [Tutorial: Copy data from an Azure blob to Azure SQL](data-factory-get-started.md).\n  \n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-vs.md). \ntest\n"
}