{
  "nodes": [
    {
      "content": "Move data to and from File System | Azure Data Factory",
      "pos": [
        28,
        82
      ]
    },
    {
      "content": "Learn how to move data to/from on-premises File System using Azure Data Factory.",
      "pos": [
        102,
        182
      ]
    },
    {
      "content": "Move data to and from On-premises File System using Azure Data Factory",
      "pos": [
        510,
        580
      ]
    },
    {
      "content": "This article outlines how you can use data factory copy activity to move data to and from on-premises file system.",
      "pos": [
        582,
        696
      ]
    },
    {
      "content": "This article builds on the <bpt id=\"p1\">[</bpt>data movement activities<ept id=\"p1\">](data-factory-data-movement-activities.md)</ept> article which presents a general overview of data movement with copy activity and supported data store combinations.",
      "pos": [
        697,
        909
      ]
    },
    {
      "content": "Data factory supports connecting to and from on-premises File System via the Data Management Gateway.",
      "pos": [
        911,
        1012
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>moving data between on-premises locations and cloud<ept id=\"p1\">](data-factory-move-data-between-onprem-and-cloud.md)</ept> article to learn about Data Management Gateway and step by step instructions on setting up the gateway.",
      "pos": [
        1013,
        1226
      ]
    },
    {
      "pos": [
        1229,
        1368
      ],
      "content": "<bpt id=\"p1\">**</bpt>Note:<ept id=\"p1\">**</ept> Apart from the Data Management Gateway no other binaries need to be installed to communicate to and from on-premises File System."
    },
    {
      "content": "Linux File Share",
      "pos": [
        1374,
        1390
      ]
    },
    {
      "content": "Perform the following two steps to use a Linux file share with the File Server Linked Service:",
      "pos": [
        1393,
        1487
      ]
    },
    {
      "pos": [
        1491,
        1552
      ],
      "content": "Install <bpt id=\"p1\">[</bpt>Samba<ept id=\"p1\">](https://www.samba.org/)</ept> on your Linux Server."
    },
    {
      "content": "Install and configure Data Management Gateway on a Windows server.",
      "pos": [
        1555,
        1621
      ]
    },
    {
      "content": "Installing gateway on a Linux server is not supported.",
      "pos": [
        1622,
        1676
      ]
    },
    {
      "content": "Sample: Copy data from On-premises File System to Azure Blob",
      "pos": [
        1683,
        1743
      ]
    },
    {
      "content": "The sample below shows:",
      "pos": [
        1745,
        1768
      ]
    },
    {
      "pos": [
        1774,
        1915
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>OnPremisesFileServer<ept id=\"p1\">](data-factory-onprem-file-system-connector.md#onpremisesfileserver-linked-service-properties)</ept>."
    },
    {
      "pos": [
        1920,
        2037
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>AzureStorage<ept id=\"p1\">](data-factory-azure-blob-connector.md#azure-storage-linked-service-properties)</ept>"
    },
    {
      "pos": [
        2042,
        2208
      ],
      "content": "An input <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>FileShare<ept id=\"p2\">](data-factory-onprem-file-system-connector.md#on-premises-file-system-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        2213,
        2359
      ],
      "content": "An output <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>AzureBlob<ept id=\"p2\">](data-factory-azure-blob-connector.md#azure-blob-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        2364,
        2642
      ],
      "content": "The <bpt id=\"p1\">[</bpt>pipeline<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> with Copy Activity that uses <bpt id=\"p2\">[</bpt>FileSystemSource<ept id=\"p2\">](data-factory-onprem-file-system-connector.md#file-share-copy-activity-type-properties)</ept> and <bpt id=\"p3\">[</bpt>BlobSink<ept id=\"p3\">](data-factory-azure-blob-connector.md#azure-blob-copy-activity-type-properties)</ept>."
    },
    {
      "content": "The sample below copies data belonging to a time series from on-premises file system to Azure blob every hour.",
      "pos": [
        2645,
        2755
      ]
    },
    {
      "content": "The JSON properties used in these samples are described in sections following the samples.",
      "pos": [
        2756,
        2846
      ]
    },
    {
      "pos": [
        2849,
        3048
      ],
      "content": "As a first step, do setup the data management gateway as per the instructions in the <bpt id=\"p1\">[</bpt>moving data between on-premises locations and cloud<ept id=\"p1\">](data-factory-move-data-between-onprem-and-cloud.md)</ept> article."
    },
    {
      "content": "On-premises File Server linked service:",
      "pos": [
        3053,
        3092
      ]
    },
    {
      "content": "Azure Blob storage linked service:",
      "pos": [
        3410,
        3444
      ]
    },
    {
      "content": "On-premises File System input dataset:",
      "pos": [
        3714,
        3752
      ]
    },
    {
      "content": "Data is picked up from a new file every hour with the path &amp; filename reflecting the specific datetime with hour granularity.",
      "pos": [
        3756,
        3881
      ]
    },
    {
      "content": "Setting “external”: ”true” and specifying externalData policy informs the Azure Data Factory service that the table is external to the data factory and not produced by an activity in the data factory.",
      "pos": [
        3884,
        4084
      ]
    },
    {
      "content": "Azure Blob output dataset:",
      "pos": [
        5598,
        5624
      ]
    },
    {
      "content": "Data is written to a new blob every hour (frequency: hour, interval: 1).",
      "pos": [
        5628,
        5700
      ]
    },
    {
      "content": "The folder path for the blob is dynamically evaluated based on the start time of the slice that is being processed.",
      "pos": [
        5701,
        5816
      ]
    },
    {
      "content": "The folder path uses year, month, day, and hours parts of the start time.",
      "pos": [
        5817,
        5890
      ]
    },
    {
      "content": "Copy activity:",
      "pos": [
        7289,
        7303
      ]
    },
    {
      "content": "The pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour.",
      "pos": [
        7307,
        7442
      ]
    },
    {
      "content": "In the pipeline JSON definition, the <bpt id=\"p1\">**</bpt>source<ept id=\"p1\">**</ept> type is set to <bpt id=\"p2\">**</bpt>FileSystemSource<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>sink<ept id=\"p3\">**</ept> type is set to <bpt id=\"p4\">**</bpt>BlobSink<ept id=\"p4\">**</ept>.",
      "pos": [
        7443,
        7568
      ]
    },
    {
      "content": "Sample: Copy data from Azure SQL to On-premises File System",
      "pos": [
        8705,
        8764
      ]
    },
    {
      "content": "The sample below shows:",
      "pos": [
        8767,
        8790
      ]
    },
    {
      "content": "A linked service of type AzureSqlDatabase.",
      "pos": [
        8796,
        8838
      ]
    },
    {
      "content": "A linked service of type OnPremisesFileServer.",
      "pos": [
        8843,
        8889
      ]
    },
    {
      "content": "An input dataset of type AzureSqlTable.",
      "pos": [
        8894,
        8933
      ]
    },
    {
      "content": "An output dataset of type FileShare.",
      "pos": [
        8939,
        8975
      ]
    },
    {
      "content": "A pipeline with Copy activity that uses SqlSource and FileSystemSink.",
      "pos": [
        8980,
        9049
      ]
    },
    {
      "content": "The sample copies data belonging to a time series from a table in Azure SQL database to a On-premises File System every hour.",
      "pos": [
        9051,
        9176
      ]
    },
    {
      "content": "The JSON properties used in these samples are described in sections following the samples.",
      "pos": [
        9177,
        9267
      ]
    },
    {
      "content": "Azure SQL linked service:",
      "pos": [
        9272,
        9297
      ]
    },
    {
      "content": "On-premises File Server linked service:",
      "pos": [
        9677,
        9716
      ]
    },
    {
      "content": "Azure SQL input dataset:",
      "pos": [
        10034,
        10058
      ]
    },
    {
      "content": "The sample assumes you have created a table “MyTable” in Azure SQL and it contains a column called “timestampcolumn” for time series data.",
      "pos": [
        10062,
        10200
      ]
    },
    {
      "content": "Setting “external”: ”true” and specifying externalData policy informs the Data Factory service that  the table is external to the data factory and is not produced by an activity in the data factory.",
      "pos": [
        10203,
        10401
      ]
    },
    {
      "content": "On-premises File System output dataset:",
      "pos": [
        10937,
        10976
      ]
    },
    {
      "content": "Data is copied to a new file every hour with the path for the blob reflecting the specific datetime with hour granularity.",
      "pos": [
        10980,
        11102
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Pipeline with a Copy activity:<ept id=\"p1\">**</ept>",
      "pos": [
        12615,
        12649
      ]
    },
    {
      "content": "The pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour.",
      "pos": [
        12650,
        12785
      ]
    },
    {
      "content": "In the pipeline JSON definition, the <bpt id=\"p1\">**</bpt>source<ept id=\"p1\">**</ept> type is set to <bpt id=\"p2\">**</bpt>SqlSource<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>sink<ept id=\"p3\">**</ept> type is set to <bpt id=\"p4\">**</bpt>FileSystemSink<ept id=\"p4\">**</ept>.",
      "pos": [
        12786,
        12910
      ]
    },
    {
      "content": "The SQL query specified for the <bpt id=\"p1\">**</bpt>SqlReaderQuery<ept id=\"p1\">**</ept> property selects the data in the past hour to copy.",
      "pos": [
        12911,
        13013
      ]
    },
    {
      "content": "OnPremisesFileServer Linked Service properties",
      "pos": [
        14336,
        14382
      ]
    },
    {
      "content": "You can link an On-premises File System to an Azure Data Factory with On-Premises File Server Linked Service.",
      "pos": [
        14384,
        14493
      ]
    },
    {
      "content": "The following table provides description for JSON elements specific to On-Premises File Server Linked Service.",
      "pos": [
        14494,
        14604
      ]
    },
    {
      "content": "Property",
      "pos": [
        14607,
        14615
      ]
    },
    {
      "content": "Description",
      "pos": [
        14618,
        14629
      ]
    },
    {
      "content": "Required",
      "pos": [
        14632,
        14640
      ]
    },
    {
      "content": "type",
      "pos": [
        14675,
        14679
      ]
    },
    {
      "pos": [
        14682,
        14741
      ],
      "content": "The type property should be set to <bpt id=\"p1\">**</bpt>OnPremisesFileServer<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Yes",
      "pos": [
        14744,
        14747
      ]
    },
    {
      "content": "host",
      "pos": [
        14749,
        14753
      ]
    },
    {
      "content": "Host name of the server.",
      "pos": [
        14756,
        14780
      ]
    },
    {
      "content": "Use ‘ \\ ’ as the escape character as in the following example: if your share is: \\\\servername, specify \\\\\\\\servername.",
      "pos": [
        14781,
        14899
      ]
    },
    {
      "content": "If the file system is local to the gateway machine, use Local or localhost.",
      "pos": [
        14902,
        14977
      ]
    },
    {
      "content": "If the file system is on a server different from the gateway machine, use \\\\\\\\servername.",
      "pos": [
        14978,
        15067
      ]
    },
    {
      "content": "Yes",
      "pos": [
        15074,
        15077
      ]
    },
    {
      "content": "userid",
      "pos": [
        15078,
        15084
      ]
    },
    {
      "content": "Specify the ID of the user who has access to the server",
      "pos": [
        15088,
        15143
      ]
    },
    {
      "content": "No (if you choose encryptedCredential)",
      "pos": [
        15146,
        15184
      ]
    },
    {
      "content": "password",
      "pos": [
        15185,
        15193
      ]
    },
    {
      "content": "Specify the password for the user (userid)",
      "pos": [
        15196,
        15238
      ]
    },
    {
      "content": "No (if you choose encryptedCredential",
      "pos": [
        15241,
        15278
      ]
    },
    {
      "content": "encryptedCredential",
      "pos": [
        15280,
        15299
      ]
    },
    {
      "content": "Specify the encrypted credentials that you can get by running the New-AzureDataFactoryEncryptValue cmdlet",
      "pos": [
        15302,
        15407
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Note:<ept id=\"p1\">**</ept> You must use the Azure PowerShell of version 0.8.14 or higher to use cmdlets such as New-AzureDataFactoryEncryptValue with type parameter set to OnPremisesFileSystemLinkedService",
      "pos": [
        15410,
        15598
      ]
    },
    {
      "content": "No (if you choose to specify userid and password in plain text)",
      "pos": [
        15605,
        15668
      ]
    },
    {
      "content": "gatewayName",
      "pos": [
        15669,
        15680
      ]
    },
    {
      "content": "Name of the gateway that the Data Factory service should use to connect to the on-premises file server",
      "pos": [
        15683,
        15785
      ]
    },
    {
      "content": "Yes",
      "pos": [
        15788,
        15791
      ]
    },
    {
      "pos": [
        15793,
        15998
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Setting Credentials and Security<ept id=\"p1\">](data-factory-move-data-between-onprem-and-cloud.md#setting-credentials-and-security)</ept> for details about setting credentials for an on-premises File System data source."
    },
    {
      "content": "Example: Using username and password in plain text",
      "pos": [
        16002,
        16052
      ]
    },
    {
      "content": "Example: Using encryptedcredential",
      "pos": [
        16378,
        16412
      ]
    },
    {
      "content": "On-premises File System Dataset type properties",
      "pos": [
        16758,
        16805
      ]
    },
    {
      "content": "For a full list of sections &amp; properties available for defining datasets, see the <bpt id=\"p1\">[</bpt>Creating datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept> article.",
      "pos": [
        16807,
        16950
      ]
    },
    {
      "content": "Sections like structure, availability, and policy of a dataset JSON are similar for all dataset types (Azure SQL, Azure Blob, Azure Table, On-premises File System, etc...).",
      "pos": [
        16951,
        17123
      ]
    },
    {
      "content": "The typeProperties section is different for each type of dataset and provides information about the location, format etc. of the data in the data store.",
      "pos": [
        17126,
        17278
      ]
    },
    {
      "content": "The typeProperties section for dataset of type <bpt id=\"p1\">**</bpt>FileShare<ept id=\"p1\">**</ept> dataset has the following properties.",
      "pos": [
        17279,
        17377
      ]
    },
    {
      "content": "Property",
      "pos": [
        17379,
        17387
      ]
    },
    {
      "content": "Description",
      "pos": [
        17390,
        17401
      ]
    },
    {
      "content": "Required",
      "pos": [
        17404,
        17412
      ]
    },
    {
      "content": "folderPath",
      "pos": [
        17447,
        17457
      ]
    },
    {
      "content": "Path to the folder.",
      "pos": [
        17460,
        17479
      ]
    },
    {
      "content": "Example: myfolder",
      "pos": [
        17480,
        17497
      ]
    },
    {
      "content": "Use escape character ‘ \\ ’ for special characters in the string.",
      "pos": [
        17500,
        17564
      ]
    },
    {
      "content": "For example: for folder\\subfolder, specify folder\\\\subfolder and for d:\\samplefolder, specify d:\\\\samplefolder.",
      "pos": [
        17565,
        17676
      ]
    },
    {
      "content": "You can combine this with <bpt id=\"p1\">**</bpt>partitionBy<ept id=\"p1\">**</ept> to have folder paths based on slice start/end date-times.",
      "pos": [
        17683,
        17782
      ]
    },
    {
      "content": "Yes",
      "pos": [
        17789,
        17792
      ]
    },
    {
      "content": "fileName",
      "pos": [
        17793,
        17801
      ]
    },
    {
      "content": "Specify the name of the file in the <bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> if you want the table to refer to a specific file in the folder.",
      "pos": [
        17804,
        17919
      ]
    },
    {
      "content": "If you do not specify any value for this property, the table points to all files in the folder.",
      "pos": [
        17920,
        18015
      ]
    },
    {
      "content": "When fileName is not specified for an output dataset, the name of the generated file would be in the following this format:",
      "pos": [
        18018,
        18141
      ]
    },
    {
      "content": "Data.",
      "pos": [
        18149,
        18154
      ]
    },
    {
      "content": ".txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt",
      "pos": [
        18160,
        18226
      ]
    },
    {
      "content": "No",
      "pos": [
        18233,
        18235
      ]
    },
    {
      "content": "partitionedBy",
      "pos": [
        18236,
        18249
      ]
    },
    {
      "content": "partitionedBy can be leveraged to specify a dynamic folderPath, filename for time series data.",
      "pos": [
        18252,
        18346
      ]
    },
    {
      "content": "For example folderPath parameterized for every hour of data.",
      "pos": [
        18347,
        18407
      ]
    },
    {
      "content": "No",
      "pos": [
        18410,
        18412
      ]
    },
    {
      "content": "Format",
      "pos": [
        18413,
        18419
      ]
    },
    {
      "content": "Two formats types are supported: <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>AvroFormat<ept id=\"p2\">**</ept>.",
      "pos": [
        18422,
        18486
      ]
    },
    {
      "content": "You need to set the type property under format to either if this value.",
      "pos": [
        18487,
        18558
      ]
    },
    {
      "content": "When the forAvroFormatmat is TextFormat you can specify additional optional properties for format.",
      "pos": [
        18559,
        18657
      ]
    },
    {
      "content": "See the format section below for more details.",
      "pos": [
        18658,
        18704
      ]
    },
    {
      "content": "No",
      "pos": [
        18707,
        18709
      ]
    },
    {
      "content": "fileFilter",
      "pos": [
        18710,
        18720
      ]
    },
    {
      "content": "Specify a filter to be used to select a subset of files in the folderPath rather than all files.",
      "pos": [
        18723,
        18819
      ]
    },
    {
      "content": "Allowed values are: * (multiple characters) and ?",
      "pos": [
        18823,
        18872
      ]
    },
    {
      "content": "(single character).",
      "pos": [
        18873,
        18892
      ]
    },
    {
      "content": "Examples 1: \"fileFilter\": \"<bpt id=\"p1\">*</bpt>.log\"<ph id=\"ph1\">&lt;/p&gt;</ph>Example 2: \"fileFilter\": 2014-1-?.txt\"<ph id=\"ph2\">&lt;/p&gt;</ph><ph id=\"ph3\">&lt;p&gt;</ph><ept id=\"p1\">*</ept><bpt id=\"p2\">*</bpt>Note<ept id=\"p2\">*</ept>*: fileFilter is applicable for an input FileShare dataset",
      "pos": [
        18899,
        19046
      ]
    },
    {
      "content": "No",
      "pos": [
        19053,
        19055
      ]
    },
    {
      "content": "compression",
      "pos": [
        19058,
        19069
      ]
    },
    {
      "content": "Specify the type and level of compression for the data.",
      "pos": [
        19072,
        19127
      ]
    },
    {
      "content": "Supported types are: GZip, Deflate, and BZip2 and supported levels are: Optimal and Fastest.",
      "pos": [
        19128,
        19220
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Compression support<ept id=\"p1\">](#compression-support)</ept> section for more details.",
      "pos": [
        19221,
        19294
      ]
    },
    {
      "content": "No",
      "pos": [
        19298,
        19300
      ]
    },
    {
      "pos": [
        19306,
        19373
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> filename and fileFilter cannot be used simultaneously."
    },
    {
      "content": "Leveraging partionedBy property",
      "pos": [
        19379,
        19410
      ]
    },
    {
      "content": "As mentioned above, you can specify a dynamic folderPath, filename for time series data with partitionedBy.",
      "pos": [
        19412,
        19519
      ]
    },
    {
      "content": "You can do so with the Data Factory macros and the system variable SliceStart, SliceEnd that indicate the logical time period for a given data slice.",
      "pos": [
        19520,
        19669
      ]
    },
    {
      "pos": [
        19672,
        19940
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Creating Datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept>, <bpt id=\"p2\">[</bpt>Scheduling &amp; Execution<ept id=\"p2\">](data-factory-scheduling-and-execution.md)</ept>, and <bpt id=\"p3\">[</bpt>Creating Pipelines<ept id=\"p3\">](data-factory-create-pipelines.md)</ept> articles to understand more details on time series datasets, scheduling and slices."
    },
    {
      "content": "Sample 1:",
      "pos": [
        19947,
        19956
      ]
    },
    {
      "content": "In the above example {Slice} is replaced with the value of Data Factory system variable SliceStart in the format (YYYYMMDDHH) specified.",
      "pos": [
        20165,
        20301
      ]
    },
    {
      "content": "The SliceStart refers to start time of the slice.",
      "pos": [
        20302,
        20351
      ]
    },
    {
      "content": "The folderPath is different for each slice.",
      "pos": [
        20352,
        20395
      ]
    },
    {
      "content": "For example: wikidatagateway/wikisampledataout/2014100103 or wikidatagateway/wikisampledataout/2014100104.",
      "pos": [
        20396,
        20502
      ]
    },
    {
      "content": "Sample 2:",
      "pos": [
        20509,
        20518
      ]
    },
    {
      "content": "In the above example, year, month, day, and time of SliceStart are extracted into separate variables that are used by folderPath and fileName properties.",
      "pos": [
        21063,
        21216
      ]
    },
    {
      "content": "Specifying TextFormat",
      "pos": [
        21222,
        21243
      ]
    },
    {
      "pos": [
        21245,
        21403
      ],
      "content": "If the format is set to <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>, you can specify the following <bpt id=\"p2\">**</bpt>optional<ept id=\"p2\">**</ept> properties in the <bpt id=\"p3\">**</bpt>Format<ept id=\"p3\">**</ept> section within the <bpt id=\"p4\">**</bpt>typeProperties<ept id=\"p4\">**</ept> section."
    },
    {
      "content": "Property",
      "pos": [
        21406,
        21414
      ]
    },
    {
      "content": "Description",
      "pos": [
        21417,
        21428
      ]
    },
    {
      "content": "Required",
      "pos": [
        21431,
        21439
      ]
    },
    {
      "content": "columnDelimiter",
      "pos": [
        21474,
        21489
      ]
    },
    {
      "content": "The character(s) used as a column separator in a file.",
      "pos": [
        21492,
        21546
      ]
    },
    {
      "content": "The default value is comma (,).",
      "pos": [
        21547,
        21578
      ]
    },
    {
      "content": "No",
      "pos": [
        21581,
        21583
      ]
    },
    {
      "content": "rowDelimiter",
      "pos": [
        21584,
        21596
      ]
    },
    {
      "content": "The character(s) used as a raw separator in file.",
      "pos": [
        21599,
        21648
      ]
    },
    {
      "content": "The default value is any of the following: [“\\r\\n”, “\\r”,” \\n”].",
      "pos": [
        21649,
        21713
      ]
    },
    {
      "content": "No",
      "pos": [
        21716,
        21718
      ]
    },
    {
      "content": "escapeChar",
      "pos": [
        21719,
        21729
      ]
    },
    {
      "content": "The special character used to escape column delimiter shown in content.",
      "pos": [
        21732,
        21803
      ]
    },
    {
      "content": "No default value.",
      "pos": [
        21804,
        21821
      ]
    },
    {
      "content": "You must specify no more than one character for this property.",
      "pos": [
        21822,
        21884
      ]
    },
    {
      "content": "For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: “Hello, world”), you can define ‘$’ as the escape character and use string “Hello$, world” in the source.",
      "pos": [
        21887,
        22107
      ]
    },
    {
      "content": "Note that you cannot specify both escapeChar and quoteChar for a table.",
      "pos": [
        22114,
        22185
      ]
    },
    {
      "content": "No",
      "pos": [
        22192,
        22194
      ]
    },
    {
      "content": "quoteChar",
      "pos": [
        22195,
        22204
      ]
    },
    {
      "content": "The special character is used to quote the string value.",
      "pos": [
        22207,
        22263
      ]
    },
    {
      "content": "The column and row delimiters inside of the quote characters would be treated as part of the string value.",
      "pos": [
        22264,
        22370
      ]
    },
    {
      "content": "No default value.",
      "pos": [
        22371,
        22388
      ]
    },
    {
      "content": "You must specify no more than one character for this property.",
      "pos": [
        22389,
        22451
      ]
    },
    {
      "content": "For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: &lt;Hello, world&gt;), you can define ‘\"’ as the quote character and use string &lt;\"Hello, world\"&gt; in the source.",
      "pos": [
        22454,
        22674
      ]
    },
    {
      "content": "This property is applicable to both input and output tables.",
      "pos": [
        22675,
        22735
      ]
    },
    {
      "content": "Note that you cannot specify both escapeChar and quoteChar for a table.",
      "pos": [
        22742,
        22813
      ]
    },
    {
      "content": "No",
      "pos": [
        22820,
        22822
      ]
    },
    {
      "content": "nullValue",
      "pos": [
        22823,
        22832
      ]
    },
    {
      "content": "The character(s) used to represent null value in blob file content.",
      "pos": [
        22835,
        22902
      ]
    },
    {
      "content": "The default value is “\\N”.&gt;",
      "pos": [
        22903,
        22930
      ]
    },
    {
      "content": "No",
      "pos": [
        22933,
        22935
      ]
    },
    {
      "content": "encodingName",
      "pos": [
        22936,
        22948
      ]
    },
    {
      "content": "Specify the encoding name.",
      "pos": [
        22951,
        22977
      ]
    },
    {
      "content": "For the list of valid encoding names, see: Encoding.EncodingName Property.",
      "pos": [
        22978,
        23052
      ]
    },
    {
      "content": "For example: windows-1250 or shift_jis.",
      "pos": [
        23056,
        23095
      ]
    },
    {
      "content": "The default value is: UTF-8.",
      "pos": [
        23096,
        23124
      ]
    },
    {
      "content": "No",
      "pos": [
        23131,
        23133
      ]
    },
    {
      "content": "Samples:",
      "pos": [
        23140,
        23148
      ]
    },
    {
      "pos": [
        23150,
        23226
      ],
      "content": "The following sample shows some of the format properties for <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>."
    },
    {
      "content": "To use an escapeChar instead of quoteChar, replace the line with quoteChar with the following:",
      "pos": [
        23534,
        23628
      ]
    },
    {
      "content": "Specifying AvroFormat",
      "pos": [
        23658,
        23679
      ]
    },
    {
      "content": "If the format is set to <bpt id=\"p1\">**</bpt>AvroFormat<ept id=\"p1\">**</ept>, you do not need to specify any properties in the Format section within the typeProperties section.",
      "pos": [
        23681,
        23819
      ]
    },
    {
      "content": "Example:",
      "pos": [
        23820,
        23828
      ]
    },
    {
      "pos": [
        23891,
        24032
      ],
      "content": "To use Avro format in a subsequent Hive table, refer to <bpt id=\"p1\">[</bpt>Apache Hive’s tutorial<ept id=\"p1\">](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe)</ept>."
    },
    {
      "content": "File Share Copy Activity type properties",
      "pos": [
        24132,
        24172
      ]
    },
    {
      "pos": [
        24174,
        24261
      ],
      "content": "<bpt id=\"p1\">**</bpt>FileSystemSource<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>FileSystemSink<ept id=\"p2\">**</ept> do not support any properties at this time."
    },
    {
      "content": "test",
      "pos": [
        24501,
        24505
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Move data to and from File System | Azure Data Factory\" \n    description=\"Learn how to move data to/from on-premises File System using Azure Data Factory.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/26/2015\" \n    ms.author=\"spelluru\"/>\n\n# Move data to and from On-premises File System using Azure Data Factory\n\nThis article outlines how you can use data factory copy activity to move data to and from on-premises file system. This article builds on the [data movement activities](data-factory-data-movement-activities.md) article which presents a general overview of data movement with copy activity and supported data store combinations.\n\nData factory supports connecting to and from on-premises File System via the Data Management Gateway. See [moving data between on-premises locations and cloud](data-factory-move-data-between-onprem-and-cloud.md) article to learn about Data Management Gateway and step by step instructions on setting up the gateway. \n\n**Note:** Apart from the Data Management Gateway no other binaries need to be installed to communicate to and from on-premises File System. \n\n## Linux File Share \n\nPerform the following two steps to use a Linux file share with the File Server Linked Service:\n\n- Install [Samba](https://www.samba.org/) on your Linux Server.\n- Install and configure Data Management Gateway on a Windows server. Installing gateway on a Linux server is not supported. \n \n## Sample: Copy data from On-premises File System to Azure Blob\n\nThe sample below shows:\n\n1.  A linked service of type [OnPremisesFileServer](data-factory-onprem-file-system-connector.md#onpremisesfileserver-linked-service-properties).\n2.  A linked service of type [AzureStorage](data-factory-azure-blob-connector.md#azure-storage-linked-service-properties)\n3.  An input [dataset](data-factory-create-datasets.md) of type [FileShare](data-factory-onprem-file-system-connector.md#on-premises-file-system-dataset-type-properties).\n4.  An output [dataset](data-factory-create-datasets.md) of type [AzureBlob](data-factory-azure-blob-connector.md#azure-blob-dataset-type-properties).\n4.  The [pipeline](data-factory-create-pipelines.md) with Copy Activity that uses [FileSystemSource](data-factory-onprem-file-system-connector.md#file-share-copy-activity-type-properties) and [BlobSink](data-factory-azure-blob-connector.md#azure-blob-copy-activity-type-properties). \n\nThe sample below copies data belonging to a time series from on-premises file system to Azure blob every hour. The JSON properties used in these samples are described in sections following the samples. \n\nAs a first step, do setup the data management gateway as per the instructions in the [moving data between on-premises locations and cloud](data-factory-move-data-between-onprem-and-cloud.md) article. \n\n**On-premises File Server linked service:**\n\n    {\n      \"Name\": \"OnPremisesFileServerLinkedService\",\n      \"properties\": {\n        \"type\": \"OnPremisesFileServer\",\n        \"typeProperties\": {\n          \"host\": \"\\\\\\\\Contosogame-Asia\",\n          \"userid\": \"Admin\",\n          \"password\": \"123456\",\n          \"gatewayName\": \"mygateway\"\n        }\n      }\n    }\n\n**Azure Blob storage linked service:**\n\n    {\n      \"name\": \"StorageLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureStorage\",\n        \"typeProperties\": {\n          \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n        }\n      }\n    }\n\n**On-premises File System input dataset:**\n\nData is picked up from a new file every hour with the path & filename reflecting the specific datetime with hour granularity. \n\nSetting “external”: ”true” and specifying externalData policy informs the Azure Data Factory service that the table is external to the data factory and not produced by an activity in the data factory.\n\n    {\n      \"name\": \"OnpremisesFileSystemInput\",\n      \"properties\": {\n        \"type\": \" FileShare\",\n        \"linkedServiceName\": \" OnPremisesFileServerLinkedService \",\n        \"typeProperties\": {\n          \"folderPath\": \"mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}\",\n          \"fileName\": \"{Hour}.csv\",\n          \"partitionedBy\": [\n            {\n              \"name\": \"Year\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"yyyy\"\n              }\n            },\n            {\n              \"name\": \"Month\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%M\"\n              }\n            },\n            {\n              \"name\": \"Day\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%d\"\n              }\n            },\n            {\n              \"name\": \"Hour\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%H\"\n              }\n            }\n          ]\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n**Azure Blob output dataset:**\n\nData is written to a new blob every hour (frequency: hour, interval: 1). The folder path for the blob is dynamically evaluated based on the start time of the slice that is being processed. The folder path uses year, month, day, and hours parts of the start time. \n\n    {\n      \"name\": \"AzureBlobOutput\",\n      \"properties\": {\n        \"type\": \"AzureBlob\",\n        \"linkedServiceName\": \"StorageLinkedService\",\n        \"typeProperties\": {\n          \"folderPath\": \"mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}/hourno={Hour}\",\n          \"partitionedBy\": [\n            {\n              \"name\": \"Year\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"yyyy\"\n              }\n            },\n            {\n              \"name\": \"Month\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%M\"\n              }\n            },\n            {\n              \"name\": \"Day\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%d\"\n              }\n            },\n            {\n              \"name\": \"Hour\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%HH\"\n              }\n            }\n          ],\n          \"format\": {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \"\\t\",\n            \"rowDelimiter\": \"\\n\"\n          }\n        },\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        }\n      }\n    }\n\n**Copy activity:**\n\nThe pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour. In the pipeline JSON definition, the **source** type is set to **FileSystemSource** and **sink** type is set to **BlobSink**. \n    \n    {  \n        \"name\":\"SamplePipeline\",\n        \"properties\":{  \n        \"start\":\"2015-06-01T18:00:00\",\n        \"end\":\"2015-06-01T19:00:00\",\n        \"description\":\"Pipeline for copy activity\",\n        \"activities\":[  \n          {\n            \"name\": \"OnpremisesFileSystemtoBlob\",\n            \"description\": \"copy activity\",\n            \"type\": \"Copy\",\n            \"inputs\": [\n              {\n                \"name\": \"OnpremisesFileSystemInput\"\n              }\n            ],\n            \"outputs\": [\n              {\n                \"name\": \"AzureBlobOutput\"\n              }\n            ],\n            \"typeProperties\": {\n              \"source\": {\n                \"type\": \"FileSystemSource\"\n              },\n              \"sink\": {\n                \"type\": \"BlobSink\"\n              }\n            },\n           \"scheduler\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            },\n            \"policy\": {\n              \"concurrency\": 1,\n              \"executionPriorityOrder\": \"OldestFirst\",\n              \"retry\": 0,\n              \"timeout\": \"01:00:00\"\n            }\n          }\n         ]\n       }\n    }\n\n##Sample: Copy data from Azure SQL to On-premises File System \n\nThe sample below shows:\n\n1.  A linked service of type AzureSqlDatabase.\n2.  A linked service of type OnPremisesFileServer.\n3.  An input dataset of type AzureSqlTable. \n3.  An output dataset of type FileShare.\n4.  A pipeline with Copy activity that uses SqlSource and FileSystemSink.\n\nThe sample copies data belonging to a time series from a table in Azure SQL database to a On-premises File System every hour. The JSON properties used in these samples are described in sections following the samples. \n\n**Azure SQL linked service:**\n\n    {\n      \"name\": \"AzureSqlLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureSqlDatabase\",\n        \"typeProperties\": {\n          \"connectionString\": \"Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30\"\n        }\n      }\n    }\n\n**On-premises File Server linked service:**\n\n    {\n      \"Name\": \"OnPremisesFileServerLinkedService\",\n      \"properties\": {\n        \"type\": \"OnPremisesFileServer\",\n        \"typeProperties\": {\n          \"host\": \"\\\\\\\\Contosogame-Asia\",\n          \"userid\": \"Admin\",\n          \"password\": \"123456\",\n          \"gatewayName\": \"mygateway\"\n        }\n      }\n    }\n\n**Azure SQL input dataset:**\n\nThe sample assumes you have created a table “MyTable” in Azure SQL and it contains a column called “timestampcolumn” for time series data. \n\nSetting “external”: ”true” and specifying externalData policy informs the Data Factory service that  the table is external to the data factory and is not produced by an activity in the data factory.\n\n    {\n      \"name\": \"AzureSqlInput\",\n      \"properties\": {\n        \"type\": \"AzureSqlTable\",\n        \"linkedServiceName\": \"AzureSqlLinkedService\",\n        \"typeProperties\": {\n          \"tableName\": \"MyTable\"\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n**On-premises File System output dataset:**\n\nData is copied to a new file every hour with the path for the blob reflecting the specific datetime with hour granularity.\n\n    {\n      \"name\": \"OnpremisesFileSystemOutput\",\n      \"properties\": {\n        \"type\": \"FileShare\",\n        \"linkedServiceName\": \" OnPremisesFileServerLinkedService \",\n        \"typeProperties\": {\n          \"folderPath\": \"mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}\",\n          \"fileName\": \"{Hour}.csv\",\n          \"partitionedBy\": [\n            {\n              \"name\": \"Year\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"yyyy\"\n              }\n            },\n            {\n              \"name\": \"Month\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%M\"\n              }\n            },\n            {\n              \"name\": \"Day\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%d\"\n              }\n            },\n            {\n              \"name\": \"Hour\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%HH\"\n              }\n            }\n          ]\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n**Pipeline with a Copy activity:**\nThe pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour. In the pipeline JSON definition, the **source** type is set to **SqlSource** and **sink** type is set to **FileSystemSink**. The SQL query specified for the **SqlReaderQuery** property selects the data in the past hour to copy.\n\n    \n    {  \n        \"name\":\"SamplePipeline\",\n        \"properties\":{  \n        \"start\":\"2015-06-01T18:00:00\",\n        \"end\":\"2015-06-01T20:00:00\",\n        \"description\":\"pipeline for copy activity\",\n        \"activities\":[  \n          {\n            \"name\": \"AzureSQLtoOnPremisesFile\",\n            \"description\": \"copy activity\",\n            \"type\": \"Copy\",\n            \"inputs\": [\n              {\n                \"name\": \"AzureSQLInput\"\n              }\n            ],\n            \"outputs\": [\n              {\n                \"name\": \"OnpremisesFileSystemOutput\"\n              }\n            ],\n            \"typeProperties\": {\n              \"source\": {\n                \"type\": \"SqlSource\",\n                \"SqlReaderQuery\": \"$$Text.Format('select * from MyTable where timestampcolumn >= \\\\'{0:yyyy-MM-dd}\\\\' AND timestampcolumn < \\\\'{1:yyyy-MM-dd}\\\\'', WindowStart, WindowEnd)\"\n              },\n              \"sink\": {\n                \"type\": \"FileSystemSink\"\n              }\n            },\n           \"scheduler\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            },\n            \"policy\": {\n              \"concurrency\": 1,\n              \"executionPriorityOrder\": \"OldestFirst\",\n              \"retry\": 3,\n              \"timeout\": \"01:00:00\"\n            }\n          }\n         ]\n       }\n    }\n\n## OnPremisesFileServer Linked Service properties\n\nYou can link an On-premises File System to an Azure Data Factory with On-Premises File Server Linked Service. The following table provides description for JSON elements specific to On-Premises File Server Linked Service. \n\nProperty | Description | Required\n-------- | ----------- | --------\ntype | The type property should be set to **OnPremisesFileServer** | Yes \nhost | Host name of the server. Use ‘ \\ ’ as the escape character as in the following example: if your share is: \\\\servername, specify \\\\\\\\servername.<p>If the file system is local to the gateway machine, use Local or localhost. If the file system is on a server different from the gateway machine, use \\\\\\\\servername.</p> | Yes\nuserid  | Specify the ID of the user who has access to the server | No (if you choose encryptedCredential)\npassword | Specify the password for the user (userid) | No (if you choose encryptedCredential \nencryptedCredential | Specify the encrypted credentials that you can get by running the New-AzureDataFactoryEncryptValue cmdlet<p>**Note:** You must use the Azure PowerShell of version 0.8.14 or higher to use cmdlets such as New-AzureDataFactoryEncryptValue with type parameter set to OnPremisesFileSystemLinkedService</p> | No (if you choose to specify userid and password in plain text)\ngatewayName | Name of the gateway that the Data Factory service should use to connect to the on-premises file server | Yes\n\nSee [Setting Credentials and Security](data-factory-move-data-between-onprem-and-cloud.md#setting-credentials-and-security) for details about setting credentials for an on-premises File System data source.\n\n**Example: Using username and password in plain text**\n    \n    {\n      \"Name\": \"OnPremisesFileServerLinkedService\",\n      \"properties\": {\n        \"type\": \"OnPremisesFileServer\",\n        \"typeProperties\": {\n          \"host\": \"\\\\\\\\Contosogame-Asia\",\n          \"userid\": \"Admin\",\n          \"password\": \"123456\",\n          \"gatewayName\": \"mygateway\"\n        }\n      }\n    }\n    \n**Example: Using encryptedcredential**\n\n    {\n      \"Name\": \" OnPremisesFileServerLinkedService \",\n      \"properties\": {\n        \"type\": \"OnPremisesFileServer\",\n        \"typeProperties\": {\n          \"host\": \"localhost\",\n          \"encryptedCredential\": \"WFuIGlzIGRpc3Rpbmd1aXNoZWQsIG5vdCBvbmx5IGJ5xxxxxxxxxxxxxxxxx\",\n          \"gatewayName\": \"mygateway\"\n        }\n      }\n    }\n\n## On-premises File System Dataset type properties\n\nFor a full list of sections & properties available for defining datasets, see the [Creating datasets](data-factory-create-datasets.md) article. Sections like structure, availability, and policy of a dataset JSON are similar for all dataset types (Azure SQL, Azure Blob, Azure Table, On-premises File System, etc...). \n\nThe typeProperties section is different for each type of dataset and provides information about the location, format etc. of the data in the data store. The typeProperties section for dataset of type **FileShare** dataset has the following properties.\n\nProperty | Description | Required\n-------- | ----------- | --------\nfolderPath | Path to the folder. Example: myfolder<p>Use escape character ‘ \\ ’ for special characters in the string. For example: for folder\\subfolder, specify folder\\\\subfolder and for d:\\samplefolder, specify d:\\\\samplefolder.</p><p>You can combine this with **partitionBy** to have folder paths based on slice start/end date-times.</p> | Yes\nfileName | Specify the name of the file in the **folderPath** if you want the table to refer to a specific file in the folder. If you do not specify any value for this property, the table points to all files in the folder.<p>When fileName is not specified for an output dataset, the name of the generated file would be in the following this format: </p><p>Data.<Guid>.txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt</p> | No\npartitionedBy | partitionedBy can be leveraged to specify a dynamic folderPath, filename for time series data. For example folderPath parameterized for every hour of data. | No\nFormat | Two formats types are supported: **TextFormat**, **AvroFormat**. You need to set the type property under format to either if this value. When the forAvroFormatmat is TextFormat you can specify additional optional properties for format. See the format section below for more details. | No\nfileFilter | Specify a filter to be used to select a subset of files in the folderPath rather than all files. <p>Allowed values are: * (multiple characters) and ? (single character).</p><p>Examples 1: \"fileFilter\": \"*.log\"</p>Example 2: \"fileFilter\": 2014-1-?.txt\"</p><p>**Note**: fileFilter is applicable for an input FileShare dataset</p> | No\n| compression | Specify the type and level of compression for the data. Supported types are: GZip, Deflate, and BZip2 and supported levels are: Optimal and Fastest. See [Compression support](#compression-support) section for more details.  | No |\n\n> [AZURE.NOTE] filename and fileFilter cannot be used simultaneously.\n\n### Leveraging partionedBy property\n\nAs mentioned above, you can specify a dynamic folderPath, filename for time series data with partitionedBy. You can do so with the Data Factory macros and the system variable SliceStart, SliceEnd that indicate the logical time period for a given data slice. \n\nSee [Creating Datasets](data-factory-create-datasets.md), [Scheduling & Execution](data-factory-scheduling-and-execution.md), and [Creating Pipelines](data-factory-create-pipelines.md) articles to understand more details on time series datasets, scheduling and slices.\n\n#### Sample 1:\n\n    \"folderPath\": \"wikidatagateway/wikisampledataout/{Slice}\",\n    \"partitionedBy\": \n    [\n        { \"name\": \"Slice\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyyMMddHH\" } },\n    ],\n\nIn the above example {Slice} is replaced with the value of Data Factory system variable SliceStart in the format (YYYYMMDDHH) specified. The SliceStart refers to start time of the slice. The folderPath is different for each slice. For example: wikidatagateway/wikisampledataout/2014100103 or wikidatagateway/wikisampledataout/2014100104.\n\n#### Sample 2:\n\n    \"folderPath\": \"wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}\",\n    \"fileName\": \"{Hour}.csv\",\n    \"partitionedBy\": \n     [\n        { \"name\": \"Year\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyy\" } },\n        { \"name\": \"Month\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"MM\" } }, \n        { \"name\": \"Day\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"dd\" } }, \n        { \"name\": \"Hour\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"hh\" } } \n    ],\n\nIn the above example, year, month, day, and time of SliceStart are extracted into separate variables that are used by folderPath and fileName properties.\n\n### Specifying TextFormat\n\nIf the format is set to **TextFormat**, you can specify the following **optional** properties in the **Format** section within the **typeProperties** section. \n\nProperty | Description | Required\n-------- | ----------- | --------\ncolumnDelimiter | The character(s) used as a column separator in a file. The default value is comma (,). | No\nrowDelimiter | The character(s) used as a raw separator in file. The default value is any of the following: [“\\r\\n”, “\\r”,” \\n”]. | No\nescapeChar | The special character used to escape column delimiter shown in content. No default value. You must specify no more than one character for this property.<p>For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: “Hello, world”), you can define ‘$’ as the escape character and use string “Hello$, world” in the source.</p><p>Note that you cannot specify both escapeChar and quoteChar for a table.</p> | No\nquoteChar | The special character is used to quote the string value. The column and row delimiters inside of the quote characters would be treated as part of the string value. No default value. You must specify no more than one character for this property.<p>For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: <Hello, world>), you can define ‘\"’ as the quote character and use string <\"Hello, world\"> in the source. This property is applicable to both input and output tables.</p><p>Note that you cannot specify both escapeChar and quoteChar for a table.</p> | No\nnullValue | The character(s) used to represent null value in blob file content. The default value is “\\N”.> | No\nencodingName | Specify the encoding name. For the list of valid encoding names, see: Encoding.EncodingName Property. <p>For example: windows-1250 or shift_jis. The default value is: UTF-8.</p> | No\n\n#### Samples:\n\nThe following sample shows some of the format properties for **TextFormat**.\n\n    \"typeProperties\":\n    {\n        \"folderPath\": \"MyFolder\",\n        \"fileName\": \"MyFileName\"\n        \"format\":\n        {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \",\",\n            \"rowDelimiter\": \";\",\n            \"quoteChar\": \"\\\"\",\n            \"NullValue\": \"NaN\"\n        }\n    },\n\nTo use an escapeChar instead of quoteChar, replace the line with quoteChar with the following:\n\n    \"escapeChar\": \"$\",\n\n### Specifying AvroFormat\n\nIf the format is set to **AvroFormat**, you do not need to specify any properties in the Format section within the typeProperties section. Example:\n\n    \"format\":\n    {\n        \"type\": \"AvroFormat\",\n    }\n    \nTo use Avro format in a subsequent Hive table, refer to [Apache Hive’s tutorial](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe).       \n\n[AZURE.INCLUDE [data-factory-compression](../../includes/data-factory-compression.md)]\n\n## File Share Copy Activity type properties\n\n**FileSystemSource** and **FileSystemSink** do not support any properties at this time.\n\n[AZURE.INCLUDE [data-factory-structure-for-rectangualr-datasets](../../includes/data-factory-structure-for-rectangualr-datasets.md)]\n\n[AZURE.INCLUDE [data-factory-column-mapping](../../includes/data-factory-column-mapping.md)]\n\n\n\n\n\n\n\n\n \n\ntest\n"
}