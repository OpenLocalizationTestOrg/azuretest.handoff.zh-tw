<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>NAMD with Microsoft HPC Pack on Linux VMs | Microsoft Azure</source>
          <target state="new">NAMD with Microsoft HPC Pack on Linux VMs | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Deploy a Microsoft HPC Pack cluster on Azure and run a NAMD simulation with charmrun on multiple Linux compute nodes.</source>
          <target state="new">Deploy a Microsoft HPC Pack cluster on Azure and run a NAMD simulation with charmrun on multiple Linux compute nodes.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Run NAMD with Microsoft HPC Pack on Linux compute nodes in Azure</source>
          <target state="new">Run NAMD with Microsoft HPC Pack on Linux compute nodes in Azure</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>This article shows you how to deploy a Microsoft HPC Pack cluster on Azure and run a <bpt id="p1">[</bpt>NAMD<ept id="p1">](http://www.ks.uiuc.edu/Research/namd/)</ept> job with <bpt id="p2">**</bpt>charmrun<ept id="p2">**</ept> on multiple Linux compute nodes in a virtual cluster network to calculate and visualize the structure of a large biomolecular system.</source>
          <target state="new">This article shows you how to deploy a Microsoft HPC Pack cluster on Azure and run a <bpt id="p1">[</bpt>NAMD<ept id="p1">](http://www.ks.uiuc.edu/Research/namd/)</ept> job with <bpt id="p2">**</bpt>charmrun<ept id="p2">**</ept> on multiple Linux compute nodes in a virtual cluster network to calculate and visualize the structure of a large biomolecular system.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>NAMD (for Nanoscale Molecular Dynamics program) is a parallel molecular dynamics package designed for high-performance simulation of large biomolecular systems containing up to millions of atoms, such as viruses, cell structures, and large proteins.</source>
          <target state="new">NAMD (for Nanoscale Molecular Dynamics program) is a parallel molecular dynamics package designed for high-performance simulation of large biomolecular systems containing up to millions of atoms, such as viruses, cell structures, and large proteins.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>NAMD scales to hundreds of cores for typical simulations and to more than 500,000 cores for the largest simulations.</source>
          <target state="new">NAMD scales to hundreds of cores for typical simulations and to more than 500,000 cores for the largest simulations.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Microsoft HPC Pack provides features to run a variety of large-scale HPC and parallel applications, including MPI applications, on clusters of Microsoft Azure virtual machines.</source>
          <target state="new">Microsoft HPC Pack provides features to run a variety of large-scale HPC and parallel applications, including MPI applications, on clusters of Microsoft Azure virtual machines.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Starting in Microsoft HPC Pack 2012 R2, HPC Pack also supports running Linux HPC applications on Linux compute node VMs deployed in an HPC Pack cluster.</source>
          <target state="new">Starting in Microsoft HPC Pack 2012 R2, HPC Pack also supports running Linux HPC applications on Linux compute node VMs deployed in an HPC Pack cluster.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id="p1">](virtual-machines-linux-cluster-hpcpack.md)</ept> for an introduction to using Linux compute nodes with HPC Pack.</source>
          <target state="new">See <bpt id="p1">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id="p1">](virtual-machines-linux-cluster-hpcpack.md)</ept> for an introduction to using Linux compute nodes with HPC Pack.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>HPC Pack cluster with Linux compute nodes<ept id="p1">**</ept> - See <bpt id="p2">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id="p2">](virtual-machines-linux-cluster-hpcpack.md)</ept> for the prerequisites and steps to deploy an HPC Pack cluster with Linux compute nodes on Azure by using an Azure PowerShell script and HPC Pack images in the Azure Marketplace.</source>
          <target state="new"><bpt id="p1">**</bpt>HPC Pack cluster with Linux compute nodes<ept id="p1">**</ept> - See <bpt id="p2">[</bpt>Get started with Linux compute nodes in an HPC Pack cluster in Azure<ept id="p2">](virtual-machines-linux-cluster-hpcpack.md)</ept> for the prerequisites and steps to deploy an HPC Pack cluster with Linux compute nodes on Azure by using an Azure PowerShell script and HPC Pack images in the Azure Marketplace.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Following is a sample XML configuration file you can use with the script to deploy an Azure-based HPC Pack cluster consisting of a Windows Server 2012 R2 head node and 4 size Large (A3) CentOS 6.6 compute nodes.</source>
          <target state="new">Following is a sample XML configuration file you can use with the script to deploy an Azure-based HPC Pack cluster consisting of a Windows Server 2012 R2 head node and 4 size Large (A3) CentOS 6.6 compute nodes.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Substitute appropriate values for your subscription and service names.</source>
          <target state="new">Substitute appropriate values for your subscription and service names.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>NAMD software and tutorial files<ept id="p1">**</ept> - Download NAMD software for Linux from the <bpt id="p2">[</bpt>NAMD<ept id="p2">](http://www.ks.uiuc.edu/Research/namd/)</ept> site.</source>
          <target state="new"><bpt id="p1">**</bpt>NAMD software and tutorial files<ept id="p1">**</ept> - Download NAMD software for Linux from the <bpt id="p2">[</bpt>NAMD<ept id="p2">](http://www.ks.uiuc.edu/Research/namd/)</ept> site.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>This article is based on NAMD version 2.10, and uses the <bpt id="p1">[</bpt>Linux-x86_64 (64-bit Intel/AMD with Ethernet)<ept id="p1">](http://www.ks.uiuc.edu/Development/Download/download.cgi?UserID=&amp;AccessCode=&amp;ArchiveID=1310)</ept> archive, which you'll use to run NAMD on multiple Linux compute nodes in a cluster network.</source>
          <target state="new">This article is based on NAMD version 2.10, and uses the <bpt id="p1">[</bpt>Linux-x86_64 (64-bit Intel/AMD with Ethernet)<ept id="p1">](http://www.ks.uiuc.edu/Development/Download/download.cgi?UserID=&amp;AccessCode=&amp;ArchiveID=1310)</ept> archive, which you'll use to run NAMD on multiple Linux compute nodes in a cluster network.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Also download the <bpt id="p1">[</bpt>NAMD tutorial files<ept id="p1">](http://www.ks.uiuc.edu/Training/Tutorials/#namd)</ept>.</source>
          <target state="new">Also download the <bpt id="p1">[</bpt>NAMD tutorial files<ept id="p1">](http://www.ks.uiuc.edu/Training/Tutorials/#namd)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Follow the instructions later in this article to extract the archive and the tutorial samples on the cluster head node.</source>
          <target state="new">Follow the instructions later in this article to extract the archive and the tutorial samples on the cluster head node.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>VMD<ept id="p1">**</ept> (optional) - To see the results of your NAMD job, download and install the molecular visualization program <bpt id="p2">[</bpt>VMD<ept id="p2">](http://www.ks.uiuc.edu/Research/vmd/)</ept> on a computer of your choice.</source>
          <target state="new"><bpt id="p1">**</bpt>VMD<ept id="p1">**</ept> (optional) - To see the results of your NAMD job, download and install the molecular visualization program <bpt id="p2">[</bpt>VMD<ept id="p2">](http://www.ks.uiuc.edu/Research/vmd/)</ept> on a computer of your choice.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The current version is 1.9.2.</source>
          <target state="new">The current version is 1.9.2.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>See the VMD download site to get started.</source>
          <target state="new">See the VMD download site to get started.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Set up mutual trust between compute nodes</source>
          <target state="new">Set up mutual trust between compute nodes</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Running a cross-node job on multiple Linux nodes requires the nodes to trust each other (by <bpt id="p1">**</bpt>rsh<ept id="p1">**</ept> or <bpt id="p2">**</bpt>ssh<ept id="p2">**</ept>).</source>
          <target state="new">Running a cross-node job on multiple Linux nodes requires the nodes to trust each other (by <bpt id="p1">**</bpt>rsh<ept id="p1">**</ept> or <bpt id="p2">**</bpt>ssh<ept id="p2">**</ept>).</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>When you create the HPC Pack cluster with the Microsoft HPC Pack IaaS deployment script, the script automatically sets up permanent mutual trust for the administrator account you specify.</source>
          <target state="new">When you create the HPC Pack cluster with the Microsoft HPC Pack IaaS deployment script, the script automatically sets up permanent mutual trust for the administrator account you specify.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>For non-administrator users you create in the cluster's domain, you have to set up temporary mutual trust among the nodes when a job is allocated to them, and destroy the relationship after the job is complete.</source>
          <target state="new">For non-administrator users you create in the cluster's domain, you have to set up temporary mutual trust among the nodes when a job is allocated to them, and destroy the relationship after the job is complete.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>To do this for each user, provide an RSA key pair to the cluster which HPC Pack uses to establish the trust relationship.</source>
          <target state="new">To do this for each user, provide an RSA key pair to the cluster which HPC Pack uses to establish the trust relationship.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Generate an RSA key pair</source>
          <target state="new">Generate an RSA key pair</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>It's easy to generate an RSA key pair, which contains a public key and a private key, by running the Linux <bpt id="p1">**</bpt>ssh-keygen<ept id="p1">**</ept> command.</source>
          <target state="new">It's easy to generate an RSA key pair, which contains a public key and a private key, by running the Linux <bpt id="p1">**</bpt>ssh-keygen<ept id="p1">**</ept> command.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Log on to a Linux computer.</source>
          <target state="new">Log on to a Linux computer.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Run the following command.</source>
          <target state="new">Run the following command.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Press <bpt id="p1">**</bpt>Enter<ept id="p1">**</ept> to use the default settings until the command is completed.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Press <bpt id="p1">**</bpt>Enter<ept id="p1">**</ept> to use the default settings until the command is completed.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Do not enter a passphrase here; when prompted for a password, just press <bpt id="p1">**</bpt>Enter<ept id="p1">**</ept>.</source>
          <target state="new">Do not enter a passphrase here; when prompted for a password, just press <bpt id="p1">**</bpt>Enter<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>![Generate an RSA key pair][keygen]</source>
          <target state="new">![Generate an RSA key pair][keygen]</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Change directory to the ~/.ssh directory.</source>
          <target state="new">Change directory to the ~/.ssh directory.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The private key is stored in id_rsa and the public key in id_rsa.pub.</source>
          <target state="new">The private key is stored in id_rsa and the public key in id_rsa.pub.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>![Private and public keys][keys]</source>
          <target state="new">![Private and public keys][keys]</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Add the key pair to the HPC Pack cluster</source>
          <target state="new">Add the key pair to the HPC Pack cluster</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Make a Remote Desktop connnection to your head node with your HPC Pack administrator account (the administrator account you set up when you ran the deployment script).</source>
          <target state="new">Make a Remote Desktop connnection to your head node with your HPC Pack administrator account (the administrator account you set up when you ran the deployment script).</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Use standard Windows Server procedures to create a domain user account in the cluster's Active Directory domain.</source>
          <target state="new">Use standard Windows Server procedures to create a domain user account in the cluster's Active Directory domain.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>For example, use the Active Directory User and Computers tool on the head node.</source>
          <target state="new">For example, use the Active Directory User and Computers tool on the head node.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>The examples in this article assume you create a domain user named hpclab\hpcuser.</source>
          <target state="new">The examples in this article assume you create a domain user named hpclab\hpcuser.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Create a file named C:\cred.xml and copy the RSA key data into it.</source>
          <target state="new">Create a file named C:\cred.xml and copy the RSA key data into it.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>You can find an example of this file in the Appendix at the end of this article.</source>
          <target state="new">You can find an example of this file in the Appendix at the end of this article.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Open a Command window and enter the following command to set the credentials data for the hpclab\hpcuser account.</source>
          <target state="new">Open a Command window and enter the following command to set the credentials data for the hpclab\hpcuser account.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>You use the <bpt id="p1">**</bpt>extendeddata<ept id="p1">**</ept> parameter to pass the name of C:\cred.xml file you created for the key data.</source>
          <target state="new">You use the <bpt id="p1">**</bpt>extendeddata<ept id="p1">**</ept> parameter to pass the name of C:\cred.xml file you created for the key data.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>This command completes successfully without output.</source>
          <target state="new">This command completes successfully without output.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>After setting the credentials for the user accounts you need to run jobs, store the cred.xml file in a secure location, or delete it.</source>
          <target state="new">After setting the credentials for the user accounts you need to run jobs, store the cred.xml file in a secure location, or delete it.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>If you generated the RSA key pair on one of your Linux nodes, remember to delete the keys after you finish using them.</source>
          <target state="new">If you generated the RSA key pair on one of your Linux nodes, remember to delete the keys after you finish using them.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>HPC Pack does not set up mutual trust if it finds an existing id_rsa file or id_rsa.pub file.</source>
          <target state="new">HPC Pack does not set up mutual trust if it finds an existing id_rsa file or id_rsa.pub file.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.IMPORTANT]</ph> We don’t recommend running a Linux job as a cluster administrator on a shared cluster, because a job submitted by an administrator runs under the root account on the Linux nodes.</source>
          <target state="new"><ph id="ph1">[AZURE.IMPORTANT]</ph> We don’t recommend running a Linux job as a cluster administrator on a shared cluster, because a job submitted by an administrator runs under the root account on the Linux nodes.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>A job submitted by a non-administrator user runs under a local Linux user account with the same name as the job user, and HPC Pack sets up mutual trust for this Linux user across all the nodes allocated to the job.</source>
          <target state="new">A job submitted by a non-administrator user runs under a local Linux user account with the same name as the job user, and HPC Pack sets up mutual trust for this Linux user across all the nodes allocated to the job.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>You can set up the Linux user manually on the Linux nodes before running the job, or HPC Pack creates the user automatically when the job is submitted.</source>
          <target state="new">You can set up the Linux user manually on the Linux nodes before running the job, or HPC Pack creates the user automatically when the job is submitted.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>If HPC Pack creates the user, HPC Pack deletes it after the job completes.</source>
          <target state="new">If HPC Pack creates the user, HPC Pack deletes it after the job completes.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>The keys are removed after job completion on the nodes to reduce security threats.</source>
          <target state="new">The keys are removed after job completion on the nodes to reduce security threats.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Set up a file share for Linux nodes</source>
          <target state="new">Set up a file share for Linux nodes</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Now set up a standard SMB share on a folder on the head node, and mount the shared folder on all Linux nodes to allow the Linux nodes to access NAMD files with a common path.</source>
          <target state="new">Now set up a standard SMB share on a folder on the head node, and mount the shared folder on all Linux nodes to allow the Linux nodes to access NAMD files with a common path.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>See the file sharing options and steps in <bpt id="p1">[</bpt>Get started with Linux compute nodes in an HPC Pack Cluster in Azure<ept id="p1">](virtual-machines-linux-cluster-hpcpack.md)</ept>.</source>
          <target state="new">See the file sharing options and steps in <bpt id="p1">[</bpt>Get started with Linux compute nodes in an HPC Pack Cluster in Azure<ept id="p1">](virtual-machines-linux-cluster-hpcpack.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>(We recommend mounting a shared folder on the head node in this article because CentOS 6.6 Linux nodes don’t currently support the Azure File service, which provides similar features.</source>
          <target state="new">(We recommend mounting a shared folder on the head node in this article because CentOS 6.6 Linux nodes don’t currently support the Azure File service, which provides similar features.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>For more about mounting an Azure File share, see <bpt id="p1">[</bpt>Persisting connections to Microsoft Azure Files<ept id="p1">](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/27/persisting-connections-to-microsoft-azure-files.aspx)</ept>.)</source>
          <target state="new">For more about mounting an Azure File share, see <bpt id="p1">[</bpt>Persisting connections to Microsoft Azure Files<ept id="p1">](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/27/persisting-connections-to-microsoft-azure-files.aspx)</ept>.)</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Create a folder on the head node, and share it to everyone by setting Read/Write privileges.</source>
          <target state="new">Create a folder on the head node, and share it to everyone by setting Read/Write privileges.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>In this example, \\\\CentOS66HN\Namd is the name of the folder, where CentOS66HN is the host name of the head node.</source>
          <target state="new">In this example, \\\\CentOS66HN\Namd is the name of the folder, where CentOS66HN is the host name of the head node.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Extract the NAMD files in the folder by using a Windows version of <bpt id="p1">**</bpt>tar<ept id="p1">**</ept> or another Windows utility that operates on .tar archives.</source>
          <target state="new">Extract the NAMD files in the folder by using a Windows version of <bpt id="p1">**</bpt>tar<ept id="p1">**</ept> or another Windows utility that operates on .tar archives.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Extract the NAMD tar archive to \\\\CentOS66HN\Namd\namd2, and extract the tutorial files under \\\\CentOS66HN\Namd\namd2\namdsample.</source>
          <target state="new">Extract the NAMD tar archive to \\\\CentOS66HN\Namd\namd2, and extract the tutorial files under \\\\CentOS66HN\Namd\namd2\namdsample.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Open a Windows PowerShell window and run the following commands to mount the shared folder.</source>
          <target state="new">Open a Windows PowerShell window and run the following commands to mount the shared folder.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The first command creates a folder named /namd2 on all nodes in the LinuxNodes group.</source>
          <target state="new">The first command creates a folder named /namd2 on all nodes in the LinuxNodes group.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The second command mounts the shared folder //CentOS66HN/Namd/namd2 onto the folder with dir_mode and file_mode bits set to 777.</source>
          <target state="new">The second command mounts the shared folder //CentOS66HN/Namd/namd2 onto the folder with dir_mode and file_mode bits set to 777.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>username<ept id="p1">*</ept> and <bpt id="p2">*</bpt>password<ept id="p2">*</ept> in the command should be the credentials of a user on the head node.</source>
          <target state="new">The <bpt id="p1">*</bpt>username<ept id="p1">*</ept> and <bpt id="p2">*</bpt>password<ept id="p2">*</ept> in the command should be the credentials of a user on the head node.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph>The “\`” symbol in the second command is an escape symbol for PowerShell.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph>The “\`” symbol in the second command is an escape symbol for PowerShell.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>“\`,” means the “,” (comma character) is a part of the command.</source>
          <target state="new">“\`,” means the “,” (comma character) is a part of the command.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Prepare to run a NAMD job</source>
          <target state="new">Prepare to run a NAMD job</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Your  NAMD job needs a <bpt id="p1">*</bpt>nodelist<ept id="p1">*</ept> file for <bpt id="p2">**</bpt>charmrun<ept id="p2">**</ept> to know the number of nodes to use when starting NAMD processes.</source>
          <target state="new">Your  NAMD job needs a <bpt id="p1">*</bpt>nodelist<ept id="p1">*</ept> file for <bpt id="p2">**</bpt>charmrun<ept id="p2">**</ept> to know the number of nodes to use when starting NAMD processes.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>You'll write a Bash script that generates the nodelist file and runs <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> with this nodelist file.</source>
          <target state="new">You'll write a Bash script that generates the nodelist file and runs <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> with this nodelist file.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>You can then submit a NAMD job in HPC Cluster Manager that calls this script.</source>
          <target state="new">You can then submit a NAMD job in HPC Cluster Manager that calls this script.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Environment variables and nodelist file</source>
          <target state="new">Environment variables and nodelist file</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Information about nodes and cores is in the $CCP_NODES_CORES environment variable, which is automatically set by the HPC Pack head node when the job is activated.</source>
          <target state="new">Information about nodes and cores is in the $CCP_NODES_CORES environment variable, which is automatically set by the HPC Pack head node when the job is activated.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>The format for the $CCP_NODES_CORES variable is as follows:</source>
          <target state="new">The format for the $CCP_NODES_CORES variable is as follows:</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>This lists the total number of nodes, node names, and number of cores on each node that are allocated to the job.</source>
          <target state="new">This lists the total number of nodes, node names, and number of cores on each node that are allocated to the job.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>For example, if the job needs 10 cores to run, the value of $CCP_NODES_CORES will be similar to:</source>
          <target state="new">For example, if the job needs 10 cores to run, the value of $CCP_NODES_CORES will be similar to:</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Following is the information in the nodelist file, which the script will generate:</source>
          <target state="new">Following is the information in the nodelist file, which the script will generate:</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Bash script to create a nodelist file</source>
          <target state="new">Bash script to create a nodelist file</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Using a text editor of your choice, create the following Bash script in the folder containing the NAMD program files and name it hpccharmrun.sh.</source>
          <target state="new">Using a text editor of your choice, create the following Bash script in the folder containing the NAMD program files and name it hpccharmrun.sh.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>A complete sample of this file is in the Appendix of this article.</source>
          <target state="new">A complete sample of this file is in the Appendix of this article.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>This bash script does the following things.</source>
          <target state="new">This bash script does the following things.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.TIP]</ph> Save your script as a text file with Linux line endings (LF only, not CR LF).</source>
          <target state="new"><ph id="ph1">[AZURE.TIP]</ph> Save your script as a text file with Linux line endings (LF only, not CR LF).</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>This ensures that it runs properly on the Linux nodes.</source>
          <target state="new">This ensures that it runs properly on the Linux nodes.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Define some variables.</source>
          <target state="new">Define some variables.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Get node information from the environment variables.</source>
          <target state="new">Get node information from the environment variables.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>$NODESCORES stores a list of split words from $CCP_NODES_CORES.</source>
          <target state="new">$NODESCORES stores a list of split words from $CCP_NODES_CORES.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>$COUNT is the size of $NODESCORES.</source>
          <target state="new">$COUNT is the size of $NODESCORES.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>If the $CCP_NODES_CORES variable is not set, just start <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> directly.</source>
          <target state="new">If the $CCP_NODES_CORES variable is not set, just start <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> directly.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>(This should only occur when you run this script directly on your Linux nodes.)</source>
          <target state="new">(This should only occur when you run this script directly on your Linux nodes.)</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Or create a nodelist file for <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept>.</source>
          <target state="new">Or create a nodelist file for <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Run <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> with the nodelist file, get its return status, and remove the nodelist file at the end.</source>
          <target state="new">Run <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> with the nodelist file, get its return status, and remove the nodelist file at the end.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>${CCP_NUMCPUS} is another environment variable set by the HPC Pack head node.</source>
          <target state="new">${CCP_NUMCPUS} is another environment variable set by the HPC Pack head node.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>It stores the number of total cores allocated to this job.</source>
          <target state="new">It stores the number of total cores allocated to this job.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>We use it to specify the number of processes for charmrun.</source>
          <target state="new">We use it to specify the number of processes for charmrun.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Exit with the <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> return status.</source>
          <target state="new">Exit with the <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> return status.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Submit a NAMD job</source>
          <target state="new">Submit a NAMD job</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>Now you are ready to submit a NAMD job in HPC Cluster Manager.</source>
          <target state="new">Now you are ready to submit a NAMD job in HPC Cluster Manager.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Connect to your cluster head node and start HPC Cluster Manager.</source>
          <target state="new">Connect to your cluster head node and start HPC Cluster Manager.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>In <bpt id="p1">**</bpt>Node Management<ept id="p1">**</ept>, ensure that the Linux compute nodes are in the <bpt id="p2">**</bpt>Online<ept id="p2">**</ept> state.</source>
          <target state="new">In <bpt id="p1">**</bpt>Node Management<ept id="p1">**</ept>, ensure that the Linux compute nodes are in the <bpt id="p2">**</bpt>Online<ept id="p2">**</ept> state.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>If they are not, select them and click <bpt id="p1">**</bpt>Bring Online<ept id="p1">**</ept>.</source>
          <target state="new">If they are not, select them and click <bpt id="p1">**</bpt>Bring Online<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>In <bpt id="p1">**</bpt>Job Management<ept id="p1">**</ept>, click <bpt id="p2">**</bpt>New Job<ept id="p2">**</ept>.</source>
          <target state="new">In <bpt id="p1">**</bpt>Job Management<ept id="p1">**</ept>, click <bpt id="p2">**</bpt>New Job<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Enter a name for job such as <bpt id="p1">*</bpt>hpccharmrun<ept id="p1">*</ept>.</source>
          <target state="new">Enter a name for job such as <bpt id="p1">*</bpt>hpccharmrun<ept id="p1">*</ept>.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>![New HPC job][namd_job]</source>
          <target state="new">![New HPC job][namd_job]</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p1">**</bpt>Job Details<ept id="p1">**</ept> page, under <bpt id="p2">**</bpt>Job Resources<ept id="p2">**</ept>, select the type of resource as <bpt id="p3">**</bpt>Node<ept id="p3">**</ept> and set the <bpt id="p4">**</bpt>Minimum<ept id="p4">**</ept> to 3.</source>
          <target state="new">On the <bpt id="p1">**</bpt>Job Details<ept id="p1">**</ept> page, under <bpt id="p2">**</bpt>Job Resources<ept id="p2">**</ept>, select the type of resource as <bpt id="p3">**</bpt>Node<ept id="p3">**</ept> and set the <bpt id="p4">**</bpt>Minimum<ept id="p4">**</ept> to 3.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>In this example we'll run the job on 3 Linux nodes and each node has 4 cores.</source>
          <target state="new">In this example we'll run the job on 3 Linux nodes and each node has 4 cores.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>![Job resources][job_resources]</source>
          <target state="new">![Job resources][job_resources]</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p1">**</bpt>Task Details and I/O Redirection<ept id="p1">**</ept> page, add a new task to the job and set the following values.</source>
          <target state="new">On the <bpt id="p1">**</bpt>Task Details and I/O Redirection<ept id="p1">**</ept> page, add a new task to the job and set the following values.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Command line<ept id="p1">**</ept> -</source>
          <target state="new"><bpt id="p1">**</bpt>Command line<ept id="p1">**</ept> -</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Working directory<ept id="p1">**</ept> - /namd2</source>
          <target state="new"><bpt id="p1">**</bpt>Working directory<ept id="p1">**</ept> - /namd2</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Minimum<ept id="p1">**</ept> - 3</source>
          <target state="new"><bpt id="p1">**</bpt>Minimum<ept id="p1">**</ept> - 3</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>![Task details][task_details]</source>
          <target state="new">![Task details][task_details]</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> You set the working directory here because <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> tries to navigate to the same working directory on each node.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> You set the working directory here because <bpt id="p1">**</bpt>charmrun<ept id="p1">**</ept> tries to navigate to the same working directory on each node.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>If the working directory isn't set, HPC Pack starts the command in a randomly named folder created on one of the Linux nodes.</source>
          <target state="new">If the working directory isn't set, HPC Pack starts the command in a randomly named folder created on one of the Linux nodes.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>This causes the following error on the other nodes:</source>
          <target state="new">This causes the following error on the other nodes:</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source><ph id="ph1">`/bin/bash: line 37: cd: /tmp/nodemanager_task_94_0.mFlQSN: No such file or directory.`</ph> To avoid this, specify a folder path which can be accessed by all nodes as the working directory.</source>
          <target state="new"><ph id="ph1">`/bin/bash: line 37: cd: /tmp/nodemanager_task_94_0.mFlQSN: No such file or directory.`</ph> To avoid this, specify a folder path which can be accessed by all nodes as the working directory.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p1">**</bpt>Submit<ept id="p1">**</ept> to run this job.</source>
          <target state="new">Click <bpt id="p1">**</bpt>Submit<ept id="p1">**</ept> to run this job.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>By default, HPC Pack submits the job as your current logged-on user account.</source>
          <target state="new">By default, HPC Pack submits the job as your current logged-on user account.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>A dialog box might prompt you to enter the user name and password after you click <bpt id="p1">**</bpt>Submit<ept id="p1">**</ept>.</source>
          <target state="new">A dialog box might prompt you to enter the user name and password after you click <bpt id="p1">**</bpt>Submit<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>![Job credentials][creds]</source>
          <target state="new">![Job credentials][creds]</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>Under some conditions HPC Pack remembers the user information you input before and won’t show this dialog box.</source>
          <target state="new">Under some conditions HPC Pack remembers the user information you input before and won’t show this dialog box.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>To make HPC Pack show it again, enter the following in a Command window and then submit the job.</source>
          <target state="new">To make HPC Pack show it again, enter the following in a Command window and then submit the job.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>The job takes several minutes to finish.</source>
          <target state="new">The job takes several minutes to finish.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>Find the job log at \\</source>
          <target state="new">Find the job log at \\</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>\Namd\namd2\namd2_hpccharmrun.log and the output files in \\</source>
          <target state="new">\Namd\namd2\namd2_hpccharmrun.log and the output files in \\</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>\Namd\namd2\namdsample\1-2-sphere\.</source>
          <target state="new">\Namd\namd2\namdsample\1-2-sphere\.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Optionally, start VMD to view your job results.</source>
          <target state="new">Optionally, start VMD to view your job results.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>The steps for visualizing the NAMD output files (in this case, a ubiquitin protein molecule in a water sphere) are beyond the scope of this article.</source>
          <target state="new">The steps for visualizing the NAMD output files (in this case, a ubiquitin protein molecule in a water sphere) are beyond the scope of this article.</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>NAMD Tutorial<ept id="p1">](http://www.life.illinois.edu/emad/biop590c/namd-tutorial-unix-590C.pdf)</ept> for details.</source>
          <target state="new">See <bpt id="p1">[</bpt>NAMD Tutorial<ept id="p1">](http://www.life.illinois.edu/emad/biop590c/namd-tutorial-unix-590C.pdf)</ept> for details.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>![Job results][vmd_view]</source>
          <target state="new">![Job results][vmd_view]</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>Appendix</source>
          <target state="new">Appendix</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>Sample hpccharmrun.sh script</source>
          <target state="new">Sample hpccharmrun.sh script</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>Sample cred.xml file</source>
          <target state="new">Sample cred.xml file</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">e8aa118ae92ec16b0d9478b035a3fc10e0cbfd28</xliffext:olfilehash>
  </header>
</xliff>