{
  "nodes": [
    {
      "content": "Common Scenarios for using Azure Data Factory",
      "pos": [
        28,
        73
      ]
    },
    {
      "content": "Learn about a few common scenarios for using the Azure Data Factory service",
      "pos": [
        93,
        168
      ]
    },
    {
      "content": "Common scenarios for using Azure Data Factory",
      "pos": [
        500,
        545
      ]
    },
    {
      "content": "This section describes a few example scenarios that the Azure Data Factory can support today, and will continue to grow as Hub scenarios.",
      "pos": [
        546,
        683
      ]
    },
    {
      "pos": [
        687,
        820
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Read through the <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">][datafactory-introduction]</ept> article before reading through this one."
    },
    {
      "content": "Scenario #1: Data Sources for the Data Hub",
      "pos": [
        827,
        869
      ]
    },
    {
      "content": "Source the Data Hub",
      "pos": [
        872,
        891
      ]
    },
    {
      "content": "Enterprises have data of disparate types located in disparate sources.",
      "pos": [
        953,
        1023
      ]
    },
    {
      "content": "The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed for subsequent processing.",
      "pos": [
        1025,
        1260
      ]
    },
    {
      "content": "Without Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.",
      "pos": [
        1262,
        1411
      ]
    },
    {
      "content": "This is expensive, and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.",
      "pos": [
        1413,
        1602
      ]
    },
    {
      "content": "With Azure Data Factory data storage and processing services are collected into a Hub container which facilitates and optimizes computation and storage activities, enables unified resource consumption management, and provides services for data movement as-needed.",
      "pos": [
        1606,
        1869
      ]
    },
    {
      "content": "Scenario #2: Operationalize Information Production",
      "pos": [
        1873,
        1923
      ]
    },
    {
      "content": "Operationalization scenarios are the next logical step after data sourcing scenarios.",
      "pos": [
        1924,
        2009
      ]
    },
    {
      "content": "Once data is present in a Hub, you want to author and operationalize data pipelines to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.",
      "pos": [
        2010,
        2223
      ]
    },
    {
      "content": "Data transformation in Azure Data Factory is through Hive, Pig and custom C# processing running on Hadoop (Azure HDInsight).",
      "pos": [
        2225,
        2349
      ]
    },
    {
      "content": "These transformations can be used to clean data, mask critical data fields, and perform other operations on the data in a wide variety of complex ways.",
      "pos": [
        2351,
        2502
      ]
    },
    {
      "content": "Ordinarily, operationalization is achieved with complex and hard to maintain infrastructure and custom services, and poses a number of challenges for implementation, management, scaling, troubleshooting, and versioning such a solution.",
      "pos": [
        2504,
        2739
      ]
    },
    {
      "content": "With Data Factory as a fully managed service, users can operationalize these pipelines by defining them with one-time or complex recurring schedules, and orchestration is handled directly by the Data Factory service.",
      "pos": [
        2743,
        2959
      ]
    },
    {
      "content": "With Hubs, cluster management for all of the data and processing within a Hub is handled on behalf of the user, so users can focus on transformative analytics instead on infrastructure management.",
      "pos": [
        2961,
        3157
      ]
    },
    {
      "content": "Azure Data Factory removes the challenges of working with brittle custom services, and enables enterprises to produce trusted information reliably and reproducibly.",
      "pos": [
        3159,
        3323
      ]
    },
    {
      "content": "Scenario #3:  Integrate Information Production with data discovery",
      "pos": [
        3328,
        3394
      ]
    },
    {
      "content": "Traditional BI approaches and technologies, while providing an “authoritative source of the truth”, almost always have a serious side effect: a constant backlog of requests due to a carefully controlled change request process.",
      "pos": [
        3395,
        3621
      ]
    },
    {
      "content": "To adapt to quickly changing business questions, there is a need for greater flexibility for enterprises to connect their information production systems with their information consumption systems.",
      "pos": [
        3623,
        3819
      ]
    },
    {
      "content": "Azure Data Factory helps address the challenge of connecting these systems with streamlined data pipelines for information production, and the information consumption challenge by making up-to-date trusted data available in easily consumable forms.",
      "pos": [
        3821,
        4069
      ]
    },
    {
      "content": "Azure Data Factory supports the following capabilities to enable simple consumption of the data produced:",
      "pos": [
        4073,
        4178
      ]
    },
    {
      "content": "Easily move (one time or scheduled) the produced data assets to relational data marts for consumption using existing BI tools (Excel, Tableau, etc…).",
      "pos": [
        4182,
        4331
      ]
    },
    {
      "content": "Consume data assets produced by a data factory directly using Power Query in Excel.",
      "pos": [
        4334,
        4417
      ]
    },
    {
      "content": "See the following topics for consuming data using Power Query:",
      "pos": [
        4419,
        4481
      ]
    },
    {
      "content": "Power Query: Connect to Microsoft Azure Table Storage",
      "pos": [
        4487,
        4540
      ]
    },
    {
      "content": "Power Query: Connect to Microsoft Azure Blob Storage",
      "pos": [
        4570,
        4622
      ]
    },
    {
      "content": "Power Query: Connect to Microsoft Azure SQL Database",
      "pos": [
        4651,
        4703
      ]
    },
    {
      "content": "Power Query: Connect to Microsoft On-premises SQL Server",
      "pos": [
        4731,
        4787
      ]
    },
    {
      "content": "test",
      "pos": [
        6262,
        6266
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Common Scenarios for using Azure Data Factory\" \n    description=\"Learn about a few common scenarios for using the Azure Data Factory service\" \n    services=\"data-factory\"     \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/07/2015\" \n    ms.author=\"spelluru\"/>\n\n# Common scenarios for using Azure Data Factory\nThis section describes a few example scenarios that the Azure Data Factory can support today, and will continue to grow as Hub scenarios.\n\n> [AZURE.NOTE] Read through the [Introduction to Azure Data Factory][datafactory-introduction] article before reading through this one.   \n\n##Scenario #1: Data Sources for the Data Hub\n![Source the Data Hub][image-data-factory-introduction-secenario1-source-datahub]\n\nEnterprises have data of disparate types located in disparate sources.  The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed for subsequent processing.\n\nWithout Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.  This is expensive, and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.\n  \nWith Azure Data Factory data storage and processing services are collected into a Hub container which facilitates and optimizes computation and storage activities, enables unified resource consumption management, and provides services for data movement as-needed.\n\n##Scenario #2: Operationalize Information Production\nOperationalization scenarios are the next logical step after data sourcing scenarios. Once data is present in a Hub, you want to author and operationalize data pipelines to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.  Data transformation in Azure Data Factory is through Hive, Pig and custom C# processing running on Hadoop (Azure HDInsight).  These transformations can be used to clean data, mask critical data fields, and perform other operations on the data in a wide variety of complex ways.  Ordinarily, operationalization is achieved with complex and hard to maintain infrastructure and custom services, and poses a number of challenges for implementation, management, scaling, troubleshooting, and versioning such a solution.\n  \nWith Data Factory as a fully managed service, users can operationalize these pipelines by defining them with one-time or complex recurring schedules, and orchestration is handled directly by the Data Factory service.  With Hubs, cluster management for all of the data and processing within a Hub is handled on behalf of the user, so users can focus on transformative analytics instead on infrastructure management.  Azure Data Factory removes the challenges of working with brittle custom services, and enables enterprises to produce trusted information reliably and reproducibly.\n\n\n##Scenario #3:  Integrate Information Production with data discovery\nTraditional BI approaches and technologies, while providing an “authoritative source of the truth”, almost always have a serious side effect: a constant backlog of requests due to a carefully controlled change request process.  To adapt to quickly changing business questions, there is a need for greater flexibility for enterprises to connect their information production systems with their information consumption systems.  Azure Data Factory helps address the challenge of connecting these systems with streamlined data pipelines for information production, and the information consumption challenge by making up-to-date trusted data available in easily consumable forms.\n  \nAzure Data Factory supports the following capabilities to enable simple consumption of the data produced:\n\n- Easily move (one time or scheduled) the produced data assets to relational data marts for consumption using existing BI tools (Excel, Tableau, etc…).\n- Consume data assets produced by a data factory directly using Power Query in Excel.\n\nSee the following topics for consuming data using Power Query: \n\n- [Power Query: Connect to Microsoft Azure Table Storage][Power-Query-Azure-Table]\n- [Power Query: Connect to Microsoft Azure Blob Storage][Power-Query-Azure-Blob]\n- [Power Query: Connect to Microsoft Azure SQL Database][Power-Query-Azure-SQL]\n- [Power Query: Connect to Microsoft On-premises SQL Server][Power-Query-OnPrem-SQL] \n\n\n[Power-Query-Azure-Table]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azuretable-storage-HA104122607.aspx\n[Power-Query-Azure-Blob]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azure-blob-storage-HA104113447.aspx\n[Power-Query-Azure-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-microsoft-azure-sql-database-HA104019809.aspx\n[Power-Query-OnPrem-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-sql-server-database-HA104019808.aspx\n\n[copy-data-with-adf]: http://azure.microsoft.com/documentation/articles/data-factory-copy-activity/\n[use-pig-hive]: http://azure.microsoft.com/documentation/articles/data-factory-pig-hive-activities/\n[run-map-reduce]: http://azure.microsoft.com/documentation/articles/data-factory-map-reduce/\n[azure-ml-adf]: http://azure.microsoft.com/documentation/articles/data-factory-create-predictive-pipelines/\n\n[msdn-stored-procedure-activity]: https://msdn.microsoft.com/library/dn912649.aspx\n\n[adf-tutorial]: data-factory-tutorial.md\n[datafactory-getstarted]: data-factory-get-started.md\n[datafactory-introduction]: data-factory-introduction.md\n\n[image-data-factory-introduction-secenario1-source-datahub]:./media/data-factory-common-scenarios/Scenario1SourceDataHub.png\n\n[image-data-factory-introduction-secenario2-operationalize-infoproduction]:./media/data-factory-common-scenarios/Scenario2-OperationalizeInformationProduction.png\n\n\n\n \n\ntest\n"
}