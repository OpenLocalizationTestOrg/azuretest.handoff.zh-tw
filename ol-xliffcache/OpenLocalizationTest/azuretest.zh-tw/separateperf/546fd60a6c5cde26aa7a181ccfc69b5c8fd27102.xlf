<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Run the Hadoop samples in HDInsight | Microsoft Azure</source>
          <target state="new">Run the Hadoop samples in HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Get started using MapReduce samples with Linux-based HDInsight.</source>
          <target state="new">Get started using MapReduce samples with Linux-based HDInsight.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Use SSH to connect to the cluster, then use the Hadoop command to run sample jobs.</source>
          <target state="new">Use SSH to connect to the cluster, then use the Hadoop command to run sample jobs.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Run the Hadoop samples in HDInsight</source>
          <target state="new">Run the Hadoop samples in HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Linux-based HDInsight clusters provide a set of MapReduce samples that can be used to familiarize yourself with running Hadoop MapReduce jobs.</source>
          <target state="new">Linux-based HDInsight clusters provide a set of MapReduce samples that can be used to familiarize yourself with running Hadoop MapReduce jobs.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>In this document, you will learn about the available samples and walk through running a few of them.</source>
          <target state="new">In this document, you will learn about the available samples and walk through running a few of them.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>An Azure subscription<ept id="p1">**</ept>: See <bpt id="p2">[</bpt>Get Azure free trial<ept id="p2">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept></source>
          <target state="new"><bpt id="p1">**</bpt>An Azure subscription<ept id="p1">**</ept>: See <bpt id="p2">[</bpt>Get Azure free trial<ept id="p2">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept></target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>A Linux-based HDInsight cluster<ept id="p1">**</ept>: See <bpt id="p2">[</bpt>Get started using Hadoop with Hive in HDInsight on Linux<ept id="p2">](hdinsight-hadoop-linux-tutorial-get-started.md)</ept></source>
          <target state="new"><bpt id="p1">**</bpt>A Linux-based HDInsight cluster<ept id="p1">**</ept>: See <bpt id="p2">[</bpt>Get started using Hadoop with Hive in HDInsight on Linux<ept id="p2">](hdinsight-hadoop-linux-tutorial-get-started.md)</ept></target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>An SSH client<ept id="p1">**</ept>: For information on using SSH with HDInsight, see the following articles:</source>
          <target state="new"><bpt id="p1">**</bpt>An SSH client<ept id="p1">**</ept>: For information on using SSH with HDInsight, see the following articles:</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Windows</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Windows</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>The samples</source>
          <target state="new">The samples</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Location<ept id="p1">**</ept>: The samples are located on the HDInsight cluster at  <bpt id="p2">**</bpt>/usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar<ept id="p2">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>Location<ept id="p1">**</ept>: The samples are located on the HDInsight cluster at  <bpt id="p2">**</bpt>/usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar<ept id="p2">**</ept></target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Contents<ept id="p1">**</ept>: The following samples are contained in this archive:</source>
          <target state="new"><bpt id="p1">**</bpt>Contents<ept id="p1">**</ept>: The following samples are contained in this archive:</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>aggregatewordcount<ept id="p1">**</ept>: An Aggregate based map/reduce program that counts the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>aggregatewordcount<ept id="p1">**</ept>: An Aggregate based map/reduce program that counts the words in the input files</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>aggregatewordhist<ept id="p1">**</ept>: An Aggregate based map/reduce program that computes the histogram of the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>aggregatewordhist<ept id="p1">**</ept>: An Aggregate based map/reduce program that computes the histogram of the words in the input files</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>bbp<ept id="p1">**</ept>: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi</source>
          <target state="new"><bpt id="p1">**</bpt>bbp<ept id="p1">**</ept>: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>dbcount<ept id="p1">**</ept>: An example job that count the pageview counts from a database</source>
          <target state="new"><bpt id="p1">**</bpt>dbcount<ept id="p1">**</ept>: An example job that count the pageview counts from a database</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>distbbp<ept id="p1">**</ept>: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi</source>
          <target state="new"><bpt id="p1">**</bpt>distbbp<ept id="p1">**</ept>: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>grep<ept id="p1">**</ept>: A map/reduce program that counts the matches of a regex in the input</source>
          <target state="new"><bpt id="p1">**</bpt>grep<ept id="p1">**</ept>: A map/reduce program that counts the matches of a regex in the input</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>join<ept id="p1">**</ept>: A job that effects a join over sorted, equally partitioned datasets</source>
          <target state="new"><bpt id="p1">**</bpt>join<ept id="p1">**</ept>: A job that effects a join over sorted, equally partitioned datasets</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>multifilewc<ept id="p1">**</ept>: A job that counts words from several files</source>
          <target state="new"><bpt id="p1">**</bpt>multifilewc<ept id="p1">**</ept>: A job that counts words from several files</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>pentomino<ept id="p1">**</ept>: A map/reduce tile laying program to find solutions to pentomino problems</source>
          <target state="new"><bpt id="p1">**</bpt>pentomino<ept id="p1">**</ept>: A map/reduce tile laying program to find solutions to pentomino problems</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>pi<ept id="p1">**</ept>: A map/reduce program that estimates Pi using a quasi-Monte Carlo method</source>
          <target state="new"><bpt id="p1">**</bpt>pi<ept id="p1">**</ept>: A map/reduce program that estimates Pi using a quasi-Monte Carlo method</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>randomtextwriter<ept id="p1">**</ept>: A map/reduce program that writes 10GB of random textual data per node</source>
          <target state="new"><bpt id="p1">**</bpt>randomtextwriter<ept id="p1">**</ept>: A map/reduce program that writes 10GB of random textual data per node</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>randomwriter<ept id="p1">**</ept>: A map/reduce program that writes 10GB of random data per node</source>
          <target state="new"><bpt id="p1">**</bpt>randomwriter<ept id="p1">**</ept>: A map/reduce program that writes 10GB of random data per node</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>secondarysort<ept id="p1">**</ept>: An example defining a secondary sort to the reduce</source>
          <target state="new"><bpt id="p1">**</bpt>secondarysort<ept id="p1">**</ept>: An example defining a secondary sort to the reduce</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>sort<ept id="p1">**</ept>: A map/reduce program that sorts the data written by the random writer</source>
          <target state="new"><bpt id="p1">**</bpt>sort<ept id="p1">**</ept>: A map/reduce program that sorts the data written by the random writer</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>sudoku<ept id="p1">**</ept>: A sudoku solver</source>
          <target state="new"><bpt id="p1">**</bpt>sudoku<ept id="p1">**</ept>: A sudoku solver</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>teragen<ept id="p1">**</ept>: Generate data for the terasort</source>
          <target state="new"><bpt id="p1">**</bpt>teragen<ept id="p1">**</ept>: Generate data for the terasort</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>terasort<ept id="p1">**</ept>: Run the terasort</source>
          <target state="new"><bpt id="p1">**</bpt>terasort<ept id="p1">**</ept>: Run the terasort</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>teravalidate<ept id="p1">**</ept>: Checking results of terasort</source>
          <target state="new"><bpt id="p1">**</bpt>teravalidate<ept id="p1">**</ept>: Checking results of terasort</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>wordcount<ept id="p1">**</ept>: A map/reduce program that counts the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>wordcount<ept id="p1">**</ept>: A map/reduce program that counts the words in the input files</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>wordmean<ept id="p1">**</ept>: A map/reduce program that counts the average length of the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>wordmean<ept id="p1">**</ept>: A map/reduce program that counts the average length of the words in the input files</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>wordmedian<ept id="p1">**</ept>: A map/reduce program that counts the median length of the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>wordmedian<ept id="p1">**</ept>: A map/reduce program that counts the median length of the words in the input files</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>wordstandarddeviation<ept id="p1">**</ept>: A map/reduce program that counts the standard deviation of the length of the words in the input files</source>
          <target state="new"><bpt id="p1">**</bpt>wordstandarddeviation<ept id="p1">**</ept>: A map/reduce program that counts the standard deviation of the length of the words in the input files</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Source code<ept id="p1">**</ept>: Source code for these samples is included on the HDInsight cluster at <bpt id="p2">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples<ept id="p2">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>Source code<ept id="p1">**</ept>: Source code for these samples is included on the HDInsight cluster at <bpt id="p2">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples<ept id="p2">**</ept></target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The <ph id="ph2">`2.2.4.9-1`</ph> in the path is the version of the Hortonworks Data Platform for the HDInsight cluster, and may change as HDInsight is updated.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The <ph id="ph2">`2.2.4.9-1`</ph> in the path is the version of the Hortonworks Data Platform for the HDInsight cluster, and may change as HDInsight is updated.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>How to run the samples</source>
          <target state="new">How to run the samples</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Connect to HDInsight using SSH as described in the following articles:</source>
          <target state="new">Connect to HDInsight using SSH as described in the following articles:</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Windows</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Windows</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>From the <ph id="ph1">`username@headnode0:~$`</ph> prompt, use the following command to list the samples:</source>
          <target state="new">From the <ph id="ph1">`username@headnode0:~$`</ph> prompt, use the following command to list the samples:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>This will generate the list of sample from the previous section of this document.</source>
          <target state="new">This will generate the list of sample from the previous section of this document.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Use the following command to get help on a specific sample.</source>
          <target state="new">Use the following command to get help on a specific sample.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>In this case, the <bpt id="p1">**</bpt>wordcount<ept id="p1">**</ept> sample:</source>
          <target state="new">In this case, the <bpt id="p1">**</bpt>wordcount<ept id="p1">**</ept> sample:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>You should receive the following message:</source>
          <target state="new">You should receive the following message:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>This indicates that you can provide several input paths for the source documents, and the final path is where the output (count of words in the source documents,) will be located.</source>
          <target state="new">This indicates that you can provide several input paths for the source documents, and the final path is where the output (count of words in the source documents,) will be located.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Use the following to count all words in the Notebooks of Leonardo Da Vinci, which are provided as sample data with your cluster:</source>
          <target state="new">Use the following to count all words in the Notebooks of Leonardo Da Vinci, which are provided as sample data with your cluster:</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Input for this job is read from <bpt id="p1">**</bpt>wasb:///example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</source>
          <target state="new">Input for this job is read from <bpt id="p1">**</bpt>wasb:///example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Output for this example will be stored in <bpt id="p1">**</bpt>wasb:///example/data/davinciwordcount<ept id="p1">**</ept>.</source>
          <target state="new">Output for this example will be stored in <bpt id="p1">**</bpt>wasb:///example/data/davinciwordcount<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> As noted in the help for the wordcount sample, you could also specify multiple input files.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> As noted in the help for the wordcount sample, you could also specify multiple input files.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph1">`hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount`</ph> would count words in both davinci.txt and ulysses.txt.</source>
          <target state="new">For example, <ph id="ph1">`hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount`</ph> would count words in both davinci.txt and ulysses.txt.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Once the job completes, use the following command to view the output:</source>
          <target state="new">Once the job completes, use the following command to view the output:</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>This will concatenate all the output files produced by the job, and display them.</source>
          <target state="new">This will concatenate all the output files produced by the job, and display them.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>For this basic example there is only one file, however if there were more this command would iterate over all of them.</source>
          <target state="new">For this basic example there is only one file, however if there were more this command would iterate over all of them.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>You will see output similar to the following from this command:</source>
          <target state="new">You will see output similar to the following from this command:</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Each line represents a word and how many times it occured in the input data.</source>
          <target state="new">Each line represents a word and how many times it occured in the input data.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Sudoku</source>
          <target state="new">Sudoku</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>The Sudoku example has somewhat unhelpful usage instructions; "Include a puzzle on the command line."</source>
          <target state="new">The Sudoku example has somewhat unhelpful usage instructions; "Include a puzzle on the command line."</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Sudoku<ept id="p1">](https://en.wikipedia.org/wiki/Sudoku)</ept> is a logic puzzle made up of nine 3x3 grids.</source>
          <target state="new"><bpt id="p1">[</bpt>Sudoku<ept id="p1">](https://en.wikipedia.org/wiki/Sudoku)</ept> is a logic puzzle made up of nine 3x3 grids.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Some cells in the grid have numbers, while others are blank, and the goal is to solve for the blank cells.</source>
          <target state="new">Some cells in the grid have numbers, while others are blank, and the goal is to solve for the blank cells.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The link above has more information on the puzzle, but the purpose of this sample is to solve for the blank cells.</source>
          <target state="new">The link above has more information on the puzzle, but the purpose of this sample is to solve for the blank cells.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>So our input should be a file that is in the following format:</source>
          <target state="new">So our input should be a file that is in the following format:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Nine rows of nine columns</source>
          <target state="new">Nine rows of nine columns</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Each column can contain either a number or <ph id="ph1">`?`</ph> (which indicates a blank cell)</source>
          <target state="new">Each column can contain either a number or <ph id="ph1">`?`</ph> (which indicates a blank cell)</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Cells are separated by a space</source>
          <target state="new">Cells are separated by a space</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Now, there's a certain way to construct Sudoku puzzles in that you can't repeat a number in a column or row.</source>
          <target state="new">Now, there's a certain way to construct Sudoku puzzles in that you can't repeat a number in a column or row.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Thankfully there's an example on the HDInsight cluster that is properly constructed.</source>
          <target state="new">Thankfully there's an example on the HDInsight cluster that is properly constructed.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>It is located at <bpt id="p1">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta<ept id="p1">**</ept> and contains the following:</source>
          <target state="new">It is located at <bpt id="p1">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta<ept id="p1">**</ept> and contains the following:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The <ph id="ph2">`2.2.4.9-1`</ph> portion of the path may change as updates are made to the HDInsight cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The <ph id="ph2">`2.2.4.9-1`</ph> portion of the path may change as updates are made to the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>To run this through the Sudoku example, use the following command:</source>
          <target state="new">To run this through the Sudoku example, use the following command:</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>The results should appear similar to the following:</source>
          <target state="new">The results should appear similar to the following:</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Pi (π)</source>
          <target state="new">Pi (π)</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>The pi sample uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</source>
          <target state="new">The pi sample uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.</source>
          <target state="new">Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.</source>
          <target state="new">The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>The larger the sample of points used, the better the estimate is.</source>
          <target state="new">The larger the sample of points used, the better the estimate is.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>The mapper for this sample generates a number of points at random inside of a unit square and then counts the number of those points that are inside the circle.</source>
          <target state="new">The mapper for this sample generates a number of points at random inside of a unit square and then counts the number of those points that are inside the circle.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>The reducer then accumulates points counted by the mappers and estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.</source>
          <target state="new">The reducer then accumulates points counted by the mappers and estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Use the following command to run this sample.</source>
          <target state="new">Use the following command to run this sample.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>This uses 16 maps with 10,000,000 samples each to estimate the value of pi:</source>
          <target state="new">This uses 16 maps with 10,000,000 samples each to estimate the value of pi:</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>The value returned by this should be similar to <bpt id="p1">**</bpt>3.14159155000000000000<ept id="p1">**</ept>.</source>
          <target state="new">The value returned by this should be similar to <bpt id="p1">**</bpt>3.14159155000000000000<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>For references, the first 10 decimal places of pi are 3.1415926535.</source>
          <target state="new">For references, the first 10 decimal places of pi are 3.1415926535.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>10GB Greysort</source>
          <target state="new">10GB Greysort</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>GraySort is a benchmark sort whose metric is the sort rate (TB/minute) that is achieved while sorting very large amounts of data, usually a 100TB minimum.</source>
          <target state="new">GraySort is a benchmark sort whose metric is the sort rate (TB/minute) that is achieved while sorting very large amounts of data, usually a 100TB minimum.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>This sample uses a modest 10GB of data so that it can be run relatively quickly.</source>
          <target state="new">This sample uses a modest 10GB of data so that it can be run relatively quickly.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).</source>
          <target state="new">It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>For more information on this and other sorting benchmarks, see the <bpt id="p1">[</bpt>Sortbenchmark<ept id="p1">](http://sortbenchmark.org/)</ept> site.</source>
          <target state="new">For more information on this and other sorting benchmarks, see the <bpt id="p1">[</bpt>Sortbenchmark<ept id="p1">](http://sortbenchmark.org/)</ept> site.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>This sample uses three sets of MapReduce programs:</source>
          <target state="new">This sample uses three sets of MapReduce programs:</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>TeraGen<ept id="p1">**</ept>: A MapReduce program that generates rows of data to sort</source>
          <target state="new"><bpt id="p1">**</bpt>TeraGen<ept id="p1">**</ept>: A MapReduce program that generates rows of data to sort</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>TeraSort<ept id="p1">**</ept>: Samples the input data and uses MapReduce to sort the data into a total order</source>
          <target state="new"><bpt id="p1">**</bpt>TeraSort<ept id="p1">**</ept>: Samples the input data and uses MapReduce to sort the data into a total order</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce.</source>
          <target state="new">TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>In particular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.</source>
          <target state="new">In particular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>This guarantees that the outputs of reduce i are all less than the output of reduce i+1.</source>
          <target state="new">This guarantees that the outputs of reduce i are all less than the output of reduce i+1.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>TeraValidate<ept id="p1">**</ept>: A MapReduce program that validates that the output is globally sorted</source>
          <target state="new"><bpt id="p1">**</bpt>TeraValidate<ept id="p1">**</ept>: A MapReduce program that validates that the output is globally sorted</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.</source>
          <target state="new">It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.</source>
          <target state="new">The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Any problems are reported as an output of the reduce with the keys that are out of order.</source>
          <target state="new">Any problems are reported as an output of the reduce with the keys that are out of order.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Use the following steps to generate data, sort, and then validate the output:</source>
          <target state="new">Use the following steps to generate data, sort, and then validate the output:</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Generate 10GB of data, which will be stored to the HDInsight cluster's default storage at <bpt id="p1">**</bpt>wasb:///example/data/10GB-sort-input<ept id="p1">**</ept>:</source>
          <target state="new">Generate 10GB of data, which will be stored to the HDInsight cluster's default storage at <bpt id="p1">**</bpt>wasb:///example/data/10GB-sort-input<ept id="p1">**</ept>:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`-Dmapred.map.tasks`</ph> tells Hadoop how many map tasks to use for this job.</source>
          <target state="new">The <ph id="ph1">`-Dmapred.map.tasks`</ph> tells Hadoop how many map tasks to use for this job.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>The final two parameters instruct the job to create 10GB worth of data and to store it at <bpt id="p1">**</bpt>wasb:///example/data/10GB-sort-input<ept id="p1">**</ept>.</source>
          <target state="new">The final two parameters instruct the job to create 10GB worth of data and to store it at <bpt id="p1">**</bpt>wasb:///example/data/10GB-sort-input<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>Use the following command to sort the data:</source>
          <target state="new">Use the following command to sort the data:</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`-Dmapred.reduce.tasks`</ph> tells Hadoop how many reduce tasks to use for the job.</source>
          <target state="new">The <ph id="ph1">`-Dmapred.reduce.tasks`</ph> tells Hadoop how many reduce tasks to use for the job.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>The final two parameters are just the input and output locations for data.</source>
          <target state="new">The final two parameters are just the input and output locations for data.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Use the following to validate the data generated by the sort:</source>
          <target state="new">Use the following to validate the data generated by the sort:</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>From this article, you learned how to run the samples included with the Linux-based HDInsight clusters.</source>
          <target state="new">From this article, you learned how to run the samples included with the Linux-based HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:</source>
          <target state="new">For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Use Pig with Hadoop on HDInsight</source>
          <target state="new">Use Pig with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Use Hive with Hadoop on HDInsight</source>
          <target state="new">Use Hive with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Use MapReduce with Hadoop on HDInsight</source>
          <target state="new">Use MapReduce with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">546fd60a6c5cde26aa7a181ccfc69b5c8fd27102</xliffext:olfilehash>
  </header>
</xliff>