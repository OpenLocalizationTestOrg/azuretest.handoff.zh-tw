{
  "nodes": [
    {
      "content": "Partitioning data in DocumentDB | Microsoft Azure",
      "pos": [
        33,
        82
      ]
    },
    {
      "content": "Learn about how to partition data in DocumentDB, and when to use Hash, Range and Lookup partitioning.",
      "pos": [
        107,
        208
      ]
    },
    {
      "content": "Partitioning data in DocumentDB",
      "pos": [
        595,
        626
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Microsoft Azure DocumentDB<ept id=\"p1\">](../../services/documentdb/)</ept> is designed to help you achieve fast, predictable performance and <bpt id=\"p2\">*</bpt>scale-out<ept id=\"p2\">*</ept> seamlessly along with your application as it grows.",
      "pos": [
        628,
        814
      ]
    },
    {
      "content": "DocumentDB has been used to power high-scale production services at Microsoft like the User Data Store that powers the MSN suite of web and mobile apps.",
      "pos": [
        815,
        967
      ]
    },
    {
      "content": "You can achieve near-infinite scale in terms of storage and throughput for your DocumentDB application by horizontally partitioning your data - a concept commonly referred to as <bpt id=\"p1\">**</bpt>sharding<ept id=\"p1\">**</ept>.",
      "pos": [
        970,
        1161
      ]
    },
    {
      "content": "DocumentDB accounts can be scaled linearly with cost via stackable units a.k.a.",
      "pos": [
        1163,
        1242
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>collections<ept id=\"p1\">**</ept>.",
      "pos": [
        1243,
        1259
      ]
    },
    {
      "content": "How you best partition your data across collections will depend on your data format and access patterns.",
      "pos": [
        1260,
        1364
      ]
    },
    {
      "content": "After reading this article you will be able to answer the following questions:",
      "pos": [
        1367,
        1445
      ]
    },
    {
      "content": "What is hash, range and lookup partitioning?",
      "pos": [
        1453,
        1497
      ]
    },
    {
      "content": "When would you use each partitioning technique and why?",
      "pos": [
        1501,
        1556
      ]
    },
    {
      "content": "How do you go about building a partitioned application on Azure DocumentDB?",
      "pos": [
        1560,
        1635
      ]
    },
    {
      "content": "This article presents some concepts about sharding.",
      "pos": [
        1637,
        1688
      ]
    },
    {
      "content": "If you're ready to write code that partitions data using the DocumentDB .NET SDK, take a look at <bpt id=\"p1\">[</bpt>Partitioning data with the DocumentDB .NET SDK<ept id=\"p1\">](documentdb-sharding.md)</ept>.",
      "pos": [
        1689,
        1859
      ]
    },
    {
      "content": "Collections = Partitions",
      "pos": [
        1864,
        1888
      ]
    },
    {
      "content": "Before we dive deeper on data partitioning techniques, it is important to understand what a collection is and what it isn't.",
      "pos": [
        1890,
        2014
      ]
    },
    {
      "content": "As you may already know, a collection is a container for your JSON documents.",
      "pos": [
        2015,
        2092
      ]
    },
    {
      "content": "Collections in DocumentDB are not just <bpt id=\"p1\">*</bpt>logical<ept id=\"p1\">*</ept> containers, but also <bpt id=\"p2\">*</bpt>physical<ept id=\"p2\">*</ept> containers.",
      "pos": [
        2093,
        2185
      ]
    },
    {
      "content": "They are the transaction boundary for stored procedures and triggers, and the entry point to queries and CRUD operations.",
      "pos": [
        2186,
        2307
      ]
    },
    {
      "content": "Each collection is assigned a reserved amount of throughput which is not shared with other collections in the same account.",
      "pos": [
        2308,
        2431
      ]
    },
    {
      "content": "Therefore you can scale out your application both in terms of storage and throughput by adding more collections, and then distributing your documents across them.",
      "pos": [
        2432,
        2594
      ]
    },
    {
      "content": "Collections are not the same as tables in relational databases.",
      "pos": [
        2596,
        2659
      ]
    },
    {
      "content": "Collections do not enforce schema.",
      "pos": [
        2660,
        2694
      ]
    },
    {
      "content": "Therefore you can store different types of documents with diverse schemas in the same collection.",
      "pos": [
        2695,
        2792
      ]
    },
    {
      "content": "You can however choose to use collections to store objects of a single type like you would with tables.",
      "pos": [
        2793,
        2896
      ]
    },
    {
      "content": "The best model depends only on how the data appears together in queries and transactions.",
      "pos": [
        2897,
        2986
      ]
    },
    {
      "content": "Partitioning with DocumentDB",
      "pos": [
        2991,
        3019
      ]
    },
    {
      "content": "The most common techniques used for partitioning data with Azure DocumentDB are <bpt id=\"p1\">*</bpt>range partitioning<ept id=\"p1\">*</ept>, <bpt id=\"p2\">*</bpt>lookup partitioning<ept id=\"p2\">*</ept>, and <bpt id=\"p3\">*</bpt>hash partitioning<ept id=\"p3\">*</ept>.",
      "pos": [
        3021,
        3170
      ]
    },
    {
      "content": "Usually you designate a single JSON property name within your document as your partition key like \"timestamp\" or \"userID\".",
      "pos": [
        3171,
        3293
      ]
    },
    {
      "content": "In some cases, this might instead be an inner JSON property, or a different property name for each distinct type of document.",
      "pos": [
        3294,
        3419
      ]
    },
    {
      "content": "Let's take a look at these techniques in some more detail.",
      "pos": [
        3421,
        3479
      ]
    },
    {
      "content": "Range partitioning",
      "pos": [
        3484,
        3502
      ]
    },
    {
      "content": "In range partitioning, partitions are assigned based on whether the partition key is within a certain range.",
      "pos": [
        3504,
        3612
      ]
    },
    {
      "content": "This is commonly used for partitioning with <bpt id=\"p1\">*</bpt>time stamp<ept id=\"p1\">*</ept> properties (e.g., eventTime between Feb 1, 2015 and Feb 2, 2015).",
      "pos": [
        3613,
        3735
      ]
    },
    {
      "pos": [
        3740,
        3867
      ],
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> You should use Range partitioning if your queries are restricted to specifc range values against the partition key."
    },
    {
      "content": "Lookup partitioning",
      "pos": [
        3872,
        3891
      ]
    },
    {
      "content": "In lookup partitioning, partitions are assigned based on a lookup map that assigns discrete partition values to specific partitions a.k.a.",
      "pos": [
        3893,
        4031
      ]
    },
    {
      "content": "a partition or shard map.",
      "pos": [
        4032,
        4057
      ]
    },
    {
      "content": "This is commonly used for partitioning by region (e.g. the partition for Scandinavia contains Norway, Denmark, and Sweden).",
      "pos": [
        4058,
        4181
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> Lookup partitioning offers the highest degree of control in managing a multi-tenant application.",
      "pos": [
        4185,
        4293
      ]
    },
    {
      "content": "You can assign multiple tenants to a single collection, single tenant to a single collection, or even a single tenant across multiple collections.",
      "pos": [
        4294,
        4440
      ]
    },
    {
      "content": "Hash partitioning",
      "pos": [
        4446,
        4463
      ]
    },
    {
      "content": "In hash partitioning, partitions are assigned based on the value of a hash function, allowing you to evenly distribute requests and data across a number of partitions.",
      "pos": [
        4465,
        4632
      ]
    },
    {
      "content": "This is commonly used to partition data produced or consumed from a large number of distinct clients, and is useful for storing user profiles, catalog items, and IoT (\"Internet of Things\") telemetry data.",
      "pos": [
        4633,
        4837
      ]
    },
    {
      "pos": [
        4842,
        5044
      ],
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> You should use hash partitioning whenever there are too many entities to enumerate through lookup partitioning (e.g. users or devices) and the request rate is fairly uniform across entities."
    },
    {
      "content": "Choosing the right partitioning technique",
      "pos": [
        5049,
        5090
      ]
    },
    {
      "content": "So which partitioning technique is right for you?",
      "pos": [
        5092,
        5141
      ]
    },
    {
      "content": "It depends on the type of data and your common access patterns.",
      "pos": [
        5142,
        5205
      ]
    },
    {
      "content": "Picking the right partitioning technique at design time allows you to avoid technical debt, and handle growth in data size and request volumes.",
      "pos": [
        5206,
        5349
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Range partitioning<ept id=\"p1\">**</ept> is generally used in the context of dates, as it gives you an easy and natural mechanism for aging out partitions by timestamp.",
      "pos": [
        5353,
        5503
      ]
    },
    {
      "content": "It is also useful when queries are generally constrained to a time range since that is aligned with the partitioning boundaries.",
      "pos": [
        5504,
        5632
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Lookup partitioning<ept id=\"p1\">**</ept> allows you to group and organize unordered and unrelated sets of data in a natural way e.g., group tenants by organization or states by geographic region.",
      "pos": [
        5636,
        5814
      ]
    },
    {
      "content": "Lookup also offers fine-grained control for migrating data between collections.",
      "pos": [
        5815,
        5894
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Hash partitioning<ept id=\"p1\">**</ept> is useful for uniform load balancing of requests to make effective use of your provisioned storage and throughput.",
      "pos": [
        5898,
        6034
      ]
    },
    {
      "content": "Using <bpt id=\"p1\">*</bpt>consistent hashing<ept id=\"p1\">*</ept> algorithms allow you to minimize the amount of data that has to be moved when adding or removing a partition.",
      "pos": [
        6035,
        6171
      ]
    },
    {
      "content": "You don't have to choose just one partitioning technique.",
      "pos": [
        6173,
        6230
      ]
    },
    {
      "content": "A <bpt id=\"p1\">*</bpt>composite<ept id=\"p1\">*</ept> of these techniques can also be useful depending on the scenario.",
      "pos": [
        6231,
        6310
      ]
    },
    {
      "content": "For example, if you're storing vehicle telemetry data, a good approach would be to partition device telemetry data by range on timestamp for easy manageability of partitions, then sub-partition on VIN (vehicle identification number) in order to scale-out for throughput (range-hash composite partitioning).",
      "pos": [
        6311,
        6617
      ]
    },
    {
      "content": "Developing a partitioned application",
      "pos": [
        6622,
        6658
      ]
    },
    {
      "content": "There are three key design areas to look at when developing a partitioned application on DocumentDB.",
      "pos": [
        6659,
        6759
      ]
    },
    {
      "content": "How you route your creates and reads (including queries) to the right collections.",
      "pos": [
        6763,
        6845
      ]
    },
    {
      "content": "How you persist and retrieve your partition resolution configuration, a.k.a.",
      "pos": [
        6848,
        6924
      ]
    },
    {
      "content": "partition maps.",
      "pos": [
        6925,
        6940
      ]
    },
    {
      "content": "How you add/remove partitions as your data and request volume increases.",
      "pos": [
        6943,
        7015
      ]
    },
    {
      "content": "Let's take a closer look at each of these areas.",
      "pos": [
        7017,
        7065
      ]
    },
    {
      "content": "Routing creates and queries",
      "pos": [
        7070,
        7097
      ]
    },
    {
      "content": "Routing document creation requests is straight-forward for all three techniques we've discussed so far.",
      "pos": [
        7099,
        7202
      ]
    },
    {
      "content": "The document is created on the partition from the hash, lookup, or range value corresponding to the partition key.",
      "pos": [
        7203,
        7317
      ]
    },
    {
      "content": "Queries and reads should typically be scoped to a single partition key, so queries can be fanned out to only the matching partitions.",
      "pos": [
        7319,
        7452
      ]
    },
    {
      "content": "Queries across all data however, would require you to <bpt id=\"p1\">*</bpt>fan-out<ept id=\"p1\">*</ept> the request across multiple partitions, then merge the results.",
      "pos": [
        7453,
        7580
      ]
    },
    {
      "content": "Keep in mind that some queries might have to perform custom logic to merge results for e.g. when fetching the top N results.",
      "pos": [
        7581,
        7705
      ]
    },
    {
      "content": "Managing your partition map",
      "pos": [
        7710,
        7737
      ]
    },
    {
      "content": "You also need to decide how you will store your partition map, how your clients load it and receive updates when it changes, and how it is shared across multiple clients.",
      "pos": [
        7739,
        7909
      ]
    },
    {
      "content": "If the partition map does not change often, you can simply save it in your application config file.",
      "pos": [
        7910,
        8009
      ]
    },
    {
      "content": "If not, you can store it in any persistent store.",
      "pos": [
        8012,
        8061
      ]
    },
    {
      "content": "A common design pattern we've seen in production is to serialize partition maps as JSON, and store them within DocumentDB collections as well.",
      "pos": [
        8062,
        8204
      ]
    },
    {
      "content": "Clients can then cache the map in order to avoid the extra round trips, and then poll for changes periodically.",
      "pos": [
        8205,
        8316
      ]
    },
    {
      "content": "If your clients might modify the shard map, ensure that they use a consistent naming schema and use optimistic concurrency (eTags) to allow consistent updates to the partition map.",
      "pos": [
        8317,
        8497
      ]
    },
    {
      "content": "Adding and removing partitions",
      "pos": [
        8502,
        8532
      ]
    },
    {
      "content": "With DocumentDB, you can add and remove collections at any time and use them to store new incoming data or re-balance data available on existing collections.",
      "pos": [
        8534,
        8691
      ]
    },
    {
      "content": "Review the <bpt id=\"p1\">[</bpt>Limits<ept id=\"p1\">](documentdb-limits.md)</ept> page for the number of collections.",
      "pos": [
        8692,
        8769
      ]
    },
    {
      "content": "You can always call us to increase these limits.",
      "pos": [
        8770,
        8818
      ]
    },
    {
      "content": "Adding and removing a new partition with lookup and range partitioning is straightforward.",
      "pos": [
        8820,
        8910
      ]
    },
    {
      "content": "For example, adding a new geographic region or new time range for recent data, you just need to append the new partitions to the partition map.",
      "pos": [
        8911,
        9054
      ]
    },
    {
      "content": "Splitting an existing partition into multiple partitions, or merge two partitions requires a little more effort.",
      "pos": [
        9055,
        9167
      ]
    },
    {
      "content": "You need to either",
      "pos": [
        9168,
        9186
      ]
    },
    {
      "content": "Take the shard offline for reads.",
      "pos": [
        9191,
        9224
      ]
    },
    {
      "content": "Route reads to both the partitions using the old partitioning configuration as well as the new partitioning configuration during migration.",
      "pos": [
        9227,
        9366
      ]
    },
    {
      "content": "Note that transactions and consistency level guarantees will not be available until migration is complete.",
      "pos": [
        9367,
        9473
      ]
    },
    {
      "content": "Hashing is relatively more complicated for adding and removing partitions.",
      "pos": [
        9475,
        9549
      ]
    },
    {
      "content": "Simple hashing techniques will cause shuffling, and require most of the data to get moved around.",
      "pos": [
        9550,
        9647
      ]
    },
    {
      "content": "Using <bpt id=\"p1\">**</bpt>consistent hashing<ept id=\"p1\">**</ept> ensures that only a fraction of data needs to get moved.",
      "pos": [
        9648,
        9733
      ]
    },
    {
      "content": "A relatively easy way to add new partitions without requiring data movement is to  \"spill over\" your data to a fresh collection, and then fan-out requests across both the old and new collections.",
      "pos": [
        9735,
        9930
      ]
    },
    {
      "content": "This approach, however, should be used only in rare situations (e.g. spill over in peak time workloads and to hold data temporarily until it can be moved).",
      "pos": [
        9931,
        10086
      ]
    },
    {
      "content": "Next Steps",
      "pos": [
        10091,
        10101
      ]
    },
    {
      "content": "In this article, we've introduced some common techniques on how you can partition data with DocumentDB, and when to use which technique or combination of techniques.",
      "pos": [
        10102,
        10267
      ]
    },
    {
      "pos": [
        10274,
        10414
      ],
      "content": "Next, take a look at this <bpt id=\"p1\">[</bpt>article<ept id=\"p1\">](documentdb-sharding.md)</ept> on how you can partition data using partition resolvers with the DocumentDB SDK."
    },
    {
      "pos": [
        10420,
        10512
      ],
      "content": "Download one of the <bpt id=\"p1\">[</bpt>supported SDKs<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn781482.aspx)</ept>"
    },
    {
      "pos": [
        10517,
        10659
      ],
      "content": "Contact us through the <bpt id=\"p1\">[</bpt>MSDN support forums<ept id=\"p1\">](https://social.msdn.microsoft.com/forums/azure/home?forum=AzureDocumentDB)</ept> if you have questions."
    },
    {
      "content": "test",
      "pos": [
        10669,
        10673
      ]
    }
  ],
  "content": "<properties      \n    pageTitle=\"Partitioning data in DocumentDB | Microsoft Azure\"      \n    description=\"Learn about how to partition data in DocumentDB, and when to use Hash, Range and Lookup partitioning.\"          \n    services=\"documentdb\"      \n    authors=\"arramac\"      \n    manager=\"jhubbard\"      \n    editor=\"monicar\"      \n    documentationCenter=\"\"/> \n<tags      \n    ms.service=\"documentdb\"      \n    ms.workload=\"data-services\"      \n    ms.tgt_pltfrm=\"na\"      \n    ms.devlang=\"na\"      \n    ms.topic=\"article\"      \n    ms.date=\"05/28/2015\"      \n    ms.author=\"arramac\"/> \n\n# Partitioning data in DocumentDB\n\n[Microsoft Azure DocumentDB](../../services/documentdb/) is designed to help you achieve fast, predictable performance and *scale-out* seamlessly along with your application as it grows. DocumentDB has been used to power high-scale production services at Microsoft like the User Data Store that powers the MSN suite of web and mobile apps. \n\nYou can achieve near-infinite scale in terms of storage and throughput for your DocumentDB application by horizontally partitioning your data - a concept commonly referred to as **sharding**.  DocumentDB accounts can be scaled linearly with cost via stackable units a.k.a. **collections**. How you best partition your data across collections will depend on your data format and access patterns. \n\nAfter reading this article you will be able to answer the following questions:   \n\n - What is hash, range and lookup partitioning?\n - When would you use each partitioning technique and why?\n - How do you go about building a partitioned application on Azure DocumentDB?\n\nThis article presents some concepts about sharding. If you're ready to write code that partitions data using the DocumentDB .NET SDK, take a look at [Partitioning data with the DocumentDB .NET SDK](documentdb-sharding.md).\n\n## Collections = Partitions\n\nBefore we dive deeper on data partitioning techniques, it is important to understand what a collection is and what it isn't. As you may already know, a collection is a container for your JSON documents. Collections in DocumentDB are not just *logical* containers, but also *physical* containers. They are the transaction boundary for stored procedures and triggers, and the entry point to queries and CRUD operations. Each collection is assigned a reserved amount of throughput which is not shared with other collections in the same account. Therefore you can scale out your application both in terms of storage and throughput by adding more collections, and then distributing your documents across them.\n\nCollections are not the same as tables in relational databases. Collections do not enforce schema. Therefore you can store different types of documents with diverse schemas in the same collection. You can however choose to use collections to store objects of a single type like you would with tables. The best model depends only on how the data appears together in queries and transactions.\n\n## Partitioning with DocumentDB\n\nThe most common techniques used for partitioning data with Azure DocumentDB are *range partitioning*, *lookup partitioning*, and *hash partitioning*. Usually you designate a single JSON property name within your document as your partition key like \"timestamp\" or \"userID\". In some cases, this might instead be an inner JSON property, or a different property name for each distinct type of document.\n\nLet's take a look at these techniques in some more detail.\n\n## Range partitioning\n\nIn range partitioning, partitions are assigned based on whether the partition key is within a certain range. This is commonly used for partitioning with *time stamp* properties (e.g., eventTime between Feb 1, 2015 and Feb 2, 2015). \n\n> [AZURE.TIP] You should use Range partitioning if your queries are restricted to specifc range values against the partition key.\n\n## Lookup partitioning\n\nIn lookup partitioning, partitions are assigned based on a lookup map that assigns discrete partition values to specific partitions a.k.a. a partition or shard map. This is commonly used for partitioning by region (e.g. the partition for Scandinavia contains Norway, Denmark, and Sweden).\n\n> [AZURE.TIP] Lookup partitioning offers the highest degree of control in managing a multi-tenant application. You can assign multiple tenants to a single collection, single tenant to a single collection, or even a single tenant across multiple collections. \n\n## Hash partitioning\n\nIn hash partitioning, partitions are assigned based on the value of a hash function, allowing you to evenly distribute requests and data across a number of partitions. This is commonly used to partition data produced or consumed from a large number of distinct clients, and is useful for storing user profiles, catalog items, and IoT (\"Internet of Things\") telemetry data. \n\n> [AZURE.TIP] You should use hash partitioning whenever there are too many entities to enumerate through lookup partitioning (e.g. users or devices) and the request rate is fairly uniform across entities.\n\n## Choosing the right partitioning technique\n\nSo which partitioning technique is right for you? It depends on the type of data and your common access patterns. Picking the right partitioning technique at design time allows you to avoid technical debt, and handle growth in data size and request volumes.\n\n- **Range partitioning** is generally used in the context of dates, as it gives you an easy and natural mechanism for aging out partitions by timestamp. It is also useful when queries are generally constrained to a time range since that is aligned with the partitioning boundaries. \n- **Lookup partitioning** allows you to group and organize unordered and unrelated sets of data in a natural way e.g., group tenants by organization or states by geographic region. Lookup also offers fine-grained control for migrating data between collections. \n- **Hash partitioning** is useful for uniform load balancing of requests to make effective use of your provisioned storage and throughput. Using *consistent hashing* algorithms allow you to minimize the amount of data that has to be moved when adding or removing a partition.\n\nYou don't have to choose just one partitioning technique. A *composite* of these techniques can also be useful depending on the scenario. For example, if you're storing vehicle telemetry data, a good approach would be to partition device telemetry data by range on timestamp for easy manageability of partitions, then sub-partition on VIN (vehicle identification number) in order to scale-out for throughput (range-hash composite partitioning).\n\n## Developing a partitioned application\nThere are three key design areas to look at when developing a partitioned application on DocumentDB.\n\n- How you route your creates and reads (including queries) to the right collections.\n- How you persist and retrieve your partition resolution configuration, a.k.a. partition maps.\n- How you add/remove partitions as your data and request volume increases.\n\nLet's take a closer look at each of these areas.\n\n## Routing creates and queries\n\nRouting document creation requests is straight-forward for all three techniques we've discussed so far. The document is created on the partition from the hash, lookup, or range value corresponding to the partition key.\n\nQueries and reads should typically be scoped to a single partition key, so queries can be fanned out to only the matching partitions. Queries across all data however, would require you to *fan-out* the request across multiple partitions, then merge the results. Keep in mind that some queries might have to perform custom logic to merge results for e.g. when fetching the top N results.\n\n## Managing your partition map\n\nYou also need to decide how you will store your partition map, how your clients load it and receive updates when it changes, and how it is shared across multiple clients. If the partition map does not change often, you can simply save it in your application config file. \n\nIf not, you can store it in any persistent store. A common design pattern we've seen in production is to serialize partition maps as JSON, and store them within DocumentDB collections as well. Clients can then cache the map in order to avoid the extra round trips, and then poll for changes periodically. If your clients might modify the shard map, ensure that they use a consistent naming schema and use optimistic concurrency (eTags) to allow consistent updates to the partition map.\n\n## Adding and removing partitions\n\nWith DocumentDB, you can add and remove collections at any time and use them to store new incoming data or re-balance data available on existing collections. Review the [Limits](documentdb-limits.md) page for the number of collections. You can always call us to increase these limits.\n\nAdding and removing a new partition with lookup and range partitioning is straightforward. For example, adding a new geographic region or new time range for recent data, you just need to append the new partitions to the partition map. Splitting an existing partition into multiple partitions, or merge two partitions requires a little more effort. You need to either \n\n- Take the shard offline for reads.\n- Route reads to both the partitions using the old partitioning configuration as well as the new partitioning configuration during migration. Note that transactions and consistency level guarantees will not be available until migration is complete.\n\nHashing is relatively more complicated for adding and removing partitions. Simple hashing techniques will cause shuffling, and require most of the data to get moved around. Using **consistent hashing** ensures that only a fraction of data needs to get moved.\n\nA relatively easy way to add new partitions without requiring data movement is to  \"spill over\" your data to a fresh collection, and then fan-out requests across both the old and new collections. This approach, however, should be used only in rare situations (e.g. spill over in peak time workloads and to hold data temporarily until it can be moved).\n\n## Next Steps\nIn this article, we've introduced some common techniques on how you can partition data with DocumentDB, and when to use which technique or combination of techniques. \n\n-   Next, take a look at this [article](documentdb-sharding.md) on how you can partition data using partition resolvers with the DocumentDB SDK. \n-   Download one of the [supported SDKs](https://msdn.microsoft.com/library/azure/dn781482.aspx)\n-   Contact us through the [MSDN support forums](https://social.msdn.microsoft.com/forums/azure/home?forum=AzureDocumentDB) if you have questions.\n   \n\n\n \n\ntest\n"
}