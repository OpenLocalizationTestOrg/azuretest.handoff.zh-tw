{
  "nodes": [
    {
      "content": "Linux tutorial: Get started with Hadoop and Hive | Microsoft Azure",
      "pos": [
        27,
        93
      ]
    },
    {
      "content": "Follow this Linux tutorial to get started using Hadoop in HDInsight.",
      "pos": [
        112,
        180
      ]
    },
    {
      "content": "Learn how to provision Linux clusters, and query data with Hive.",
      "pos": [
        181,
        245
      ]
    },
    {
      "content": "Hadoop tutorial: Get started using Hadoop with Hive in HDInsight on Linux (preview)",
      "pos": [
        578,
        661
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        665,
        681
      ]
    },
    {
      "content": "Windows",
      "pos": [
        685,
        692
      ]
    },
    {
      "content": "Linux",
      "pos": [
        747,
        752
      ]
    },
    {
      "content": "This Hadoop tutorial gets you started quickly with Azure HDInsight on Linux by showing you how to provision an Hadoop cluster on Linux and run a Hive query.",
      "pos": [
        803,
        959
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you are new to Hadoop and big data, you can read more about the terms <ph id=\"ph2\">&lt;a href=\"http://go.microsoft.com/fwlink/?LinkId=510084\" target=\"_blank\"&gt;</ph>Apache Hadoop<ph id=\"ph3\">&lt;/a&gt;</ph>, <ph id=\"ph4\">&lt;a href=\"http://go.microsoft.com/fwlink/?LinkId=510086\" target=\"_blank\"&gt;</ph>MapReduce<ph id=\"ph5\">&lt;/a&gt;</ph>, <ph id=\"ph6\">&lt;a href=\"http://go.microsoft.com/fwlink/?LinkId=510087\" target=\"_blank\"&gt;</ph>Hadoop Distributed File System (HDFS)<ph id=\"ph7\">&lt;/a&gt;</ph>, and <ph id=\"ph8\">&lt;a href=\"http://go.microsoft.com/fwlink/?LinkId=510085\" target=\"_blank\"&gt;</ph>Hive<ph id=\"ph9\">&lt;/a&gt;</ph>.",
      "pos": [
        964,
        1428
      ]
    },
    {
      "content": "To understand how HDInsight enables Hadoop in Azure, see <bpt id=\"p1\">[</bpt>Introduction to Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-introduction.md)</ept>.",
      "pos": [
        1429,
        1558
      ]
    },
    {
      "content": "What does this tutorial accomplish?",
      "pos": [
        1564,
        1599
      ]
    },
    {
      "content": "Assume you have a large unstructured data set and you want to run queries on it to extract some meaningful information.",
      "pos": [
        1601,
        1720
      ]
    },
    {
      "content": "Here's how you achieve this:",
      "pos": [
        1721,
        1749
      ]
    },
    {
      "content": "Hadoop tutorial steps: Create a Storage account; provision a Hadoop cluster; query data with Hive.",
      "pos": [
        1756,
        1854
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        1943,
        1956
      ]
    },
    {
      "content": "Before you begin this Linux tutorial for Hadoop, you must have the following:",
      "pos": [
        1958,
        2035
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>An Azure subscription<ept id=\"p1\">**</ept>.",
      "pos": [
        2039,
        2065
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Get Azure free trial<ept id=\"p1\">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "pos": [
        2066,
        2196
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Secure Shell (SSH) keys<ept id=\"p1\">**</ept>.",
      "pos": [
        2199,
        2227
      ]
    },
    {
      "content": "If you want to remote into a Linux cluster by using SSH with a key instead of a password.",
      "pos": [
        2228,
        2317
      ]
    },
    {
      "content": "Using a key is the recommended method as it is more secure.",
      "pos": [
        2318,
        2377
      ]
    },
    {
      "content": "For instructions on how to generate SSH keys, refer to the following articles:",
      "pos": [
        2378,
        2456
      ]
    },
    {
      "pos": [
        2464,
        2600
      ],
      "content": "From a Linux computer - <bpt id=\"p1\">[</bpt>Use SSH with Linux-based HDInsight (Hadoop) from Linux, Unix, or OS X<ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>."
    },
    {
      "pos": [
        2608,
        2736
      ],
      "content": "From a Windows computer - <bpt id=\"p1\">[</bpt>Use SSH with Linux-based HDInsight (Hadoop) from Windows<ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>."
    },
    {
      "pos": [
        2738,
        2780
      ],
      "content": "<bpt id=\"p1\">**</bpt>Estimated time to complete:<ept id=\"p1\">**</ept> 30 minutes"
    },
    {
      "pos": [
        2785,
        2848
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"provision\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Provision an HDInsight cluster on Linux"
    },
    {
      "content": "When you provision a cluster, you provision Azure compute resources that contain Hadoop and related applications.",
      "pos": [
        2850,
        2963
      ]
    },
    {
      "content": "In this section, you provision an HDInsight version 3.2 cluster.",
      "pos": [
        2964,
        3028
      ]
    },
    {
      "content": "You can also create Hadoop clusters for other versions.",
      "pos": [
        3029,
        3084
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Provision HDInsight clusters using custom options<ept id=\"p1\">][hdinsight-provision]</ept>.",
      "pos": [
        3085,
        3180
      ]
    },
    {
      "content": "For information about HDInsight versions and their SLAs, see <bpt id=\"p1\">[</bpt>HDInsight component versioning<ept id=\"p1\">](hdinsight-component-versioning.md)</ept>.",
      "pos": [
        3181,
        3310
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>  You can also create Hadoop clusters running the Windows Server operating system.",
      "pos": [
        3313,
        3407
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Get Started with HDInsight on Windows<ept id=\"p1\">](hdinsight-hadoop-tutorial-get-started-windows.md)</ept>.",
      "pos": [
        3408,
        3520
      ]
    },
    {
      "content": "To provision an HDInsight cluster",
      "pos": [
        3525,
        3558
      ]
    },
    {
      "pos": [
        3565,
        3633
      ],
      "content": "Sign in to the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](https://ms.portal.azure.com/)</ept>."
    },
    {
      "pos": [
        3637,
        3707
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept>, Click <bpt id=\"p2\">**</bpt>Data Analytics<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>HDInsight<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Creating a new cluster in the Azure Preview Portal",
      "pos": [
        3715,
        3765
      ]
    },
    {
      "content": "Enter a <bpt id=\"p1\">**</bpt>Cluster Name<ept id=\"p1\">**</ept>, select <bpt id=\"p2\">**</bpt>Hadoop<ept id=\"p2\">**</ept> for the <bpt id=\"p3\">**</bpt>Cluster Type<ept id=\"p3\">**</ept>, and from the <bpt id=\"p4\">**</bpt>Cluster Operating System<ept id=\"p4\">**</ept> drop-down, select <bpt id=\"p5\">**</bpt>Ubuntu<ept id=\"p5\">**</ept>.",
      "pos": [
        3901,
        4042
      ]
    },
    {
      "content": "A green check will appear beside the cluster name if it is available.",
      "pos": [
        4043,
        4112
      ]
    },
    {
      "content": "Enter cluster name and type",
      "pos": [
        4120,
        4147
      ]
    },
    {
      "pos": [
        4260,
        4400
      ],
      "content": "If you have more than one subscription, click the <bpt id=\"p1\">**</bpt>Subscription<ept id=\"p1\">**</ept> entry to select the Azure subscription that will be used for the cluster."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Resource Group<ept id=\"p1\">**</ept> to see a list of existing resource groups and then select the one to create the cluster in.",
      "pos": [
        4405,
        4521
      ]
    },
    {
      "content": "Or, you can click <bpt id=\"p1\">**</bpt>Create New<ept id=\"p1\">**</ept> and then enter the name of the new resource group.",
      "pos": [
        4522,
        4605
      ]
    },
    {
      "content": "A green check will appear to indicate if the new group name is available.",
      "pos": [
        4606,
        4679
      ]
    },
    {
      "pos": [
        4687,
        4786
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> This entry will default to one of your existing resource groups, if any are available."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Credentials<ept id=\"p1\">**</ept> and then enter a password for the admin user.",
      "pos": [
        4791,
        4858
      ]
    },
    {
      "content": "You must also enter an <bpt id=\"p1\">**</bpt>SSH Username<ept id=\"p1\">**</ept> and either a <bpt id=\"p2\">**</bpt>PASSWORD<ept id=\"p2\">**</ept> or <bpt id=\"p3\">**</bpt>PUBLIC KEY<ept id=\"p3\">**</ept>, which will be used to authenticate the SSH user.",
      "pos": [
        4859,
        4992
      ]
    },
    {
      "content": "Using a public key is the recommended approach.",
      "pos": [
        4993,
        5040
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> at the bottom to save the credentials configuration.",
      "pos": [
        5041,
        5110
      ]
    },
    {
      "content": "Provide cluster credentials",
      "pos": [
        5118,
        5145
      ]
    },
    {
      "content": "For more information on using SSH with HDInsight, see one of the following articles:",
      "pos": [
        5259,
        5343
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X",
      "pos": [
        5352,
        5422
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Windows",
      "pos": [
        5471,
        5528
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Data Source<ept id=\"p1\">**</ept> to choose an existing data source for the cluster, or create a new one.",
      "pos": [
        5578,
        5671
      ]
    },
    {
      "content": "When you provision a Hadoop cluster in HDInsight, you specify an Azure Storage account.",
      "pos": [
        5672,
        5759
      ]
    },
    {
      "content": "A specific Blob storage container from that account is designated as the default file system, like in the Hadoop distributed file system (HDFS).",
      "pos": [
        5760,
        5904
      ]
    },
    {
      "content": "By default, the HDInsight cluster is provisioned in the same data center as the storage account you specify.",
      "pos": [
        5905,
        6013
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Use Azure Blob storage with HDInsight<ept id=\"p1\">](hdinsight-use-blob-storage.md)</ept>",
      "pos": [
        6014,
        6110
      ]
    },
    {
      "content": "Data source blade",
      "pos": [
        6118,
        6135
      ]
    },
    {
      "content": "Currently you can select an Azure Storage Account as the data source for an HDInsight cluster.",
      "pos": [
        6255,
        6349
      ]
    },
    {
      "content": "Use the following to understand the entries on the <bpt id=\"p1\">**</bpt>Data Source<ept id=\"p1\">**</ept> blade.",
      "pos": [
        6350,
        6423
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Selection Method<ept id=\"p1\">**</ept>: Set this to <bpt id=\"p2\">**</bpt>From all subscriptions<ept id=\"p2\">**</ept> to enable browsing of storage accounts from all your subscriptions.",
      "pos": [
        6431,
        6559
      ]
    },
    {
      "content": "Set this to <bpt id=\"p1\">**</bpt>Access Key<ept id=\"p1\">**</ept> if you want to enter the <bpt id=\"p2\">**</bpt>Storage Name<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>Access Key<ept id=\"p3\">**</ept> of an existing storage account.",
      "pos": [
        6560,
        6679
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Select storage account / Create New<ept id=\"p1\">**</ept>: Click <bpt id=\"p2\">**</bpt>Select storage account<ept id=\"p2\">**</ept> to browse and select an existing storage account you want to associate with the cluster.",
      "pos": [
        6687,
        6849
      ]
    },
    {
      "content": "Or, click <bpt id=\"p1\">**</bpt>Create New<ept id=\"p1\">**</ept> to create a new storage account.",
      "pos": [
        6850,
        6907
      ]
    },
    {
      "content": "Use the field that appears to enter the name of the storage account.",
      "pos": [
        6908,
        6976
      ]
    },
    {
      "content": "A green check will appear if the name is available.",
      "pos": [
        6977,
        7028
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Choose Default Container<ept id=\"p1\">**</ept>: Use this to enter the name of the default container to use for the cluster.",
      "pos": [
        7036,
        7141
      ]
    },
    {
      "content": "While you can enter any name here, we recommend using the same name as the cluster so that you can easily recognize that the container is used for this specific cluster.",
      "pos": [
        7142,
        7311
      ]
    },
    {
      "pos": [
        7319,
        7409
      ],
      "content": "<bpt id=\"p1\">**</bpt>Location<ept id=\"p1\">**</ept>: The geographic region that the storage account is in, or will be created in."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Selecting the location for the default data source will also set the location of the HDInsight cluster.",
      "pos": [
        7421,
        7542
      ]
    },
    {
      "content": "The cluster and default data source must be located in the same region.",
      "pos": [
        7543,
        7614
      ]
    },
    {
      "pos": [
        7620,
        7675
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> to save the data source configuration."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Node Pricing Tiers<ept id=\"p1\">**</ept> to display information about the nodes that will be created for this cluster.",
      "pos": [
        7680,
        7786
      ]
    },
    {
      "content": "Set the number of worker nodes that you need for the cluster.",
      "pos": [
        7787,
        7848
      ]
    },
    {
      "content": "The estimated cost of the cluster will be shown within the blade.",
      "pos": [
        7849,
        7914
      ]
    },
    {
      "content": "Node pricing tiers blade",
      "pos": [
        7922,
        7946
      ]
    },
    {
      "pos": [
        8064,
        8120
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> to save the node pricing configuration."
    },
    {
      "content": "On the <bpt id=\"p1\">**</bpt>New HDInsight Cluster<ept id=\"p1\">**</ept> blade, ensure that <bpt id=\"p2\">**</bpt>Pin to Startboard<ept id=\"p2\">**</ept> is selected, and then click <bpt id=\"p3\">**</bpt>Create<ept id=\"p3\">**</ept>.",
      "pos": [
        8125,
        8238
      ]
    },
    {
      "content": "This will create the cluster and add a tile for it to the Startboard of your Azure Portal.",
      "pos": [
        8239,
        8329
      ]
    },
    {
      "content": "The icon will indicate that the cluster is provisioning, and will change to display the HDInsight icon once provisioning has completed.",
      "pos": [
        8330,
        8465
      ]
    },
    {
      "content": "While provisioning",
      "pos": [
        8467,
        8485
      ]
    },
    {
      "content": "Provisioning complete",
      "pos": [
        8486,
        8507
      ]
    },
    {
      "content": "Provisioning indicator on startboard",
      "pos": [
        8555,
        8591
      ]
    },
    {
      "content": "Provisioned cluster tile",
      "pos": [
        8665,
        8689
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> It will take some time for the cluster to be created, usually around 15 minutes.",
      "pos": [
        8763,
        8856
      ]
    },
    {
      "content": "Use the tile on the Startboard, or the <bpt id=\"p1\">**</bpt>Notifications<ept id=\"p1\">**</ept> entry on the left of the page to check on the provisioning process.",
      "pos": [
        8857,
        8981
      ]
    },
    {
      "content": "Once the provisioning is completed, click the tile for the cluster from the Startboard to launch the cluster blade.",
      "pos": [
        8983,
        9098
      ]
    },
    {
      "pos": [
        9103,
        9159
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"hivequery\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Submit a Hive job on the cluster"
    },
    {
      "content": "Now that you have an HDInsight Linux cluster provisioned, the next step is to run a sample Hive job to query sample data (sample.log) that comes with HDInsight clusters.",
      "pos": [
        9160,
        9329
      ]
    },
    {
      "content": "The sample data contains log information, including trace, warnings, info, and errors.",
      "pos": [
        9330,
        9416
      ]
    },
    {
      "content": "We query this data to retrieve all the error logs with a specific severity.",
      "pos": [
        9417,
        9492
      ]
    },
    {
      "content": "You must perform the following steps to run a Hive query on an HDInsight Linux cluster:",
      "pos": [
        9493,
        9580
      ]
    },
    {
      "content": "Connect to a Linux cluster",
      "pos": [
        9584,
        9610
      ]
    },
    {
      "content": "Run a Hive job",
      "pos": [
        9613,
        9627
      ]
    },
    {
      "content": "To connect to a cluster",
      "pos": [
        9635,
        9658
      ]
    },
    {
      "content": "You can connect to an HDInsight cluster on Linux from a Linux computer or a Windows-based computer by using SSH.",
      "pos": [
        9660,
        9772
      ]
    },
    {
      "content": "To connect from a Linux computer",
      "pos": [
        9776,
        9808
      ]
    },
    {
      "content": "Open a terminal and enter the following command:",
      "pos": [
        9815,
        9863
      ]
    },
    {
      "content": "Because you provisioned a cluster with the Quick Create option, the default SSH user name is <bpt id=\"p1\">**</bpt>hdiuser<ept id=\"p1\">**</ept>.",
      "pos": [
        9930,
        10035
      ]
    },
    {
      "content": "So, the command must be:",
      "pos": [
        10036,
        10060
      ]
    },
    {
      "content": "When prompted, enter the password that you provided while provisioning the cluster.",
      "pos": [
        10128,
        10211
      ]
    },
    {
      "content": "After you are successfully connected, the prompt will change to the following:",
      "pos": [
        10212,
        10290
      ]
    },
    {
      "content": "To connect from a Windows-based computer",
      "pos": [
        10326,
        10366
      ]
    },
    {
      "pos": [
        10373,
        10508
      ],
      "content": "Download <ph id=\"ph1\">&lt;a href=\"http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\" target=\"_blank\"&gt;</ph>PuTTY<ph id=\"ph2\">&lt;/a&gt;</ph> for Windows-based clients."
    },
    {
      "content": "Open PuTTY.",
      "pos": [
        10513,
        10524
      ]
    },
    {
      "content": "In <bpt id=\"p1\">**</bpt>Category<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Session<ept id=\"p2\">**</ept>.",
      "pos": [
        10525,
        10560
      ]
    },
    {
      "content": "From the <bpt id=\"p1\">**</bpt>Basic options for your PuTTY session<ept id=\"p1\">**</ept> screen, enter the SSH address of your HDInsight server in the <bpt id=\"p2\">**</bpt>Host Name (or IP address)<ept id=\"p2\">**</ept> field.",
      "pos": [
        10561,
        10709
      ]
    },
    {
      "content": "The SSH address is your cluster name, followed by<bpt id=\"p1\">**</bpt>-ssh.azurehdinsight.net<ept id=\"p1\">**</ept>.",
      "pos": [
        10710,
        10787
      ]
    },
    {
      "content": "For example, <bpt id=\"p1\">**</bpt>myhdinsightcluster-ssh.azurehdinsight.net<ept id=\"p1\">**</ept>.",
      "pos": [
        10788,
        10847
      ]
    },
    {
      "content": "Connect to an HDInsight cluster on Linux using PuTTY",
      "pos": [
        10855,
        10907
      ]
    },
    {
      "content": "To save the connection information for future use, enter a name for this connection under <bpt id=\"p1\">**</bpt>Saved Sessions<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Save<ept id=\"p2\">**</ept>.",
      "pos": [
        10994,
        11128
      ]
    },
    {
      "content": "The connection will be added to the list of saved sessions.",
      "pos": [
        11129,
        11188
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Open<ept id=\"p1\">**</ept> to connect to the cluster.",
      "pos": [
        11193,
        11234
      ]
    },
    {
      "content": "When prompted for the user name, enter <bpt id=\"p1\">**</bpt>hdiuser<ept id=\"p1\">**</ept>.",
      "pos": [
        11235,
        11286
      ]
    },
    {
      "content": "For the password, enter the password you specified while provisioning the cluster.",
      "pos": [
        11287,
        11369
      ]
    },
    {
      "content": "After you are successfully connected, the prompt will change to the following:",
      "pos": [
        11370,
        11448
      ]
    },
    {
      "content": "To run a Hive job",
      "pos": [
        11485,
        11502
      ]
    },
    {
      "content": "Once you are connected to the cluster via SSH, use the following commands to run a Hive query.",
      "pos": [
        11504,
        11598
      ]
    },
    {
      "content": "Start the Hive command-line interface (CLI) by using the following command at the prompt:",
      "pos": [
        11603,
        11692
      ]
    },
    {
      "pos": [
        11711,
        11857
      ],
      "content": "Using the CLI, enter the following statements to create a new table named <bpt id=\"p1\">**</bpt>log4jLogs<ept id=\"p1\">**</ept> by using the sample data already available on the cluster:"
    },
    {
      "content": "These statements perform the following actions:",
      "pos": [
        12217,
        12264
      ]
    },
    {
      "pos": [
        12272,
        12359
      ],
      "content": "<bpt id=\"p1\">**</bpt>DROP TABLE<ept id=\"p1\">**</ept> - Deletes the table and the data file, in case the table already exists."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p1\">**</ept> - Creates a new \"external\" table in Hive.",
      "pos": [
        12366,
        12433
      ]
    },
    {
      "content": "External tables store only the table definition in Hive; the data is left in the original location.",
      "pos": [
        12434,
        12533
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>ROW FORMAT<ept id=\"p1\">**</ept> - Tells Hive how the data is formatted.",
      "pos": [
        12540,
        12594
      ]
    },
    {
      "content": "In this case, the fields in each log are separated by a space.",
      "pos": [
        12595,
        12657
      ]
    },
    {
      "pos": [
        12664,
        12794
      ],
      "content": "<bpt id=\"p1\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p1\">**</ept> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text."
    },
    {
      "pos": [
        12801,
        12885
      ],
      "content": "<bpt id=\"p1\">**</bpt>SELECT<ept id=\"p1\">**</ept> - Selects a count of all rows where column t4 contains the value [ERROR]."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but you always want Hive queries to use the latest data.",
      "pos": [
        12892,
        13142
      ]
    },
    {
      "content": "Dropping an external table does <bpt id=\"p1\">*</bpt>not<ept id=\"p1\">*</ept> delete the data, only the table definition.",
      "pos": [
        13143,
        13224
      ]
    },
    {
      "content": "This returns the following output:",
      "pos": [
        13230,
        13264
      ]
    },
    {
      "pos": [
        14751,
        14845
      ],
      "content": "Note that the output contains <bpt id=\"p1\">**</bpt>[ERROR]  3<ept id=\"p1\">**</ept>, as there are three rows that contain this value."
    },
    {
      "pos": [
        14850,
        14932
      ],
      "content": "Use the following statements to create a new \"internal\" table named <bpt id=\"p1\">**</bpt>errorLogs<ept id=\"p1\">**</ept>:"
    },
    {
      "content": "These statements perform the following actions:",
      "pos": [
        15190,
        15237
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p1\">**</ept> - Creates a table, if it does not already exist.",
      "pos": [
        15245,
        15324
      ]
    },
    {
      "content": "Since the <bpt id=\"p1\">**</bpt>EXTERNAL<ept id=\"p1\">**</ept> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
      "pos": [
        15325,
        15473
      ]
    },
    {
      "content": "Unlike external tables, dropping an internal table will delete the underlying data as well.",
      "pos": [
        15474,
        15565
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>STORED AS ORC<ept id=\"p1\">**</ept> - Stores the data in Optimized Row Columnar (ORC) format.",
      "pos": [
        15572,
        15647
      ]
    },
    {
      "content": "This is a highly optimized and efficient format for storing Hive data.",
      "pos": [
        15648,
        15718
      ]
    },
    {
      "pos": [
        15725,
        15878
      ],
      "content": "<bpt id=\"p1\">**</bpt>INSERT OVERWRITE ... SELECT<ept id=\"p1\">**</ept> - Selects rows from the <bpt id=\"p2\">**</bpt>log4jLogs<ept id=\"p2\">**</ept> table that contain [ERROR], and then inserts the data into the <bpt id=\"p3\">**</bpt>errorLogs<ept id=\"p3\">**</ept> table."
    },
    {
      "pos": [
        15883,
        16050
      ],
      "content": "To verify that only rows containing [ERROR] in column t4 were stored to the <bpt id=\"p1\">**</bpt>errorLogs<ept id=\"p1\">**</ept> table, use the following statement to return all the rows from <bpt id=\"p2\">**</bpt>errorLogs<ept id=\"p2\">**</ept>:"
    },
    {
      "content": "The following output should be displayed on the console:",
      "pos": [
        16090,
        16146
      ]
    },
    {
      "content": "The returned data should all correspond to [ERROR] logs.",
      "pos": [
        16431,
        16487
      ]
    },
    {
      "pos": [
        16492,
        16526
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "In this Linux tutorial, you have learned how to provision a Hadoop cluster on Linux with HDInsight and run a Hive query on it by using SSH.",
      "pos": [
        16527,
        16666
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        16667,
        16709
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Manage HDInsight clusters using Ambari<ept id=\"p1\">](hdinsight-hadoop-manage-ambari.md)</ept>: Linux-based HDInsight clusters use Ambari for management and monitoring of Hadoop services.",
      "pos": [
        16713,
        16881
      ]
    },
    {
      "content": "The Ambari web UI is available on each cluster at https://CLUSTERNAME.azurehdinsight.net.",
      "pos": [
        16882,
        16971
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> While many sections of the Ambari web are directly accessible through the Internet, the web UI for Hadoop services such as Resource Manager or Job History require the use of an SSH tunnel.",
      "pos": [
        16979,
        17185
      ]
    },
    {
      "content": "For more information on using an SSH tunnel with HDInsight, see the following articles:",
      "pos": [
        17186,
        17273
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X",
      "pos": [
        17289,
        17359
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Windows",
      "pos": [
        17417,
        17474
      ]
    },
    {
      "pos": [
        17529,
        17689
      ],
      "content": "<bpt id=\"p1\">[</bpt>Provision HDInsight on Linux using custom options<ept id=\"p1\">](hdinsight-hadoop-provision-linux-clusters.md)</ept>: Learn more details about how to provision HDInsight clusters."
    },
    {
      "pos": [
        17693,
        17898
      ],
      "content": "<bpt id=\"p1\">[</bpt>Working with HDInsight on Linux<ept id=\"p1\">](hdinsight-hadoop-linux-information.md)</ept>: If you are already familiar with Hadoop on Linux platforms, this document provides guidance on Azure specific information, such as:"
    },
    {
      "content": "URLs for services hosted on the cluster, such as Ambari and WebHCat",
      "pos": [
        17906,
        17973
      ]
    },
    {
      "content": "The location of Hadoop files and examples on the local file system",
      "pos": [
        17980,
        18046
      ]
    },
    {
      "content": "The use of Azure Storage (WASB) instead of HDFS as the default data store",
      "pos": [
        18053,
        18126
      ]
    },
    {
      "content": "For more information on Hive, or to learn about Pig and MapReduce, see the following:",
      "pos": [
        18130,
        18215
      ]
    },
    {
      "content": "Use MapReduce with HDInsight",
      "pos": [
        18224,
        18252
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        18286,
        18309
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        18338,
        18360
      ]
    },
    {
      "content": "For more information on how to work with the Azure Storage used by your HDInsight cluster, see the following:",
      "pos": [
        18384,
        18493
      ]
    },
    {
      "content": "Use Azure Blob storage with HDInsight",
      "pos": [
        18502,
        18539
      ]
    },
    {
      "content": "Upload data to HDInsight",
      "pos": [
        18582,
        18606
      ]
    },
    {
      "content": "test",
      "pos": [
        20261,
        20265
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Linux tutorial: Get started with Hadoop and Hive | Microsoft Azure\"\n    description=\"Follow this Linux tutorial to get started using Hadoop in HDInsight. Learn how to provision Linux clusters, and query data with Hive.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"nitinme\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.devlang=\"na\"\n    ms.topic=\"hero-article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"big-data\"\n    ms.date=\"08/07/2015\"\n    ms.author=\"nitinme\"/>\n\n# Hadoop tutorial: Get started using Hadoop with Hive in HDInsight on Linux (preview)\n\n> [AZURE.SELECTOR]\n- [Windows](hdinsight-hadoop-tutorial-get-started-windows.md)\n- [Linux](hdinsight-hadoop-linux-tutorial-get-started.md)\n\nThis Hadoop tutorial gets you started quickly with Azure HDInsight on Linux by showing you how to provision an Hadoop cluster on Linux and run a Hive query.\n\n\n> [AZURE.NOTE] If you are new to Hadoop and big data, you can read more about the terms <a href=\"http://go.microsoft.com/fwlink/?LinkId=510084\" target=\"_blank\">Apache Hadoop</a>, <a href=\"http://go.microsoft.com/fwlink/?LinkId=510086\" target=\"_blank\">MapReduce</a>, <a href=\"http://go.microsoft.com/fwlink/?LinkId=510087\" target=\"_blank\">Hadoop Distributed File System (HDFS)</a>, and <a href=\"http://go.microsoft.com/fwlink/?LinkId=510085\" target=\"_blank\">Hive</a>. To understand how HDInsight enables Hadoop in Azure, see [Introduction to Hadoop in HDInsight](hdinsight-hadoop-introduction.md).\n\n\n## What does this tutorial accomplish?\n\nAssume you have a large unstructured data set and you want to run queries on it to extract some meaningful information. Here's how you achieve this:\n\n   ![Hadoop tutorial steps: Create a Storage account; provision a Hadoop cluster; query data with Hive.](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.Linux.GetStartedFlow.png)\n\n\n## Prerequisites\n\nBefore you begin this Linux tutorial for Hadoop, you must have the following:\n\n- **An Azure subscription**. See [Get Azure free trial](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- **Secure Shell (SSH) keys**. If you want to remote into a Linux cluster by using SSH with a key instead of a password. Using a key is the recommended method as it is more secure. For instructions on how to generate SSH keys, refer to the following articles:\n    -  From a Linux computer - [Use SSH with Linux-based HDInsight (Hadoop) from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md).\n    -  From a Windows computer - [Use SSH with Linux-based HDInsight (Hadoop) from Windows](hdinsight-hadoop-linux-use-ssh-windows.md).\n\n**Estimated time to complete:** 30 minutes\n\n## <a name=\"provision\"></a>Provision an HDInsight cluster on Linux\n\nWhen you provision a cluster, you provision Azure compute resources that contain Hadoop and related applications. In this section, you provision an HDInsight version 3.2 cluster. You can also create Hadoop clusters for other versions. For instructions, see [Provision HDInsight clusters using custom options][hdinsight-provision]. For information about HDInsight versions and their SLAs, see [HDInsight component versioning](hdinsight-component-versioning.md).\n\n>[AZURE.NOTE]  You can also create Hadoop clusters running the Windows Server operating system. For instructions, see [Get Started with HDInsight on Windows](hdinsight-hadoop-tutorial-get-started-windows.md).\n\n\n**To provision an HDInsight cluster**\n\n1. Sign in to the [Azure Preview Portal](https://ms.portal.azure.com/).\n2. Click **NEW**, Click **Data Analytics**, and then click **HDInsight**.\n\n    ![Creating a new cluster in the Azure Preview Portal](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.CreateCluster.1.png \"Creating a new cluster in the Azure Preview Portal\")\n\n3. Enter a **Cluster Name**, select **Hadoop** for the **Cluster Type**, and from the **Cluster Operating System** drop-down, select **Ubuntu**. A green check will appear beside the cluster name if it is available.\n\n    ![Enter cluster name and type](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.CreateCluster.2.png \"Enter cluster name and type\")\n\n4. If you have more than one subscription, click the **Subscription** entry to select the Azure subscription that will be used for the cluster.\n\n5. Click **Resource Group** to see a list of existing resource groups and then select the one to create the cluster in. Or, you can click **Create New** and then enter the name of the new resource group. A green check will appear to indicate if the new group name is available.\n\n    > [AZURE.NOTE] This entry will default to one of your existing resource groups, if any are available.\n\n6. Click **Credentials** and then enter a password for the admin user. You must also enter an **SSH Username** and either a **PASSWORD** or **PUBLIC KEY**, which will be used to authenticate the SSH user. Using a public key is the recommended approach. Click **Select** at the bottom to save the credentials configuration.\n\n    ![Provide cluster credentials](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.CreateCluster.3.png \"Provide cluster credentials\")\n\n    For more information on using SSH with HDInsight, see one of the following articles:\n\n    * [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n    * [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n\n7. Click **Data Source** to choose an existing data source for the cluster, or create a new one. When you provision a Hadoop cluster in HDInsight, you specify an Azure Storage account. A specific Blob storage container from that account is designated as the default file system, like in the Hadoop distributed file system (HDFS). By default, the HDInsight cluster is provisioned in the same data center as the storage account you specify. For more information, see [Use Azure Blob storage with HDInsight](hdinsight-use-blob-storage.md)\n\n    ![Data source blade](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.CreateCluster.4.png \"Provide data source configuration\")\n\n    Currently you can select an Azure Storage Account as the data source for an HDInsight cluster. Use the following to understand the entries on the **Data Source** blade.\n\n    - **Selection Method**: Set this to **From all subscriptions** to enable browsing of storage accounts from all your subscriptions. Set this to **Access Key** if you want to enter the **Storage Name** and **Access Key** of an existing storage account.\n\n    - **Select storage account / Create New**: Click **Select storage account** to browse and select an existing storage account you want to associate with the cluster. Or, click **Create New** to create a new storage account. Use the field that appears to enter the name of the storage account. A green check will appear if the name is available.\n\n    - **Choose Default Container**: Use this to enter the name of the default container to use for the cluster. While you can enter any name here, we recommend using the same name as the cluster so that you can easily recognize that the container is used for this specific cluster.\n\n    - **Location**: The geographic region that the storage account is in, or will be created in.\n\n        > [AZURE.IMPORTANT] Selecting the location for the default data source will also set the location of the HDInsight cluster. The cluster and default data source must be located in the same region.\n\n    Click **Select** to save the data source configuration.\n\n8. Click **Node Pricing Tiers** to display information about the nodes that will be created for this cluster. Set the number of worker nodes that you need for the cluster. The estimated cost of the cluster will be shown within the blade.\n\n    ![Node pricing tiers blade](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.CreateCluster.5.png \"Specify number of cluster nodes\")\n\n    Click **Select** to save the node pricing configuration.\n\n9. On the **New HDInsight Cluster** blade, ensure that **Pin to Startboard** is selected, and then click **Create**. This will create the cluster and add a tile for it to the Startboard of your Azure Portal. The icon will indicate that the cluster is provisioning, and will change to display the HDInsight icon once provisioning has completed.\n\nWhile provisioning|Provisioning complete\n------------------|---------------------\n    ![Provisioning indicator on startboard](./media/hdinsight-hadoop-linux-tutorial-get-started/provisioning.png)|![Provisioned cluster tile](./media/hdinsight-hadoop-linux-tutorial-get-started/provisioned.png)\n\n> [AZURE.NOTE] It will take some time for the cluster to be created, usually around 15 minutes. Use the tile on the Startboard, or the **Notifications** entry on the left of the page to check on the provisioning process.\n\nOnce the provisioning is completed, click the tile for the cluster from the Startboard to launch the cluster blade.\n\n## <a name=\"hivequery\"></a>Submit a Hive job on the cluster\nNow that you have an HDInsight Linux cluster provisioned, the next step is to run a sample Hive job to query sample data (sample.log) that comes with HDInsight clusters. The sample data contains log information, including trace, warnings, info, and errors. We query this data to retrieve all the error logs with a specific severity. You must perform the following steps to run a Hive query on an HDInsight Linux cluster:\n\n- Connect to a Linux cluster\n- Run a Hive job\n\n\n\n### To connect to a cluster\n\nYou can connect to an HDInsight cluster on Linux from a Linux computer or a Windows-based computer by using SSH.\n\n**To connect from a Linux computer**\n\n1. Open a terminal and enter the following command:\n\n        ssh <username>@<clustername>-ssh.azurehdinsight.net\n\n    Because you provisioned a cluster with the Quick Create option, the default SSH user name is **hdiuser**. So, the command must be:\n\n        ssh hdiuser@myhdinsightcluster-ssh.azurehdinsight.net\n\n2. When prompted, enter the password that you provided while provisioning the cluster. After you are successfully connected, the prompt will change to the following:\n\n        hdiuser@headnode-0:~$\n\n\n**To connect from a Windows-based computer**\n\n1. Download <a href=\"http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\" target=\"_blank\">PuTTY</a> for Windows-based clients.\n\n2. Open PuTTY. In **Category**, click **Session**. From the **Basic options for your PuTTY session** screen, enter the SSH address of your HDInsight server in the **Host Name (or IP address)** field. The SSH address is your cluster name, followed by**-ssh.azurehdinsight.net**. For example, **myhdinsightcluster-ssh.azurehdinsight.net**.\n\n    ![Connect to an HDInsight cluster on Linux using PuTTY](./media/hdinsight-hadoop-linux-tutorial-get-started/HDI.linux.connect.putty.png)\n\n3. To save the connection information for future use, enter a name for this connection under **Saved Sessions**, and then click **Save**. The connection will be added to the list of saved sessions.\n\n4. Click **Open** to connect to the cluster. When prompted for the user name, enter **hdiuser**. For the password, enter the password you specified while provisioning the cluster. After you are successfully connected, the prompt will change to the following:\n\n        hdiuser@headnode-0:~$\n\n### To run a Hive job\n\nOnce you are connected to the cluster via SSH, use the following commands to run a Hive query.\n\n1. Start the Hive command-line interface (CLI) by using the following command at the prompt:\n\n        hive\n\n2. Using the CLI, enter the following statements to create a new table named **log4jLogs** by using the sample data already available on the cluster:\n\n        DROP TABLE log4jLogs;\n        CREATE EXTERNAL TABLE log4jLogs(t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n        STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n        SELECT t4 AS sev, COUNT(*) AS cnt FROM log4jLogs WHERE t4 = '[ERROR]' GROUP BY t4;\n\n    These statements perform the following actions:\n\n    - **DROP TABLE** - Deletes the table and the data file, in case the table already exists.\n    - **CREATE EXTERNAL TABLE** - Creates a new \"external\" table in Hive. External tables store only the table definition in Hive; the data is left in the original location.\n    - **ROW FORMAT** - Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.\n    - **STORED AS TEXTFILE LOCATION** - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.\n    - **SELECT** - Selects a count of all rows where column t4 contains the value [ERROR].\n\n    >[AZURE.NOTE] External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but you always want Hive queries to use the latest data. Dropping an external table does *not* delete the data, only the table definition.\n\n    This returns the following output:\n\n        Query ID = hdiuser_20150116000202_cceb9c6b-4356-4931-b9a7-2c373ebba493\n        Total jobs = 1\n        Launching Job 1 out of 1\n        Number of reduce tasks not specified. Estimated from input data size: 1\n        In order to change the average load for a reducer (in bytes):\n          set hive.exec.reducers.bytes.per.reducer=<number>\n        In order to limit the maximum number of reducers:\n          set hive.exec.reducers.max=<number>\n        In order to set a constant number of reducers:\n          set mapreduce.job.reduces=<number>\n        Starting Job = job_1421200049012_0006, Tracking URL = <URL>:8088/proxy/application_1421200049012_0006/\n        Kill Command = /usr/hdp/2.2.1.0-2165/hadoop/bin/hadoop job  -kill job_1421200049012_0006\n        Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n        2015-01-16 00:02:40,823 Stage-1 map = 0%,  reduce = 0%\n        2015-01-16 00:02:55,488 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.32 sec\n        2015-01-16 00:03:05,298 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.62 sec\n        MapReduce Total cumulative CPU time: 5 seconds 620 msec\n        Ended Job = job_1421200049012_0006\n        MapReduce Jobs Launched:\n        Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.62 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS\n        Total MapReduce CPU Time Spent: 5 seconds 620 msec\n        OK\n        [ERROR]    3\n        Time taken: 60.991 seconds, Fetched: 1 row(s)\n\n    Note that the output contains **[ERROR]  3**, as there are three rows that contain this value.\n\n3. Use the following statements to create a new \"internal\" table named **errorLogs**:\n\n        CREATE TABLE IF NOT EXISTS errorLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string) STORED AS ORC;\n        INSERT OVERWRITE TABLE errorLogs SELECT t1, t2, t3, t4, t5, t6, t7 FROM log4jLogs WHERE t4 = '[ERROR]';\n\n\n    These statements perform the following actions:\n\n    - **CREATE TABLE IF NOT EXISTS** - Creates a table, if it does not already exist. Since the **EXTERNAL** keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive. Unlike external tables, dropping an internal table will delete the underlying data as well.\n    - **STORED AS ORC** - Stores the data in Optimized Row Columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.\n    - **INSERT OVERWRITE ... SELECT** - Selects rows from the **log4jLogs** table that contain [ERROR], and then inserts the data into the **errorLogs** table.\n\n4. To verify that only rows containing [ERROR] in column t4 were stored to the **errorLogs** table, use the following statement to return all the rows from **errorLogs**:\n\n        SELECT * from errorLogs;\n\n    The following output should be displayed on the console:\n\n        2012-02-03  18:35:34    SampleClass0    [ERROR]  incorrect      id\n        2012-02-03  18:55:54    SampleClass1    [ERROR]  incorrect      id\n        2012-02-03  19:25:27    SampleClass4    [ERROR]  incorrect      id\n        Time taken: 0.987 seconds, Fetched: 3 row(s)\n\n    The returned data should all correspond to [ERROR] logs.\n\n## <a name=\"nextsteps\"></a>Next steps\nIn this Linux tutorial, you have learned how to provision a Hadoop cluster on Linux with HDInsight and run a Hive query on it by using SSH. To learn more, see the following articles:\n\n- [Manage HDInsight clusters using Ambari](hdinsight-hadoop-manage-ambari.md): Linux-based HDInsight clusters use Ambari for management and monitoring of Hadoop services. The Ambari web UI is available on each cluster at https://CLUSTERNAME.azurehdinsight.net.\n\n    > [AZURE.IMPORTANT] While many sections of the Ambari web are directly accessible through the Internet, the web UI for Hadoop services such as Resource Manager or Job History require the use of an SSH tunnel. For more information on using an SSH tunnel with HDInsight, see the following articles:\n    >\n    > * [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md#tunnel)\n    > * [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md#tunnel)\n\n- [Provision HDInsight on Linux using custom options](hdinsight-hadoop-provision-linux-clusters.md): Learn more details about how to provision HDInsight clusters.\n\n- [Working with HDInsight on Linux](hdinsight-hadoop-linux-information.md): If you are already familiar with Hadoop on Linux platforms, this document provides guidance on Azure specific information, such as:\n\n    * URLs for services hosted on the cluster, such as Ambari and WebHCat\n    * The location of Hadoop files and examples on the local file system\n    * The use of Azure Storage (WASB) instead of HDFS as the default data store\n\n- For more information on Hive, or to learn about Pig and MapReduce, see the following:\n\n    - [Use MapReduce with HDInsight][hdinsight-use-mapreduce]\n    - [Use Hive with HDInsight][hdinsight-use-hive]\n    - [Use Pig with HDInsight][hdinsight-use-pig]\n\n- For more information on how to work with the Azure Storage used by your HDInsight cluster, see the following:\n\n    - [Use Azure Blob storage with HDInsight](../hdinsight-use-blob-storage.md)\n    - [Upload data to HDInsight][hdinsight-upload-data]\n\n\n[1]: ../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n\n[powershell-download]: http://go.microsoft.com/fwlink/p/?linkid=320376&clcid=0x409\n[powershell-install-configure]: ../install-configure-powershell.md\n[powershell-open]: ../install-configure-powershell.md#Install\n\n[img-hdi-dashboard]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.dashboard.png\n[img-hdi-dashboard-query-select]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.dashboard.query.select.png\n[img-hdi-dashboard-query-select-result]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.dashboard.query.select.result.png\n[img-hdi-dashboard-query-select-result-output]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.dashboard.query.select.result.output.png\n[img-hdi-dashboard-query-browse-output]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.dashboard.query.browse.output.png\n[image-hdi-clusterstatus]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.ClusterStatus.png\n[image-hdi-gettingstarted-powerquery-importdata]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.GettingStarted.PowerQuery.ImportData.png\n[image-hdi-gettingstarted-powerquery-importdata2]: ./media/hdinsight-hadoop-tutorial-get-started-windows/HDI.GettingStarted.PowerQuery.ImportData2.png\n\ntest\n"
}