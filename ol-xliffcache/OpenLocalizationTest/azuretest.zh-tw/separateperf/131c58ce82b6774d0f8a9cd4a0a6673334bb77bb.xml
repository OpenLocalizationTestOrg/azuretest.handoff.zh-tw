{
  "nodes": [
    {
      "content": "Build your first Azure Data Factory pipeline using Data Factory Editor",
      "pos": [
        27,
        97
      ]
    },
    {
      "content": "In this tutorial, you will create a sample Azure Data Factory pipeline using Data Factory Editor in the Azure Portal.",
      "pos": [
        116,
        233
      ]
    },
    {
      "content": "Build your first Azure Data Factory pipeline using Data Factory Editor (Azure Portal)",
      "pos": [
        550,
        635
      ]
    },
    {
      "content": "[AZURE.SELECTOR]",
      "pos": [
        638,
        654
      ]
    },
    {
      "content": "Tutorial Overview",
      "pos": [
        658,
        675
      ]
    },
    {
      "content": "Using Data Factory Editor",
      "pos": [
        723,
        748
      ]
    },
    {
      "content": "Using PowerShell",
      "pos": [
        809,
        825
      ]
    },
    {
      "content": "Using Visual Studio",
      "pos": [
        890,
        909
      ]
    },
    {
      "content": "In this article, you will learn how to use the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](https://portal.azure.com/)</ept> to create your first pipeline.",
      "pos": [
        965,
        1092
      ]
    },
    {
      "content": "This tutorial consists of the following steps:",
      "pos": [
        1093,
        1139
      ]
    },
    {
      "content": "Creating the data factory",
      "pos": [
        1145,
        1170
      ]
    },
    {
      "content": "Creating the linked services (data stores, computes) and datasets",
      "pos": [
        1175,
        1240
      ]
    },
    {
      "content": "Creating the pipeline",
      "pos": [
        1245,
        1266
      ]
    },
    {
      "content": "This article does not provide a conceptual overview of the Azure Data Factory service.",
      "pos": [
        1268,
        1354
      ]
    },
    {
      "content": "For a detailed overview of the service, see the <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">](data-factory-introduction.md)</ept> article.",
      "pos": [
        1355,
        1478
      ]
    },
    {
      "content": "Step 1: Creating the data factory",
      "pos": [
        1483,
        1516
      ]
    },
    {
      "pos": [
        1522,
        1612
      ],
      "content": "After logging into the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](http://portal.azure.com/)</ept>, do the following:"
    },
    {
      "pos": [
        1621,
        1652
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>NEW<ept id=\"p1\">**</ept> on the left menu."
    },
    {
      "pos": [
        1662,
        1711
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Data analytics<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        1720,
        1775
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Data Factory<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>Data analytics<ept id=\"p2\">**</ept> blade."
    },
    {
      "content": "Create blade",
      "pos": [
        1787,
        1799
      ]
    },
    {
      "pos": [
        1884,
        1969
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade, enter <bpt id=\"p2\">**</bpt>DataFactoryMyFirstPipeline<ept id=\"p2\">**</ept> for the Name."
    },
    {
      "content": "New data factory blade",
      "pos": [
        1977,
        1999
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Azure Data Factory names are globally unique.",
      "pos": [
        2096,
        2159
      ]
    },
    {
      "content": "You will need to prefix the name of the data factory with your name, to enable the successful creation of the factory.",
      "pos": [
        2160,
        2278
      ]
    },
    {
      "content": "If you have not created any resource group,  you will need to create a resource group.",
      "pos": [
        2284,
        2370
      ]
    },
    {
      "content": "To do this:",
      "pos": [
        2371,
        2382
      ]
    },
    {
      "pos": [
        2391,
        2424
      ],
      "content": "Click on <bpt id=\"p1\">**</bpt>RESOURCE GROUP NAME<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        2433,
        2504
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>Create a new resource group<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">**</bpt>Resource group<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        2513,
        2583
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>ADF<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>Name<ept id=\"p2\">**</ept> in the <bpt id=\"p3\">**</bpt>Create resource group<ept id=\"p3\">**</ept> blade."
    },
    {
      "pos": [
        2592,
        2605
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Create resource group",
      "pos": [
        2621,
        2642
      ]
    },
    {
      "content": "After you have selected the resource group, verify that you are using the correct subscription where you want the data factory to be created.",
      "pos": [
        2735,
        2876
      ]
    },
    {
      "pos": [
        2881,
        2932
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Create<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>New data factory<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        2937,
        3042
      ],
      "content": "You will see the data factory being created in the <bpt id=\"p1\">**</bpt>Startboard<ept id=\"p1\">**</ept> of the Azure Preview Portal as follows:"
    },
    {
      "content": "Creating data factory status",
      "pos": [
        3053,
        3081
      ]
    },
    {
      "content": "Congratulations!",
      "pos": [
        3179,
        3195
      ]
    },
    {
      "content": "You have successfully created your first data factory.",
      "pos": [
        3196,
        3250
      ]
    },
    {
      "content": "After the data factory has been created successfully, you will see the data factory page, which shows you the contents of the data factory.",
      "pos": [
        3251,
        3390
      ]
    },
    {
      "content": "Data Factory blade",
      "pos": [
        3400,
        3418
      ]
    },
    {
      "content": "In the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial.",
      "pos": [
        3505,
        3637
      ]
    },
    {
      "content": "Step 2: Create linked services and datasets",
      "pos": [
        3643,
        3686
      ]
    },
    {
      "content": "In this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then create a dataset to represent the output data from Hive processing.",
      "pos": [
        3687,
        3880
      ]
    },
    {
      "content": "Create Azure Storage linked service",
      "pos": [
        3886,
        3921
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Author and deploy<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>DATA FACTORY<ept id=\"p2\">**</ept> blade for <bpt id=\"p3\">**</bpt>DataFactoryFirstPipeline<ept id=\"p3\">**</ept>.",
      "pos": [
        3926,
        4017
      ]
    },
    {
      "content": "This launches the Data Factory Editor.",
      "pos": [
        4018,
        4056
      ]
    },
    {
      "content": "Author and deploy tile",
      "pos": [
        4070,
        4092
      ]
    },
    {
      "pos": [
        4190,
        4243
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>New data store<ept id=\"p1\">**</ept> and choose <bpt id=\"p2\">**</bpt>Azure storage<ept id=\"p2\">**</ept>"
    },
    {
      "content": "Azure Storage linked service",
      "pos": [
        4255,
        4283
      ]
    },
    {
      "content": "You should see the JSON script for creating an Azure Storage linked service in the editor.",
      "pos": [
        4384,
        4474
      ]
    },
    {
      "content": "Replace <bpt id=\"p1\">**</bpt>account name<ept id=\"p1\">**</ept> with the name of your Azure storage account and <bpt id=\"p2\">**</bpt>account key<ept id=\"p2\">**</ept> with the access key of the Azure storage account.",
      "pos": [
        4479,
        4617
      ]
    },
    {
      "content": "To learn how to get your storage access key, see <bpt id=\"p1\">[</bpt>View, copy and regenerate storage access keys<ept id=\"p1\">](../storage/storage-create-storage-account.md/#view-copy-and-regenerate-storage-access-keys)</ept>",
      "pos": [
        4618,
        4806
      ]
    },
    {
      "pos": [
        4810,
        4875
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the linked service."
    },
    {
      "content": "Deploy button",
      "pos": [
        4883,
        4896
      ]
    },
    {
      "content": "Create Azure HDInsight linked service",
      "pos": [
        4982,
        5019
      ]
    },
    {
      "content": "Now, you will create a linked service for an on-demand HDInsight cluster that will be used to run the Hive script.",
      "pos": [
        5020,
        5134
      ]
    },
    {
      "pos": [
        5140,
        5256
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New compute<ept id=\"p2\">**</ept> on the command bar and select <bpt id=\"p3\">**</bpt>On-demand HDInsight cluster<ept id=\"p3\">**</ept>."
    },
    {
      "content": "New compute",
      "pos": [
        5264,
        5275
      ]
    },
    {
      "content": "Copy and paste the snippet below to the Draft-1 window.",
      "pos": [
        5362,
        5417
      ]
    },
    {
      "content": "The JSON snippet describes the properties that will be used to create the HDInsight cluster on-demand.",
      "pos": [
        5418,
        5520
      ]
    },
    {
      "content": "The following table provides descriptions for the JSON properties used in the snippet:",
      "pos": [
        5933,
        6019
      ]
    },
    {
      "content": "Property",
      "pos": [
        6029,
        6037
      ]
    },
    {
      "content": "Description",
      "pos": [
        6040,
        6051
      ]
    },
    {
      "content": "Version",
      "pos": [
        6083,
        6090
      ]
    },
    {
      "content": "This specifies that the version of the HDInsight created to be 3.1.",
      "pos": [
        6093,
        6160
      ]
    },
    {
      "content": "ClusterSize",
      "pos": [
        6166,
        6177
      ]
    },
    {
      "content": "This creates a one node HDInsight cluster.",
      "pos": [
        6180,
        6222
      ]
    },
    {
      "content": "TimeToLive",
      "pos": [
        6228,
        6238
      ]
    },
    {
      "content": "This specifies that the idle time for the HDInsight cluster, before it is deleted.",
      "pos": [
        6241,
        6323
      ]
    },
    {
      "content": "JobsContainer",
      "pos": [
        6328,
        6341
      ]
    },
    {
      "content": "This specifies the name of the job container that will be created to store the logs that are generated by HDInsight",
      "pos": [
        6344,
        6459
      ]
    },
    {
      "content": "linkedServiceName",
      "pos": [
        6464,
        6481
      ]
    },
    {
      "content": "This specifies the storage account that will be used to store the logs that are generated by HDInsight",
      "pos": [
        6484,
        6586
      ]
    },
    {
      "pos": [
        6590,
        6655
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the linked service."
    },
    {
      "content": "Confirm that you see both StorageLinkedService and HDInsightOnDemandLinkedService in the tree view on the left.",
      "pos": [
        6660,
        6771
      ]
    },
    {
      "content": "Tree view with linked services",
      "pos": [
        6779,
        6809
      ]
    },
    {
      "content": "Create the output dataset",
      "pos": [
        6908,
        6933
      ]
    },
    {
      "content": "Now, you will create the output dataset to represent the data stored in the Azure Blob storage.",
      "pos": [
        6934,
        7029
      ]
    },
    {
      "pos": [
        7035,
        7142
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New dataset<ept id=\"p2\">**</ept> on the command bar and select <bpt id=\"p3\">**</bpt>Azure Blob storage<ept id=\"p3\">**</ept>."
    },
    {
      "content": "New dataset",
      "pos": [
        7152,
        7163
      ]
    },
    {
      "content": "Copy and paste the snippet below to the Draft-1 window.",
      "pos": [
        7246,
        7301
      ]
    },
    {
      "content": "In the JSON snippet, you are creating a dataset called <bpt id=\"p1\">**</bpt>AzureBlobOutput<ept id=\"p1\">**</ept>, and specifying the structure of the data that will be produced by the Hive script.",
      "pos": [
        7302,
        7460
      ]
    },
    {
      "content": "In addition, you specify that the results are stored in the blob container called <bpt id=\"p1\">**</bpt>data<ept id=\"p1\">**</ept> and the folder called <bpt id=\"p2\">**</bpt>partitioneddata<ept id=\"p2\">**</ept>.",
      "pos": [
        7461,
        7594
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> section specifies that the output dataset is produced on a monthly basis.",
      "pos": [
        7595,
        7689
      ]
    },
    {
      "pos": [
        8210,
        8282
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the newly created dataset."
    },
    {
      "content": "Verify that the dataset is created successfully.",
      "pos": [
        8286,
        8334
      ]
    },
    {
      "content": "Tree view with linked services",
      "pos": [
        8342,
        8372
      ]
    },
    {
      "content": "Step 3: Creating your first pipeline",
      "pos": [
        8462,
        8498
      ]
    },
    {
      "content": "In this step, you will create your first pipeline.",
      "pos": [
        8499,
        8549
      ]
    },
    {
      "pos": [
        8554,
        8640
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Elipsis (…)<ept id=\"p2\">**</ept> and then click <bpt id=\"p3\">**</bpt>New pipeline<ept id=\"p3\">**</ept>."
    },
    {
      "content": "new pipeline button",
      "pos": [
        8652,
        8671
      ]
    },
    {
      "content": "Copy and paste the snippet below to the Draft-1 window.",
      "pos": [
        8761,
        8816
      ]
    },
    {
      "pos": [
        8824,
        8924
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Replace <bpt id=\"p1\">**</bpt>storageaccountname<ept id=\"p1\">**</ept> with the name of your storage account in the  JSON."
    },
    {
      "content": "In the JSON snippet, you are creating a pipeline that consists of a single activity that uses Hive to process Data on an HDInsight cluster.",
      "pos": [
        10115,
        10254
      ]
    },
    {
      "pos": [
        10264,
        10461
      ],
      "content": "The Hive script file, <bpt id=\"p1\">**</bpt>partitionweblogs.hql<ept id=\"p1\">**</ept>, is stored in the Azure storage account (specified by the scriptLinkedService, called <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept>), and in a container called <bpt id=\"p3\">**</bpt>script<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        10467,
        10644
      ],
      "content": "The <bpt id=\"p1\">**</bpt>extendedProperties<ept id=\"p1\">**</ept> section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (e.g ${hiveconf:PartitionedData})."
    },
    {
      "pos": [
        10650,
        10747
      ],
      "content": "The <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> properties of the pipeline specifies the active period of the pipeline."
    },
    {
      "pos": [
        10753,
        10897
      ],
      "content": "In the activity JSON, you specify that the Hive script runs on the compute specified by the linked service – <bpt id=\"p1\">**</bpt>HDInsightOnDemandLinkedService<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        10901,
        10960
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the pipeline."
    },
    {
      "content": "Confirm that you see the pipeline in the tree view.",
      "pos": [
        10964,
        11015
      ]
    },
    {
      "content": "Tree view with pipeline",
      "pos": [
        11023,
        11046
      ]
    },
    {
      "content": "Congratulations, you have successfully created your first pipeline!",
      "pos": [
        11135,
        11202
      ]
    },
    {
      "pos": [
        11206,
        11327
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> to close Data Factory Editor blades and to navigate back to the Data Factory blade, and click on <bpt id=\"p2\">**</bpt>Diagram<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Diagram tile",
      "pos": [
        11337,
        11349
      ]
    },
    {
      "content": "In the Diagram View, you will see an overview of the pipelines, and datasets used in this tutorial.",
      "pos": [
        11432,
        11531
      ]
    },
    {
      "content": "Diagram View",
      "pos": [
        11543,
        11555
      ]
    },
    {
      "content": "In the Diagram View, double-click on the dataset <bpt id=\"p1\">**</bpt>AzureBlobOutput<ept id=\"p1\">**</ept>.",
      "pos": [
        11641,
        11710
      ]
    },
    {
      "content": "You will see that the slice that is currently being processed.",
      "pos": [
        11711,
        11773
      ]
    },
    {
      "content": "Dataset",
      "pos": [
        11781,
        11788
      ]
    },
    {
      "content": "When processing is done, you will see the slice in <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state.",
      "pos": [
        11872,
        11939
      ]
    },
    {
      "content": "Note that the creation of an on-demand HDInsight cluster usually takes sometime.",
      "pos": [
        11940,
        12020
      ]
    },
    {
      "content": "Dataset",
      "pos": [
        12029,
        12036
      ]
    },
    {
      "pos": [
        12128,
        12270
      ],
      "content": "When the slice is in <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state, check the <bpt id=\"p2\">**</bpt>partitioneddata<ept id=\"p2\">**</ept> folder in the <bpt id=\"p3\">**</bpt>data<ept id=\"p3\">**</ept> container in your blob storage for the output data."
    },
    {
      "content": "Next Steps",
      "pos": [
        12282,
        12292
      ]
    },
    {
      "content": "In this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand HDInsight cluster.",
      "pos": [
        12293,
        12448
      ]
    },
    {
      "content": "To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see <bpt id=\"p1\">[</bpt>Tutorial: Copy data from an Azure blob to Azure SQL<ept id=\"p1\">](./data-factory-get-started.md)</ept>.",
      "pos": [
        12449,
        12618
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        12626,
        12639
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        12640,
        12697
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-editor.md)</ept>.",
      "pos": [
        12698,
        12860
      ]
    },
    {
      "content": "test",
      "pos": [
        12862,
        12866
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Build your first Azure Data Factory pipeline using Data Factory Editor\"\n    description=\"In this tutorial, you will create a sample Azure Data Factory pipeline using Data Factory Editor in the Azure Portal.\"\n    services=\"data-factory\"\n    documentationCenter=\"\"\n    authors=\"spelluru\"\n    manager=\"jhubbard\"\n    editor=\"monicar\"/>\n\n<tags\n    ms.service=\"data-factory\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\" \n    ms.date=\"07/27/2015\"\n    ms.author=\"spelluru\"/>\n\n# Build your first Azure Data Factory pipeline using Data Factory Editor (Azure Portal)\n> [AZURE.SELECTOR]\n- [Tutorial Overview](data-factory-build-your-first-pipeline.md)\n- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)\n- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)\n- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)\n\n\nIn this article, you will learn how to use the [Azure Preview Portal](https://portal.azure.com/) to create your first pipeline. This tutorial consists of the following steps:\n\n1.  Creating the data factory\n2.  Creating the linked services (data stores, computes) and datasets\n3.  Creating the pipeline\n\nThis article does not provide a conceptual overview of the Azure Data Factory service. For a detailed overview of the service, see the [Introduction to Azure Data Factory](data-factory-introduction.md) article.\n\n## Step 1: Creating the data factory\n\n1.  After logging into the [Azure Preview Portal](http://portal.azure.com/), do the following:\n    1.  Click **NEW** on the left menu. \n    2.  Click **Data analytics** in the **Create** blade.\n    3.  Click **Data Factory** on the **Data analytics** blade.\n\n        ![Create blade](./media/data-factory-build-your-first-pipeline-using-editor/create-blade.png)\n\n2.  In the **New data factory** blade, enter **DataFactoryMyFirstPipeline** for the Name.\n\n    ![New data factory blade](./media/data-factory-build-your-first-pipeline-using-editor/new-data-factory-blade.png)\n\n    > [AZURE.IMPORTANT] Azure Data Factory names are globally unique. You will need to prefix the name of the data factory with your name, to enable the successful creation of the factory. \n3.  If you have not created any resource group,  you will need to create a resource group. To do this:\n    1.  Click on **RESOURCE GROUP NAME**.\n    2.  Select **Create a new resource group** in the **Resource group** blade.\n    3.  Enter **ADF** for the **Name** in the **Create resource group** blade.\n    4.  Click **OK**.\n    \n        ![Create resource group](./media/data-factory-build-your-first-pipeline-using-editor/create-resource-group.png)\n4.  After you have selected the resource group, verify that you are using the correct subscription where you want the data factory to be created.\n5.  Click **Create** on the **New data factory** blade.\n6.  You will see the data factory being created in the **Startboard** of the Azure Preview Portal as follows:   \n\n    ![Creating data factory status](./media/data-factory-build-your-first-pipeline-using-editor/creating-data-factory-image.png)\n7. Congratulations! You have successfully created your first data factory. After the data factory has been created successfully, you will see the data factory page, which shows you the contents of the data factory.  \n\n    ![Data Factory blade](./media/data-factory-build-your-first-pipeline-using-editor/data-factory-blade.png)\n\nIn the subsequent steps, you will learn how to create the linked services, datasets and pipeline that you will use in this tutorial. \n\n## Step 2: Create linked services and datasets\nIn this step, you will link your Azure Storage account and an on-demand Azure HDInsight cluster to your data factory and then create a dataset to represent the output data from Hive processing.\n\n### Create Azure Storage linked service\n1.  Click **Author and deploy** on the **DATA FACTORY** blade for **DataFactoryFirstPipeline**. This launches the Data Factory Editor. \n     \n    ![Author and deploy tile](./media/data-factory-build-your-first-pipeline-using-editor/data-factory-author-deploy.png)\n2.  Click **New data store** and choose **Azure storage**\n    \n    ![Azure Storage linked service](./media/data-factory-build-your-first-pipeline-using-editor/azure-storage-linked-service.png)\n\n    You should see the JSON script for creating an Azure Storage linked service in the editor. \n4. Replace **account name** with the name of your Azure storage account and **account key** with the access key of the Azure storage account. To learn how to get your storage access key, see [View, copy and regenerate storage access keys](../storage/storage-create-storage-account.md/#view-copy-and-regenerate-storage-access-keys)\n5. Click **Deploy** on the command bar to deploy the linked service.\n\n    ![Deploy button](./media/data-factory-build-your-first-pipeline-using-editor/deploy-button.png)\n\n### Create Azure HDInsight linked service\nNow, you will create a linked service for an on-demand HDInsight cluster that will be used to run the Hive script. \n\n1. In the **Data Factory Editor**, click **New compute** on the command bar and select **On-demand HDInsight cluster**.\n\n    ![New compute](./media/data-factory-build-your-first-pipeline-using-editor/new-compute-menu.png)\n2. Copy and paste the snippet below to the Draft-1 window. The JSON snippet describes the properties that will be used to create the HDInsight cluster on-demand. \n\n        {\n          \"name\": \"HDInsightOnDemandLinkedService\",\n          \"properties\": {\n            \"type\": \"HDInsightOnDemand\",\n            \"typeProperties\": {\n              \"version\": \"3.1\",\n              \"clusterSize\": 1,\n              \"timeToLive\": \"00:05:00\",\n              \"jobsContainer\": \"adfjobs\",\n              \"linkedServiceName\": \"StorageLinkedService\"\n            }\n          }\n        }\n    \n    The following table provides descriptions for the JSON properties used in the snippet:\n    \n    Property | Description\n    -------- | -----------\n    Version | This specifies that the version of the HDInsight created to be 3.1. \n    ClusterSize | This creates a one node HDInsight cluster. \n    TimeToLive | This specifies that the idle time for the HDInsight cluster, before it is deleted.\n    JobsContainer | This specifies the name of the job container that will be created to store the logs that are generated by HDInsight\n    linkedServiceName | This specifies the storage account that will be used to store the logs that are generated by HDInsight\n3. Click **Deploy** on the command bar to deploy the linked service. \n4. Confirm that you see both StorageLinkedService and HDInsightOnDemandLinkedService in the tree view on the left.\n\n    ![Tree view with linked services](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-linked-services.png)\n \n### Create the output dataset\nNow, you will create the output dataset to represent the data stored in the Azure Blob storage. \n\n1. In the **Data Factory Editor**, click **New dataset** on the command bar and select **Azure Blob storage**.  \n\n    ![New dataset](./media/data-factory-build-your-first-pipeline-using-editor/new-data-set.png)\n2. Copy and paste the snippet below to the Draft-1 window. In the JSON snippet, you are creating a dataset called **AzureBlobOutput**, and specifying the structure of the data that will be produced by the Hive script. In addition, you specify that the results are stored in the blob container called **data** and the folder called **partitioneddata**. The **availability** section specifies that the output dataset is produced on a monthly basis.\n    \n        {\n          \"name\": \"AzureBlobOutput\",\n          \"properties\": {\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"data/partitioneddata\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"availability\": {\n              \"frequency\": \"Month\",\n              \"interval\": 1\n            }\n          }\n        }\n\n3. Click **Deploy** on the command bar to deploy the newly created dataset.\n4. Verify that the dataset is created successfully.\n\n    ![Tree view with linked services](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-data-set.png)\n\n## Step 3: Creating your first pipeline\nIn this step, you will create your first pipeline.\n\n1. In the **Data Factory Editor**, click **Elipsis (…)** and then click **New pipeline**.\n    \n    ![new pipeline button](./media/data-factory-build-your-first-pipeline-using-editor/new-pipeline-button.png)\n2. Copy and paste the snippet below to the Draft-1 window.\n\n    > [AZURE.IMPORTANT] Replace **storageaccountname** with the name of your storage account in the  JSON.\n\n        {\n          \"name\": \"MyFirstPipeline\",\n          \"properties\": {\n            \"description\": \"My first Azure Data Factory pipeline\",\n            \"activities\": [\n              {\n                \"type\": \"HDInsightHive\",\n                \"typeProperties\": {\n                  \"scriptPath\": \"script/partitionweblogs.hql\",\n                  \"scriptLinkedService\": \"StorageLinkedService\",\n                  \"defines\": {\n                    \"partitionedtable\": \"wasb://data@<storageaccountname>.blob.core.windows.net/partitioneddata\"\n                  }\n                },\n                \"outputs\": [\n                  {\n                    \"name\": \"AzureBlobOutput\"\n                  }\n                ],\n                \"scheduler\": {\n                    \"frequency\": \"Month\",\n                    \"interval\": 1\n                },\n                \"policy\": {\n                  \"concurrency\": 1,\n                  \"retry\": 3\n                },\n                \"name\": \"RunSampleHiveActivity\",\n                \"linkedServiceName\": \"HDInsightOnDemandLinkedService\"\n              }\n            ],\n            \"start\": \"2014-01-01\",\n            \"end\": \"2014-01-02\"\n          }\n        }\n \n    In the JSON snippet, you are creating a pipeline that consists of a single activity that uses Hive to process Data on an HDInsight cluster.\n    \n    The Hive script file, **partitionweblogs.hql**, is stored in the Azure storage account (specified by the scriptLinkedService, called **StorageLinkedService**), and in a container called **script**.\n\n    The **extendedProperties** section is used to specify the runtime settings that will be passed to the hive script as Hive configuration values (e.g ${hiveconf:PartitionedData}).\n\n    The **start** and **end** properties of the pipeline specifies the active period of the pipeline.\n\n    In the activity JSON, you specify that the Hive script runs on the compute specified by the linked service – **HDInsightOnDemandLinkedService**.\n3. Click **Deploy** on the command bar to deploy the pipeline.\n4. Confirm that you see the pipeline in the tree view.\n\n    ![Tree view with pipeline](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-pipeline.png)\n5. Congratulations, you have successfully created your first pipeline!\n6. Click **X** to close Data Factory Editor blades and to navigate back to the Data Factory blade, and click on **Diagram**.\n  \n    ![Diagram tile](./media/data-factory-build-your-first-pipeline-using-editor/diagram-tile.png)\n7. In the Diagram View, you will see an overview of the pipelines, and datasets used in this tutorial.\n    \n    ![Diagram View](./media/data-factory-build-your-first-pipeline-using-editor/diagram-view-2.png) \n8. In the Diagram View, double-click on the dataset **AzureBlobOutput**. You will see that the slice that is currently being processed.\n\n    ![Dataset](./media/data-factory-build-your-first-pipeline-using-editor/dataset-blade.png)\n9. When processing is done, you will see the slice in **Ready** state. Note that the creation of an on-demand HDInsight cluster usually takes sometime. \n\n    ![Dataset](./media/data-factory-build-your-first-pipeline-using-editor/dataset-slice-ready.png) \n10. When the slice is in **Ready** state, check the **partitioneddata** folder in the **data** container in your blob storage for the output data.  \n \n\n \n\n## Next Steps\nIn this article, you have created a pipeline with a transformation activity (HDInsight Activity) that runs a Hive script on an on-demand HDInsight cluster. To see how to use a Copy Activity to copy data from an Azure Blob to Azure SQL, see [Tutorial: Copy data from an Azure blob to Azure SQL](./data-factory-get-started.md).\n  \n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-editor.md). \ntest\n"
}