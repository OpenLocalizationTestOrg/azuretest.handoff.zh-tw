{
  "nodes": [
    {
      "content": "Use Script Action to install Spark on Hadoop cluster | Microsoft Azure",
      "pos": [
        28,
        98
      ]
    },
    {
      "content": "Learn how to customize an HDInsight cluster with Spark.",
      "pos": [
        118,
        173
      ]
    },
    {
      "content": "You'll use a Script Action configuration option to use a script to install Spark.",
      "pos": [
        174,
        255
      ]
    },
    {
      "content": "Install and use Spark on HDInsight Hadoop clusters",
      "pos": [
        573,
        623
      ]
    },
    {
      "content": "In this document, you will learn how to install Spark by using Script Action.",
      "pos": [
        625,
        702
      ]
    },
    {
      "content": "Script Action lets you run scripts to customize a cluster, only when the cluster is being created.",
      "pos": [
        703,
        801
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p1\">][hdinsight-cluster-customize]</ept>.",
      "pos": [
        802,
        907
      ]
    },
    {
      "content": "Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.",
      "pos": [
        908,
        1004
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster.",
      "pos": [
        1008,
        1164
      ]
    },
    {
      "content": "However, this is limited to Windows-based clusters currently.",
      "pos": [
        1165,
        1226
      ]
    },
    {
      "content": "Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1.",
      "pos": [
        1227,
        1336
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p1\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.",
      "pos": [
        1337,
        1471
      ]
    },
    {
      "pos": [
        1477,
        1512
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"whatis\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>What is Spark?"
    },
    {
      "content": "<ph id=\"ph1\">&lt;a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\"&gt;</ph>Apache Spark<ph id=\"ph2\">&lt;/a&gt;</ph> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.",
      "pos": [
        1514,
        1746
      ]
    },
    {
      "content": "Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
      "pos": [
        1747,
        1880
      ]
    },
    {
      "content": "Spark can also be used to perform conventional disk-based data processing.",
      "pos": [
        1882,
        1956
      ]
    },
    {
      "content": "Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.",
      "pos": [
        1957,
        2062
      ]
    },
    {
      "content": "Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
      "pos": [
        2063,
        2214
      ]
    },
    {
      "content": "This topic provides instructions on how to customize an HDInsight cluster to install Spark.",
      "pos": [
        2217,
        2308
      ]
    },
    {
      "pos": [
        2313,
        2371
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"whatis\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Which version of Spark can I install?"
    },
    {
      "content": "In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster.",
      "pos": [
        2373,
        2466
      ]
    },
    {
      "content": "This script installs Spark 1.3.1.",
      "pos": [
        2467,
        2500
      ]
    },
    {
      "content": "You can modify this script or create your own script to install other versions of Spark.",
      "pos": [
        2502,
        2590
      ]
    },
    {
      "content": "What the script does",
      "pos": [
        2595,
        2615
      ]
    },
    {
      "pos": [
        2617,
        2688
      ],
      "content": "This script installs Spark version 1.3.1 into <ph id=\"ph1\">`/usr/hdp/current/spark`</ph>."
    },
    {
      "pos": [
        2693,
        2749
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Install Spark using Script Actions"
    },
    {
      "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p1\">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh<ept id=\"p1\">](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh)</ept>.",
      "pos": [
        2751,
        3055
      ]
    },
    {
      "content": "This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal.",
      "pos": [
        3056,
        3180
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script.",
      "pos": [
        3185,
        3296
      ]
    },
    {
      "content": "For more information on using these methods, see <bpt id=\"p1\">[</bpt>Customize HDInsight clusters with Script Actions<ept id=\"p1\">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.",
      "pos": [
        3297,
        3442
      ]
    },
    {
      "pos": [
        3447,
        3621
      ],
      "content": "Start provisioning a cluster by using the steps in <bpt id=\"p1\">[</bpt>Provision Linux-based HDInsight clusters<ept id=\"p1\">](hdinsight-provision-linux-clusters.md#portal)</ept>, but do not complete provisioning."
    },
    {
      "pos": [
        3626,
        3728
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Optional Configuration<ept id=\"p1\">**</ept> blade, select <bpt id=\"p2\">**</bpt>Script Actions<ept id=\"p2\">**</ept>, and provide the information below:"
    },
    {
      "pos": [
        3736,
        3790
      ],
      "content": "<bpt id=\"p1\">__</bpt>NAME<ept id=\"p1\">__</ept>: Enter a friendly name for the script action."
    },
    {
      "pos": [
        3797,
        3908
      ],
      "content": "<bpt id=\"p1\">__</bpt>SCRIPT URI<ept id=\"p1\">__</ept>: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh"
    },
    {
      "pos": [
        3915,
        3942
      ],
      "content": "<bpt id=\"p1\">__</bpt>HEAD<ept id=\"p1\">__</ept>: Check this option"
    },
    {
      "pos": [
        3949,
        3978
      ],
      "content": "<bpt id=\"p1\">__</bpt>WORKER<ept id=\"p1\">__</ept>: Check this option"
    },
    {
      "pos": [
        3985,
        4051
      ],
      "content": "<bpt id=\"p1\">__</bpt>ZOOKEEPER<ept id=\"p1\">__</ept>: Check this option to install on the Zookeeper node."
    },
    {
      "pos": [
        4058,
        4096
      ],
      "content": "<bpt id=\"p1\">__</bpt>PARAMETERS<ept id=\"p1\">__</ept>: Leave this field blank"
    },
    {
      "content": "At the bottom of the <bpt id=\"p1\">**</bpt>Script Actions<ept id=\"p1\">**</ept>, use the <bpt id=\"p2\">**</bpt>Select<ept id=\"p2\">**</ept> button to save the configuration.",
      "pos": [
        4101,
        4194
      ]
    },
    {
      "content": "Finally, use the <bpt id=\"p1\">**</bpt>Select<ept id=\"p1\">**</ept> button at the bottom of the <bpt id=\"p2\">**</bpt>Optional Configuration<ept id=\"p2\">**</ept> blade to save the optional configuration information.",
      "pos": [
        4195,
        4331
      ]
    },
    {
      "pos": [
        4336,
        4474
      ],
      "content": "Continue provisining the cluster as described in <bpt id=\"p1\">[</bpt>Provision Linux-based HDInsight clusters<ept id=\"p1\">](hdinsight-provision-linux-clusters.md#portal)</ept>."
    },
    {
      "pos": [
        4479,
        4534
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"usespark\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>How do I use Spark in HDInsight?"
    },
    {
      "content": "Spark provides APIs in Scala, Python, and Java.",
      "pos": [
        4536,
        4583
      ]
    },
    {
      "content": "You can also use the interactive Spark shell to run Spark queries.",
      "pos": [
        4584,
        4650
      ]
    },
    {
      "content": "Once your cluster has finished provisioning, use the following to connect to your HDInsight cluster:",
      "pos": [
        4651,
        4751
      ]
    },
    {
      "content": "For more information on using SSH with HDInsight, see the following:",
      "pos": [
        4810,
        4878
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X",
      "pos": [
        4883,
        4953
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Windows",
      "pos": [
        4999,
        5056
      ]
    },
    {
      "content": "Once connected, use the following sections for specific steps on using Spark:",
      "pos": [
        5102,
        5179
      ]
    },
    {
      "content": "Using the Spark shell to run interactive queries",
      "pos": [
        5184,
        5232
      ]
    },
    {
      "content": "Using the Spark shell to run Spark SQL queries",
      "pos": [
        5250,
        5296
      ]
    },
    {
      "content": "Using a standalone Scala program",
      "pos": [
        5313,
        5345
      ]
    },
    {
      "pos": [
        5364,
        5437
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"sparkshell\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using the Spark shell to run interactive queries"
    },
    {
      "content": "Run the following command to start the Spark shell:",
      "pos": [
        5442,
        5493
      ]
    },
    {
      "content": "After the command finishes running, you should get a Scala prompt:",
      "pos": [
        5562,
        5628
      ]
    },
    {
      "content": "On the Scala prompt, enter the Spark query shown below.",
      "pos": [
        5650,
        5705
      ]
    },
    {
      "content": "This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
      "pos": [
        5706,
        5889
      ]
    },
    {
      "content": "The output should resemble the following:",
      "pos": [
        6108,
        6149
      ]
    },
    {
      "content": "Enter :q to exit the Scala prompt.",
      "pos": [
        6234,
        6268
      ]
    },
    {
      "pos": [
        6285,
        6354
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"sparksql\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using the Spark shell to run Spark SQL queries"
    },
    {
      "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.",
      "pos": [
        6356,
        6479
      ]
    },
    {
      "content": "In this section, we look at using Spark to run a Hive query on a sample Hive table.",
      "pos": [
        6480,
        6563
      ]
    },
    {
      "content": "The Hive table used in this section (called <bpt id=\"p1\">**</bpt>hivesampletable<ept id=\"p1\">**</ept>) is available by default when you provision a cluster.",
      "pos": [
        6564,
        6682
      ]
    },
    {
      "content": "Run the following command to start the Spark shell:",
      "pos": [
        6687,
        6738
      ]
    },
    {
      "content": "After the command finishes running, you should get a Scala prompt:",
      "pos": [
        6807,
        6873
      ]
    },
    {
      "content": "On the Scala prompt, set the Hive context.",
      "pos": [
        6895,
        6937
      ]
    },
    {
      "content": "This is required to work with Hive queries by using Spark.",
      "pos": [
        6938,
        6996
      ]
    },
    {
      "pos": [
        7077,
        7186
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>  <ph id=\"ph2\">`sc`</ph> in this statement is the default Spark context that is set when you start the Spark shell."
    },
    {
      "content": "Run a Hive query by using the Hive context and print the output to the console.",
      "pos": [
        7191,
        7270
      ]
    },
    {
      "content": "The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
      "pos": [
        7271,
        7375
      ]
    },
    {
      "content": "You should see an output like the following:",
      "pos": [
        7507,
        7551
      ]
    },
    {
      "content": "Enter :q to exit the Scala prompt.",
      "pos": [
        7830,
        7864
      ]
    },
    {
      "pos": [
        7882,
        7939
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"standalone\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Using a standalone Scala program"
    },
    {
      "content": "In this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.)",
      "pos": [
        7941,
        8121
      ]
    },
    {
      "content": "Use the following commands to install the Scala Build Tool:",
      "pos": [
        8127,
        8186
      ]
    },
    {
      "pos": [
        8363,
        8403
      ],
      "content": "When prompted, select <bpt id=\"p1\">__</bpt>Y<ept id=\"p1\">__</ept> to continue."
    },
    {
      "content": "Create the directory structure for the Scala project:",
      "pos": [
        8408,
        8461
      ]
    },
    {
      "pos": [
        8522,
        8624
      ],
      "content": "Create a new file named <bpt id=\"p1\">__</bpt>simple.sbt<ept id=\"p1\">__</ept>, which contains the configuration information for this project."
    },
    {
      "content": "Use the following as the contents of the file:",
      "pos": [
        8689,
        8735
      ]
    },
    {
      "pos": [
        8922,
        8991
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Make sure you retain the empty lines between each entry."
    },
    {
      "pos": [
        9001,
        9059
      ],
      "content": "Use <bpt id=\"p1\">__</bpt>Ctrl+X<ept id=\"p1\">__</ept>, then <bpt id=\"p2\">__</bpt>Y<ept id=\"p2\">__</ept> and <bpt id=\"p3\">__</bpt>Enter<ept id=\"p3\">__</ept> to save the file."
    },
    {
      "pos": [
        9064,
        9188
      ],
      "content": "Use the following command to create a new file named <bpt id=\"p1\">__</bpt>SimpleApp.scala<ept id=\"p1\">__</ept> in the <bpt id=\"p2\">__</bpt>SimpleScalaApp/src/main/scala<ept id=\"p2\">__</ept> directory:"
    },
    {
      "content": "Use the following as the contents of the file:",
      "pos": [
        9239,
        9285
      ]
    },
    {
      "pos": [
        10087,
        10146
      ],
      "content": "Use <bpt id=\"p1\">__</bpt>Ctrl+X<ept id=\"p1\">__</ept>, then <bpt id=\"p2\">__</bpt>Y<ept id=\"p2\">__</ept>, and <bpt id=\"p3\">__</bpt>Enter<ept id=\"p3\">__</ept> to save the file."
    },
    {
      "pos": [
        10151,
        10269
      ],
      "content": "From the <bpt id=\"p1\">__</bpt>SimpleScalaApp<ept id=\"p1\">__</ept> directory, use the following command to build the application, and store it in a jar file:"
    },
    {
      "pos": [
        10296,
        10439
      ],
      "content": "Once the application is compiled, you will see a <bpt id=\"p1\">**</bpt>simpleapp_2.10-1.0.jar<ept id=\"p1\">**</ept> file created in the __SimpleScalaApp/target/scala-2.10** directory."
    },
    {
      "content": "Use the following command to run the SimpleApp.scala program:",
      "pos": [
        10444,
        10505
      ]
    },
    {
      "content": "When the program finishes running, the output is displayed on the console.",
      "pos": [
        10636,
        10710
      ]
    },
    {
      "content": "Next steps",
      "pos": [
        10765,
        10775
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install and use Hue on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-hue-linux.md)</ept>.",
      "pos": [
        10779,
        10854
      ]
    },
    {
      "content": "Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.",
      "pos": [
        10855,
        10998
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install R on HDInsight clusters<ept id=\"p1\">][hdinsight-install-r]</ept> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.",
      "pos": [
        11002,
        11165
      ]
    },
    {
      "content": "R is an open-source language and environment for statistical computing.",
      "pos": [
        11166,
        11237
      ]
    },
    {
      "content": "It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.",
      "pos": [
        11238,
        11394
      ]
    },
    {
      "content": "It also provides extensive graphical capabilities.",
      "pos": [
        11395,
        11445
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-giraph-install-linux.md)</ept>.",
      "pos": [
        11449,
        11530
      ]
    },
    {
      "content": "Use cluster customization to install Giraph on HDInsight Hadoop clusters.",
      "pos": [
        11531,
        11604
      ]
    },
    {
      "content": "Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
      "pos": [
        11605,
        11705
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-solr-install-linux.md)</ept>.",
      "pos": [
        11709,
        11786
      ]
    },
    {
      "content": "Use cluster customization to install Solr on HDInsight Hadoop clusters.",
      "pos": [
        11787,
        11858
      ]
    },
    {
      "content": "Solr allows you to perform powerful search operations on data stored.",
      "pos": [
        11859,
        11928
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Install Hue on HDInsight clusters<ept id=\"p1\">](hdinsight-hadoop-hue-linux.md)</ept>.",
      "pos": [
        11932,
        11999
      ]
    },
    {
      "content": "Use cluster customization to install Hue on HDInsight Hadoop clusters.",
      "pos": [
        12000,
        12070
      ]
    },
    {
      "content": "Hue is a set of Web applications used to interact with a Hadoop cluster.",
      "pos": [
        12071,
        12143
      ]
    },
    {
      "content": "test",
      "pos": [
        12411,
        12415
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Use Script Action to install Spark on Hadoop cluster | Microsoft Azure\" \n    description=\"Learn how to customize an HDInsight cluster with Spark. You'll use a Script Action configuration option to use a script to install Spark.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"Blackmist\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/20/2015\" \n    ms.author=\"larryfr\"/>\n\n# Install and use Spark on HDInsight Hadoop clusters\n\nIn this document, you will learn how to install Spark by using Script Action. Script Action lets you run scripts to customize a cluster, only when the cluster is being created. For more information, see [Customize HDInsight cluster using Script Action][hdinsight-cluster-customize]. Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.\n\n> [AZURE.NOTE] HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster. However, this is limited to Windows-based clusters currently. Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1. For more information, see [Get Started with Apache Spark on HDInsight](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md).\n\n\n## <a name=\"whatis\"></a>What is Spark?\n\n<a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.\n\nSpark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark. \n\nThis topic provides instructions on how to customize an HDInsight cluster to install Spark.\n\n## <a name=\"whatis\"></a>Which version of Spark can I install?\n\nIn this topic, we use a Script Action custom script to install Spark on an HDInsight cluster. This script installs Spark 1.3.1.\n\nYou can modify this script or create your own script to install other versions of Spark.\n\n## What the script does\n\nThis script installs Spark version 1.3.1 into `/usr/hdp/current/spark`.\n\n## <a name=\"install\"></a>Install Spark using Script Actions\n\nA sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at [https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh). This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal. \n\n> [AZURE.NOTE] You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script. For more information on using these methods, see [Customize HDInsight clusters with Script Actions](hdinsight-hadoop-customize-cluster-linux.md).\n\n1. Start provisioning a cluster by using the steps in [Provision Linux-based HDInsight clusters](hdinsight-provision-linux-clusters.md#portal), but do not complete provisioning.\n\n2. On the **Optional Configuration** blade, select **Script Actions**, and provide the information below:\n\n    * __NAME__: Enter a friendly name for the script action.\n    * __SCRIPT URI__: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh\n    * __HEAD__: Check this option\n    * __WORKER__: Check this option\n    * __ZOOKEEPER__: Check this option to install on the Zookeeper node.\n    * __PARAMETERS__: Leave this field blank\n\n3. At the bottom of the **Script Actions**, use the **Select** button to save the configuration. Finally, use the **Select** button at the bottom of the **Optional Configuration** blade to save the optional configuration information.\n\n4. Continue provisining the cluster as described in [Provision Linux-based HDInsight clusters](hdinsight-provision-linux-clusters.md#portal).\n\n## <a name=\"usespark\"></a>How do I use Spark in HDInsight?\n\nSpark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. Once your cluster has finished provisioning, use the following to connect to your HDInsight cluster:\n\n    ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.net\n    \nFor more information on using SSH with HDInsight, see the following:\n\n* [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n* [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\nOnce connected, use the following sections for specific steps on using Spark:\n\n- [Using the Spark shell to run interactive queries](#sparkshell)\n- [Using the Spark shell to run Spark SQL queries](#sparksql) \n- [Using a standalone Scala program](#standalone)\n\n###<a name=\"sparkshell\"></a>Using the Spark shell to run interactive queries\n\n1. Run the following command to start the Spark shell:\n\n         /usr/hdp/current/spark/bin/spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n5. On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.\n\n        val file = sc.textFile(\"/example/data/gutenberg/davinci.txt\")\n        val counts = file.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n        counts.toArray().foreach(println)\n\n6. The output should resemble the following:\n\n        (felt,,1)\n        (axle-tree,1)\n        (deals,9)\n        (virtuous,4)\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n###<a name=\"sparksql\"></a>Using the Spark shell to run Spark SQL queries\n\nSpark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called **hivesampletable**) is available by default when you provision a cluster.\n\n1. Run the following command to start the Spark shell:\n\n         /usr/hdp/current/spark/bin/spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n2. On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.\n\n        val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n\n    > [AZURE.NOTE]  `sc` in this statement is the default Spark context that is set when you start the Spark shell.\n\n5. Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.\n\n        hiveContext.sql(\"\"\"SELECT * FROM hivesampletable WHERE devicemake LIKE \"HTC%\" LIMIT 20\"\"\").collect().foreach(println)\n\n6. You should see an output like the following:\n\n        [820,11:35:17,en-US,Android,HTC,Inspire 4G,Louisiana,UnitedStates, 2.7383836,0,1]\n        [1055,17:24:08,en-US,Android,HTC,Incredible,Ohio,United States,18.0894738,0,0]\n        [1067,03:42:29,en-US,Windows Phone,HTC,HD7,District Of Columbia,United States,null,0,0]\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n### <a name=\"standalone\"></a>Using a standalone Scala program\n\nIn this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.) \n\n1. Use the following commands to install the Scala Build Tool:\n\n        echo \"deb http://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\n        sudo apt-get update\n        sudo apt-get install sbt\n        \n    When prompted, select __Y__ to continue.\n\n2. Create the directory structure for the Scala project:\n\n        mkdir -p SimpleScalaApp/src/main/scala\n        \n3. Create a new file named __simple.sbt__, which contains the configuration information for this project.\n\n        cd SimpleScalaApp\n        nano simple.sbt\n        \n    Use the following as the contents of the file:\n\n        name := \"SimpleApp\"\n    \n        version := \"1.0\"\n    \n        scalaVersion := \"2.10.4\"\n    \n        libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n\n\n    > [AZURE.NOTE] Make sure you retain the empty lines between each entry.\n    \n    Use __Ctrl+X__, then __Y__ and __Enter__ to save the file.\n\n4. Use the following command to create a new file named __SimpleApp.scala__ in the __SimpleScalaApp/src/main/scala__ directory:\n\n        nano src/main/scala/SimpleApp.scala\n\n    Use the following as the contents of the file:\n\n        /* SimpleApp.scala */\n        import org.apache.spark.SparkContext\n        import org.apache.spark.SparkContext._\n        import org.apache.spark.SparkConf\n        \n        object SimpleApp {\n          def main(args: Array[String]) {\n            val logFile = \"/example/data/gutenberg/davinci.txt\"         //Location of the sample data file on Azure Blob storage\n            val conf = new SparkConf().setAppName(\"SimpleApplication\")\n            val sc = new SparkContext(conf)\n            val logData = sc.textFile(logFile, 2).cache()\n            val numAs = logData.filter(line => line.contains(\"a\")).count()\n            val numBs = logData.filter(line => line.contains(\"b\")).count()\n            println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n          }\n        }\n\n    Use __Ctrl+X__, then __Y__, and __Enter__ to save the file.\n\n5. From the __SimpleScalaApp__ directory, use the following command to build the application, and store it in a jar file:\n\n        sbt package\n\n    Once the application is compiled, you will see a **simpleapp_2.10-1.0.jar** file created in the __SimpleScalaApp/target/scala-2.10** directory.\n\n6. Use the following command to run the SimpleApp.scala program:\n\n\n        /usr/hdp/current/spark/bin/spark-submit --class \"SimpleApp\" --master local target/scala-2.10/simpleapp_2.10-1.0.jar\n\n4. When the program finishes running, the output is displayed on the console.\n\n        Lines with a: 21374, Lines with b: 11430\n\n## Next steps\n\n- [Install and use Hue on HDInsight clusters](hdinsight-hadoop-hue-linux.md). Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.\n\n- [Install R on HDInsight clusters][hdinsight-install-r] provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.\n\n- [Install Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install-linux.md). Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.\n\n- [Install Solr on HDInsight clusters](hdinsight-hadoop-solr-install-linux.md). Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.\n\n- [Install Hue on HDInsight clusters](hdinsight-hadoop-hue-linux.md). Use cluster customization to install Hue on HDInsight Hadoop clusters. Hue is a set of Web applications used to interact with a Hadoop cluster.\n\n\n\n[hdinsight-provision]: hdinsight-provision-clusters-linux.md\n[hdinsight-install-r]: hdinsight-hadoop-r-scripts-linux.md\n[hdinsight-cluster-customize]: hdinsight-hadoop-customize-cluster-linux.md\n[powershell-install-configure]: ../install-configure-powershell.md\n \ntest\n"
}