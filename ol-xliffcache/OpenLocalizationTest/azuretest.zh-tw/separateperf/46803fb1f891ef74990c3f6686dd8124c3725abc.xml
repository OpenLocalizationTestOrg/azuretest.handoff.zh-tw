{
  "nodes": [
    {
      "content": "Caching guidance | Microsoft Azure",
      "pos": [
        26,
        60
      ]
    },
    {
      "content": "Guidance on caching to improve performance and scalability.",
      "pos": [
        78,
        137
      ]
    },
    {
      "content": "Caching guidance",
      "pos": [
        425,
        441
      ]
    },
    {
      "content": "Caching is a common technique that aims to improve the performance and",
      "pos": [
        491,
        561
      ]
    },
    {
      "content": "scalability of a system by temporarily copying frequently accessed data",
      "pos": [
        562,
        633
      ]
    },
    {
      "content": "to fast storage located close to the application.",
      "pos": [
        634,
        683
      ]
    },
    {
      "content": "If this fast data storage",
      "pos": [
        684,
        709
      ]
    },
    {
      "content": "is located closer to the application than the original source then caching",
      "pos": [
        710,
        784
      ]
    },
    {
      "content": "can significantly improve response times for client applications by serving",
      "pos": [
        785,
        860
      ]
    },
    {
      "content": "data more quickly.",
      "pos": [
        861,
        879
      ]
    },
    {
      "content": "Caching is most effective when a client instance repeatedly",
      "pos": [
        880,
        939
      ]
    },
    {
      "content": "reads the same data, especially if the data remains relatively static and",
      "pos": [
        940,
        1013
      ]
    },
    {
      "content": "the original data store is slow relative to the speed of the cache, is",
      "pos": [
        1014,
        1084
      ]
    },
    {
      "content": "subject to a high level of contention, or is far away when network latency",
      "pos": [
        1085,
        1159
      ]
    },
    {
      "content": "can cause access to be slow.",
      "pos": [
        1160,
        1188
      ]
    },
    {
      "content": "Caching in distributed applications",
      "pos": [
        1193,
        1228
      ]
    },
    {
      "content": "Distributed applications typically implement either or both of the",
      "pos": [
        1230,
        1296
      ]
    },
    {
      "content": "following strategies when caching data:",
      "pos": [
        1297,
        1336
      ]
    },
    {
      "content": "Using a private cache, where data is held locally on the computer running an instance of an application or service.",
      "pos": [
        1340,
        1455
      ]
    },
    {
      "content": "Using a shared cache, serving as a common source which can be accessed by multiple processes and/or machines.",
      "pos": [
        1458,
        1567
      ]
    },
    {
      "content": "In both cases, caching could be performed client-side (by the process providing",
      "pos": [
        1569,
        1648
      ]
    },
    {
      "content": "the user interface for a system, such as a web browser or desktop application),",
      "pos": [
        1649,
        1728
      ]
    },
    {
      "content": "and/or server-side (by the process providing the business services",
      "pos": [
        1729,
        1795
      ]
    },
    {
      "content": "running remotely).",
      "pos": [
        1796,
        1814
      ]
    },
    {
      "content": "Private caching",
      "pos": [
        1820,
        1835
      ]
    },
    {
      "content": "The most basic type of cache is an in-memory store, held in the address",
      "pos": [
        1837,
        1908
      ]
    },
    {
      "content": "space of a single process and accessed directly by the code that runs",
      "pos": [
        1909,
        1978
      ]
    },
    {
      "content": "in that process.",
      "pos": [
        1979,
        1995
      ]
    },
    {
      "content": "This type of cache is very quick to access, and it can",
      "pos": [
        1996,
        2050
      ]
    },
    {
      "content": "provide an extremely effective strategy for storing modest amounts of",
      "pos": [
        2051,
        2120
      ]
    },
    {
      "content": "static data as the size of a cache is typically constrained by the",
      "pos": [
        2121,
        2187
      ]
    },
    {
      "content": "volume of memory available on the machine hosting the process.",
      "pos": [
        2188,
        2250
      ]
    },
    {
      "content": "If you",
      "pos": [
        2251,
        2257
      ]
    },
    {
      "content": "need to cache more information than is physically possible in memory,",
      "pos": [
        2258,
        2327
      ]
    },
    {
      "content": "you can write cached data to the local file system.",
      "pos": [
        2328,
        2379
      ]
    },
    {
      "content": "This will",
      "pos": [
        2380,
        2389
      ]
    },
    {
      "content": "necessarily be slower to access than data held in-memory, but should",
      "pos": [
        2390,
        2458
      ]
    },
    {
      "content": "still be faster and more reliable than retrieving data across a network.",
      "pos": [
        2459,
        2531
      ]
    },
    {
      "content": "If you have multiple instances of an application that uses this model",
      "pos": [
        2533,
        2602
      ]
    },
    {
      "content": "running concurrently, each application instance will have its own",
      "pos": [
        2603,
        2668
      ]
    },
    {
      "content": "independent cache holding its own copy of data.",
      "pos": [
        2669,
        2716
      ]
    },
    {
      "content": "You should think of a cache as a snapshot of the original data at some",
      "pos": [
        2718,
        2788
      ]
    },
    {
      "content": "point in the past.",
      "pos": [
        2789,
        2807
      ]
    },
    {
      "content": "If this data is not static, it is likely that",
      "pos": [
        2808,
        2853
      ]
    },
    {
      "content": "different application instances will hold different versions of the",
      "pos": [
        2854,
        2921
      ]
    },
    {
      "content": "data in their caches.",
      "pos": [
        2922,
        2943
      ]
    },
    {
      "content": "Therefore, the same query performed by these",
      "pos": [
        2944,
        2988
      ]
    },
    {
      "content": "instances could return different results, as shown in Figure 1.",
      "pos": [
        2989,
        3052
      ]
    },
    {
      "content": "Using an in-memory cache in different instances of an application",
      "pos": [
        3056,
        3121
      ]
    },
    {
      "content": "Figure 1: Using an in-memory cache in different instances of an application",
      "pos": [
        3167,
        3242
      ]
    },
    {
      "content": "Shared Caching",
      "pos": [
        3249,
        3263
      ]
    },
    {
      "content": "Using a shared cache can help to alleviate the concern that data may",
      "pos": [
        3265,
        3333
      ]
    },
    {
      "content": "differ in each cache, as can occur with in-memory caching.",
      "pos": [
        3334,
        3392
      ]
    },
    {
      "content": "Shared",
      "pos": [
        3393,
        3399
      ]
    },
    {
      "content": "caching ensures that different application instances see the same",
      "pos": [
        3400,
        3465
      ]
    },
    {
      "content": "view of cached data by locating the cache in a separate location,",
      "pos": [
        3466,
        3531
      ]
    },
    {
      "content": "typically hosted as part of a separate service, as shown in Figure 2.",
      "pos": [
        3532,
        3601
      ]
    },
    {
      "content": "Using a shared cache_",
      "pos": [
        3605,
        3626
      ]
    },
    {
      "content": "Figure 2: Using a shared cache",
      "pos": [
        3672,
        3702
      ]
    },
    {
      "content": "An important benefit of using the shared caching approach is the",
      "pos": [
        3705,
        3769
      ]
    },
    {
      "content": "scalability it can help to provide.",
      "pos": [
        3770,
        3805
      ]
    },
    {
      "content": "Many shared cache services are",
      "pos": [
        3806,
        3836
      ]
    },
    {
      "content": "implemented by using a cluster of servers, and utilize software that",
      "pos": [
        3837,
        3905
      ]
    },
    {
      "content": "distributes the data across the cluster in a transparent manner.",
      "pos": [
        3906,
        3970
      ]
    },
    {
      "content": "An",
      "pos": [
        3971,
        3973
      ]
    },
    {
      "content": "application instance simply sends a request to the cache service,",
      "pos": [
        3974,
        4039
      ]
    },
    {
      "content": "and the underlying infrastructure is responsible for determining the",
      "pos": [
        4040,
        4108
      ]
    },
    {
      "content": "location of the cached data in the cluster.",
      "pos": [
        4109,
        4152
      ]
    },
    {
      "content": "You can easily scale the",
      "pos": [
        4153,
        4177
      ]
    },
    {
      "content": "cache by adding more servers.",
      "pos": [
        4178,
        4207
      ]
    },
    {
      "content": "The disadvantages of the shared caching approach are that the cache",
      "pos": [
        4209,
        4276
      ]
    },
    {
      "content": "is slower to access because it is no longer held locally to each",
      "pos": [
        4277,
        4341
      ]
    },
    {
      "content": "application instance, and the requirement to implement a separate",
      "pos": [
        4342,
        4407
      ]
    },
    {
      "content": "cache service may add complexity to the solution.",
      "pos": [
        4408,
        4457
      ]
    },
    {
      "content": "Considerations for using caching",
      "pos": [
        4462,
        4494
      ]
    },
    {
      "content": "The following sections describe in more detail the considerations",
      "pos": [
        4496,
        4561
      ]
    },
    {
      "content": "for designing and using a cache.",
      "pos": [
        4562,
        4594
      ]
    },
    {
      "content": "When should data be cached?",
      "pos": [
        4600,
        4627
      ]
    },
    {
      "content": "Caching can dramatically improve performance, scalability, and availability.",
      "pos": [
        4629,
        4705
      ]
    },
    {
      "content": "The more data",
      "pos": [
        4706,
        4719
      ]
    },
    {
      "content": "that you have and the larger the number of users that need to access this data, the greater",
      "pos": [
        4720,
        4811
      ]
    },
    {
      "content": "the benefits of caching become by reducing latency and contention associated with handling",
      "pos": [
        4812,
        4902
      ]
    },
    {
      "content": "large volumes of concurrent requests in the original data store.",
      "pos": [
        4903,
        4967
      ]
    },
    {
      "content": "For example, a database",
      "pos": [
        4968,
        4991
      ]
    },
    {
      "content": "may support a limited number of concurrent connections, but retrieving data from a shared",
      "pos": [
        4992,
        5081
      ]
    },
    {
      "content": "cache rather than the underlying database allows a client application to access this data",
      "pos": [
        5082,
        5171
      ]
    },
    {
      "content": "even if the number of available connections is currently exhausted.",
      "pos": [
        5172,
        5239
      ]
    },
    {
      "content": "Additionally, if the",
      "pos": [
        5240,
        5260
      ]
    },
    {
      "content": "database becomes unavailable, client applications may be able to continue by using the",
      "pos": [
        5261,
        5347
      ]
    },
    {
      "content": "data held in the cache.",
      "pos": [
        5348,
        5371
      ]
    },
    {
      "content": "You should consider caching data that is read frequently but that is modified infrequently",
      "pos": [
        5373,
        5463
      ]
    },
    {
      "content": "(the data has a high proportion of read operations compared to write operations).",
      "pos": [
        5464,
        5545
      ]
    },
    {
      "content": "However,",
      "pos": [
        5546,
        5554
      ]
    },
    {
      "content": "you should not use the cache as the authoritative store of critical information; you should",
      "pos": [
        5555,
        5646
      ]
    },
    {
      "content": "ensure that all changes that your application cannot afford to lose are always saved to a",
      "pos": [
        5647,
        5736
      ]
    },
    {
      "content": "persistent data store.",
      "pos": [
        5737,
        5759
      ]
    },
    {
      "content": "In this way, if the cache is unavailable, your application can",
      "pos": [
        5760,
        5822
      ]
    },
    {
      "content": "still continue to operate by using the data store and you will not lose important",
      "pos": [
        5823,
        5904
      ]
    },
    {
      "content": "information.",
      "pos": [
        5905,
        5917
      ]
    },
    {
      "content": "Types of data and cache population strategies",
      "pos": [
        5923,
        5968
      ]
    },
    {
      "content": "The key to using a cache effectively lies in determining the most appropriate data to",
      "pos": [
        5970,
        6055
      ]
    },
    {
      "content": "cache, and caching it at the appropriate time.",
      "pos": [
        6056,
        6102
      ]
    },
    {
      "content": "The data may be added to the cache on",
      "pos": [
        6103,
        6140
      ]
    },
    {
      "content": "demand the first time it is retrieved by an application, so that the application needs",
      "pos": [
        6141,
        6227
      ]
    },
    {
      "content": "fetch the data only once from the data store and subsequent accesses can be satisfied",
      "pos": [
        6228,
        6313
      ]
    },
    {
      "content": "by using the cache.",
      "pos": [
        6314,
        6333
      ]
    },
    {
      "content": "Alternatively, a cache may be partially or fully populated with data in advance,",
      "pos": [
        6335,
        6415
      ]
    },
    {
      "content": "typically when the application starts (an approach known as seeding).",
      "pos": [
        6416,
        6485
      ]
    },
    {
      "content": "However, it may",
      "pos": [
        6486,
        6501
      ]
    },
    {
      "content": "not be advisable to implement seeding for a large cache as this approach can impose",
      "pos": [
        6502,
        6585
      ]
    },
    {
      "content": "a sudden, high load on the original data store when the application starts running.",
      "pos": [
        6586,
        6669
      ]
    },
    {
      "content": "Often an analysis of usage patterns can help to decide whether to fully or partially",
      "pos": [
        6671,
        6755
      ]
    },
    {
      "content": "prepopulate a cache, and to choose the data that should be cached.",
      "pos": [
        6756,
        6822
      ]
    },
    {
      "content": "For example, it",
      "pos": [
        6823,
        6838
      ]
    },
    {
      "content": "would probably be useful to seed the cache with the static user profile data for",
      "pos": [
        6839,
        6919
      ]
    },
    {
      "content": "customers who use the application regularly (perhaps every day), but not for",
      "pos": [
        6920,
        6996
      ]
    },
    {
      "content": "customers who use the application only once a week.",
      "pos": [
        6997,
        7048
      ]
    },
    {
      "content": "Caching typically works well with data that is immutable or that changes",
      "pos": [
        7050,
        7122
      ]
    },
    {
      "content": "infrequently.",
      "pos": [
        7123,
        7136
      ]
    },
    {
      "content": "Examples include reference information such as product and pricing",
      "pos": [
        7137,
        7203
      ]
    },
    {
      "content": "information in an ecommerce application, or shared static resources that are costly",
      "pos": [
        7204,
        7287
      ]
    },
    {
      "content": "to construct.",
      "pos": [
        7288,
        7301
      ]
    },
    {
      "content": "Some or all of this data can be loaded into the cache at application",
      "pos": [
        7302,
        7370
      ]
    },
    {
      "content": "startup to minimize demand on resources and to improve performance.",
      "pos": [
        7371,
        7438
      ]
    },
    {
      "content": "It may also be",
      "pos": [
        7439,
        7453
      ]
    },
    {
      "content": "appropriate to have a background process that periodically updates reference data",
      "pos": [
        7454,
        7535
      ]
    },
    {
      "content": "in the cache to ensure it is up to date, or refreshes the cache when reference",
      "pos": [
        7536,
        7614
      ]
    },
    {
      "content": "data changes.",
      "pos": [
        7615,
        7628
      ]
    },
    {
      "content": "Caching may be less useful for dynamic data, although there are some exceptions to",
      "pos": [
        7630,
        7712
      ]
    },
    {
      "content": "this consideration (see the section Caching Highly Dynamic Data later in this",
      "pos": [
        7713,
        7790
      ]
    },
    {
      "content": "guidance for more information).",
      "pos": [
        7791,
        7822
      ]
    },
    {
      "content": "When the original data regularly changes, either",
      "pos": [
        7823,
        7871
      ]
    },
    {
      "content": "the cached information can become stale very quickly or the overhead of keeping",
      "pos": [
        7872,
        7951
      ]
    },
    {
      "content": "the cache synchronized with the original data store reduces the effectiveness of",
      "pos": [
        7952,
        8032
      ]
    },
    {
      "content": "caching.",
      "pos": [
        8033,
        8041
      ]
    },
    {
      "content": "Note that a cache does not have to include the complete data for an",
      "pos": [
        8042,
        8109
      ]
    },
    {
      "content": "entity.",
      "pos": [
        8110,
        8117
      ]
    },
    {
      "content": "For example, if a data item represents a multivalued object such as a bank",
      "pos": [
        8118,
        8192
      ]
    },
    {
      "content": "customer with a name, address, and account balance, some of these elements may",
      "pos": [
        8193,
        8271
      ]
    },
    {
      "content": "remain static (the name and address), while others (such as the account balance)",
      "pos": [
        8272,
        8352
      ]
    },
    {
      "content": "may be more dynamic.",
      "pos": [
        8353,
        8373
      ]
    },
    {
      "content": "In these situations, it could be useful to cache the static",
      "pos": [
        8374,
        8433
      ]
    },
    {
      "content": "portions of the data and only retrieve (or calculate) the remaining information as",
      "pos": [
        8434,
        8516
      ]
    },
    {
      "content": "and when it is required.",
      "pos": [
        8517,
        8541
      ]
    },
    {
      "content": "Performance testing and usage analysis should be carried out to determine whether",
      "pos": [
        8543,
        8624
      ]
    },
    {
      "content": "pre-population or on-demand loading of the cache, or a combination of both, is",
      "pos": [
        8625,
        8703
      ]
    },
    {
      "content": "appropriate.",
      "pos": [
        8704,
        8716
      ]
    },
    {
      "content": "The decision should be based on a combination of the volatility and",
      "pos": [
        8717,
        8784
      ]
    },
    {
      "content": "usage pattern of the data.",
      "pos": [
        8785,
        8811
      ]
    },
    {
      "content": "Cache utilization and performance analysis is",
      "pos": [
        8812,
        8857
      ]
    },
    {
      "content": "particularly important in applications that encounter heavy loads and must be",
      "pos": [
        8858,
        8935
      ]
    },
    {
      "content": "highly scalable.",
      "pos": [
        8936,
        8952
      ]
    },
    {
      "content": "For example, in highly scalable scenarios it may make sense to",
      "pos": [
        8953,
        9015
      ]
    },
    {
      "content": "seed the cache to reduce the load on the data store at peak times.",
      "pos": [
        9016,
        9082
      ]
    },
    {
      "content": "Caching can also be used to avoid repeating computations as the application is",
      "pos": [
        9084,
        9162
      ]
    },
    {
      "content": "running.",
      "pos": [
        9163,
        9171
      ]
    },
    {
      "content": "If an operation transforms data or performs a complicated calculation,",
      "pos": [
        9172,
        9242
      ]
    },
    {
      "content": "it can save the results of the operation in the cache.",
      "pos": [
        9243,
        9297
      ]
    },
    {
      "content": "If the same calculation",
      "pos": [
        9298,
        9321
      ]
    },
    {
      "content": "is required subsequently, the application can simply retrieve the results from",
      "pos": [
        9322,
        9400
      ]
    },
    {
      "content": "the cache.",
      "pos": [
        9401,
        9411
      ]
    },
    {
      "content": "An application can modify data held in a cache, but you should consider the",
      "pos": [
        9413,
        9488
      ]
    },
    {
      "content": "cache as a transient data store that could disappear at any time.",
      "pos": [
        9489,
        9554
      ]
    },
    {
      "content": "Do not store",
      "pos": [
        9555,
        9567
      ]
    },
    {
      "content": "valuable data only in the cache, but make sure that you maintain the information",
      "pos": [
        9568,
        9648
      ]
    },
    {
      "content": "in the original data store as well.",
      "pos": [
        9649,
        9684
      ]
    },
    {
      "content": "In this way, if the cache should become",
      "pos": [
        9685,
        9724
      ]
    },
    {
      "content": "unavailable, you minimize the chance of losing data.",
      "pos": [
        9725,
        9777
      ]
    },
    {
      "content": "Caching highly dynamic data",
      "pos": [
        9783,
        9810
      ]
    },
    {
      "content": "Storing information that changes rapidly in a persistent data store can impose",
      "pos": [
        9812,
        9890
      ]
    },
    {
      "content": "an overhead on the system.",
      "pos": [
        9891,
        9917
      ]
    },
    {
      "content": "For example, consider a device that continually reports",
      "pos": [
        9918,
        9973
      ]
    },
    {
      "content": "status or some other measurement.",
      "pos": [
        9974,
        10007
      ]
    },
    {
      "content": "If an application chooses not to cache this",
      "pos": [
        10008,
        10051
      ]
    },
    {
      "content": "data on the basis that the cached information will nearly always be outdated, then",
      "pos": [
        10052,
        10134
      ]
    },
    {
      "content": "the same consideration could be true when storing and retrieving this information",
      "pos": [
        10135,
        10216
      ]
    },
    {
      "content": "from the data store; in the time taken to save and fetch this data it may have",
      "pos": [
        10217,
        10295
      ]
    },
    {
      "content": "changed.",
      "pos": [
        10296,
        10304
      ]
    },
    {
      "content": "In a situation such as this, consider the benefits of storing the dynamic",
      "pos": [
        10305,
        10378
      ]
    },
    {
      "content": "information directly in the cache instead of the persistent data store.",
      "pos": [
        10379,
        10450
      ]
    },
    {
      "content": "If the",
      "pos": [
        10451,
        10457
      ]
    },
    {
      "content": "data is non-critical and does not require to be audited, then it does not matter",
      "pos": [
        10458,
        10538
      ]
    },
    {
      "content": "if the occasional change is lost.",
      "pos": [
        10539,
        10572
      ]
    },
    {
      "content": "Managing data expiration in a cache",
      "pos": [
        10578,
        10613
      ]
    },
    {
      "content": "In most cases, data held in a cache is a copy of the data held in the original data",
      "pos": [
        10615,
        10698
      ]
    },
    {
      "content": "store.",
      "pos": [
        10699,
        10705
      ]
    },
    {
      "content": "The data in the original data store might change after it was cached, causing",
      "pos": [
        10706,
        10783
      ]
    },
    {
      "content": "the cached data to become stale.",
      "pos": [
        10784,
        10816
      ]
    },
    {
      "content": "Many caching systems enable you to configure the",
      "pos": [
        10817,
        10865
      ]
    },
    {
      "content": "cache to expire data and reduce the period for which data may be out of date.",
      "pos": [
        10866,
        10943
      ]
    },
    {
      "content": "When cached data expires it is removed from the cache, and the application must",
      "pos": [
        10945,
        11024
      ]
    },
    {
      "content": "retrieve the data from the original data store (it can put the newly-fetched",
      "pos": [
        11025,
        11101
      ]
    },
    {
      "content": "information back into cache).",
      "pos": [
        11102,
        11131
      ]
    },
    {
      "content": "You can set a default expiration policy when you",
      "pos": [
        11132,
        11180
      ]
    },
    {
      "content": "configure the cache.",
      "pos": [
        11181,
        11201
      ]
    },
    {
      "content": "In many cache services you can also stipulate the expiration",
      "pos": [
        11202,
        11262
      ]
    },
    {
      "content": "period for individual objects when you store them programmatically in the cache",
      "pos": [
        11263,
        11342
      ]
    },
    {
      "content": "(some caches enable you to specify the expiration period as an absolute value, or",
      "pos": [
        11343,
        11424
      ]
    },
    {
      "content": "as a sliding value that causes the item to be removed from cache if it is not",
      "pos": [
        11425,
        11502
      ]
    },
    {
      "content": "accessed within the specified time.",
      "pos": [
        11503,
        11538
      ]
    },
    {
      "content": "This setting overrides any cache-wide",
      "pos": [
        11539,
        11576
      ]
    },
    {
      "content": "expiration policy, but only for the specified objects.",
      "pos": [
        11577,
        11631
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Consider the expiration period for the cache and the objects that it contains carefully.",
      "pos": [
        11635,
        11736
      ]
    },
    {
      "content": "If you make it too short, objects will expire too quickly and you will reduce the benefits of using the cache.",
      "pos": [
        11737,
        11847
      ]
    },
    {
      "content": "If you make the period too long, you risk the data becoming stale.",
      "pos": [
        11848,
        11914
      ]
    },
    {
      "content": "It is also possible that the cache might fill up if data is allowed to remain",
      "pos": [
        11916,
        11993
      ]
    },
    {
      "content": "resident for a long time.",
      "pos": [
        11994,
        12019
      ]
    },
    {
      "content": "In this case, any requests to add new items to the",
      "pos": [
        12020,
        12070
      ]
    },
    {
      "content": "cache might cause some items to be forcibly removed, in a process known as",
      "pos": [
        12071,
        12145
      ]
    },
    {
      "content": "eviction.",
      "pos": [
        12146,
        12155
      ]
    },
    {
      "content": "Cache services typically evict data on a least-recently-used (LRU)",
      "pos": [
        12156,
        12222
      ]
    },
    {
      "content": "basis, but you can usually override this policy and prevent items from being",
      "pos": [
        12223,
        12299
      ]
    },
    {
      "content": "evicted.",
      "pos": [
        12300,
        12308
      ]
    },
    {
      "content": "However, if you adopt this approach you risk your cache exceeding the",
      "pos": [
        12309,
        12378
      ]
    },
    {
      "content": "memory that it has available, and an application that attempts to add an item",
      "pos": [
        12379,
        12456
      ]
    },
    {
      "content": "to the cache will fail with an exception.",
      "pos": [
        12457,
        12498
      ]
    },
    {
      "content": "Some caching implementations may provide additional eviction policies.",
      "pos": [
        12500,
        12570
      ]
    },
    {
      "content": "These",
      "pos": [
        12571,
        12576
      ]
    },
    {
      "content": "typically include the most-recently-used policy (in the expectation that the",
      "pos": [
        12577,
        12653
      ]
    },
    {
      "content": "data will not be required again), first-in-first-out policy (oldest data is",
      "pos": [
        12654,
        12729
      ]
    },
    {
      "content": "evicted first), or explicit removal based on a triggered event (such as the",
      "pos": [
        12730,
        12805
      ]
    },
    {
      "content": "data being modified).",
      "pos": [
        12806,
        12827
      ]
    },
    {
      "content": "Invalidating data in a client-side cache",
      "pos": [
        12833,
        12873
      ]
    },
    {
      "content": "Data held in a client-side cache is generally considered to be outside of",
      "pos": [
        12875,
        12948
      ]
    },
    {
      "content": "the auspices of the service providing the data to the client; a service",
      "pos": [
        12949,
        13020
      ]
    },
    {
      "content": "cannot directly force a client to add or remove information from a",
      "pos": [
        13021,
        13087
      ]
    },
    {
      "content": "client-side cache.",
      "pos": [
        13088,
        13106
      ]
    },
    {
      "content": "This means that it is possible for a client that uses",
      "pos": [
        13107,
        13160
      ]
    },
    {
      "content": "a poorly configured cache (for example, expiration policies are not",
      "pos": [
        13161,
        13228
      ]
    },
    {
      "content": "properly implemented) to continue using outdated information cached",
      "pos": [
        13229,
        13296
      ]
    },
    {
      "content": "locally when the information in the original data source has changed.",
      "pos": [
        13297,
        13366
      ]
    },
    {
      "content": "If you are building a web application that serves data over an HTTP",
      "pos": [
        13368,
        13435
      ]
    },
    {
      "content": "connection, you can implicitly force a web client (such as a browser or",
      "pos": [
        13436,
        13507
      ]
    },
    {
      "content": "web proxy) to fetch the most recent information if a resource is updated",
      "pos": [
        13508,
        13580
      ]
    },
    {
      "content": "by changing the URI of that resource.",
      "pos": [
        13581,
        13618
      ]
    },
    {
      "content": "Web clients typically use the URI",
      "pos": [
        13619,
        13652
      ]
    },
    {
      "content": "of a resource as the key in the client-side cache, so changing the URI",
      "pos": [
        13653,
        13723
      ]
    },
    {
      "content": "causes the web client to ignore any previously cached version of a",
      "pos": [
        13724,
        13790
      ]
    },
    {
      "content": "resource and fetch the new version instead.",
      "pos": [
        13791,
        13834
      ]
    },
    {
      "content": "Managing concurrency in a cache",
      "pos": [
        13839,
        13870
      ]
    },
    {
      "content": "Caches are often designed to be shared by multiple instances of an",
      "pos": [
        13872,
        13938
      ]
    },
    {
      "content": "application.",
      "pos": [
        13939,
        13951
      ]
    },
    {
      "content": "Each application instance can read and modify data in",
      "pos": [
        13952,
        14005
      ]
    },
    {
      "content": "the cache.",
      "pos": [
        14006,
        14016
      ]
    },
    {
      "content": "Consequently, the same concurrency issues that arise with",
      "pos": [
        14017,
        14074
      ]
    },
    {
      "content": "any shared data store are also applicable to a cache.",
      "pos": [
        14075,
        14128
      ]
    },
    {
      "content": "In a situation",
      "pos": [
        14129,
        14143
      ]
    },
    {
      "content": "where an application needs to modify data held in the cache, you may",
      "pos": [
        14144,
        14212
      ]
    },
    {
      "content": "need to ensure that updates made by one instance of the application",
      "pos": [
        14213,
        14280
      ]
    },
    {
      "content": "do not blindly overwrite the changes made by another instance.",
      "pos": [
        14281,
        14343
      ]
    },
    {
      "content": "Depending on the nature of the data and the likelihood of collisions,",
      "pos": [
        14345,
        14414
      ]
    },
    {
      "content": "you can adopt one of two approaches to concurrency:",
      "pos": [
        14415,
        14466
      ]
    },
    {
      "content": "Optimistic.",
      "pos": [
        14472,
        14483
      ]
    },
    {
      "content": "The application checks to see whether the data in the cache has changed since it was retrieved, immediately prior to updating it.",
      "pos": [
        14486,
        14615
      ]
    },
    {
      "content": "If the data is still the same, the change can be made.",
      "pos": [
        14616,
        14670
      ]
    },
    {
      "content": "Otherwise, the application has to decide whether to update it (the business logic that drives this decision will be application-specific).",
      "pos": [
        14671,
        14809
      ]
    },
    {
      "content": "This approach is suitable for situations where updates are infrequent, or where collisions are unlikely to occur.",
      "pos": [
        14810,
        14923
      ]
    },
    {
      "content": "Pessimistic.",
      "pos": [
        14928,
        14940
      ]
    },
    {
      "content": "The application locks the data in the cache when it retrieves it to prevent another instance from changing the data.",
      "pos": [
        14943,
        15059
      ]
    },
    {
      "content": "This process ensures that collisions cannot occur, but could block other instances that need to process the same data.",
      "pos": [
        15060,
        15178
      ]
    },
    {
      "content": "Pessimistic concurrency can affect the scalability of the solution and should be used only for short-lived operations.",
      "pos": [
        15179,
        15297
      ]
    },
    {
      "content": "This approach may be appropriate for situations where collisions are more likely, especially if an application updates multiple items in the cache and must ensure that these changes are applied consistently.",
      "pos": [
        15298,
        15505
      ]
    },
    {
      "content": "Implementing high availability, and scalability, and improving performance",
      "pos": [
        15511,
        15585
      ]
    },
    {
      "content": "A cache should not be the primary repository of data; this is the role",
      "pos": [
        15587,
        15657
      ]
    },
    {
      "content": "of the original data store from which the cache is populated.",
      "pos": [
        15658,
        15719
      ]
    },
    {
      "content": "The",
      "pos": [
        15720,
        15723
      ]
    },
    {
      "content": "original data store is responsible for ensuring the persistence of the",
      "pos": [
        15724,
        15794
      ]
    },
    {
      "content": "data.",
      "pos": [
        15795,
        15800
      ]
    },
    {
      "content": "Be careful not to introduce critical dependencies on the availability",
      "pos": [
        15802,
        15871
      ]
    },
    {
      "content": "of a shared cache service into your solutions.",
      "pos": [
        15872,
        15918
      ]
    },
    {
      "content": "An application should be",
      "pos": [
        15919,
        15943
      ]
    },
    {
      "content": "able to continue functioning if the service providing the shared cache",
      "pos": [
        15944,
        16014
      ]
    },
    {
      "content": "is unavailable; the application should not hang or fail while waiting",
      "pos": [
        16015,
        16084
      ]
    },
    {
      "content": "for the cache service to resume.",
      "pos": [
        16085,
        16117
      ]
    },
    {
      "content": "Therefore, the application must be",
      "pos": [
        16118,
        16152
      ]
    },
    {
      "content": "prepared to detect the availability of the cache service and fall back",
      "pos": [
        16153,
        16223
      ]
    },
    {
      "content": "to the original data store if the cache is inaccessible.",
      "pos": [
        16224,
        16280
      ]
    },
    {
      "content": "The",
      "pos": [
        16281,
        16284
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Circuit-Breaker Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589784.aspx)</ept> is useful for handling this scenario.",
      "pos": [
        16285,
        16397
      ]
    },
    {
      "content": "The",
      "pos": [
        16398,
        16401
      ]
    },
    {
      "content": "service providing the cache can be recovered, and once it becomes",
      "pos": [
        16402,
        16467
      ]
    },
    {
      "content": "available the cache can be repopulated as data is read form the",
      "pos": [
        16468,
        16531
      ]
    },
    {
      "content": "original data store, following a strategy such as the <bpt id=\"p1\">[</bpt>Cache-Aside pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589799.aspx)</ept>.",
      "pos": [
        16532,
        16657
      ]
    },
    {
      "content": "However, falling back to the original data store if the cache is",
      "pos": [
        16659,
        16723
      ]
    },
    {
      "content": "temporarily unavailable may have a scalability impact on the system;",
      "pos": [
        16724,
        16792
      ]
    },
    {
      "content": "while the data store is being recovered the original data store",
      "pos": [
        16793,
        16856
      ]
    },
    {
      "content": "could be swamped with requests for data, resulting in timeouts and",
      "pos": [
        16857,
        16923
      ]
    },
    {
      "content": "failed connections.",
      "pos": [
        16924,
        16943
      ]
    },
    {
      "content": "A strategy that you should consider is to",
      "pos": [
        16944,
        16985
      ]
    },
    {
      "content": "implement a local, private cache in each instance of an application",
      "pos": [
        16986,
        17053
      ]
    },
    {
      "content": "together with the shared cache that all application instances",
      "pos": [
        17054,
        17115
      ]
    },
    {
      "content": "access.",
      "pos": [
        17116,
        17123
      ]
    },
    {
      "content": "When the application retrieves an item, it can check first",
      "pos": [
        17124,
        17182
      ]
    },
    {
      "content": "in its local cache, then the shared cache, and finally the original",
      "pos": [
        17183,
        17250
      ]
    },
    {
      "content": "data store.",
      "pos": [
        17251,
        17262
      ]
    },
    {
      "content": "The local cache can be populated using the data in the",
      "pos": [
        17263,
        17317
      ]
    },
    {
      "content": "shared cache, or the database if the shared cache is unavailable.",
      "pos": [
        17318,
        17383
      ]
    },
    {
      "content": "This approach requires careful configuration to prevent the local",
      "pos": [
        17384,
        17449
      ]
    },
    {
      "content": "cache becoming too stale with respect to the shared cache, but it",
      "pos": [
        17450,
        17515
      ]
    },
    {
      "content": "acts as a buffer if the shared cache is unreachable.",
      "pos": [
        17516,
        17568
      ]
    },
    {
      "content": "Figure 3",
      "pos": [
        17569,
        17577
      ]
    },
    {
      "content": "shows this structure.",
      "pos": [
        17578,
        17599
      ]
    },
    {
      "content": "<ph id=\"ph1\">![</ph>Using a local, private cache with a shared cache_<ph id=\"ph2\">](media/best-practices-caching/Caching3.png)</ph>",
      "pos": [
        17601,
        17696
      ]
    },
    {
      "content": "<bpt id=\"p1\">_</bpt>Figure 3: Using a local, private cache with a shared cache<ept id=\"p1\">_</ept>",
      "pos": [
        17697,
        17757
      ]
    },
    {
      "content": "To support large caches that hold relatively long-lived data, some",
      "pos": [
        17759,
        17825
      ]
    },
    {
      "content": "cache services provide a high-availability option that implements",
      "pos": [
        17826,
        17891
      ]
    },
    {
      "content": "automatic failover if the cache becomes unavailable.",
      "pos": [
        17892,
        17944
      ]
    },
    {
      "content": "This approach",
      "pos": [
        17945,
        17958
      ]
    },
    {
      "content": "typically involves replicating the cached data stored on a primary",
      "pos": [
        17959,
        18025
      ]
    },
    {
      "content": "cache server to a secondary cache server, and switching to the",
      "pos": [
        18026,
        18088
      ]
    },
    {
      "content": "secondary server if the primary server fails or connectivity is",
      "pos": [
        18089,
        18152
      ]
    },
    {
      "content": "lost.",
      "pos": [
        18153,
        18158
      ]
    },
    {
      "content": "To reduce the latency associated with writing to multiple",
      "pos": [
        18159,
        18216
      ]
    },
    {
      "content": "destinations, when data is written to the cache on the primary",
      "pos": [
        18217,
        18279
      ]
    },
    {
      "content": "server, the replication to the secondary server may occur",
      "pos": [
        18280,
        18337
      ]
    },
    {
      "content": "asynchronously.",
      "pos": [
        18338,
        18353
      ]
    },
    {
      "content": "This approach leads to the possibility that some",
      "pos": [
        18355,
        18403
      ]
    },
    {
      "content": "cached information may be lost in the event of a failure, but the",
      "pos": [
        18404,
        18469
      ]
    },
    {
      "content": "proportion of this data should be small compared to the overall",
      "pos": [
        18470,
        18533
      ]
    },
    {
      "content": "size of the cache.",
      "pos": [
        18534,
        18552
      ]
    },
    {
      "content": "If a shared cache is large, it may be beneficial to partition the",
      "pos": [
        18554,
        18619
      ]
    },
    {
      "content": "cached data across nodes to reduce the chances of contention and",
      "pos": [
        18620,
        18684
      ]
    },
    {
      "content": "improve scalability.",
      "pos": [
        18685,
        18705
      ]
    },
    {
      "content": "Many shared caches support the ability to",
      "pos": [
        18706,
        18747
      ]
    },
    {
      "content": "dynamically add (and remove) nodes and rebalance the data across",
      "pos": [
        18748,
        18812
      ]
    },
    {
      "content": "partitions.",
      "pos": [
        18813,
        18824
      ]
    },
    {
      "content": "This approach may involve clustering whereby the",
      "pos": [
        18825,
        18873
      ]
    },
    {
      "content": "collection of nodes is presented to client applications as a",
      "pos": [
        18874,
        18934
      ]
    },
    {
      "content": "seamless, single cache, but internally the data is dispersed",
      "pos": [
        18935,
        18995
      ]
    },
    {
      "content": "between nodes following some predefined distribution strategy",
      "pos": [
        18996,
        19057
      ]
    },
    {
      "content": "which balances the load evenly.",
      "pos": [
        19058,
        19089
      ]
    },
    {
      "content": "The <bpt id=\"p1\">[</bpt>Data Partitioning Guidance document<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589795.aspx)</ept>",
      "pos": [
        19090,
        19180
      ]
    },
    {
      "content": "on the Microsoft website provides more information about possible",
      "pos": [
        19181,
        19246
      ]
    },
    {
      "content": "partitioning strategies.",
      "pos": [
        19247,
        19271
      ]
    },
    {
      "content": "Clustering can also add further availability of the cache; if a",
      "pos": [
        19273,
        19336
      ]
    },
    {
      "content": "node fails, the remainder of the cache is still accessible.",
      "pos": [
        19337,
        19396
      ]
    },
    {
      "content": "Clustering is frequently used in conjunction with replication",
      "pos": [
        19397,
        19458
      ]
    },
    {
      "content": "and failover; each node can be replicated and the replica",
      "pos": [
        19459,
        19516
      ]
    },
    {
      "content": "quickly brought online if the node fails.",
      "pos": [
        19517,
        19558
      ]
    },
    {
      "content": "Many read and write operations will likely involve single data",
      "pos": [
        19560,
        19622
      ]
    },
    {
      "content": "values or objects.",
      "pos": [
        19623,
        19641
      ]
    },
    {
      "content": "However, there may be times when it is",
      "pos": [
        19642,
        19680
      ]
    },
    {
      "content": "necessary to store or retrieve large volumes of data quickly.",
      "pos": [
        19681,
        19742
      ]
    },
    {
      "content": "For example, seeding a cache could involve writing hundreds or",
      "pos": [
        19743,
        19805
      ]
    },
    {
      "content": "thousands of items to the cache, or an application may need to",
      "pos": [
        19806,
        19868
      ]
    },
    {
      "content": "retrieve a large number of related items from the cache as",
      "pos": [
        19869,
        19927
      ]
    },
    {
      "content": "part of the same request.",
      "pos": [
        19928,
        19953
      ]
    },
    {
      "content": "Many large-scale caches provide batch",
      "pos": [
        19954,
        19991
      ]
    },
    {
      "content": "operations for these purposes, enabling a client application to",
      "pos": [
        19992,
        20055
      ]
    },
    {
      "content": "package up a large volume of items into a single request and",
      "pos": [
        20056,
        20116
      ]
    },
    {
      "content": "reducing the overhead associated with performing a large number",
      "pos": [
        20117,
        20180
      ]
    },
    {
      "content": "of small requests.",
      "pos": [
        20181,
        20199
      ]
    },
    {
      "content": "Caching and eventual consistency",
      "pos": [
        20204,
        20236
      ]
    },
    {
      "content": "The Cache-Aside pattern depends on the instance of the application",
      "pos": [
        20238,
        20304
      ]
    },
    {
      "content": "populating the cache having access to the most recent and",
      "pos": [
        20305,
        20362
      ]
    },
    {
      "content": "consistent version of the data.",
      "pos": [
        20363,
        20394
      ]
    },
    {
      "content": "In a system that implements",
      "pos": [
        20395,
        20422
      ]
    },
    {
      "content": "eventual consistency (such as a replicated data store) this may",
      "pos": [
        20423,
        20486
      ]
    },
    {
      "content": "not be the case.",
      "pos": [
        20487,
        20503
      ]
    },
    {
      "content": "One instance of an application could modify a",
      "pos": [
        20504,
        20549
      ]
    },
    {
      "content": "data item and invalidate the cached version of that item.",
      "pos": [
        20550,
        20607
      ]
    },
    {
      "content": "Another",
      "pos": [
        20608,
        20615
      ]
    },
    {
      "content": "instance of the application may attempt to read this item from",
      "pos": [
        20616,
        20678
      ]
    },
    {
      "content": "cache which causes a cache-miss, so it reads the data from the",
      "pos": [
        20679,
        20741
      ]
    },
    {
      "content": "data store and adds it to the cache.",
      "pos": [
        20742,
        20778
      ]
    },
    {
      "content": "However, if the data store",
      "pos": [
        20779,
        20805
      ]
    },
    {
      "content": "has not been fully synchronized with the other replicas the",
      "pos": [
        20806,
        20865
      ]
    },
    {
      "content": "application instance could read and populate the cache with the",
      "pos": [
        20866,
        20929
      ]
    },
    {
      "content": "old value.",
      "pos": [
        20930,
        20940
      ]
    },
    {
      "content": "For more information about handling data consistency, see the",
      "pos": [
        20942,
        21003
      ]
    },
    {
      "content": "Data Consistency Guidance page on the Microsoft website.",
      "pos": [
        21004,
        21060
      ]
    },
    {
      "content": "Protecting cached data",
      "pos": [
        21066,
        21088
      ]
    },
    {
      "content": "Irrespective of the cache service you use, you should consider",
      "pos": [
        21090,
        21152
      ]
    },
    {
      "content": "how to protect the data held in the cache from unauthorized",
      "pos": [
        21153,
        21212
      ]
    },
    {
      "content": "access.",
      "pos": [
        21213,
        21220
      ]
    },
    {
      "content": "There are two main concerns:",
      "pos": [
        21221,
        21249
      ]
    },
    {
      "content": "The privacy of the data in the cache.",
      "pos": [
        21253,
        21290
      ]
    },
    {
      "pos": [
        21293,
        21381
      ],
      "content": "The privacy of data as it flows between the cache and the\napplication using the cache.",
      "leadings": [
        "",
        "  "
      ],
      "nodes": [
        {
          "content": "The privacy of data as it flows between the cache and the",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "application using the cache.",
          "pos": [
            58,
            86
          ]
        }
      ]
    },
    {
      "content": "To protect data in the cache, the cache service may implement",
      "pos": [
        21383,
        21444
      ]
    },
    {
      "content": "an authentication mechanism requiring that applications",
      "pos": [
        21445,
        21500
      ]
    },
    {
      "content": "identify themselves, and an authorization scheme that",
      "pos": [
        21501,
        21554
      ]
    },
    {
      "content": "specifies which identities can access data in the cache, and",
      "pos": [
        21555,
        21615
      ]
    },
    {
      "content": "the operations (read and write) that these identities are",
      "pos": [
        21616,
        21673
      ]
    },
    {
      "content": "allowed to perform.",
      "pos": [
        21674,
        21693
      ]
    },
    {
      "content": "To reduce overheads associated with",
      "pos": [
        21694,
        21729
      ]
    },
    {
      "content": "reading and writing data, once an identity has been granted",
      "pos": [
        21730,
        21789
      ]
    },
    {
      "content": "write and/or read access to the cache, that identity can use",
      "pos": [
        21790,
        21850
      ]
    },
    {
      "content": "any data in the cache.",
      "pos": [
        21851,
        21873
      ]
    },
    {
      "content": "If you need to restrict access to",
      "pos": [
        21874,
        21907
      ]
    },
    {
      "content": "subsets of the cached data, you can:",
      "pos": [
        21908,
        21944
      ]
    },
    {
      "pos": [
        21948,
        22112
      ],
      "content": "Split the cache into partitions (by using different cache\nservers) and only grant access to identities for the\npartitions that they should be allowed to use, or",
      "leadings": [
        "",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "Split the cache into partitions (by using different cache",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "servers) and only grant access to identities for the",
          "pos": [
            58,
            110
          ]
        },
        {
          "content": "partitions that they should be allowed to use, or",
          "pos": [
            111,
            160
          ]
        }
      ]
    },
    {
      "pos": [
        22115,
        22424
      ],
      "content": "Encrypt the data in each subset by using different keys\nand only provide the encryption keys to identities that\nshould have access to each subset. A client application\nmay still be able to retrieve all of the data in the cache,\nbut it will only be able to decrypt the data for which it\nhas the keys.",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "Encrypt the data in each subset by using different keys",
          "pos": [
            0,
            55
          ]
        },
        {
          "content": "and only provide the encryption keys to identities that",
          "pos": [
            56,
            111
          ]
        },
        {
          "content": "should have access to each subset. A client application",
          "pos": [
            112,
            167
          ],
          "nodes": [
            {
              "content": "should have access to each subset.",
              "pos": [
                0,
                34
              ]
            },
            {
              "content": "A client application",
              "pos": [
                35,
                55
              ]
            }
          ]
        },
        {
          "content": "may still be able to retrieve all of the data in the cache,",
          "pos": [
            168,
            227
          ]
        },
        {
          "content": "but it will only be able to decrypt the data for which it",
          "pos": [
            228,
            285
          ]
        },
        {
          "content": "has the keys.",
          "pos": [
            286,
            299
          ]
        }
      ]
    },
    {
      "content": "To protect the data as it flows into and out of the cache you",
      "pos": [
        22426,
        22487
      ]
    },
    {
      "content": "are dependent on the security features provided by the network",
      "pos": [
        22488,
        22550
      ]
    },
    {
      "content": "infrastructure that client applications use to connect to the",
      "pos": [
        22551,
        22612
      ]
    },
    {
      "content": "cache.",
      "pos": [
        22613,
        22619
      ]
    },
    {
      "content": "If the cache is implemented using an on-site server",
      "pos": [
        22620,
        22671
      ]
    },
    {
      "content": "within the same organization that hosts the client applications,",
      "pos": [
        22672,
        22736
      ]
    },
    {
      "content": "then the isolation of the network itself may not require you to",
      "pos": [
        22737,
        22800
      ]
    },
    {
      "content": "take any additional steps.",
      "pos": [
        22801,
        22827
      ]
    },
    {
      "content": "If the cache is located remotely and",
      "pos": [
        22828,
        22864
      ]
    },
    {
      "content": "requires a TCP or HTTP connection over a public network (such",
      "pos": [
        22865,
        22926
      ]
    },
    {
      "content": "as the Internet), you should consider implementing SSL.",
      "pos": [
        22927,
        22982
      ]
    },
    {
      "content": "Considerations for implementing caching with Microsoft Azure",
      "pos": [
        22987,
        23047
      ]
    },
    {
      "content": "Azure provides the Azure Redis Cache.",
      "pos": [
        23049,
        23086
      ]
    },
    {
      "content": "This is an implementation",
      "pos": [
        23087,
        23112
      ]
    },
    {
      "content": "of the open source Redis Cache that runs as a service in an",
      "pos": [
        23113,
        23172
      ]
    },
    {
      "content": "Azure datacenter.",
      "pos": [
        23173,
        23190
      ]
    },
    {
      "content": "It provides a caching service that can be",
      "pos": [
        23191,
        23232
      ]
    },
    {
      "content": "accessed from any Azure application, whether the application",
      "pos": [
        23233,
        23293
      ]
    },
    {
      "content": "is implemented as a cloud service, a website, or inside an",
      "pos": [
        23294,
        23352
      ]
    },
    {
      "content": "Azure virtual machine.",
      "pos": [
        23353,
        23375
      ]
    },
    {
      "content": "Caches can be shared by client",
      "pos": [
        23376,
        23406
      ]
    },
    {
      "content": "applications that have the appropriate access key.",
      "pos": [
        23407,
        23457
      ]
    },
    {
      "content": "Redis is a high-performance caching solution that provides",
      "pos": [
        23459,
        23517
      ]
    },
    {
      "content": "availability, scalability and security.",
      "pos": [
        23518,
        23557
      ]
    },
    {
      "content": "It typically runs",
      "pos": [
        23558,
        23575
      ]
    },
    {
      "content": "as a service spread across one or more dedicated machines and",
      "pos": [
        23576,
        23637
      ]
    },
    {
      "content": "attempts to store as much information as it can in memory to",
      "pos": [
        23638,
        23698
      ]
    },
    {
      "content": "ensure fast access.",
      "pos": [
        23699,
        23718
      ]
    },
    {
      "content": "This architecture is intended to provide",
      "pos": [
        23719,
        23759
      ]
    },
    {
      "content": "low latency and high throughput by reducing the need to",
      "pos": [
        23760,
        23815
      ]
    },
    {
      "content": "perform slow I/O operations.",
      "pos": [
        23816,
        23844
      ]
    },
    {
      "content": "The Azure Redis cache is compatible with many of the various",
      "pos": [
        23846,
        23906
      ]
    },
    {
      "content": "APIs used by client applications.",
      "pos": [
        23907,
        23940
      ]
    },
    {
      "content": "If you have existing",
      "pos": [
        23941,
        23961
      ]
    },
    {
      "content": "applications that already use Redis running on-premises, the",
      "pos": [
        23962,
        24022
      ]
    },
    {
      "content": "Azure Redis cache provides a quick migration path to caching",
      "pos": [
        24023,
        24083
      ]
    },
    {
      "content": "in the cloud.",
      "pos": [
        24084,
        24097
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Azure also provides the Managed Cache Service.",
      "pos": [
        24101,
        24160
      ]
    },
    {
      "content": "This",
      "pos": [
        24161,
        24165
      ]
    },
    {
      "content": "service is based on the Microsoft AppFabric Cache engine.",
      "pos": [
        24168,
        24225
      ]
    },
    {
      "content": "It",
      "pos": [
        24226,
        24228
      ]
    },
    {
      "content": "enables you to create a distributed cache that can be shared",
      "pos": [
        24231,
        24291
      ]
    },
    {
      "content": "by loosely-coupled applications.",
      "pos": [
        24294,
        24326
      ]
    },
    {
      "content": "The cache is hosted on",
      "pos": [
        24327,
        24349
      ]
    },
    {
      "content": "high-performance servers running in an Azure datacenter.",
      "pos": [
        24352,
        24408
      ]
    },
    {
      "content": "However, this option is no longer recommended and is only",
      "pos": [
        24411,
        24468
      ]
    },
    {
      "content": "provided to support existing applications that have been built",
      "pos": [
        24471,
        24533
      ]
    },
    {
      "content": "to use it.",
      "pos": [
        24536,
        24546
      ]
    },
    {
      "content": "For all new development, use the Azure Redis",
      "pos": [
        24547,
        24591
      ]
    },
    {
      "content": "Cache instead.",
      "pos": [
        24594,
        24608
      ]
    },
    {
      "content": "Additionally, Azure supports in-role caching.",
      "pos": [
        24613,
        24658
      ]
    },
    {
      "content": "This feature",
      "pos": [
        24659,
        24671
      ]
    },
    {
      "content": "enables you to create a cache specific to a cloud service.",
      "pos": [
        24674,
        24732
      ]
    },
    {
      "content": "The cache is hosted by instances of a web or worker role, and",
      "pos": [
        24735,
        24796
      ]
    },
    {
      "content": "can only be accessed by roles operating as part of the same",
      "pos": [
        24799,
        24858
      ]
    },
    {
      "content": "cloud service deployment unit (a deployment unit is the set",
      "pos": [
        24861,
        24920
      ]
    },
    {
      "content": "of role instances deployed as a cloud service to a specific",
      "pos": [
        24923,
        24982
      ]
    },
    {
      "content": "region).",
      "pos": [
        24985,
        24993
      ]
    },
    {
      "content": "The cache is clustered, and all instances of the",
      "pos": [
        24994,
        25042
      ]
    },
    {
      "content": "role within the same deployment unit that host the cache",
      "pos": [
        25045,
        25101
      ]
    },
    {
      "content": "become part of the same cache cluster.",
      "pos": [
        25104,
        25142
      ]
    },
    {
      "content": "Existing applications",
      "pos": [
        25143,
        25164
      ]
    },
    {
      "content": "that use in-role caching can continue to do so, but",
      "pos": [
        25167,
        25218
      ]
    },
    {
      "content": "migrating to the Azure Redis Cache may bring more benefits.",
      "pos": [
        25221,
        25280
      ]
    },
    {
      "content": "For more information about whether to use Azure Redis Cache",
      "pos": [
        25283,
        25342
      ]
    },
    {
      "content": "or an in-role cache, visit the page",
      "pos": [
        25345,
        25380
      ]
    },
    {
      "content": "<bpt id=\"p1\">  [</bpt>Which Azure Cache offering is right for me?<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn766201.aspx)</ept> on the Microsoft website.",
      "pos": [
        25381,
        25509
      ]
    },
    {
      "content": "Features of Redis",
      "pos": [
        25516,
        25533
      ]
    },
    {
      "content": "Redis is more than a simple cache server; it provides a distributed in-memory",
      "pos": [
        25535,
        25612
      ]
    },
    {
      "content": "database with an extensive command set that supports many common scenarios,",
      "pos": [
        25613,
        25688
      ]
    },
    {
      "content": "as described in the section Use-cases for Redis caching later in this",
      "pos": [
        25689,
        25758
      ]
    },
    {
      "content": "document.",
      "pos": [
        25759,
        25768
      ]
    },
    {
      "content": "This section summarizes some of the key features that Redis",
      "pos": [
        25769,
        25828
      ]
    },
    {
      "content": "provides.",
      "pos": [
        25829,
        25838
      ]
    },
    {
      "content": "Redis as an in-memory database",
      "pos": [
        25844,
        25874
      ]
    },
    {
      "content": "Redis supports both reading and writing operations.",
      "pos": [
        25876,
        25927
      ]
    },
    {
      "content": "Unlike many caches (which should be considered as transitory data stores), writes can be protected from system failure either by being stored in periodically in a local snapshot file or in an append-only log file.",
      "pos": [
        25928,
        26141
      ]
    },
    {
      "content": "All writes are asynchronous and do not block clients reading and writing data.",
      "pos": [
        26142,
        26220
      ]
    },
    {
      "content": "When Redis starts running, it reads the data from the snapshot or log file and uses it to construct the in-memory cache.",
      "pos": [
        26221,
        26341
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Redis Persistence<ept id=\"p1\">](http://redis.io/topics/persistence)</ept> on the Redis website.",
      "pos": [
        26342,
        26445
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Redis does not guarantee that all writes will be saved in the event",
      "pos": [
        26449,
        26529
      ]
    },
    {
      "content": "of a catastrophic failure, but at worst you should only lose a few-seconds",
      "pos": [
        26532,
        26606
      ]
    },
    {
      "content": "worth of data.",
      "pos": [
        26609,
        26623
      ]
    },
    {
      "content": "Remember that a cache is not intended to act as an",
      "pos": [
        26624,
        26674
      ]
    },
    {
      "content": "authoritative data source, and it is the responsibility of the applications",
      "pos": [
        26677,
        26752
      ]
    },
    {
      "content": "using the cache to ensure that critical data is saved successfully to an",
      "pos": [
        26755,
        26827
      ]
    },
    {
      "content": "appropriate data store.",
      "pos": [
        26830,
        26853
      ]
    },
    {
      "content": "For more information, see the Cache-Aside pattern.",
      "pos": [
        26854,
        26904
      ]
    },
    {
      "content": "Redis data types",
      "pos": [
        26911,
        26927
      ]
    },
    {
      "content": "Redis is a key-value store, where values can contain simple types or complex data structures such as hashes, lists, and sets.",
      "pos": [
        26929,
        27054
      ]
    },
    {
      "content": "It supports a set of atomic operations on these data types.",
      "pos": [
        27055,
        27114
      ]
    },
    {
      "content": "Keys can be permanent or tagged with a limited time to live at which point the key and its corresponding value are automatically removed from the cache.",
      "pos": [
        27115,
        27267
      ]
    },
    {
      "content": "For more information about redis keys and values, visit the page <bpt id=\"p1\">[</bpt>An Introduction to Redis data types and abstractions<ept id=\"p1\">](http://redis.io/topics/data-types-intro)</ept> on the Redis website.",
      "pos": [
        27268,
        27450
      ]
    },
    {
      "content": "Redis replication and clustering",
      "pos": [
        27457,
        27489
      ]
    },
    {
      "content": "Redis supports master/subordinate replication to help ensure availability and maintain throughput; write operations to a Redis master node are replicated to one or more subordinate nodes, and read operations can be served by the master or any of the subordinates.",
      "pos": [
        27491,
        27754
      ]
    },
    {
      "content": "In the event of a network partition, subordinates can continue to serve data and then transparently resynchronize with the master when the connection is reestablished.",
      "pos": [
        27755,
        27922
      ]
    },
    {
      "content": "For further details, visit the <bpt id=\"p1\">[</bpt>Replication<ept id=\"p1\">](http://redis.io/topics/replication)</ept> page on the Redis website.",
      "pos": [
        27923,
        28030
      ]
    },
    {
      "content": "Redis also provides clustering, enabling you to transparently partition data into shards across servers and spread the load.",
      "pos": [
        28032,
        28156
      ]
    },
    {
      "content": "This feature improves scalability as new Redis servers can be added and the data repartitioned as the size of the cache increases.",
      "pos": [
        28157,
        28287
      ]
    },
    {
      "content": "Furthermore, each server in the cluster can be replicated by using master/subordinate replication to ensure availability across each node in the cluster.",
      "pos": [
        28288,
        28441
      ]
    },
    {
      "content": "For more information about clustering and sharding, visit the <bpt id=\"p1\">[</bpt>Redis Cluster Tutorial page<ept id=\"p1\">](http://redis.io/topics/cluster-tutorial)</ept> on the Redis website.",
      "pos": [
        28442,
        28596
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Azure Redis Cache does not currently support clustering.",
      "pos": [
        28600,
        28669
      ]
    },
    {
      "content": "If you wish to create a Redis cluster you can build your own custom Redis server.",
      "pos": [
        28670,
        28751
      ]
    },
    {
      "content": "For more information, see the section Building a Custom Redis Cache later in this document.",
      "pos": [
        28752,
        28843
      ]
    },
    {
      "content": "Redis memory use",
      "pos": [
        28849,
        28865
      ]
    },
    {
      "content": "A Redis cache has a finite size depending on the resources available on the host computer.",
      "pos": [
        28867,
        28957
      ]
    },
    {
      "content": "When you configure a Redis server, you can specify the maximum amount of memory it can use.",
      "pos": [
        28958,
        29049
      ]
    },
    {
      "content": "A key in a Redis cache can be configured with an expiration time, after which it is automatically removed from the cache.",
      "pos": [
        29050,
        29171
      ]
    },
    {
      "content": "This feature can help prevent the in-memory cache from being filled with old or stale data.",
      "pos": [
        29172,
        29263
      ]
    },
    {
      "content": "As memory fills up, Redis can automatically evict keys and their values by following a number of policies.",
      "pos": [
        29265,
        29371
      ]
    },
    {
      "content": "The default is LRU (least recently used), but you can also select other policies such as evicting keys at random, or turning off eviction altogether (in which case, attempts to add items to the cache will fail if it is full).",
      "pos": [
        29372,
        29597
      ]
    },
    {
      "content": "The page <bpt id=\"p1\">[</bpt>Using Redis as an LRU Cache<ept id=\"p1\">](http://redis.io/topics/lru-cache)</ept> provides more information.",
      "pos": [
        29598,
        29697
      ]
    },
    {
      "content": "Redis transactions and batches",
      "pos": [
        29703,
        29733
      ]
    },
    {
      "content": "Redis enables a client application to submit a series of operations that read and write data in the cache as an atomic transaction.",
      "pos": [
        29735,
        29866
      ]
    },
    {
      "content": "All of the commands in the transaction are guaranteed to be executed sequentially and no commands issued by other concurrent clients will be interwoven between them.",
      "pos": [
        29867,
        30032
      ]
    },
    {
      "content": "However, these are not true transactions as a relational database would perform them.",
      "pos": [
        30033,
        30118
      ]
    },
    {
      "content": "Transaction processing consists of two stages; command queuing and command execution.",
      "pos": [
        30119,
        30204
      ]
    },
    {
      "content": "During the command queuing stage, the commands that comprise the transaction are submitted by the client.",
      "pos": [
        30205,
        30310
      ]
    },
    {
      "content": "If some sort of error occurs at this point (such as a syntax error, or the wrong number of parameters) then Redis will refuse to process the entire transaction and discard it.",
      "pos": [
        30311,
        30486
      ]
    },
    {
      "content": "During the execution phase, Redis performs each queued command in sequence.",
      "pos": [
        30487,
        30562
      ]
    },
    {
      "content": "If a command fails during this phase Redis will continue with the next queued command and it does not roll back the effects of any commands that have already been executed.",
      "pos": [
        30563,
        30735
      ]
    },
    {
      "content": "This simplified form of transaction helps to maintain performance and avoid performance problems caused by contention.",
      "pos": [
        30736,
        30854
      ]
    },
    {
      "content": "Redis does implement a form of optimistic locking to assist in maintaining consistency.",
      "pos": [
        30855,
        30942
      ]
    },
    {
      "content": "For detailed information about transactions and locking with Redis, visit the <bpt id=\"p1\">[</bpt>Transactions page<ept id=\"p1\">](http://redis.io/topics/transactions)</ept> on the Redis website.",
      "pos": [
        30943,
        31099
      ]
    },
    {
      "content": "Redis also supports non-transactional batching of requests.",
      "pos": [
        31101,
        31160
      ]
    },
    {
      "content": "The Redis protocol that clients use to send commands to a Redis server enables a client to send a series of operations as part of the same request.",
      "pos": [
        31161,
        31308
      ]
    },
    {
      "content": "This can help to reduce packet fragmentation on the network.",
      "pos": [
        31309,
        31369
      ]
    },
    {
      "content": "When the batch is processed, each command is performed.",
      "pos": [
        31370,
        31425
      ]
    },
    {
      "content": "Unlike a transaction, if any of these commands are malformed they will be rejected but the remaining commands will be performed.",
      "pos": [
        31426,
        31554
      ]
    },
    {
      "content": "There is also no guarantee on the order in which the commands in the batch will be processed.",
      "pos": [
        31555,
        31648
      ]
    },
    {
      "content": "Redis security",
      "pos": [
        31654,
        31668
      ]
    },
    {
      "content": "Redis is focused purely on providing fast access to data, and is designed to run inside a trusted environment and be accessed only by trusted clients.",
      "pos": [
        31670,
        31820
      ]
    },
    {
      "content": "Redis only supports a limited security model based on password authentication (it is possible to remove authentication completely, although this is not recommended).",
      "pos": [
        31821,
        31986
      ]
    },
    {
      "content": "All authenticated clients share the same global password, and have access to the same resources.",
      "pos": [
        31987,
        32083
      ]
    },
    {
      "content": "If you need more comprehensive login security, you must implement your own security layer in front of the Redis server and all client requests should pass through this additional layer; Redis should not be directly exposed to untrusted or unauthenticated clients.",
      "pos": [
        32084,
        32347
      ]
    },
    {
      "content": "You can restrict access to commands by disabling them or renaming them (and only providing privileged clients with the new names).",
      "pos": [
        32349,
        32479
      ]
    },
    {
      "content": "Redis does not directly support any form of data encryption, so all encoding must be performed by client applications.",
      "pos": [
        32481,
        32599
      ]
    },
    {
      "content": "Additionally, Redis does not provide any form of transport security, so if you need to protect data as it flows across the network you should implement an SSL proxy.",
      "pos": [
        32600,
        32765
      ]
    },
    {
      "pos": [
        32767,
        32875
      ],
      "content": "For more information, visit the <bpt id=\"p1\">[</bpt>Redis Security<ept id=\"p1\">](http://redis.io/topics/security)</ept> page on the Redis website."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Azure Redis Cache provides its own security layer through which clients connect; the underlying Redis",
      "pos": [
        32879,
        32993
      ]
    },
    {
      "content": "servers are not exposed to the public network.",
      "pos": [
        32996,
        33042
      ]
    },
    {
      "content": "Using the Azure Redis cache",
      "pos": [
        33048,
        33075
      ]
    },
    {
      "content": "The Azure Redis Cache provides access to Redis servers running on servers hosted at an Azure datacenter; it acts as a façade that provides access control and security.",
      "pos": [
        33077,
        33244
      ]
    },
    {
      "content": "You can provision a cache by using the Azure Management portal.",
      "pos": [
        33245,
        33308
      ]
    },
    {
      "content": "The portal provides a number of predefined configurations, ranging from a 53GB cache running as a dedicated service that supports SSL communications (for privacy) and master/subordinate replication with an SLA of 99.9% availability, down to a 250MB cache without replication (no availability guarantees) running on shared hardware.",
      "pos": [
        33309,
        33640
      ]
    },
    {
      "content": "Using the Azure Management portal you can also configure the eviction policy of the cache, and control access to the cache by adding users to the roles provided; Owner, Contributor, Reader.",
      "pos": [
        33642,
        33831
      ]
    },
    {
      "content": "These roles define the operations that members can perform.",
      "pos": [
        33832,
        33891
      ]
    },
    {
      "content": "For example, members of the Owner role have complete control over the cache (including security) and its contents, members of the Contributor role can read and write information in the cache, and members of the Reader role can only retrieve data from the cache.",
      "pos": [
        33892,
        34153
      ]
    },
    {
      "content": "Most administrative tasks are performed through the Azure Management portal, and for this reason many of the administrative commands available in the standard version of Redis are not available, including the ability to modify the configuration programmatically, shutdown the Redis server, configure additional slaves, or forcibly save data to disk.",
      "pos": [
        34155,
        34504
      ]
    },
    {
      "content": "The Azure management portal includes a convenient graphical display that enables you to monitor the performance of the cache.",
      "pos": [
        34506,
        34631
      ]
    },
    {
      "content": "For example, you can view the number of connections being made, the number of requests performed, the volume of reads and writes, and the number of cache hits versus cache misses.",
      "pos": [
        34632,
        34811
      ]
    },
    {
      "content": "Using this information you can determine the effectiveness of the cache and if necessary switch to a different configuration or change the eviction policy.",
      "pos": [
        34812,
        34967
      ]
    },
    {
      "content": "Additionally, you can create alerts that send email messages to an administrator if one or more critical metrics fall outside of an expected range.",
      "pos": [
        34968,
        35115
      ]
    },
    {
      "content": "For example, if the number of cache misses exceeds a specified value in the last hour, an administrator could be alerted as the cache may be too small or data may be being evicted too quickly.",
      "pos": [
        35116,
        35308
      ]
    },
    {
      "content": "You can also monitor CPU, memory, and network usage for the cache.",
      "pos": [
        35310,
        35376
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Azure Redis Cache is intended to act purely as a cache rather than a database.",
      "pos": [
        35380,
        35471
      ]
    },
    {
      "content": "As a result, it does not currently implement Redis persistence.",
      "pos": [
        35472,
        35535
      ]
    },
    {
      "pos": [
        35537,
        35778
      ],
      "content": "For further information and examples showing how to create and configure an Azure Redis Cache, visit the page <bpt id=\"p1\">[</bpt>Lap around Azure Redis Cache<ept id=\"p1\">](http://azure.microsoft.com/blog/2014/06/04/lap-around-azure-redis-cache-preview/)</ept> on the Azure blog."
    },
    {
      "content": "Caching session state and HTML output",
      "pos": [
        35783,
        35820
      ]
    },
    {
      "content": "If you building ASP.NET web applications that run by using Azure web roles, you can save session state information and HTML output in an Azure Redis Cache.",
      "pos": [
        35822,
        35977
      ]
    },
    {
      "content": "The Session State Provider for Azure Redis Cache enables you to share session information between different instances of an ASP.NET web application, and is very useful in web farm situations where client-server affinity is not available and caching session data in-memory would not be appropriate.",
      "pos": [
        35978,
        36275
      ]
    },
    {
      "content": "Using the Session State Provider with Azure Redis Cache delivers several benefits, including:",
      "pos": [
        36277,
        36370
      ]
    },
    {
      "content": "It can share session state amongst a large number of instances of an ASP.NET web application, providing improved scalability,",
      "pos": [
        36374,
        36499
      ]
    },
    {
      "content": "It supports controlled, concurrent access to the same session state data for multiple readers and a single writer, and",
      "pos": [
        36502,
        36620
      ]
    },
    {
      "content": "It can use compression to save memory and improve network performance.",
      "pos": [
        36623,
        36693
      ]
    },
    {
      "pos": [
        36695,
        36866
      ],
      "content": "For more information visit the <bpt id=\"p1\">[</bpt>ASP.NET Session State Provider for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn690522.aspx)</ept> page on the Microsoft website."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Do not use the Session State Provider for Azure Redis Cache for ASP.NET applications that run outside of the Azure environment.",
      "pos": [
        36870,
        37010
      ]
    },
    {
      "content": "The latency of accessing the cache from outside of Azure can eliminate the performance benefits of caching data.",
      "pos": [
        37011,
        37123
      ]
    },
    {
      "content": "Similarly, the Output Cache Provider for Azure Redis Cache enables you to save the HTTP responses generated by an ASP.NET web application.",
      "pos": [
        37125,
        37263
      ]
    },
    {
      "content": "Using the Output Cache Provider with Azure Redis Cache can improve the response times of applications that render complex HTML output; application instances generating similar responses can make use of the shared output fragments in the cache rather than generating this HTML output afresh.",
      "pos": [
        37264,
        37554
      ]
    },
    {
      "content": "For more information visit the <bpt id=\"p1\">[</bpt>ASP.NET Output Cache Provider for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn798898.aspx)</ept> page on the Microsoft website.",
      "pos": [
        37556,
        37726
      ]
    },
    {
      "content": "Building a custom Redis cache",
      "pos": [
        37731,
        37760
      ]
    },
    {
      "content": "The Azure Redis cache acts as a façade to the underlying Redis servers.",
      "pos": [
        37762,
        37833
      ]
    },
    {
      "content": "Currently it supports a fixed set of configurations but does not provide for Redis clustering.",
      "pos": [
        37834,
        37928
      ]
    },
    {
      "content": "If you require an advanced configuration that is not covered by the Azure Redis cache (such as a cache bigger than 53GB) you can build and host your own Redis servers by using Azure virtual machines.",
      "pos": [
        37929,
        38128
      ]
    },
    {
      "content": "This is a potentially complex process as you may need to create several VMs to act as master and subordinate nodes if you want to implement replication.",
      "pos": [
        38129,
        38281
      ]
    },
    {
      "content": "Furthermore, if you wish to create a cluster, then you will need multiple masters and subordinate servers; a minimal clustered, replication topology that provides a high degree of availability and scalability comprises at least 6 VMs organized as 3 pairs of master/subordinate servers (a cluster must contain at least 3 master nodes).",
      "pos": [
        38282,
        38616
      ]
    },
    {
      "content": "Each master/subordinate pair should be located close together to minimize latency, but each set of pairs can be running in different Azure datacenters located in different regions if you wish to locate cached data close to the applications that are most likely to use it.",
      "pos": [
        38617,
        38888
      ]
    },
    {
      "content": "The page <bpt id=\"p1\">[</bpt>Running Redis on a CentOS Linux VM in Azure<ept id=\"p1\">](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx)</ept> on the Microsoft website walks through an example showing how to build and configure a Redis node running as an Azure VM.",
      "pos": [
        38889,
        39173
      ]
    },
    {
      "content": "Note that if you implement your own Redis cache in this way, you are responsible for monitoring, managing, and securing the service.",
      "pos": [
        39175,
        39307
      ]
    },
    {
      "content": "Partitioning a Redis cache",
      "pos": [
        39312,
        39338
      ]
    },
    {
      "content": "Partitioning the cache involves splitting the cache across multiple computers.",
      "pos": [
        39340,
        39418
      ]
    },
    {
      "content": "This structure gives you several advantages over using a single cache server, including:",
      "pos": [
        39419,
        39507
      ]
    },
    {
      "content": "Creating a cache that is much bigger than can be stored on a single server.",
      "pos": [
        39511,
        39586
      ]
    },
    {
      "content": "Distributing data across servers, improving availability.",
      "pos": [
        39589,
        39646
      ]
    },
    {
      "content": "If one server fails or becomes inaccessible, only the data that it holds is unavailable; the data on the remaining servers can still be accessed.",
      "pos": [
        39647,
        39792
      ]
    },
    {
      "content": "For a cache, this is not crucial as the cached data is only a transient copy of the data held in a database, and cached data on a server that becomes inaccessible can be cached on a different server instead.",
      "pos": [
        39793,
        40000
      ]
    },
    {
      "content": "Spreading the load across servers, thereby improving performance and scalability.",
      "pos": [
        40003,
        40084
      ]
    },
    {
      "content": "Geolocating data close to the users that access it, reducing latency.",
      "pos": [
        40087,
        40156
      ]
    },
    {
      "content": "For a cache, the most common form of partitioning is sharding.",
      "pos": [
        40158,
        40220
      ]
    },
    {
      "content": "In this strategy each partition (or shard) is a Redis cache in its own right.",
      "pos": [
        40221,
        40298
      ]
    },
    {
      "content": "Data is directed to a specific partition by using sharding logic, which can use a variety of approaches to distribute the data.",
      "pos": [
        40299,
        40426
      ]
    },
    {
      "content": "The <bpt id=\"p1\">[</bpt>Sharding Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589797.aspx)</ept> provides more information on implementing sharding.",
      "pos": [
        40427,
        40550
      ]
    },
    {
      "content": "To implement partitioning in a Redis cache, you can adopt one of the following approaches:",
      "pos": [
        40552,
        40642
      ]
    },
    {
      "pos": [
        40646,
        41552
      ],
      "content": "_Server-side query routing._ In this technique, a client application sends a request to any of the\nRedis servers that comprise the cache (probably the closest server). Each Redis server stores\nmetadata that describes the partition that it holds, and also contains information about which\npartitions are located on other servers. The Redis server examines the client request and if it\ncan be resolved locally it will perform the requested operation, otherwise it will forward the\nrequest on to the appropriate server. This model is implemented by Redis clustering, and is\ndescribed in more detail on the [Redis cluster tutorial](http://redis.io/topics/cluster-tutorial) page on the Redis website. Redis clustering\nis transparent to client applications, and additional Redis servers can be added to the cluster\n(and the data re-partitioned) without requiring that you reconfigure the clients.",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  ",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "_Server-side query routing._ In this technique, a client application sends a request to any of the",
          "pos": [
            0,
            98
          ],
          "nodes": [
            {
              "content": "Server-side query routing.",
              "pos": [
                1,
                27
              ]
            },
            {
              "content": "In this technique, a client application sends a request to any of the",
              "pos": [
                29,
                98
              ]
            }
          ]
        },
        {
          "content": "Redis servers that comprise the cache (probably the closest server). Each Redis server stores",
          "pos": [
            99,
            192
          ],
          "nodes": [
            {
              "content": "Redis servers that comprise the cache (probably the closest server).",
              "pos": [
                0,
                68
              ]
            },
            {
              "content": "Each Redis server stores",
              "pos": [
                69,
                93
              ]
            }
          ]
        },
        {
          "content": "metadata that describes the partition that it holds, and also contains information about which",
          "pos": [
            193,
            287
          ]
        },
        {
          "content": "partitions are located on other servers. The Redis server examines the client request and if it",
          "pos": [
            288,
            383
          ],
          "nodes": [
            {
              "content": "partitions are located on other servers.",
              "pos": [
                0,
                40
              ]
            },
            {
              "content": "The Redis server examines the client request and if it",
              "pos": [
                41,
                95
              ]
            }
          ]
        },
        {
          "content": "can be resolved locally it will perform the requested operation, otherwise it will forward the",
          "pos": [
            384,
            478
          ]
        },
        {
          "content": "request on to the appropriate server. This model is implemented by Redis clustering, and is",
          "pos": [
            479,
            570
          ],
          "nodes": [
            {
              "content": "request on to the appropriate server.",
              "pos": [
                0,
                37
              ]
            },
            {
              "content": "This model is implemented by Redis clustering, and is",
              "pos": [
                38,
                91
              ]
            }
          ]
        },
        {
          "content": "described in more detail on the [Redis cluster tutorial](http://redis.io/topics/cluster-tutorial) page on the Redis website. Redis clustering",
          "pos": [
            571,
            712
          ],
          "nodes": [
            {
              "content": "described in more detail on the <bpt id=\"p1\">[</bpt>Redis cluster tutorial<ept id=\"p1\">](http://redis.io/topics/cluster-tutorial)</ept> page on the Redis website.",
              "pos": [
                0,
                124
              ]
            },
            {
              "content": "Redis clustering",
              "pos": [
                125,
                141
              ]
            }
          ]
        },
        {
          "content": "is transparent to client applications, and additional Redis servers can be added to the cluster",
          "pos": [
            713,
            808
          ]
        },
        {
          "content": "(and the data re-partitioned) without requiring that you reconfigure the clients.",
          "pos": [
            809,
            890
          ]
        }
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Azure Redis Cache does not currently support Redis clustering.",
      "pos": [
        41558,
        41638
      ]
    },
    {
      "content": "If you wish to",
      "pos": [
        41639,
        41653
      ]
    },
    {
      "content": "implement this approach then you should build a custom Redis cache as described earlier.",
      "pos": [
        41656,
        41744
      ]
    },
    {
      "pos": [
        41748,
        42271
      ],
      "content": "_Client-side partitioning._ In this model, the client application contains logic (possibly in\nthe form of a library) that routes requests to the appropriate Redis server. This approach\ncan be used with Azure Redis Cache; create multiple Azure Redis Caches (one for each data\npartition) and implement the client-side logic that routes the requests to the correct\ncache. If the partitioning scheme changes (if additional Azure Redis Caches are created,\nfor example), client applications may need to be reconfigured.",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "_Client-side partitioning._ In this model, the client application contains logic (possibly in",
          "pos": [
            0,
            93
          ],
          "nodes": [
            {
              "content": "Client-side partitioning.",
              "pos": [
                1,
                26
              ]
            },
            {
              "content": "In this model, the client application contains logic (possibly in",
              "pos": [
                28,
                93
              ]
            }
          ]
        },
        {
          "content": "the form of a library) that routes requests to the appropriate Redis server. This approach",
          "pos": [
            94,
            184
          ],
          "nodes": [
            {
              "content": "the form of a library) that routes requests to the appropriate Redis server.",
              "pos": [
                0,
                76
              ]
            },
            {
              "content": "This approach",
              "pos": [
                77,
                90
              ]
            }
          ]
        },
        {
          "content": "can be used with Azure Redis Cache; create multiple Azure Redis Caches (one for each data",
          "pos": [
            185,
            274
          ]
        },
        {
          "content": "partition) and implement the client-side logic that routes the requests to the correct",
          "pos": [
            275,
            361
          ]
        },
        {
          "content": "cache. If the partitioning scheme changes (if additional Azure Redis Caches are created,",
          "pos": [
            362,
            450
          ],
          "nodes": [
            {
              "content": "cache.",
              "pos": [
                0,
                6
              ]
            },
            {
              "content": "If the partitioning scheme changes (if additional Azure Redis Caches are created,",
              "pos": [
                7,
                88
              ]
            }
          ]
        },
        {
          "content": "for example), client applications may need to be reconfigured.",
          "pos": [
            451,
            513
          ]
        }
      ]
    },
    {
      "pos": [
        42275,
        42786
      ],
      "content": "_Proxy-assisted partitioning._ In this scheme, client applications send requests to an\nintermediary proxy service which understands how the data is partitioned and then routes\nthe request to the appropriate Redis server. This approach can also be used with Azure\nRedis Cache; the proxy service could be implemented as an Azure cloud service. This\napproach requires an additional level of complexity to implement the service, and\nrequests may take longer to perform than using client-side partitioning.",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "_Proxy-assisted partitioning._ In this scheme, client applications send requests to an",
          "pos": [
            0,
            86
          ],
          "nodes": [
            {
              "content": "Proxy-assisted partitioning.",
              "pos": [
                1,
                29
              ]
            },
            {
              "content": "In this scheme, client applications send requests to an",
              "pos": [
                31,
                86
              ]
            }
          ]
        },
        {
          "content": "intermediary proxy service which understands how the data is partitioned and then routes",
          "pos": [
            87,
            175
          ]
        },
        {
          "content": "the request to the appropriate Redis server. This approach can also be used with Azure",
          "pos": [
            176,
            262
          ],
          "nodes": [
            {
              "content": "the request to the appropriate Redis server.",
              "pos": [
                0,
                44
              ]
            },
            {
              "content": "This approach can also be used with Azure",
              "pos": [
                45,
                86
              ]
            }
          ]
        },
        {
          "content": "Redis Cache; the proxy service could be implemented as an Azure cloud service. This",
          "pos": [
            263,
            346
          ],
          "nodes": [
            {
              "content": "Redis Cache; the proxy service could be implemented as an Azure cloud service.",
              "pos": [
                0,
                78
              ]
            },
            {
              "content": "This",
              "pos": [
                79,
                83
              ]
            }
          ]
        },
        {
          "content": "approach requires an additional level of complexity to implement the service, and",
          "pos": [
            347,
            428
          ]
        },
        {
          "content": "requests may take longer to perform than using client-side partitioning.",
          "pos": [
            429,
            501
          ]
        }
      ]
    },
    {
      "content": "The page <bpt id=\"p1\">[</bpt>Partitioning: how to split data among multiple Redis instances<ept id=\"p1\">](http://redis.io/topics/partitioning)</ept>",
      "pos": [
        42788,
        42898
      ]
    },
    {
      "content": "on the Redis website provides further information about implementing partitioning with Redis.",
      "pos": [
        42899,
        42992
      ]
    },
    {
      "content": "Implementing Redis cache client applications",
      "pos": [
        42998,
        43042
      ]
    },
    {
      "content": "Redis supports client applications written in numerous programming languages.",
      "pos": [
        43044,
        43121
      ]
    },
    {
      "content": "If you are building new applications by using the .NET Framework, the recommended approach is to use the StackExchange.Redis client library.",
      "pos": [
        43122,
        43262
      ]
    },
    {
      "content": "This library provides a .NET Framework object model that abstracts the details for connecting to a Redis server, sending commands, and receiving responses.",
      "pos": [
        43263,
        43418
      ]
    },
    {
      "content": "It is available in Visual Studio as a NuGet package.",
      "pos": [
        43419,
        43471
      ]
    },
    {
      "content": "You can use this same library to connect to an Azure Redis cache, or a custom Redis cache hosted on a VM.",
      "pos": [
        43472,
        43577
      ]
    },
    {
      "content": "To connect to a Redis server you use the static <ph id=\"ph1\">`Connect`</ph> method of the <ph id=\"ph2\">`ConnectionMultiplexer`</ph> class.",
      "pos": [
        43579,
        43681
      ]
    },
    {
      "content": "The connection that this method creates is designed to be used throughout the lifetime of the client application, and the same connection can be used by multiple concurrent threads; do not reconnect and disconnect each time you perform a Redis operation as this can degrade performance.",
      "pos": [
        43682,
        43968
      ]
    },
    {
      "content": "You can specify the connection parameters, such as the address of the Redis host and the password.",
      "pos": [
        43969,
        44067
      ]
    },
    {
      "content": "If you are using the Azure Redis cache, the password this is either the primary or secondary key generated for the Azure Redis Cache by using the Azure Management portal.",
      "pos": [
        44068,
        44238
      ]
    },
    {
      "content": "After you have connected to the Redis server, you can obtain a handle on the Redis database that acts as the cache.",
      "pos": [
        44240,
        44355
      ]
    },
    {
      "content": "The Redis connection provides the <ph id=\"ph1\">`GetDatabase`</ph> method to do this.",
      "pos": [
        44356,
        44422
      ]
    },
    {
      "content": "You can then retrieve items from the cache and store data in the cache by using the <ph id=\"ph1\">`StringGet`</ph> and <ph id=\"ph2\">`StringSet`</ph> methods.",
      "pos": [
        44423,
        44543
      ]
    },
    {
      "content": "These methods expect a key as a parameter, and either return the item in the cache that has a matching value (<ph id=\"ph1\">`StringGet`</ph>) or add the item to the cache with this key (<ph id=\"ph2\">`StringSet`</ph>).",
      "pos": [
        44544,
        44724
      ]
    },
    {
      "content": "Depending on the location of the Redis server, many operations may incur some latency while a request is transmitted to the server and a response returned to the client.",
      "pos": [
        44726,
        44895
      ]
    },
    {
      "content": "The StackExchange library provides asynchronous versions of many of the methods that it exposes to help client applications remain responsive.",
      "pos": [
        44896,
        45038
      ]
    },
    {
      "content": "These methods support the <bpt id=\"p1\">[</bpt>Task-based Asynchronous Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/hh873175.aspx)</ept> in the .NET Framework.",
      "pos": [
        45039,
        45170
      ]
    },
    {
      "content": "The following code snippet shows a method named <ph id=\"ph1\">`RetrieveItem`</ph> that illustrates an example of an implementation of the cache-aside pattern based on Redis and the StackExchange library.",
      "pos": [
        45172,
        45356
      ]
    },
    {
      "content": "The method takes a string key value, and attempts to retrieve the corresponding item from the Redis cache by calling the <ph id=\"ph1\">`StringGetAsync`</ph> method (the asynchronous version of <ph id=\"ph2\">`StringGet`</ph>).",
      "pos": [
        45357,
        45544
      ]
    },
    {
      "content": "If the item is not found, it is fetched from the underlying data source using the <ph id=\"ph1\">`GetItemFromDataSourceAsync`</ph> method (which is a local method and not part of the StackExchange library), and then added to the cache by using the <ph id=\"ph2\">`StringSetAsync`</ph> method so it can be retrieved more quickly next time.",
      "pos": [
        45545,
        45843
      ]
    },
    {
      "content": "The <ph id=\"ph1\">`StringGet`</ph> and <ph id=\"ph2\">`StringSet`</ph> methods are not restricted to retrieving or storing string values; they can take any item serialized as an array of bytes.",
      "pos": [
        46750,
        46904
      ]
    },
    {
      "content": "If you need to save a .NET object you can serialize it as a byte stream and use the StringSet method to write it to the cache.",
      "pos": [
        46905,
        47031
      ]
    },
    {
      "content": "Similarly, you can read an object from cache by using the StringGet method and deserialize it as a .NET object.",
      "pos": [
        47032,
        47143
      ]
    },
    {
      "content": "The following code shows a set of extension methods for the IDatabase interface (the GetDatabase method of a Redis connection returns an <ph id=\"ph1\">`IDatabase`</ph> object),  and some sample code that uses these methods to read and write a BlogPost object to the cache:",
      "pos": [
        47144,
        47397
      ]
    },
    {
      "pos": [
        48792,
        48989
      ],
      "content": "The following code illustrates a method named <ph id=\"ph1\">`RetrieveBlogPost`</ph> that uses these extension methods to read and write a serializable <ph id=\"ph2\">`BlogPost`</ph> object to the cache following the cache-aside pattern:"
    },
    {
      "content": "Redis supports command pipelining if a client application sends multiple asynchronous requests.",
      "pos": [
        49842,
        49937
      ]
    },
    {
      "content": "Redis can multiplex the requests using the same connection rather than receiving and responding to commands in a strict sequence.",
      "pos": [
        49938,
        50067
      ]
    },
    {
      "content": "This approach helps to reduce latency by making more efficient use of the network.",
      "pos": [
        50068,
        50150
      ]
    },
    {
      "content": "The following code snippet shows an example that retrieves the details of two customers concurrently.",
      "pos": [
        50151,
        50252
      ]
    },
    {
      "content": "The code submits two requests and then performs some other processing (not shown) before waiting to receive the results.",
      "pos": [
        50253,
        50373
      ]
    },
    {
      "content": "The Wait method of the cache object is similar to the .NET Framework Task.Wait method:",
      "pos": [
        50374,
        50460
      ]
    },
    {
      "content": "The page <bpt id=\"p1\">[</bpt>Develop for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn690520.aspx)</ept> on the Microsoft website provides more information on how to write client applications that can use the Azure Redis cache.",
      "pos": [
        50753,
        50971
      ]
    },
    {
      "content": "Additional information is available on the <bpt id=\"p1\">[</bpt>Basic Usage page<ept id=\"p1\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md)</ept> on the StackExchange.Redis website, and the page <bpt id=\"p2\">[</bpt>Pipelines and Multiplexers<ept id=\"p2\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/PipelinesMultiplexers.md)</ept> on the same website provides more information about asynchronous operations and pipelining with Redis and the StackExchange library.",
      "pos": [
        50972,
        51421
      ]
    },
    {
      "content": "The section Use-Cases for Redis Caching later in this guidance provides examples of some of the more advanced techniques that you can apply to data held in a Redis cache.",
      "pos": [
        51423,
        51593
      ]
    },
    {
      "content": "Use-cases for Redis caching",
      "pos": [
        51598,
        51625
      ]
    },
    {
      "content": "The simplest use of Redis for caching concerns storing key/value pairs where the value is an uninterpreted string of arbitrary length that can contain any binary data (it is essentially  an array of bytes that can be treated as a string).",
      "pos": [
        51627,
        51865
      ]
    },
    {
      "content": "This scenario was illustrated in the section Implementing Redis Cache Client Applications earlier in this guidance.",
      "pos": [
        51866,
        51981
      ]
    },
    {
      "content": "You should note that keys also contain uninterpreted data, so you can use any binary information as the key, although the longer the key is the more space it will take to store, and the longer it will take to perform lookup operations.",
      "pos": [
        51982,
        52217
      ]
    },
    {
      "content": "For usability and ease of maintenance design your keyspace carefully and use meaningful (but not verbose) keys.",
      "pos": [
        52218,
        52329
      ]
    },
    {
      "content": "For example, use structured keys such as \"customer:100\" to represent the key for the customer with ID 100 rather than simply \"100\".",
      "pos": [
        52330,
        52461
      ]
    },
    {
      "content": "This scheme enables you to easily distinguish between values that store different data types.",
      "pos": [
        52462,
        52555
      ]
    },
    {
      "content": "For example, you could also use the key \"orders:100\" to represent the key for the order with ID 100.",
      "pos": [
        52556,
        52656
      ]
    },
    {
      "content": "Apart from one-dimensional binary strings, a value in a Redis key/value pair can also hold more structured information, including lists, sets (sorted and unsorted), and hashes.",
      "pos": [
        52658,
        52834
      ]
    },
    {
      "content": "Redis provides a comprehensive command set that can manipulate these types, and many of these commands are available to .NET Framework applications through a client library such as StackExchange.",
      "pos": [
        52835,
        53030
      ]
    },
    {
      "content": "The page <bpt id=\"p1\">[</bpt>An introduction to Redis data types and abstractions<ept id=\"p1\">](http://redis.io/topics/data-types-intro)</ept> on the Redis website provides a more detailed overview of these types and the commands that you can use to manipulate them.",
      "pos": [
        53031,
        53259
      ]
    },
    {
      "content": "This section summarizes some common use-cases for these data types and commands.",
      "pos": [
        53261,
        53341
      ]
    },
    {
      "content": "Performing atomic and batch operations",
      "pos": [
        53347,
        53385
      ]
    },
    {
      "content": "Redis supports a series of atomic get-and-set operations on string values.",
      "pos": [
        53387,
        53461
      ]
    },
    {
      "content": "These operations remove the possible race hazards that might occur when using separate <ph id=\"ph1\">`GET`</ph> and <ph id=\"ph2\">`SET`</ph> commands.",
      "pos": [
        53462,
        53574
      ]
    },
    {
      "content": "The operations available include:",
      "pos": [
        53575,
        53608
      ]
    },
    {
      "pos": [
        53612,
        54031
      ],
      "content": "`INCR`, `INCRBY`, `DECR`, and `DECRBY` which perform atomic increment and decrement operations on\ninteger numeric data values. The StackExchange library provides overloaded versions of the\n`IDatabase.StringIncrementAsync` and `IDatabase.StringDecrementAsync` methods to perform\nthese operations and return the resulting value stored in the cache. The following code\nsnippet illustrates how to use these methods:",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "<ph id=\"ph1\">`INCR`</ph>, <ph id=\"ph2\">`INCRBY`</ph>, <ph id=\"ph3\">`DECR`</ph>, and <ph id=\"ph4\">`DECRBY`</ph> which perform atomic increment and decrement operations on",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "integer numeric data values. The StackExchange library provides overloaded versions of the",
          "pos": [
            98,
            188
          ],
          "nodes": [
            {
              "content": "integer numeric data values.",
              "pos": [
                0,
                28
              ]
            },
            {
              "content": "The StackExchange library provides overloaded versions of the",
              "pos": [
                29,
                90
              ]
            }
          ]
        },
        {
          "content": "<ph id=\"ph1\">`IDatabase.StringIncrementAsync`</ph> and <ph id=\"ph2\">`IDatabase.StringDecrementAsync`</ph> methods to perform",
          "pos": [
            189,
            277
          ]
        },
        {
          "content": "these operations and return the resulting value stored in the cache. The following code",
          "pos": [
            278,
            365
          ],
          "nodes": [
            {
              "content": "these operations and return the resulting value stored in the cache.",
              "pos": [
                0,
                68
              ]
            },
            {
              "content": "The following code",
              "pos": [
                69,
                87
              ]
            }
          ]
        },
        {
          "content": "snippet illustrates how to use these methods:",
          "pos": [
            366,
            411
          ]
        }
      ]
    },
    {
      "pos": [
        54473,
        54915
      ],
      "content": "`GETSET` which retrieves the value associated with a key and changes it to a new value. The\nStackExchange library makes this operation available through the `IDatabase.StringGetSetAsync`\nmethod. The code snippet below shows an example of this method. This code returns the current\nvalue associated with the key \"data:counter\" from the previous example and resets the value\nfor this key back to zero, all as part of the same operation:",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "`GETSET` which retrieves the value associated with a key and changes it to a new value. The",
          "pos": [
            0,
            91
          ],
          "nodes": [
            {
              "content": "<ph id=\"ph1\">`GETSET`</ph> which retrieves the value associated with a key and changes it to a new value.",
              "pos": [
                0,
                87
              ]
            },
            {
              "content": "The",
              "pos": [
                88,
                91
              ]
            }
          ]
        },
        {
          "content": "StackExchange library makes this operation available through the <ph id=\"ph1\">`IDatabase.StringGetSetAsync`</ph>",
          "pos": [
            92,
            186
          ]
        },
        {
          "content": "method. The code snippet below shows an example of this method. This code returns the current",
          "pos": [
            187,
            280
          ],
          "nodes": [
            {
              "content": "method.",
              "pos": [
                0,
                7
              ]
            },
            {
              "content": "The code snippet below shows an example of this method.",
              "pos": [
                8,
                63
              ]
            },
            {
              "content": "This code returns the current",
              "pos": [
                64,
                93
              ]
            }
          ]
        },
        {
          "content": "value associated with the key \"data:counter\" from the previous example and resets the value",
          "pos": [
            281,
            372
          ]
        },
        {
          "content": "for this key back to zero, all as part of the same operation:",
          "pos": [
            373,
            434
          ]
        }
      ]
    },
    {
      "pos": [
        55120,
        55366
      ],
      "content": "`MGET` and `MSET`, which can return or change a set of string values as a single operation. The\n`IDatabase.StringGetAsync` and `IDatabase.StringSetAsync` methods are overloaded to support\nthis functionality, as shown in the following example:",
      "leadings": [
        "",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "`MGET` and `MSET`, which can return or change a set of string values as a single operation. The",
          "pos": [
            0,
            95
          ],
          "nodes": [
            {
              "content": "<ph id=\"ph1\">`MGET`</ph> and <ph id=\"ph2\">`MSET`</ph>, which can return or change a set of string values as a single operation.",
              "pos": [
                0,
                91
              ]
            },
            {
              "content": "The",
              "pos": [
                92,
                95
              ]
            }
          ]
        },
        {
          "content": "<ph id=\"ph1\">`IDatabase.StringGetAsync`</ph> and <ph id=\"ph2\">`IDatabase.StringSetAsync`</ph> methods are overloaded to support",
          "pos": [
            96,
            187
          ]
        },
        {
          "content": "this functionality, as shown in the following example:",
          "pos": [
            188,
            242
          ]
        }
      ]
    },
    {
      "content": "You can also combine multiple operations into a single Redis transaction as described in the Redis Transactions and Batches section in this guidance.",
      "pos": [
        56189,
        56338
      ]
    },
    {
      "content": "The StackExchange library provides support for transactions through the <ph id=\"ph1\">`ITransaction`</ph> interface.",
      "pos": [
        56339,
        56436
      ]
    },
    {
      "content": "You can create an ITransaction object by using the IDatabase.CreateTransaction method, and invoke commands to the transaction by using the methods provided <ph id=\"ph1\">`ITransaction`</ph> object.",
      "pos": [
        56437,
        56615
      ]
    },
    {
      "content": "The <ph id=\"ph1\">`ITransaction`</ph> interface provides access to a similar set of methods as the <ph id=\"ph2\">`IDatabase`</ph> interface except that all the methods are asynchronous; they are only performed when the <ph id=\"ph3\">`ITransaction.Execute`</ph> method is invoked.",
      "pos": [
        56616,
        56838
      ]
    },
    {
      "content": "The value returned by the execute method indicates whether the transaction was created successfully (true) or it failed (false).",
      "pos": [
        56839,
        56967
      ]
    },
    {
      "content": "The following code snippet shows an example that increments and decrements two counters as part of the same transaction:",
      "pos": [
        56969,
        57089
      ]
    },
    {
      "content": "Remember that Redis transactions are unlike transactions in relational databases.",
      "pos": [
        57614,
        57695
      ]
    },
    {
      "content": "The Execute method simply queues all the commands that comprise the transaction for execution, and if any of them is malformed then the transaction is aborted.",
      "pos": [
        57696,
        57855
      ]
    },
    {
      "content": "If all the commands have been queued successfully, each command will be run asynchronously.",
      "pos": [
        57856,
        57947
      ]
    },
    {
      "content": "If any command fails, the others will still continue processing.",
      "pos": [
        57948,
        58012
      ]
    },
    {
      "content": "If you need to verify that a command has completed successfully you must fetch the results of the command by using the Result property of the corresponding task, as shown in the example above.",
      "pos": [
        58013,
        58205
      ]
    },
    {
      "content": "Reading the Result property will block until the task has completed.",
      "pos": [
        58206,
        58274
      ]
    },
    {
      "pos": [
        58276,
        58457
      ],
      "content": "For more information, see the <bpt id=\"p1\">[</bpt>Transactions in Redis<ept id=\"p1\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Transactions.md)</ept> page on the StackExchange.Redis website."
    },
    {
      "content": "For performing batch operations, you can use the IBatch interface of the StackExchange library.",
      "pos": [
        58459,
        58554
      ]
    },
    {
      "content": "This interface provides access to a similar set of methods as the IDatabase interface except that all the methods are asynchronous.",
      "pos": [
        58555,
        58686
      ]
    },
    {
      "content": "You create an IBatch object by using the IDatabase.CreateBatch method and then run the batch by using the IBatch.Execute method, as shown in the following example.",
      "pos": [
        58687,
        58850
      ]
    },
    {
      "content": "This code simply sets a string value, and increments and decrements the same counters used in the previous example and displays the results:",
      "pos": [
        58851,
        58991
      ]
    },
    {
      "content": "It is important to understand that unlike a transaction, if a command in a batch fails because it is malformed the other commands may still run; the IBatch.Execute method does not return any indication of success or failure.",
      "pos": [
        59388,
        59612
      ]
    },
    {
      "content": "Performing fire-and-forget cache operations",
      "pos": [
        59618,
        59661
      ]
    },
    {
      "content": "Redis supports fire-and-forget operations by using command flags.",
      "pos": [
        59663,
        59728
      ]
    },
    {
      "content": "In this situation, the client simply initiates an operation but has no interest in the result and does not wait for the command to be completed.",
      "pos": [
        59729,
        59873
      ]
    },
    {
      "content": "The example below shows how to perform the INCR command as a fire-and-forget operation:",
      "pos": [
        59874,
        59961
      ]
    },
    {
      "content": "Automatically expiring keys",
      "pos": [
        60208,
        60235
      ]
    },
    {
      "content": "When you store an item in a Redis cache, you can specify a timeout after which the item will be automatically removed from the cache.",
      "pos": [
        60237,
        60370
      ]
    },
    {
      "content": "You can also query how much more time a key has before it expires by using the <ph id=\"ph1\">`TTL`</ph> command; this command is available to StackExchange applications by using the IDatabase.KeyTimeToLive method.",
      "pos": [
        60371,
        60565
      ]
    },
    {
      "content": "The following code snippet shows an example of setting an expiration time of 20 seconds on a key, and querying the remaining lifetime of the key:",
      "pos": [
        60567,
        60712
      ]
    },
    {
      "content": "You can also set the expiration time to a specific date and time by using the EXPIRE command, available in the StackExchange library as the KeyExpireAsync method:",
      "pos": [
        61137,
        61299
      ]
    },
    {
      "pos": [
        61640,
        61812
      ],
      "content": "<bpt id=\"p1\">_</bpt>Tip:<ept id=\"p1\">_</ept> You can manually remove an item from the cache by using the DEL command, which is available through the StackExchange library as the IDatabase.KeyDeleteAsync method."
    },
    {
      "content": "Using tags to cross-correlate cached items",
      "pos": [
        61818,
        61860
      ]
    },
    {
      "content": "A Redis set is a collection of multiple items that share a single key.",
      "pos": [
        61862,
        61932
      ]
    },
    {
      "content": "You can create a set by using the SADD command.",
      "pos": [
        61933,
        61980
      ]
    },
    {
      "content": "You can retrieve the items in a set by using the SMEMBERS command.",
      "pos": [
        61981,
        62047
      ]
    },
    {
      "content": "The StackExchange library implements the SADD command through the IDatabase.SetAddAsync method, and the SMEMBERS command with the IDatabase.SetMembersAsync method.",
      "pos": [
        62048,
        62211
      ]
    },
    {
      "content": "You can also combine existing sets to create new sets by using the SDIFF (set difference), SINTER (set intersection), and SUNION (set union) commands.",
      "pos": [
        62212,
        62362
      ]
    },
    {
      "content": "The StackExchange library unifies these operations in the IDatabase.SetCombineAsync method; the first parameter to this method specifies the set operation to perform.",
      "pos": [
        62363,
        62529
      ]
    },
    {
      "content": "The following code snippets show how sets can be useful for quickly storing and retrieving collections of related items.",
      "pos": [
        62531,
        62651
      ]
    },
    {
      "content": "This code uses the BlogPost type described in the section Implementing Redis Cache Client Applications.",
      "pos": [
        62652,
        62755
      ]
    },
    {
      "content": "A BlogPost object contains four fields—an ID, a title, a ranking score, and a collection of tags.",
      "pos": [
        62756,
        62853
      ]
    },
    {
      "content": "The first code snippet below shows the sample data used for populating a C# list of BlogPost objects:",
      "pos": [
        62854,
        62955
      ]
    },
    {
      "content": "You can store the tags for each BlogPost object as a set in a Redis cache and associate each set with the ID of the BlogPost.",
      "pos": [
        64031,
        64156
      ]
    },
    {
      "content": "This enables an application to quickly find all the tags belonging to a specific blog post.",
      "pos": [
        64157,
        64248
      ]
    },
    {
      "content": "To enable searching in the opposite direction and find all blog posts that share a specific tag, you can create another set that holds the blog posts referencing the tag ID in the key:",
      "pos": [
        64249,
        64433
      ]
    },
    {
      "content": "These structures enable you to perform many common queries very efficiently.",
      "pos": [
        65139,
        65215
      ]
    },
    {
      "content": "For example, you can find and display all of the tags for blog post 1 like this:",
      "pos": [
        65216,
        65296
      ]
    },
    {
      "content": "You can find all tags that are common to blog post 1 and blog post 2 by performing a set intersection operation, as follows:",
      "pos": [
        65453,
        65577
      ]
    },
    {
      "content": "And you can find all blog posts that contain a specific tag:",
      "pos": [
        65820,
        65880
      ]
    },
    {
      "content": "Finding recently accessed items",
      "pos": [
        66067,
        66098
      ]
    },
    {
      "content": "A common problem required by many applications is to find the most recently accessed items.",
      "pos": [
        66100,
        66191
      ]
    },
    {
      "content": "For example, a blogging site might want to display information about the most recently read blog posts.",
      "pos": [
        66192,
        66295
      ]
    },
    {
      "content": "You can implement this functionality by using a Redis list.",
      "pos": [
        66296,
        66355
      ]
    },
    {
      "content": "A Redis list contains multiple items that share the same key, but the list acts as a double-ended queue.",
      "pos": [
        66356,
        66460
      ]
    },
    {
      "content": "You can push items on to either end of the list by using the LPUSH (left push) and RPUSH (right push) commands.",
      "pos": [
        66461,
        66572
      ]
    },
    {
      "content": "You can retrieve items from either end of the list by using the LPOP and RPOP commands.",
      "pos": [
        66573,
        66660
      ]
    },
    {
      "content": "You can also return a set of elements by using the LRANGE and RRANGE commands.",
      "pos": [
        66662,
        66740
      ]
    },
    {
      "content": "The code snippets below show how you can perform these operations by using the StackExchange library.",
      "pos": [
        66741,
        66842
      ]
    },
    {
      "content": "This code uses the BlogPost type from the previous examples.",
      "pos": [
        66843,
        66903
      ]
    },
    {
      "content": "As a blog post is read by a user, the title of the blog post is pushed onto a list associated with the key \"blog:recent_posts\" in the Redis cache by using the IDatabase.ListLeftPushAsync method:",
      "pos": [
        66904,
        67098
      ]
    },
    {
      "content": "As more blog posts are read, their titles are pushed onto the same list.",
      "pos": [
        67437,
        67509
      ]
    },
    {
      "content": "The list is ordered by the sequence in which they have been added; the most recently read blog posts are towards the left end of the list (if the same blog post is read more than once, it will have multiple entries in the list).",
      "pos": [
        67510,
        67738
      ]
    },
    {
      "content": "You can display the titles of the most recently read posts by using the IDatabase.ListRange method.",
      "pos": [
        67739,
        67838
      ]
    },
    {
      "content": "This method takes the key that contains the list, a starting point, and an ending point.",
      "pos": [
        67839,
        67927
      ]
    },
    {
      "content": "The following code retrieves the titles of the 10 blog posts (items from 0 to 9) at the left-most end of the list:",
      "pos": [
        67928,
        68042
      ]
    },
    {
      "content": "Note that ListRangeAsync does not remove items from the list; to do this you can use the IDatabase.ListLeftPopAsync and IDatabase.ListRightPopAsync methods.",
      "pos": [
        68195,
        68351
      ]
    },
    {
      "content": "To prevent the list from growing indefinitely, you can periodically cull items by trimming the list.",
      "pos": [
        68353,
        68453
      ]
    },
    {
      "content": "The code snippet below, removes all but the 5 left-most items from the list:",
      "pos": [
        68454,
        68530
      ]
    },
    {
      "content": "Implementing a leader board",
      "pos": [
        68594,
        68621
      ]
    },
    {
      "content": "By default the items in a set are not held in any specific order.",
      "pos": [
        68623,
        68688
      ]
    },
    {
      "content": "You can create an ordered set by using the ZADD command (the IDatabase.SortedSetAdd method in the StackExchange library).",
      "pos": [
        68689,
        68810
      ]
    },
    {
      "content": "The items are ordered by using a numeric value called a score provided as a parameter to the command.",
      "pos": [
        68811,
        68912
      ]
    },
    {
      "content": "The following code snippet adds the title of a blog post to an ordered list.",
      "pos": [
        68913,
        68989
      ]
    },
    {
      "content": "In the example, each blog post also has a score field that contains the ranking of the blog post.",
      "pos": [
        68990,
        69087
      ]
    },
    {
      "content": "You can retrieve the blog post titles and scores in ascending score order by using the IDatabase.SortedSetRangeByRankWithScores method:",
      "pos": [
        69401,
        69536
      ]
    },
    {
      "pos": [
        69668,
        69838
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The StackExchange library also provides the IDatabase.SortedSetRangeByRankAsync method which returns the data in score order, but does not return the scores."
    },
    {
      "content": "You can also retrieve items in descending order of scores, and limit the number of items returned by providing additional parameters to the IDatabase.SortedSetRangeByRankWithScoresAsync method.",
      "pos": [
        69840,
        70033
      ]
    },
    {
      "content": "The next example displays the titles and scores of the top 10 ranked blog posts:",
      "pos": [
        70034,
        70114
      ]
    },
    {
      "content": "The next example uses the IDatabase.SortedSetRangeByScoreWithScoresAsync method which you can use to limit the items returned to those that fall within a given score range:",
      "pos": [
        70300,
        70472
      ]
    },
    {
      "content": "Messaging by using channels",
      "pos": [
        70703,
        70730
      ]
    },
    {
      "content": "Apart from acting as a data cache, a Redis server provides messaging through a high-performance publisher/subscriber mechanism.",
      "pos": [
        70732,
        70859
      ]
    },
    {
      "content": "Client applications can subscribe to a channel, and other applications or services can publish messages to the channel.",
      "pos": [
        70860,
        70979
      ]
    },
    {
      "content": "Subscribing applications will then receive these messages and can process them.",
      "pos": [
        70980,
        71059
      ]
    },
    {
      "content": "To subscribe to channel, Redis provides the SUBSCRIBE command.",
      "pos": [
        71061,
        71123
      ]
    },
    {
      "content": "This command expects the name of one or more channels on which the application will accept messages.",
      "pos": [
        71124,
        71224
      ]
    },
    {
      "content": "The StackExchange library includes the ISubscription interface which enables a .NET Framework application to subscribe and publish to channels.",
      "pos": [
        71225,
        71368
      ]
    },
    {
      "content": "You create an ISubscription object by using the GetSubscriber method of the connection to the Redis server, and then listen for messages on a channel by using the SubscribeAsync method of this object.",
      "pos": [
        71369,
        71569
      ]
    },
    {
      "content": "The following code example shows how to subscribe to a channel named \"messages:blogPosts\":",
      "pos": [
        71570,
        71660
      ]
    },
    {
      "content": "The first parameter to the Subscribe method is the name of the channel.",
      "pos": [
        71923,
        71994
      ]
    },
    {
      "content": "This name follows the same conventions as that used by keys in the cache, and can contain any binary data, although it is advisable to use relatively short, meaningful strings to help ensure good performance and maintainability.",
      "pos": [
        71995,
        72223
      ]
    },
    {
      "content": "You should also note that the namespace used by channels is separate from that used by keys, so you can have channels and keys that have the same name, although this may make your application code more difficult to maintain.",
      "pos": [
        72224,
        72448
      ]
    },
    {
      "content": "The second parameter is an Action delegate.",
      "pos": [
        72450,
        72493
      ]
    },
    {
      "content": "This delegate runs asynchronously whenever a new message appears on the channel.",
      "pos": [
        72494,
        72574
      ]
    },
    {
      "content": "This example simply displays the message on the console (the message will contain the title of a blog post).",
      "pos": [
        72575,
        72683
      ]
    },
    {
      "content": "To publish to a channel, an application can use the Redis PUBLISH command.",
      "pos": [
        72685,
        72759
      ]
    },
    {
      "content": "The StackExchange library provides the IServer.PublishAsync method to perform this operation.",
      "pos": [
        72760,
        72853
      ]
    },
    {
      "content": "The next code snippet shows how to publish a message to the \"messages:blogPosts\" channel:",
      "pos": [
        72854,
        72943
      ]
    },
    {
      "content": "There are several points you should understand about the publish/subscribe mechanism:",
      "pos": [
        73163,
        73248
      ]
    },
    {
      "content": "Multiple subscribers can subscribe to the same channel, and they will all receive the messages published to that channel.",
      "pos": [
        73252,
        73373
      ]
    },
    {
      "content": "Subscribers only receive messages that have been published after they have subscribed.",
      "pos": [
        73376,
        73462
      ]
    },
    {
      "content": "Channels are not buffered, and once a message is published the Redis infrastructure pushes the message to each subscriber and then removes it.",
      "pos": [
        73463,
        73605
      ]
    },
    {
      "pos": [
        73608,
        74165
      ],
      "content": "By default, messages are received by subscribers in the order in which they are sent. In a highly active system with a large number\nof messages and many subscribers and publishers, guaranteed sequential delivery of messages can slow performance of the system. If\neach message is independent and the order is immaterial you can enable concurrent processing by the Redis system which can help to\nimprove responsiveness. You can achieve this in a StackExchange client by setting the PreserveAsyncOrder of the connection used by\nthe subscriber to false:",
      "leadings": [
        "",
        "  ",
        "  ",
        "  ",
        "  "
      ],
      "nodes": [
        {
          "content": "By default, messages are received by subscribers in the order in which they are sent. In a highly active system with a large number",
          "pos": [
            0,
            131
          ],
          "nodes": [
            {
              "content": "By default, messages are received by subscribers in the order in which they are sent.",
              "pos": [
                0,
                85
              ]
            },
            {
              "content": "In a highly active system with a large number",
              "pos": [
                86,
                131
              ]
            }
          ]
        },
        {
          "content": "of messages and many subscribers and publishers, guaranteed sequential delivery of messages can slow performance of the system. If",
          "pos": [
            132,
            262
          ],
          "nodes": [
            {
              "content": "of messages and many subscribers and publishers, guaranteed sequential delivery of messages can slow performance of the system.",
              "pos": [
                0,
                127
              ]
            },
            {
              "content": "If",
              "pos": [
                128,
                130
              ]
            }
          ]
        },
        {
          "content": "each message is independent and the order is immaterial you can enable concurrent processing by the Redis system which can help to",
          "pos": [
            263,
            393
          ]
        },
        {
          "content": "improve responsiveness. You can achieve this in a StackExchange client by setting the PreserveAsyncOrder of the connection used by",
          "pos": [
            394,
            524
          ],
          "nodes": [
            {
              "content": "improve responsiveness.",
              "pos": [
                0,
                23
              ]
            },
            {
              "content": "You can achieve this in a StackExchange client by setting the PreserveAsyncOrder of the connection used by",
              "pos": [
                24,
                130
              ]
            }
          ]
        },
        {
          "content": "the subscriber to false:",
          "pos": [
            525,
            549
          ]
        }
      ]
    },
    {
      "content": "Related Patterns and Guidance",
      "pos": [
        74353,
        74382
      ]
    },
    {
      "content": "The following pattern may also be relevant to your scenario when implementing caching in your applications:",
      "pos": [
        74384,
        74491
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Cache-Aside Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589799.aspx)</ept>: This pattern describes how to load data on-demand into a cache from a data store.",
      "pos": [
        74495,
        74648
      ]
    },
    {
      "content": "This pattern also helps to maintain consistency between data held in the cache and the data in the original data store.",
      "pos": [
        74649,
        74768
      ]
    },
    {
      "pos": [
        74771,
        74981
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Sharding Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589797.aspx)</ept> provides information on implementing horizontal partitioning to help improve scalability when storing and accessing large volumes of data."
    },
    {
      "content": "More Information",
      "pos": [
        74986,
        75002
      ]
    },
    {
      "pos": [
        75006,
        75135
      ],
      "content": "The <bpt id=\"p1\">[</bpt>MemoryCache Class<ept id=\"p1\">](http://msdn.microsoft.com/library/system.runtime.caching.memorycache.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        75138,
        75258
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Microsoft Azure Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsazure/gg278356.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        75261,
        75396
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Which Azure Cache offering is right for me?<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn766201.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        75399,
        75517
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Configuration Model<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsazure/hh914149.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        75520,
        75637
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Task-based Asynchronous Pattern<ept id=\"p1\">](http://msdn.microsoft.com/library/hh873175.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        75640,
        75813
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Pipelines and Multiplexers<ept id=\"p1\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/PipelinesMultiplexers.md)</ept> page on the StackExchange.Redis GitHub repo."
    },
    {
      "pos": [
        75816,
        75902
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Redis Persistence<ept id=\"p1\">](http://redis.io/topics/persistence)</ept> page on the Redis website."
    },
    {
      "pos": [
        75905,
        75985
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Replication page<ept id=\"p1\">](http://redis.io/topics/replication)</ept> on the Redis website."
    },
    {
      "pos": [
        75988,
        76084
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Redis Cluster Tutorial<ept id=\"p1\">](http://redis.io/topics/cluster-tutorial)</ept> page on the Redis website."
    },
    {
      "pos": [
        76087,
        76219
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Partitioning: how to split data among multiple Redis instances<ept id=\"p1\">](http://redis.io/topics/partitioning)</ept> page on the Redis website."
    },
    {
      "pos": [
        76222,
        76316
      ],
      "content": "The page <bpt id=\"p1\">[</bpt>Using Redis as an LRU Cache<ept id=\"p1\">](http://redis.io/topics/lru-cache)</ept> on the Redis website."
    },
    {
      "pos": [
        76319,
        76401
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Transactions page<ept id=\"p1\">](http://redis.io/topics/transactions)</ept> on the Redis website."
    },
    {
      "pos": [
        76404,
        76484
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Redis Security<ept id=\"p1\">](http://redis.io/topics/security)</ept> page on the Redis website."
    },
    {
      "pos": [
        76487,
        76627
      ],
      "content": "The page <bpt id=\"p1\">[</bpt>Lap around Azure Redis Cache<ept id=\"p1\">](http://azure.microsoft.com/blog/2014/06/04/lap-around-azure-redis-cache-preview/)</ept> on the Azure blog."
    },
    {
      "pos": [
        76630,
        76818
      ],
      "content": "The page <bpt id=\"p1\">[</bpt>Running Redis on a CentOS Linux VM<ept id=\"p1\">](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx)</ept> in Azure on the Microsoft website."
    },
    {
      "pos": [
        76821,
        76965
      ],
      "content": "The <bpt id=\"p1\">[</bpt>ASP.NET Session State Provider for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn690522.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        76968,
        77111
      ],
      "content": "The <bpt id=\"p1\">[</bpt>ASP.NET Output Cache Provider for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn798898.aspx)</ept> page on the Microsoft website."
    },
    {
      "pos": [
        77114,
        77228
      ],
      "content": "The page <bpt id=\"p1\">[</bpt>Develop for Azure Redis Cache<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn690520.aspx)</ept> on the Azure site."
    },
    {
      "pos": [
        77231,
        77357
      ],
      "content": "The page <bpt id=\"p1\">[</bpt>An Introduction to Redis data types and abstractions<ept id=\"p1\">](http://redis.io/topics/data-types-intro)</ept> on the Redis website."
    },
    {
      "pos": [
        77360,
        77499
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Basic Usage<ept id=\"p1\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md)</ept> page on the StackExchange.Redis website."
    },
    {
      "pos": [
        77502,
        77654
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Transactions in Redis<ept id=\"p1\">](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Transactions.md)</ept> page on the StackExchange.Redis repo."
    },
    {
      "pos": [
        77657,
        77761
      ],
      "content": "The <bpt id=\"p1\">[</bpt>Data Partitioning Guide<ept id=\"p1\">](http://msdn.microsoft.com/library/dn589795.aspx)</ept> on the Microsoft website."
    },
    {
      "content": "test111111111111122233333",
      "pos": [
        77763,
        77788
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Caching guidance | Microsoft Azure\"\n   description=\"Guidance on caching to improve performance and scalability.\"\n   services=\"\"\n   documentationCenter=\"na\"\n   authors=\"dragon119\"\n   manager=\"masimms\"\n   editor=\"\"\n   tags=\"\"/>\n\n<tags\n   ms.service=\"best-practice\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"na\"\n   ms.date=\"04/28/2015\"\n   ms.author=\"masashin\"/>\n\n# Caching guidance\n\n![](media/best-practices-caching/pnp-logo.png)\n\nCaching is a common technique that aims to improve the performance and\nscalability of a system by temporarily copying frequently accessed data\nto fast storage located close to the application. If this fast data storage\nis located closer to the application than the original source then caching\ncan significantly improve response times for client applications by serving\ndata more quickly. Caching is most effective when a client instance repeatedly\nreads the same data, especially if the data remains relatively static and\nthe original data store is slow relative to the speed of the cache, is\nsubject to a high level of contention, or is far away when network latency\ncan cause access to be slow.\n\n## Caching in distributed applications\n\nDistributed applications typically implement either or both of the\nfollowing strategies when caching data:\n\n- Using a private cache, where data is held locally on the computer running an instance of an application or service.\n- Using a shared cache, serving as a common source which can be accessed by multiple processes and/or machines.\n\nIn both cases, caching could be performed client-side (by the process providing\nthe user interface for a system, such as a web browser or desktop application),\nand/or server-side (by the process providing the business services\nrunning remotely).\n\n### Private caching\n\nThe most basic type of cache is an in-memory store, held in the address\nspace of a single process and accessed directly by the code that runs\nin that process. This type of cache is very quick to access, and it can\nprovide an extremely effective strategy for storing modest amounts of\nstatic data as the size of a cache is typically constrained by the\nvolume of memory available on the machine hosting the process. If you\nneed to cache more information than is physically possible in memory,\nyou can write cached data to the local file system. This will\nnecessarily be slower to access than data held in-memory, but should\nstill be faster and more reliable than retrieving data across a network.\n\nIf you have multiple instances of an application that uses this model\nrunning concurrently, each application instance will have its own\nindependent cache holding its own copy of data.\n\nYou should think of a cache as a snapshot of the original data at some\npoint in the past. If this data is not static, it is likely that\ndifferent application instances will hold different versions of the\ndata in their caches. Therefore, the same query performed by these\ninstances could return different results, as shown in Figure 1.\n\n![Using an in-memory cache in different instances of an application](media/best-practices-caching/Figure1.png)\n\n_Figure 1: Using an in-memory cache in different instances of an application_\n\n### Shared Caching\n\nUsing a shared cache can help to alleviate the concern that data may\ndiffer in each cache, as can occur with in-memory caching. Shared\ncaching ensures that different application instances see the same\nview of cached data by locating the cache in a separate location,\ntypically hosted as part of a separate service, as shown in Figure 2.\n\n![Using a shared cache_](media/best-practices-caching/Figure2.png)\n\n_Figure 2: Using a shared cache_\n\nAn important benefit of using the shared caching approach is the\nscalability it can help to provide. Many shared cache services are\nimplemented by using a cluster of servers, and utilize software that\ndistributes the data across the cluster in a transparent manner. An\napplication instance simply sends a request to the cache service,\nand the underlying infrastructure is responsible for determining the\nlocation of the cached data in the cluster. You can easily scale the\ncache by adding more servers.\n\nThe disadvantages of the shared caching approach are that the cache\nis slower to access because it is no longer held locally to each\napplication instance, and the requirement to implement a separate\ncache service may add complexity to the solution.\n\n## Considerations for using caching\n\nThe following sections describe in more detail the considerations\nfor designing and using a cache.\n\n### When should data be cached?\n\nCaching can dramatically improve performance, scalability, and availability. The more data\nthat you have and the larger the number of users that need to access this data, the greater\nthe benefits of caching become by reducing latency and contention associated with handling\nlarge volumes of concurrent requests in the original data store. For example, a database\nmay support a limited number of concurrent connections, but retrieving data from a shared\ncache rather than the underlying database allows a client application to access this data\neven if the number of available connections is currently exhausted. Additionally, if the\ndatabase becomes unavailable, client applications may be able to continue by using the\ndata held in the cache.\n\nYou should consider caching data that is read frequently but that is modified infrequently\n(the data has a high proportion of read operations compared to write operations). However,\nyou should not use the cache as the authoritative store of critical information; you should\nensure that all changes that your application cannot afford to lose are always saved to a\npersistent data store. In this way, if the cache is unavailable, your application can\nstill continue to operate by using the data store and you will not lose important\ninformation.\n\n### Types of data and cache population strategies\n\nThe key to using a cache effectively lies in determining the most appropriate data to\ncache, and caching it at the appropriate time. The data may be added to the cache on\ndemand the first time it is retrieved by an application, so that the application needs\nfetch the data only once from the data store and subsequent accesses can be satisfied\nby using the cache.\n\nAlternatively, a cache may be partially or fully populated with data in advance,\ntypically when the application starts (an approach known as seeding). However, it may\nnot be advisable to implement seeding for a large cache as this approach can impose\na sudden, high load on the original data store when the application starts running.\n\nOften an analysis of usage patterns can help to decide whether to fully or partially\nprepopulate a cache, and to choose the data that should be cached. For example, it\nwould probably be useful to seed the cache with the static user profile data for\ncustomers who use the application regularly (perhaps every day), but not for\ncustomers who use the application only once a week.\n\nCaching typically works well with data that is immutable or that changes\ninfrequently. Examples include reference information such as product and pricing\ninformation in an ecommerce application, or shared static resources that are costly\nto construct. Some or all of this data can be loaded into the cache at application\nstartup to minimize demand on resources and to improve performance. It may also be\nappropriate to have a background process that periodically updates reference data\nin the cache to ensure it is up to date, or refreshes the cache when reference\ndata changes.\n\nCaching may be less useful for dynamic data, although there are some exceptions to\nthis consideration (see the section Caching Highly Dynamic Data later in this\nguidance for more information). When the original data regularly changes, either\nthe cached information can become stale very quickly or the overhead of keeping\nthe cache synchronized with the original data store reduces the effectiveness of\ncaching. Note that a cache does not have to include the complete data for an\nentity. For example, if a data item represents a multivalued object such as a bank\ncustomer with a name, address, and account balance, some of these elements may\nremain static (the name and address), while others (such as the account balance)\nmay be more dynamic. In these situations, it could be useful to cache the static\nportions of the data and only retrieve (or calculate) the remaining information as\nand when it is required.\n\nPerformance testing and usage analysis should be carried out to determine whether\npre-population or on-demand loading of the cache, or a combination of both, is\nappropriate. The decision should be based on a combination of the volatility and\nusage pattern of the data. Cache utilization and performance analysis is\nparticularly important in applications that encounter heavy loads and must be\nhighly scalable. For example, in highly scalable scenarios it may make sense to\nseed the cache to reduce the load on the data store at peak times.\n\nCaching can also be used to avoid repeating computations as the application is\nrunning. If an operation transforms data or performs a complicated calculation,\nit can save the results of the operation in the cache. If the same calculation\nis required subsequently, the application can simply retrieve the results from\nthe cache.\n\nAn application can modify data held in a cache, but you should consider the\ncache as a transient data store that could disappear at any time. Do not store\nvaluable data only in the cache, but make sure that you maintain the information\nin the original data store as well. In this way, if the cache should become\nunavailable, you minimize the chance of losing data.\n\n### Caching highly dynamic data\n\nStoring information that changes rapidly in a persistent data store can impose\nan overhead on the system. For example, consider a device that continually reports\nstatus or some other measurement. If an application chooses not to cache this\ndata on the basis that the cached information will nearly always be outdated, then\nthe same consideration could be true when storing and retrieving this information\nfrom the data store; in the time taken to save and fetch this data it may have\nchanged. In a situation such as this, consider the benefits of storing the dynamic\ninformation directly in the cache instead of the persistent data store. If the\ndata is non-critical and does not require to be audited, then it does not matter\nif the occasional change is lost.\n\n### Managing data expiration in a cache\n\nIn most cases, data held in a cache is a copy of the data held in the original data\nstore. The data in the original data store might change after it was cached, causing\nthe cached data to become stale. Many caching systems enable you to configure the\ncache to expire data and reduce the period for which data may be out of date.\n\nWhen cached data expires it is removed from the cache, and the application must\nretrieve the data from the original data store (it can put the newly-fetched\ninformation back into cache). You can set a default expiration policy when you\nconfigure the cache. In many cache services you can also stipulate the expiration\nperiod for individual objects when you store them programmatically in the cache\n(some caches enable you to specify the expiration period as an absolute value, or\nas a sliding value that causes the item to be removed from cache if it is not\naccessed within the specified time. This setting overrides any cache-wide\nexpiration policy, but only for the specified objects.\n\n> [AZURE.NOTE] Consider the expiration period for the cache and the objects that it contains carefully. If you make it too short, objects will expire too quickly and you will reduce the benefits of using the cache. If you make the period too long, you risk the data becoming stale.\n\nIt is also possible that the cache might fill up if data is allowed to remain\nresident for a long time. In this case, any requests to add new items to the\ncache might cause some items to be forcibly removed, in a process known as\neviction. Cache services typically evict data on a least-recently-used (LRU)\nbasis, but you can usually override this policy and prevent items from being\nevicted. However, if you adopt this approach you risk your cache exceeding the\nmemory that it has available, and an application that attempts to add an item\nto the cache will fail with an exception.\n\nSome caching implementations may provide additional eviction policies. These\ntypically include the most-recently-used policy (in the expectation that the\ndata will not be required again), first-in-first-out policy (oldest data is\nevicted first), or explicit removal based on a triggered event (such as the\ndata being modified).\n\n### Invalidating data in a client-side cache\n\nData held in a client-side cache is generally considered to be outside of\nthe auspices of the service providing the data to the client; a service\ncannot directly force a client to add or remove information from a\nclient-side cache. This means that it is possible for a client that uses\na poorly configured cache (for example, expiration policies are not\nproperly implemented) to continue using outdated information cached\nlocally when the information in the original data source has changed.\n\nIf you are building a web application that serves data over an HTTP\nconnection, you can implicitly force a web client (such as a browser or\nweb proxy) to fetch the most recent information if a resource is updated\nby changing the URI of that resource. Web clients typically use the URI\nof a resource as the key in the client-side cache, so changing the URI\ncauses the web client to ignore any previously cached version of a\nresource and fetch the new version instead.\n\n## Managing concurrency in a cache\n\nCaches are often designed to be shared by multiple instances of an\napplication. Each application instance can read and modify data in\nthe cache. Consequently, the same concurrency issues that arise with\nany shared data store are also applicable to a cache. In a situation\nwhere an application needs to modify data held in the cache, you may\nneed to ensure that updates made by one instance of the application\ndo not blindly overwrite the changes made by another instance.\n\nDepending on the nature of the data and the likelihood of collisions,\nyou can adopt one of two approaches to concurrency:\n\n- __Optimistic.__ The application checks to see whether the data in the cache has changed since it was retrieved, immediately prior to updating it. If the data is still the same, the change can be made. Otherwise, the application has to decide whether to update it (the business logic that drives this decision will be application-specific). This approach is suitable for situations where updates are infrequent, or where collisions are unlikely to occur.\n- __Pessimistic.__ The application locks the data in the cache when it retrieves it to prevent another instance from changing the data. This process ensures that collisions cannot occur, but could block other instances that need to process the same data. Pessimistic concurrency can affect the scalability of the solution and should be used only for short-lived operations. This approach may be appropriate for situations where collisions are more likely, especially if an application updates multiple items in the cache and must ensure that these changes are applied consistently.\n\n### Implementing high availability, and scalability, and improving performance\n\nA cache should not be the primary repository of data; this is the role\nof the original data store from which the cache is populated. The\noriginal data store is responsible for ensuring the persistence of the\ndata.\n\nBe careful not to introduce critical dependencies on the availability\nof a shared cache service into your solutions. An application should be\nable to continue functioning if the service providing the shared cache\nis unavailable; the application should not hang or fail while waiting\nfor the cache service to resume. Therefore, the application must be\nprepared to detect the availability of the cache service and fall back\nto the original data store if the cache is inaccessible. The\n[Circuit-Breaker Pattern](http://msdn.microsoft.com/library/dn589784.aspx) is useful for handling this scenario. The\nservice providing the cache can be recovered, and once it becomes\navailable the cache can be repopulated as data is read form the\noriginal data store, following a strategy such as the [Cache-Aside pattern](http://msdn.microsoft.com/library/dn589799.aspx).\n\nHowever, falling back to the original data store if the cache is\ntemporarily unavailable may have a scalability impact on the system;\nwhile the data store is being recovered the original data store\ncould be swamped with requests for data, resulting in timeouts and\nfailed connections. A strategy that you should consider is to\nimplement a local, private cache in each instance of an application\ntogether with the shared cache that all application instances\naccess. When the application retrieves an item, it can check first\nin its local cache, then the shared cache, and finally the original\ndata store. The local cache can be populated using the data in the\nshared cache, or the database if the shared cache is unavailable.\nThis approach requires careful configuration to prevent the local\ncache becoming too stale with respect to the shared cache, but it\nacts as a buffer if the shared cache is unreachable. Figure 3\nshows this structure.\n\n![Using a local, private cache with a shared cache_](media/best-practices-caching/Caching3.png)\n_Figure 3: Using a local, private cache with a shared cache_\n\nTo support large caches that hold relatively long-lived data, some\ncache services provide a high-availability option that implements\nautomatic failover if the cache becomes unavailable. This approach\ntypically involves replicating the cached data stored on a primary\ncache server to a secondary cache server, and switching to the\nsecondary server if the primary server fails or connectivity is\nlost. To reduce the latency associated with writing to multiple\ndestinations, when data is written to the cache on the primary\nserver, the replication to the secondary server may occur\nasynchronously.  This approach leads to the possibility that some\ncached information may be lost in the event of a failure, but the\nproportion of this data should be small compared to the overall\nsize of the cache.\n\nIf a shared cache is large, it may be beneficial to partition the\ncached data across nodes to reduce the chances of contention and\nimprove scalability. Many shared caches support the ability to\ndynamically add (and remove) nodes and rebalance the data across\npartitions. This approach may involve clustering whereby the\ncollection of nodes is presented to client applications as a\nseamless, single cache, but internally the data is dispersed\nbetween nodes following some predefined distribution strategy\nwhich balances the load evenly. The [Data Partitioning Guidance document](http://msdn.microsoft.com/library/dn589795.aspx)\non the Microsoft website provides more information about possible\npartitioning strategies.\n\nClustering can also add further availability of the cache; if a\nnode fails, the remainder of the cache is still accessible.\nClustering is frequently used in conjunction with replication\nand failover; each node can be replicated and the replica\nquickly brought online if the node fails.\n\nMany read and write operations will likely involve single data\nvalues or objects. However, there may be times when it is\nnecessary to store or retrieve large volumes of data quickly.\nFor example, seeding a cache could involve writing hundreds or\nthousands of items to the cache, or an application may need to\nretrieve a large number of related items from the cache as\npart of the same request. Many large-scale caches provide batch\noperations for these purposes, enabling a client application to\npackage up a large volume of items into a single request and\nreducing the overhead associated with performing a large number\nof small requests.\n\n## Caching and eventual consistency\n\nThe Cache-Aside pattern depends on the instance of the application\npopulating the cache having access to the most recent and\nconsistent version of the data. In a system that implements\neventual consistency (such as a replicated data store) this may\nnot be the case. One instance of an application could modify a\ndata item and invalidate the cached version of that item. Another\ninstance of the application may attempt to read this item from\ncache which causes a cache-miss, so it reads the data from the\ndata store and adds it to the cache. However, if the data store\nhas not been fully synchronized with the other replicas the\napplication instance could read and populate the cache with the\nold value.\n\nFor more information about handling data consistency, see the\nData Consistency Guidance page on the Microsoft website.\n\n### Protecting cached data\n\nIrrespective of the cache service you use, you should consider\nhow to protect the data held in the cache from unauthorized\naccess. There are two main concerns:\n\n- The privacy of the data in the cache.\n- The privacy of data as it flows between the cache and the\n  application using the cache.\n\nTo protect data in the cache, the cache service may implement\nan authentication mechanism requiring that applications\nidentify themselves, and an authorization scheme that\nspecifies which identities can access data in the cache, and\nthe operations (read and write) that these identities are\nallowed to perform. To reduce overheads associated with\nreading and writing data, once an identity has been granted\nwrite and/or read access to the cache, that identity can use\nany data in the cache. If you need to restrict access to\nsubsets of the cached data, you can:\n\n- Split the cache into partitions (by using different cache\n  servers) and only grant access to identities for the\n  partitions that they should be allowed to use, or\n- Encrypt the data in each subset by using different keys\n  and only provide the encryption keys to identities that\n  should have access to each subset. A client application\n  may still be able to retrieve all of the data in the cache,\n  but it will only be able to decrypt the data for which it\n  has the keys.\n\nTo protect the data as it flows into and out of the cache you\nare dependent on the security features provided by the network\ninfrastructure that client applications use to connect to the\ncache. If the cache is implemented using an on-site server\nwithin the same organization that hosts the client applications,\nthen the isolation of the network itself may not require you to\ntake any additional steps. If the cache is located remotely and\nrequires a TCP or HTTP connection over a public network (such\nas the Internet), you should consider implementing SSL.\n\n## Considerations for implementing caching with Microsoft Azure\n\nAzure provides the Azure Redis Cache. This is an implementation\nof the open source Redis Cache that runs as a service in an\nAzure datacenter. It provides a caching service that can be\naccessed from any Azure application, whether the application\nis implemented as a cloud service, a website, or inside an\nAzure virtual machine. Caches can be shared by client\napplications that have the appropriate access key.\n\nRedis is a high-performance caching solution that provides\navailability, scalability and security. It typically runs\nas a service spread across one or more dedicated machines and\nattempts to store as much information as it can in memory to\nensure fast access. This architecture is intended to provide\nlow latency and high throughput by reducing the need to\nperform slow I/O operations.\n\nThe Azure Redis cache is compatible with many of the various\nAPIs used by client applications. If you have existing\napplications that already use Redis running on-premises, the\nAzure Redis cache provides a quick migration path to caching\nin the cloud.\n\n> [AZURE.NOTE] Azure also provides the Managed Cache Service. This\n  service is based on the Microsoft AppFabric Cache engine. It\n  enables you to create a distributed cache that can be shared\n  by loosely-coupled applications. The cache is hosted on\n  high-performance servers running in an Azure datacenter.\n  However, this option is no longer recommended and is only\n  provided to support existing applications that have been built\n  to use it. For all new development, use the Azure Redis\n  Cache instead.\n>\n> Additionally, Azure supports in-role caching. This feature\n  enables you to create a cache specific to a cloud service.\n  The cache is hosted by instances of a web or worker role, and\n  can only be accessed by roles operating as part of the same\n  cloud service deployment unit (a deployment unit is the set\n  of role instances deployed as a cloud service to a specific\n  region). The cache is clustered, and all instances of the\n  role within the same deployment unit that host the cache\n  become part of the same cache cluster. Existing applications\n  that use in-role caching can continue to do so, but\n  migrating to the Azure Redis Cache may bring more benefits.\n  For more information about whether to use Azure Redis Cache\n  or an in-role cache, visit the page\n  [Which Azure Cache offering is right for me?](http://msdn.microsoft.com/library/azure/dn766201.aspx) on the Microsoft website.\n\n\n### Features of Redis\n\nRedis is more than a simple cache server; it provides a distributed in-memory\ndatabase with an extensive command set that supports many common scenarios,\nas described in the section Use-cases for Redis caching later in this\ndocument. This section summarizes some of the key features that Redis\nprovides.\n\n### Redis as an in-memory database\n\nRedis supports both reading and writing operations. Unlike many caches (which should be considered as transitory data stores), writes can be protected from system failure either by being stored in periodically in a local snapshot file or in an append-only log file. All writes are asynchronous and do not block clients reading and writing data. When Redis starts running, it reads the data from the snapshot or log file and uses it to construct the in-memory cache. For more information, see [Redis Persistence](http://redis.io/topics/persistence) on the Redis website.\n\n> [AZURE.NOTE] Redis does not guarantee that all writes will be saved in the event\n  of a catastrophic failure, but at worst you should only lose a few-seconds\n  worth of data. Remember that a cache is not intended to act as an\n  authoritative data source, and it is the responsibility of the applications\n  using the cache to ensure that critical data is saved successfully to an\n  appropriate data store. For more information, see the Cache-Aside pattern.\n\n#### Redis data types\n\nRedis is a key-value store, where values can contain simple types or complex data structures such as hashes, lists, and sets. It supports a set of atomic operations on these data types. Keys can be permanent or tagged with a limited time to live at which point the key and its corresponding value are automatically removed from the cache. For more information about redis keys and values, visit the page [An Introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro) on the Redis website.\n\n#### Redis replication and clustering\n\nRedis supports master/subordinate replication to help ensure availability and maintain throughput; write operations to a Redis master node are replicated to one or more subordinate nodes, and read operations can be served by the master or any of the subordinates. In the event of a network partition, subordinates can continue to serve data and then transparently resynchronize with the master when the connection is reestablished. For further details, visit the [Replication](http://redis.io/topics/replication) page on the Redis website.\n\nRedis also provides clustering, enabling you to transparently partition data into shards across servers and spread the load. This feature improves scalability as new Redis servers can be added and the data repartitioned as the size of the cache increases. Furthermore, each server in the cluster can be replicated by using master/subordinate replication to ensure availability across each node in the cluster. For more information about clustering and sharding, visit the [Redis Cluster Tutorial page](http://redis.io/topics/cluster-tutorial) on the Redis website.\n\n> [AZURE.NOTE] Azure Redis Cache does not currently support clustering. If you wish to create a Redis cluster you can build your own custom Redis server. For more information, see the section Building a Custom Redis Cache later in this document.\n\n### Redis memory use\n\nA Redis cache has a finite size depending on the resources available on the host computer. When you configure a Redis server, you can specify the maximum amount of memory it can use. A key in a Redis cache can be configured with an expiration time, after which it is automatically removed from the cache. This feature can help prevent the in-memory cache from being filled with old or stale data.\n\nAs memory fills up, Redis can automatically evict keys and their values by following a number of policies. The default is LRU (least recently used), but you can also select other policies such as evicting keys at random, or turning off eviction altogether (in which case, attempts to add items to the cache will fail if it is full). The page [Using Redis as an LRU Cache](http://redis.io/topics/lru-cache) provides more information.\n\n### Redis transactions and batches\n\nRedis enables a client application to submit a series of operations that read and write data in the cache as an atomic transaction. All of the commands in the transaction are guaranteed to be executed sequentially and no commands issued by other concurrent clients will be interwoven between them. However, these are not true transactions as a relational database would perform them. Transaction processing consists of two stages; command queuing and command execution. During the command queuing stage, the commands that comprise the transaction are submitted by the client. If some sort of error occurs at this point (such as a syntax error, or the wrong number of parameters) then Redis will refuse to process the entire transaction and discard it. During the execution phase, Redis performs each queued command in sequence. If a command fails during this phase Redis will continue with the next queued command and it does not roll back the effects of any commands that have already been executed. This simplified form of transaction helps to maintain performance and avoid performance problems caused by contention. Redis does implement a form of optimistic locking to assist in maintaining consistency. For detailed information about transactions and locking with Redis, visit the [Transactions page](http://redis.io/topics/transactions) on the Redis website.\n\nRedis also supports non-transactional batching of requests. The Redis protocol that clients use to send commands to a Redis server enables a client to send a series of operations as part of the same request. This can help to reduce packet fragmentation on the network. When the batch is processed, each command is performed. Unlike a transaction, if any of these commands are malformed they will be rejected but the remaining commands will be performed. There is also no guarantee on the order in which the commands in the batch will be processed.\n\n### Redis security\n\nRedis is focused purely on providing fast access to data, and is designed to run inside a trusted environment and be accessed only by trusted clients. Redis only supports a limited security model based on password authentication (it is possible to remove authentication completely, although this is not recommended). All authenticated clients share the same global password, and have access to the same resources. If you need more comprehensive login security, you must implement your own security layer in front of the Redis server and all client requests should pass through this additional layer; Redis should not be directly exposed to untrusted or unauthenticated clients.\n\nYou can restrict access to commands by disabling them or renaming them (and only providing privileged clients with the new names).\n\nRedis does not directly support any form of data encryption, so all encoding must be performed by client applications. Additionally, Redis does not provide any form of transport security, so if you need to protect data as it flows across the network you should implement an SSL proxy.\n\nFor more information, visit the [Redis Security](http://redis.io/topics/security) page on the Redis website.\n\n> [AZURE.NOTE] Azure Redis Cache provides its own security layer through which clients connect; the underlying Redis\n  servers are not exposed to the public network.\n\n### Using the Azure Redis cache\n\nThe Azure Redis Cache provides access to Redis servers running on servers hosted at an Azure datacenter; it acts as a façade that provides access control and security. You can provision a cache by using the Azure Management portal. The portal provides a number of predefined configurations, ranging from a 53GB cache running as a dedicated service that supports SSL communications (for privacy) and master/subordinate replication with an SLA of 99.9% availability, down to a 250MB cache without replication (no availability guarantees) running on shared hardware.\n\nUsing the Azure Management portal you can also configure the eviction policy of the cache, and control access to the cache by adding users to the roles provided; Owner, Contributor, Reader. These roles define the operations that members can perform. For example, members of the Owner role have complete control over the cache (including security) and its contents, members of the Contributor role can read and write information in the cache, and members of the Reader role can only retrieve data from the cache.\n\nMost administrative tasks are performed through the Azure Management portal, and for this reason many of the administrative commands available in the standard version of Redis are not available, including the ability to modify the configuration programmatically, shutdown the Redis server, configure additional slaves, or forcibly save data to disk.\n\nThe Azure management portal includes a convenient graphical display that enables you to monitor the performance of the cache. For example, you can view the number of connections being made, the number of requests performed, the volume of reads and writes, and the number of cache hits versus cache misses. Using this information you can determine the effectiveness of the cache and if necessary switch to a different configuration or change the eviction policy. Additionally, you can create alerts that send email messages to an administrator if one or more critical metrics fall outside of an expected range. For example, if the number of cache misses exceeds a specified value in the last hour, an administrator could be alerted as the cache may be too small or data may be being evicted too quickly.\n\nYou can also monitor CPU, memory, and network usage for the cache.\n\n> [AZURE.NOTE] Azure Redis Cache is intended to act purely as a cache rather than a database. As a result, it does not currently implement Redis persistence.\n\nFor further information and examples showing how to create and configure an Azure Redis Cache, visit the page [Lap around Azure Redis Cache](http://azure.microsoft.com/blog/2014/06/04/lap-around-azure-redis-cache-preview/) on the Azure blog.\n\n## Caching session state and HTML output\n\nIf you building ASP.NET web applications that run by using Azure web roles, you can save session state information and HTML output in an Azure Redis Cache. The Session State Provider for Azure Redis Cache enables you to share session information between different instances of an ASP.NET web application, and is very useful in web farm situations where client-server affinity is not available and caching session data in-memory would not be appropriate.\n\nUsing the Session State Provider with Azure Redis Cache delivers several benefits, including:\n\n- It can share session state amongst a large number of instances of an ASP.NET web application, providing improved scalability,\n- It supports controlled, concurrent access to the same session state data for multiple readers and a single writer, and\n- It can use compression to save memory and improve network performance.\n\nFor more information visit the [ASP.NET Session State Provider for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn690522.aspx) page on the Microsoft website.\n\n> [AZURE.NOTE] Do not use the Session State Provider for Azure Redis Cache for ASP.NET applications that run outside of the Azure environment. The latency of accessing the cache from outside of Azure can eliminate the performance benefits of caching data.\n\nSimilarly, the Output Cache Provider for Azure Redis Cache enables you to save the HTTP responses generated by an ASP.NET web application. Using the Output Cache Provider with Azure Redis Cache can improve the response times of applications that render complex HTML output; application instances generating similar responses can make use of the shared output fragments in the cache rather than generating this HTML output afresh.  For more information visit the [ASP.NET Output Cache Provider for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn798898.aspx) page on the Microsoft website.\n\n## Building a custom Redis cache\n\nThe Azure Redis cache acts as a façade to the underlying Redis servers. Currently it supports a fixed set of configurations but does not provide for Redis clustering. If you require an advanced configuration that is not covered by the Azure Redis cache (such as a cache bigger than 53GB) you can build and host your own Redis servers by using Azure virtual machines. This is a potentially complex process as you may need to create several VMs to act as master and subordinate nodes if you want to implement replication. Furthermore, if you wish to create a cluster, then you will need multiple masters and subordinate servers; a minimal clustered, replication topology that provides a high degree of availability and scalability comprises at least 6 VMs organized as 3 pairs of master/subordinate servers (a cluster must contain at least 3 master nodes). Each master/subordinate pair should be located close together to minimize latency, but each set of pairs can be running in different Azure datacenters located in different regions if you wish to locate cached data close to the applications that are most likely to use it. The page [Running Redis on a CentOS Linux VM in Azure](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx) on the Microsoft website walks through an example showing how to build and configure a Redis node running as an Azure VM.\n\nNote that if you implement your own Redis cache in this way, you are responsible for monitoring, managing, and securing the service.\n\n## Partitioning a Redis cache\n\nPartitioning the cache involves splitting the cache across multiple computers. This structure gives you several advantages over using a single cache server, including:\n\n- Creating a cache that is much bigger than can be stored on a single server.\n- Distributing data across servers, improving availability. If one server fails or becomes inaccessible, only the data that it holds is unavailable; the data on the remaining servers can still be accessed. For a cache, this is not crucial as the cached data is only a transient copy of the data held in a database, and cached data on a server that becomes inaccessible can be cached on a different server instead.\n- Spreading the load across servers, thereby improving performance and scalability.\n- Geolocating data close to the users that access it, reducing latency.\n\nFor a cache, the most common form of partitioning is sharding. In this strategy each partition (or shard) is a Redis cache in its own right. Data is directed to a specific partition by using sharding logic, which can use a variety of approaches to distribute the data. The [Sharding Pattern](http://msdn.microsoft.com/library/dn589797.aspx) provides more information on implementing sharding.\n\nTo implement partitioning in a Redis cache, you can adopt one of the following approaches:\n\n- _Server-side query routing._ In this technique, a client application sends a request to any of the\n  Redis servers that comprise the cache (probably the closest server). Each Redis server stores\n  metadata that describes the partition that it holds, and also contains information about which\n  partitions are located on other servers. The Redis server examines the client request and if it\n  can be resolved locally it will perform the requested operation, otherwise it will forward the\n  request on to the appropriate server. This model is implemented by Redis clustering, and is\n  described in more detail on the [Redis cluster tutorial](http://redis.io/topics/cluster-tutorial) page on the Redis website. Redis clustering\n  is transparent to client applications, and additional Redis servers can be added to the cluster\n  (and the data re-partitioned) without requiring that you reconfigure the clients.\n\n  > [AZURE.IMPORTANT] Azure Redis Cache does not currently support Redis clustering. If you wish to\n  implement this approach then you should build a custom Redis cache as described earlier.\n\n- _Client-side partitioning._ In this model, the client application contains logic (possibly in\n  the form of a library) that routes requests to the appropriate Redis server. This approach\n  can be used with Azure Redis Cache; create multiple Azure Redis Caches (one for each data\n  partition) and implement the client-side logic that routes the requests to the correct\n  cache. If the partitioning scheme changes (if additional Azure Redis Caches are created,\n  for example), client applications may need to be reconfigured.\n\n- _Proxy-assisted partitioning._ In this scheme, client applications send requests to an\n  intermediary proxy service which understands how the data is partitioned and then routes\n  the request to the appropriate Redis server. This approach can also be used with Azure\n  Redis Cache; the proxy service could be implemented as an Azure cloud service. This\n  approach requires an additional level of complexity to implement the service, and\n  requests may take longer to perform than using client-side partitioning.\n\nThe page [Partitioning: how to split data among multiple Redis instances](http://redis.io/topics/partitioning)\non the Redis website provides further information about implementing partitioning with Redis.\n\n### Implementing Redis cache client applications\n\nRedis supports client applications written in numerous programming languages. If you are building new applications by using the .NET Framework, the recommended approach is to use the StackExchange.Redis client library. This library provides a .NET Framework object model that abstracts the details for connecting to a Redis server, sending commands, and receiving responses. It is available in Visual Studio as a NuGet package. You can use this same library to connect to an Azure Redis cache, or a custom Redis cache hosted on a VM.\n\nTo connect to a Redis server you use the static `Connect` method of the `ConnectionMultiplexer` class. The connection that this method creates is designed to be used throughout the lifetime of the client application, and the same connection can be used by multiple concurrent threads; do not reconnect and disconnect each time you perform a Redis operation as this can degrade performance. You can specify the connection parameters, such as the address of the Redis host and the password. If you are using the Azure Redis cache, the password this is either the primary or secondary key generated for the Azure Redis Cache by using the Azure Management portal.\n\nAfter you have connected to the Redis server, you can obtain a handle on the Redis database that acts as the cache. The Redis connection provides the `GetDatabase` method to do this. You can then retrieve items from the cache and store data in the cache by using the `StringGet` and `StringSet` methods. These methods expect a key as a parameter, and either return the item in the cache that has a matching value (`StringGet`) or add the item to the cache with this key (`StringSet`).\n\nDepending on the location of the Redis server, many operations may incur some latency while a request is transmitted to the server and a response returned to the client. The StackExchange library provides asynchronous versions of many of the methods that it exposes to help client applications remain responsive. These methods support the [Task-based Asynchronous Pattern](http://msdn.microsoft.com/library/hh873175.aspx) in the .NET Framework.\n\nThe following code snippet shows a method named `RetrieveItem` that illustrates an example of an implementation of the cache-aside pattern based on Redis and the StackExchange library. The method takes a string key value, and attempts to retrieve the corresponding item from the Redis cache by calling the `StringGetAsync` method (the asynchronous version of `StringGet`). If the item is not found, it is fetched from the underlying data source using the `GetItemFromDataSourceAsync` method (which is a local method and not part of the StackExchange library), and then added to the cache by using the `StringSetAsync` method so it can be retrieved more quickly next time.\n\n```csharp\n// Connect to the Azure Redis cache\nConfigurationOptions config = new ConfigurationOptions();\nconfig.EndPoints.Add(\"<your DNS name>.redis.cache.windows.net\");\nconfig.Password = \"<Redis cache key from management portal>\";\nConnectionMultiplexer redisHostConnection = ConnectionMultiplexer.Connect(config);\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nprivate async Task<string> RetrieveItem(string itemKey)\n{\n    // Attempt to retrieve the item from the Redis cache\n    string itemValue = await cache.StringGetAsync(itemKey);\n\n    // If the value returned is null, the item was not found in the cache\n    // So retrieve the item from the data source and add it to the cache\n    if (itemValue == null)\n    {\n        itemValue = await GetItemFromDataSourceAsync(itemKey);\n        await cache.StringSetAsync(itemKey, itemValue);\n    }\n\n    // Return the item\n    return itemValue;\n}\n```\n\nThe `StringGet` and `StringSet` methods are not restricted to retrieving or storing string values; they can take any item serialized as an array of bytes. If you need to save a .NET object you can serialize it as a byte stream and use the StringSet method to write it to the cache. Similarly, you can read an object from cache by using the StringGet method and deserialize it as a .NET object. The following code shows a set of extension methods for the IDatabase interface (the GetDatabase method of a Redis connection returns an `IDatabase` object),  and some sample code that uses these methods to read and write a BlogPost object to the cache:\n\n```csharp\npublic static class RedisCacheExtensions\n{\n    public static async Task<T> GetAsync<T>(this IDatabase cache, string key)\n    {\n        return Deserialize<T>(await cache.StringGetAsync(key));\n    }\n\n    public static async Task<object> GetAsync(this IDatabase cache, string key)\n    {\n        return Deserialize<object>(await cache.StringGetAsync(key));\n    }\n\n    public static async Task SetAsync(this IDatabase cache, string key, object value)\n    {\n        await cache.StringSetAsync(key, Serialize(value));\n    }\n\n    static byte[] Serialize(object o)\n    {\n        byte[] objectDataAsStream = null;\n\n        if (o != null)\n        {\n            BinaryFormatter binaryFormatter = new BinaryFormatter();\n            using (MemoryStream memoryStream = new MemoryStream())\n            {\n                binaryFormatter.Serialize(memoryStream, o);\n                objectDataAsStream = memoryStream.ToArray();\n            }\n        }\n\n        return objectDataAsStream;\n    }\n\n    static T Deserialize<T>(byte[] stream)\n    {\n        T result = default(T);\n\n        if (stream != null)\n        {\n            BinaryFormatter binaryFormatter = new BinaryFormatter();\n            using (MemoryStream memoryStream = new MemoryStream(stream))\n            {\n                result = (T)binaryFormatter.Deserialize(memoryStream);\n            }\n        }\n\n        return result;\n    }\n}\n```\n\nThe following code illustrates a method named `RetrieveBlogPost` that uses these extension methods to read and write a serializable `BlogPost` object to the cache following the cache-aside pattern:\n\n```csharp\n// The BlogPost type\n[Serializable]\nprivate class BlogPost\n{\n    private HashSet<string> tags = new HashSet<string>();\n\n    public BlogPost(int id, string title, int score, IEnumerable<string> tags)\n    {\n        this.Id = id;\n        this.Title = title;\n        this.Score = score;\n        this.tags = new HashSet<string>(tags);\n    }\n\n    public int Id { get; set; }\n    public string Title { get; set; }\n    public int Score { get; set; }\n    public ICollection<string> Tags { get { return this.tags; } }\n}\n...\nprivate async Task<BlogPost> RetrieveBlogPost(string blogPostKey)\n{\n    BlogPost blogPost = await cache.GetAsync<BlogPost>(blogPostKey);\n    if (blogPost == null)\n    {\n        blogPost = await GetBlogPostFromDataSourceAsync(blogPostKey);\n        await cache.SetAsync(blogPostKey, blogPost);\n    }\n\n    return blogPost;\n}\n```\n\nRedis supports command pipelining if a client application sends multiple asynchronous requests. Redis can multiplex the requests using the same connection rather than receiving and responding to commands in a strict sequence. This approach helps to reduce latency by making more efficient use of the network. The following code snippet shows an example that retrieves the details of two customers concurrently. The code submits two requests and then performs some other processing (not shown) before waiting to receive the results. The Wait method of the cache object is similar to the .NET Framework Task.Wait method:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nvar task1 = cache.StringGetAsync(\"customer:1\");\nvar task2 = cache.StringGetAsync(\"customer:2\");\n...\nvar customer1 = cache.Wait(task1);\nvar customer2 = cache.Wait(task2);\n```\n\nThe page [Develop for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn690520.aspx) on the Microsoft website provides more information on how to write client applications that can use the Azure Redis cache. Additional information is available on the [Basic Usage page](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md) on the StackExchange.Redis website, and the page [Pipelines and Multiplexers](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/PipelinesMultiplexers.md) on the same website provides more information about asynchronous operations and pipelining with Redis and the StackExchange library.  The section Use-Cases for Redis Caching later in this guidance provides examples of some of the more advanced techniques that you can apply to data held in a Redis cache.\n\n## Use-cases for Redis caching\n\nThe simplest use of Redis for caching concerns storing key/value pairs where the value is an uninterpreted string of arbitrary length that can contain any binary data (it is essentially  an array of bytes that can be treated as a string). This scenario was illustrated in the section Implementing Redis Cache Client Applications earlier in this guidance. You should note that keys also contain uninterpreted data, so you can use any binary information as the key, although the longer the key is the more space it will take to store, and the longer it will take to perform lookup operations. For usability and ease of maintenance design your keyspace carefully and use meaningful (but not verbose) keys. For example, use structured keys such as \"customer:100\" to represent the key for the customer with ID 100 rather than simply \"100\". This scheme enables you to easily distinguish between values that store different data types. For example, you could also use the key \"orders:100\" to represent the key for the order with ID 100.\n\nApart from one-dimensional binary strings, a value in a Redis key/value pair can also hold more structured information, including lists, sets (sorted and unsorted), and hashes. Redis provides a comprehensive command set that can manipulate these types, and many of these commands are available to .NET Framework applications through a client library such as StackExchange. The page [An introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro) on the Redis website provides a more detailed overview of these types and the commands that you can use to manipulate them.\n\nThis section summarizes some common use-cases for these data types and commands.\n\n### Performing atomic and batch operations\n\nRedis supports a series of atomic get-and-set operations on string values. These operations remove the possible race hazards that might occur when using separate `GET` and `SET` commands. The operations available include:\n\n- `INCR`, `INCRBY`, `DECR`, and `DECRBY` which perform atomic increment and decrement operations on\n  integer numeric data values. The StackExchange library provides overloaded versions of the\n  `IDatabase.StringIncrementAsync` and `IDatabase.StringDecrementAsync` methods to perform\n  these operations and return the resulting value stored in the cache. The following code\n  snippet illustrates how to use these methods:\n\n  ```csharp\n  ConnectionMultiplexer redisHostConnection = ...;\n  IDatabase cache = redisHostConnection.GetDatabase();\n  ...\n  await cache.StringSetAsync(\"data:counter\", 99);\n  ...\n  long oldValue = await cache.StringIncrementAsync(\"data:counter\");\n  // Increment by 1 (the default)\n  // oldValue should be 100\n\n  long newValue = await cache.StringDecrementAsync(\"data:counter\", 50);\n  // Decrement by 50\n  // newValue should be 50\n  ```\n\n- `GETSET` which retrieves the value associated with a key and changes it to a new value. The\n  StackExchange library makes this operation available through the `IDatabase.StringGetSetAsync`\n  method. The code snippet below shows an example of this method. This code returns the current\n  value associated with the key \"data:counter\" from the previous example and resets the value\n  for this key back to zero, all as part of the same operation:\n\n  ```csharp\n  ConnectionMultiplexer redisHostConnection = ...;\n  IDatabase cache = redisHostConnection.GetDatabase();\n  ...\n  string oldValue = await cache.StringGetSetAsync(\"data:counter\", 0);\n  ```\n\n- `MGET` and `MSET`, which can return or change a set of string values as a single operation. The\n  `IDatabase.StringGetAsync` and `IDatabase.StringSetAsync` methods are overloaded to support\n  this functionality, as shown in the following example:\n\n  ```csharp\n  ConnectionMultiplexer redisHostConnection = ...;\n  IDatabase cache = redisHostConnection.GetDatabase();\n  ...\n  // Create a list of key/value pairs\n  var keysAndValues =\n      new List<KeyValuePair<RedisKey, RedisValue>>()\n      {\n          new KeyValuePair<RedisKey, RedisValue>(\"data:key1\", \"value1\"),\n          new KeyValuePair<RedisKey, RedisValue>(\"data:key99\", \"value2\"),\n          new KeyValuePair<RedisKey, RedisValue>(\"data:key322\", \"value3\")\n      };\n\n  // Store the list of key/value pairs in the cache\n  cache.StringSet(keysAndValues.ToArray());\n  ...\n  // Find all values that match a list of keys\n  RedisKey[] keys = { \"data:key1\", \"data:key99\", \"data:key322\"};\n  RedisValue[] values = null;\n  values = cache.StringGet(keys);\n  // values should contain { \"value1\", \"value2\", \"value3\" }\n  ```\n\nYou can also combine multiple operations into a single Redis transaction as described in the Redis Transactions and Batches section in this guidance. The StackExchange library provides support for transactions through the `ITransaction` interface. You can create an ITransaction object by using the IDatabase.CreateTransaction method, and invoke commands to the transaction by using the methods provided `ITransaction` object. The `ITransaction` interface provides access to a similar set of methods as the `IDatabase` interface except that all the methods are asynchronous; they are only performed when the `ITransaction.Execute` method is invoked. The value returned by the execute method indicates whether the transaction was created successfully (true) or it failed (false).\n\nThe following code snippet shows an example that increments and decrements two counters as part of the same transaction:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nITransaction transaction = cache.CreateTransaction();\nvar tx1 = transaction.StringIncrementAsync(\"data:counter1\");\nvar tx2 = transaction.StringDecrementAsync(\"data:counter2\");\nbool result = transaction.Execute();\nConsole.WriteLine(\"Transaction {0}\", result ? \"succeeded\" : \"failed\");\nConsole.WriteLine(\"Result of increment: {0}\", tx1.Result);\nConsole.WriteLine(\"Result of decrement: {0}\", tx2.Result);\n```\n\nRemember that Redis transactions are unlike transactions in relational databases. The Execute method simply queues all the commands that comprise the transaction for execution, and if any of them is malformed then the transaction is aborted. If all the commands have been queued successfully, each command will be run asynchronously. If any command fails, the others will still continue processing. If you need to verify that a command has completed successfully you must fetch the results of the command by using the Result property of the corresponding task, as shown in the example above. Reading the Result property will block until the task has completed.\n\nFor more information, see the [Transactions in Redis](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Transactions.md) page on the StackExchange.Redis website.\n\nFor performing batch operations, you can use the IBatch interface of the StackExchange library. This interface provides access to a similar set of methods as the IDatabase interface except that all the methods are asynchronous. You create an IBatch object by using the IDatabase.CreateBatch method and then run the batch by using the IBatch.Execute method, as shown in the following example. This code simply sets a string value, and increments and decrements the same counters used in the previous example and displays the results:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nIBatch batch = cache.CreateBatch();\nbatch.StringSetAsync(\"data:key1\", 11);\nvar t1 = batch.StringIncrementAsync(\"data:counter1\");\nvar t2 = batch.StringDecrementAsync(\"data:counter2\");\nbatch.Execute();\nConsole.WriteLine(\"{0}\", t1.Result);\nConsole.WriteLine(\"{0}\", t2.Result);\n```\n\nIt is important to understand that unlike a transaction, if a command in a batch fails because it is malformed the other commands may still run; the IBatch.Execute method does not return any indication of success or failure.\n\n### Performing fire-and-forget cache operations\n\nRedis supports fire-and-forget operations by using command flags. In this situation, the client simply initiates an operation but has no interest in the result and does not wait for the command to be completed. The example below shows how to perform the INCR command as a fire-and-forget operation:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nawait cache.StringSetAsync(\"data:key1\", 99);\n...\ncache.StringIncrement(\"data:key1\", flags: CommandFlags.FireAndForget);\n```\n\n### Automatically expiring keys\n\nWhen you store an item in a Redis cache, you can specify a timeout after which the item will be automatically removed from the cache. You can also query how much more time a key has before it expires by using the `TTL` command; this command is available to StackExchange applications by using the IDatabase.KeyTimeToLive method.\n\nThe following code snippet shows an example of setting an expiration time of 20 seconds on a key, and querying the remaining lifetime of the key:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\n// Add a key with an expiration time of 20 seconds\nawait cache.StringSetAsync(\"data:key1\", 99, TimeSpan.FromSeconds(20));\n...\n// Query how much time a key has left to live\n// If the key has already expired, the KeyTimeToLive function returns a null\nTimeSpan? expiry = cache.KeyTimeToLive(\"data:key1\");\n```\n\nYou can also set the expiration time to a specific date and time by using the EXPIRE command, available in the StackExchange library as the KeyExpireAsync method:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\n// Add a key with an expiration date of midnight on 1st January 2015\nawait cache.StringSetAsync(\"data:key1\", 99);\nawait cache.KeyExpireAsync(\"data:key1\",\n    new DateTime(2015, 1, 1, 0, 0, 0, DateTimeKind.Utc));\n...\n```\n\n> _Tip:_ You can manually remove an item from the cache by using the DEL command, which is available through the StackExchange library as the IDatabase.KeyDeleteAsync method.\n\n### Using tags to cross-correlate cached items\n\nA Redis set is a collection of multiple items that share a single key. You can create a set by using the SADD command. You can retrieve the items in a set by using the SMEMBERS command. The StackExchange library implements the SADD command through the IDatabase.SetAddAsync method, and the SMEMBERS command with the IDatabase.SetMembersAsync method. You can also combine existing sets to create new sets by using the SDIFF (set difference), SINTER (set intersection), and SUNION (set union) commands. The StackExchange library unifies these operations in the IDatabase.SetCombineAsync method; the first parameter to this method specifies the set operation to perform.\n\nThe following code snippets show how sets can be useful for quickly storing and retrieving collections of related items. This code uses the BlogPost type described in the section Implementing Redis Cache Client Applications. A BlogPost object contains four fields—an ID, a title, a ranking score, and a collection of tags. The first code snippet below shows the sample data used for populating a C# list of BlogPost objects:\n\n```csharp\nList<string[]> tags = new List<string[]>()\n{\n    new string[] { \"iot\",\"csharp\" },\n    new string[] { \"iot\",\"azure\",\"csharp\" },\n    new string[] { \"csharp\",\"git\",\"big data\" },\n    new string[] { \"iot\",\"git\",\"database\" },\n    new string[] { \"database\",\"git\" },\n    new string[] { \"csharp\",\"database\" },\n    new string[] { \"iot\" },\n    new string[] { \"iot\",\"database\",\"git\" },\n    new string[] { \"azure\",\"database\",\"big data\",\"git\",\"csharp\" },\n    new string[] { \"azure\" }\n};\n\nList<BlogPost> posts = new List<BlogPost>();\nint blogKey = 0;\nint blogPostId = 0;\nint numberOfPosts = 20;\nRandom random = new Random();\nfor (int i = 0; i < numberOfPosts; i++)\n{\n    blogPostId = blogKey++;\n    posts.Add(new BlogPost(\n        blogPostId,               // Blog post ID\n        string.Format(CultureInfo.InvariantCulture, \"Blog Post #{0}\",\n            blogPostId),          // Blog post title\n        random.Next(100, 10000),  // Ranking score\n        tags[i % tags.Count]));   // Tags – assigned from a collection\n                                  // in the tags list\n}\n```\n\nYou can store the tags for each BlogPost object as a set in a Redis cache and associate each set with the ID of the BlogPost. This enables an application to quickly find all the tags belonging to a specific blog post. To enable searching in the opposite direction and find all blog posts that share a specific tag, you can create another set that holds the blog posts referencing the tag ID in the key:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\n// Tags are easily represented as Redis Sets\nforeach (BlogPost post in posts)\n{\n    string redisKey = string.Format(CultureInfo.InvariantCulture,\n        \"blog:posts:{0}:tags\", post.Id);\n    // Add tags to the blog post in redis\n    await cache.SetAddAsync(\n        redisKey, post.Tags.Select(s => (RedisValue)s).ToArray());\n\n    // Now do the inverse so we can figure how which blog posts have a given tag.\n    foreach (var tag in post.Tags)\n    {\n        await cache.SetAddAsync(string.Format(CultureInfo.InvariantCulture,\n            \"tag:{0}:blog:posts\", tag), post.Id);\n    }\n}\n```\n\nThese structures enable you to perform many common queries very efficiently. For example, you can find and display all of the tags for blog post 1 like this:\n\n```csharp\n// Show the tags for blog post #1\nforeach (var value in await cache.SetMembersAsync(\"blog:posts:1:tags\"))\n{\n    Console.WriteLine(value);\n}\n```\n\nYou can find all tags that are common to blog post 1 and blog post 2 by performing a set intersection operation, as follows:\n\n```csharp\n// Show the tags in common for blog posts #1 and #2\nforeach (var value in await cache.SetCombineAsync(SetOperation.Intersect, new RedisKey[]\n    { \"blog:posts:1:tags\", \"blog:posts:2:tags\" }))\n{\n    Console.WriteLine(value);\n}\n```\n\nAnd you can find all blog posts that contain a specific tag:\n\n```csharp\n// Show the ids of the blog posts that have the tag \"iot\".\nforeach (var value in await cache.SetMembersAsync(\"tag:iot:blog:posts\"))\n{\n    Console.WriteLine(value);\n}\n```\n\n### Finding recently accessed items\n\nA common problem required by many applications is to find the most recently accessed items. For example, a blogging site might want to display information about the most recently read blog posts. You can implement this functionality by using a Redis list. A Redis list contains multiple items that share the same key, but the list acts as a double-ended queue. You can push items on to either end of the list by using the LPUSH (left push) and RPUSH (right push) commands. You can retrieve items from either end of the list by using the LPOP and RPOP commands.  You can also return a set of elements by using the LRANGE and RRANGE commands. The code snippets below show how you can perform these operations by using the StackExchange library. This code uses the BlogPost type from the previous examples. As a blog post is read by a user, the title of the blog post is pushed onto a list associated with the key \"blog:recent_posts\" in the Redis cache by using the IDatabase.ListLeftPushAsync method:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nstring redisKey = \"blog:recent_posts\";\nBlogPost blogPost = ...; // reference to the blog post that has just been read\nawait cache.ListLeftPushAsync(\n    redisKey, blogPost.Title); // push the blog post onto the list\n```\n\nAs more blog posts are read, their titles are pushed onto the same list. The list is ordered by the sequence in which they have been added; the most recently read blog posts are towards the left end of the list (if the same blog post is read more than once, it will have multiple entries in the list). You can display the titles of the most recently read posts by using the IDatabase.ListRange method. This method takes the key that contains the list, a starting point, and an ending point. The following code retrieves the titles of the 10 blog posts (items from 0 to 9) at the left-most end of the list:\n\n```csharp\n// Show latest ten posts\nforeach (string postTitle in await cache.ListRangeAsync(redisKey, 0, 9))\n{\n    Console.WriteLine(postTitle);\n}\n```\n\nNote that ListRangeAsync does not remove items from the list; to do this you can use the IDatabase.ListLeftPopAsync and IDatabase.ListRightPopAsync methods.\n\nTo prevent the list from growing indefinitely, you can periodically cull items by trimming the list. The code snippet below, removes all but the 5 left-most items from the list:\n\n```csharp\nawait cache.ListTrimAsync(redisKey, 0, 5);\n```\n\n### Implementing a leader board\n\nBy default the items in a set are not held in any specific order. You can create an ordered set by using the ZADD command (the IDatabase.SortedSetAdd method in the StackExchange library). The items are ordered by using a numeric value called a score provided as a parameter to the command. The following code snippet adds the title of a blog post to an ordered list. In the example, each blog post also has a score field that contains the ranking of the blog post.\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nIDatabase cache = redisHostConnection.GetDatabase();\n...\nstring redisKey = \"blog:post_rankings\";\nBlogPost blogPost = ...; // reference to a blog post that has just been rated\nawait cache.SortedSetAddAsync(redisKey, blogPost.Title, blogpost.Score);\n```\n\nYou can retrieve the blog post titles and scores in ascending score order by using the IDatabase.SortedSetRangeByRankWithScores method:\n\n```csharp\nforeach (var post in await cache.SortedSetRangeByRankWithScoresAsync(redisKey))\n{\n    Console.WriteLine(post);\n}\n```\n\n> [AZURE.NOTE] The StackExchange library also provides the IDatabase.SortedSetRangeByRankAsync method which returns the data in score order, but does not return the scores.\n\nYou can also retrieve items in descending order of scores, and limit the number of items returned by providing additional parameters to the IDatabase.SortedSetRangeByRankWithScoresAsync method. The next example displays the titles and scores of the top 10 ranked blog posts:\n\n```csharp\nforeach (var post in await cache.SortedSetRangeByRankWithScoresAsync(\n                               redisKey, 0, 9, Order.Descending))\n{\n    Console.WriteLine(post);\n}\n```\n\nThe next example uses the IDatabase.SortedSetRangeByScoreWithScoresAsync method which you can use to limit the items returned to those that fall within a given score range:\n\n```csharp\n// Blog posts with scores between 5000 and 100000\nforeach (var post in await cache.SortedSetRangeByScoreWithScoresAsync(\n                               redisKey, 5000, 100000))\n{\n    Console.WriteLine(post);\n}\n```\n\n### Messaging by using channels\n\nApart from acting as a data cache, a Redis server provides messaging through a high-performance publisher/subscriber mechanism. Client applications can subscribe to a channel, and other applications or services can publish messages to the channel. Subscribing applications will then receive these messages and can process them.\n\nTo subscribe to channel, Redis provides the SUBSCRIBE command. This command expects the name of one or more channels on which the application will accept messages. The StackExchange library includes the ISubscription interface which enables a .NET Framework application to subscribe and publish to channels. You create an ISubscription object by using the GetSubscriber method of the connection to the Redis server, and then listen for messages on a channel by using the SubscribeAsync method of this object. The following code example shows how to subscribe to a channel named \"messages:blogPosts\":\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nISubscriber subscriber = redisHostConnection.GetSubscriber();\n...\nawait subscriber.SubscribeAsync(\"messages:blogPosts\", (channel, message) =>\n{\n    Console.WriteLine(\"Title is: {0}\", message);\n});\n```\n\nThe first parameter to the Subscribe method is the name of the channel. This name follows the same conventions as that used by keys in the cache, and can contain any binary data, although it is advisable to use relatively short, meaningful strings to help ensure good performance and maintainability. You should also note that the namespace used by channels is separate from that used by keys, so you can have channels and keys that have the same name, although this may make your application code more difficult to maintain.\n\nThe second parameter is an Action delegate. This delegate runs asynchronously whenever a new message appears on the channel. This example simply displays the message on the console (the message will contain the title of a blog post).\n\nTo publish to a channel, an application can use the Redis PUBLISH command. The StackExchange library provides the IServer.PublishAsync method to perform this operation. The next code snippet shows how to publish a message to the \"messages:blogPosts\" channel:\n\n```csharp\nConnectionMultiplexer redisHostConnection = ...;\nISubscriber subscriber = redisHostConnection.GetSubscriber();\n...\nBlogPost blogpost = ...;\nsubscriber.PublishAsync(\"messages:blogPosts\", blogPost.Title);\n```\n\nThere are several points you should understand about the publish/subscribe mechanism:\n\n- Multiple subscribers can subscribe to the same channel, and they will all receive the messages published to that channel.\n- Subscribers only receive messages that have been published after they have subscribed. Channels are not buffered, and once a message is published the Redis infrastructure pushes the message to each subscriber and then removes it.\n- By default, messages are received by subscribers in the order in which they are sent. In a highly active system with a large number\n  of messages and many subscribers and publishers, guaranteed sequential delivery of messages can slow performance of the system. If\n  each message is independent and the order is immaterial you can enable concurrent processing by the Redis system which can help to\n  improve responsiveness. You can achieve this in a StackExchange client by setting the PreserveAsyncOrder of the connection used by\n  the subscriber to false:\n  ```csharp\n  ConnectionMultiplexer redisHostConnection = ...;\n  redisHostConnection.PreserveAsyncOrder = false;\n  ISubscriber subscriber = redisHostConnection.GetSubscriber();\n  ```\n\n## Related Patterns and Guidance\n\nThe following pattern may also be relevant to your scenario when implementing caching in your applications:\n\n- [Cache-Aside Pattern](http://msdn.microsoft.com/library/dn589799.aspx): This pattern describes how to load data on-demand into a cache from a data store. This pattern also helps to maintain consistency between data held in the cache and the data in the original data store.\n- The [Sharding Pattern](http://msdn.microsoft.com/library/dn589797.aspx) provides information on implementing horizontal partitioning to help improve scalability when storing and accessing large volumes of data.\n\n## More Information\n\n- The [MemoryCache Class](http://msdn.microsoft.com/library/system.runtime.caching.memorycache.aspx) page on the Microsoft website.\n- The [Microsoft Azure Cache](http://msdn.microsoft.com/library/windowsazure/gg278356.aspx) page on the Microsoft website.\n- The [Which Azure Cache offering is right for me?](http://msdn.microsoft.com/library/azure/dn766201.aspx) page on the Microsoft website.\n- The [Configuration Model](http://msdn.microsoft.com/library/windowsazure/hh914149.aspx) page on the Microsoft website.\n- The [Task-based Asynchronous Pattern](http://msdn.microsoft.com/library/hh873175.aspx) page on the Microsoft website.\n- The [Pipelines and Multiplexers](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/PipelinesMultiplexers.md) page on the StackExchange.Redis GitHub repo.\n- The [Redis Persistence](http://redis.io/topics/persistence) page on the Redis website.\n- The [Replication page](http://redis.io/topics/replication) on the Redis website.\n- The [Redis Cluster Tutorial](http://redis.io/topics/cluster-tutorial) page on the Redis website.\n- The [Partitioning: how to split data among multiple Redis instances](http://redis.io/topics/partitioning) page on the Redis website.\n- The page [Using Redis as an LRU Cache](http://redis.io/topics/lru-cache) on the Redis website.\n- The [Transactions page](http://redis.io/topics/transactions) on the Redis website.\n- The [Redis Security](http://redis.io/topics/security) page on the Redis website.\n- The page [Lap around Azure Redis Cache](http://azure.microsoft.com/blog/2014/06/04/lap-around-azure-redis-cache-preview/) on the Azure blog.\n- The page [Running Redis on a CentOS Linux VM](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx) in Azure on the Microsoft website.\n- The [ASP.NET Session State Provider for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn690522.aspx) page on the Microsoft website.\n- The [ASP.NET Output Cache Provider for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn798898.aspx) page on the Microsoft website.\n- The page [Develop for Azure Redis Cache](http://msdn.microsoft.com/library/azure/dn690520.aspx) on the Azure site.\n- The page [An Introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro) on the Redis website.\n- The [Basic Usage](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md) page on the StackExchange.Redis website.\n- The [Transactions in Redis](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Transactions.md) page on the StackExchange.Redis repo.\n- The [Data Partitioning Guide](http://msdn.microsoft.com/library/dn589795.aspx) on the Microsoft website.\n\ntest111111111111122233333\n"
}