{
  "nodes": [
    {
      "content": "Troubleshooting Service Fabric application upgrades",
      "pos": [
        26,
        77
      ]
    },
    {
      "content": "This article covers some common issues around upgrading a Service Fabric application and how to resolve them.",
      "pos": [
        95,
        204
      ]
    },
    {
      "content": "Troubleshoot application upgrades",
      "pos": [
        506,
        539
      ]
    },
    {
      "content": "This article covers some of the common issues around upgrading a Service Fabric application and how to resolve them.",
      "pos": [
        541,
        657
      ]
    },
    {
      "content": "Troubleshoot a failed application upgrade",
      "pos": [
        662,
        703
      ]
    },
    {
      "content": "When an upgrade fails, the output of the <bpt id=\"p1\">**</bpt>Get-ServiceFabricApplicationUpgrade<ept id=\"p1\">**</ept> command will contain some additional information for debugging the failure.",
      "pos": [
        705,
        861
      ]
    },
    {
      "content": "This information can be used to:",
      "pos": [
        862,
        894
      ]
    },
    {
      "content": "Identify the failure type",
      "pos": [
        899,
        924
      ]
    },
    {
      "content": "Identify the failure reason",
      "pos": [
        928,
        955
      ]
    },
    {
      "content": "Isolate the failing component(s) for further investigation",
      "pos": [
        959,
        1017
      ]
    },
    {
      "pos": [
        1019,
        1182
      ],
      "content": "This information will be available as soon as Service Fabric detects the failure regardless of whether the <bpt id=\"p1\">**</bpt>FailureAction<ept id=\"p1\">**</ept> is to rollback or suspend the upgrade."
    },
    {
      "content": "Identify the failure type",
      "pos": [
        1188,
        1213
      ]
    },
    {
      "content": "In the output of <bpt id=\"p1\">**</bpt>Get-ServiceFabricApplicationUpgrade<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>FailureTimestampUtc<ept id=\"p2\">**</ept> identifies the timestamp (in UTC) at which an upgrade failure was detected by Service Fabric and the <bpt id=\"p3\">**</bpt>FailureAction<ept id=\"p3\">**</ept> was triggered.",
      "pos": [
        1215,
        1430
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>FailureReason<ept id=\"p1\">**</ept> identifies one of three potential high-level causes of the failure:",
      "pos": [
        1431,
        1520
      ]
    },
    {
      "pos": [
        1525,
        1654
      ],
      "content": "UpgradeDomainTimeout - Indicates that a particular upgrade domain took too long to complete and <bpt id=\"p1\">**</bpt>UpgradeDomainTimeout<ept id=\"p1\">**</ept> expired."
    },
    {
      "pos": [
        1658,
        1774
      ],
      "content": "OverallUpgradeTimeout - Indicates that the overall upgrade took too long to complete and <bpt id=\"p1\">**</bpt>UpgradeTimeout<ept id=\"p1\">**</ept> expired."
    },
    {
      "pos": [
        1778,
        1960
      ],
      "content": "HealthCheck - Indicates that after upgrading an Upgrade Domain, the application remained unhealthy according to the specified health policies and <bpt id=\"p1\">**</bpt>HealthCheckRetryTimeout<ept id=\"p1\">**</ept> expired."
    },
    {
      "content": "These entries will only show up in the output when the upgrade fails and starts rolling back.",
      "pos": [
        1962,
        2055
      ]
    },
    {
      "content": "Further information will be displayed depending on the type of the failure.",
      "pos": [
        2056,
        2131
      ]
    },
    {
      "content": "Investigate upgrade timeouts",
      "pos": [
        2137,
        2165
      ]
    },
    {
      "content": "Upgrade timeout failures are most commonly caused by service availability issues.",
      "pos": [
        2167,
        2248
      ]
    },
    {
      "content": "The output below is typical of upgrades where service replicas or instances fail to start in the new code version.",
      "pos": [
        2249,
        2363
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>UpgradeDomainProgressAtFailure<ept id=\"p1\">**</ept> field captures a snapshot of any pending upgrade work at the time of failure.",
      "pos": [
        2364,
        2480
      ]
    },
    {
      "content": "In this example, we can see that the upgrade failed at upgrade domain <bpt id=\"p1\">*</bpt>MYUD1<ept id=\"p1\">*</ept> and two partitions (<bpt id=\"p2\">*</bpt>744c8d9f-1d26-417e-a60e-cd48f5c098f0<ept id=\"p2\">*</ept> and <bpt id=\"p3\">*</bpt>4b43f4d8-b26b-424e-9307-7a7a62e79750<ept id=\"p3\">*</ept>) were stuck, unable to place primary replicas (<bpt id=\"p4\">*</bpt>WaitForPrimaryPlacement<ept id=\"p4\">*</ept>) on target nodes <bpt id=\"p5\">*</bpt>Node1<ept id=\"p5\">*</ept> and <bpt id=\"p6\">*</bpt>Node4<ept id=\"p6\">*</ept>.",
      "pos": [
        4052,
        4342
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Get-ServiceFabricNode<ept id=\"p1\">**</ept> command can be used to verify that these two nodes are in upgrade domain <bpt id=\"p2\">*</bpt>MYUD1<ept id=\"p2\">*</ept>.",
      "pos": [
        4343,
        4454
      ]
    },
    {
      "content": "The <bpt id=\"p1\">*</bpt>UpgradePhase<ept id=\"p1\">*</ept> says <bpt id=\"p2\">*</bpt>PostUpgradeSafetyCheck<ept id=\"p2\">*</ept>, which means that these safety checks are occurring after all nodes in the upgrade domain had finished upgrading.",
      "pos": [
        4455,
        4617
      ]
    },
    {
      "content": "All this information combined points to a potential issue with the new version of the application code.",
      "pos": [
        4618,
        4721
      ]
    },
    {
      "content": "The most common issues are service errors in the open or promotion to primary code paths.",
      "pos": [
        4722,
        4811
      ]
    },
    {
      "content": "An <bpt id=\"p1\">*</bpt>UpgradePhase<ept id=\"p1\">*</ept> of <bpt id=\"p2\">*</bpt>PreUpgradeSafetyCheck<ept id=\"p2\">*</ept> means there were issues preparing the upgrade domain before actually performing the upgrade.",
      "pos": [
        4813,
        4950
      ]
    },
    {
      "content": "The most common issues in this case are service errors in the close or demotion from primary code paths.",
      "pos": [
        4951,
        5055
      ]
    },
    {
      "content": "The current <bpt id=\"p1\">**</bpt>UpgradeState<ept id=\"p1\">**</ept> is <bpt id=\"p2\">*</bpt>RollingBackCompleted<ept id=\"p2\">*</ept>, so the original upgrade must have been performed with a rollback <bpt id=\"p3\">**</bpt>FailureAction<ept id=\"p3\">**</ept>, which automatically rolled back the upgrade upon failure.",
      "pos": [
        5057,
        5254
      ]
    },
    {
      "content": "If the original upgrade had been performed with a manual <bpt id=\"p1\">**</bpt>FailureAction<ept id=\"p1\">**</ept>, then the upgrade would instead be in a suspended state to allow live debugging of the application.",
      "pos": [
        5255,
        5429
      ]
    },
    {
      "content": "Investigate health check failures",
      "pos": [
        5435,
        5468
      ]
    },
    {
      "content": "Health check failures can be triggered by a variety of additional issues that can happen after all nodes in an upgrade domain finish upgrading, passing all safety checks.",
      "pos": [
        5470,
        5640
      ]
    },
    {
      "content": "The output below is typical of an upgrade failure due to failed health checks.",
      "pos": [
        5641,
        5719
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>UnhealthyEvaluations<ept id=\"p1\">**</ept> field captures a snapshot of all failing health checks at the time of the upgrade failure according the user-specified <bpt id=\"p2\">[</bpt>Health Policy<ept id=\"p2\">](service-fabric-health-introduction.md)</ept>.",
      "pos": [
        5720,
        5923
      ]
    },
    {
      "content": "Investigating health check failures first requires an understanding of the Service Fabric health model, but even without such an in-depth understanding, we can see that two services are unhealthy: <bpt id=\"p1\">*</bpt>fabric:/DemoApp/Svc3<ept id=\"p1\">*</ept> and <bpt id=\"p2\">*</bpt>fabric:/DemoApp/Svc2<ept id=\"p2\">*</ept> along with the error health reports (\"InjectedFault\" in this case).",
      "pos": [
        8683,
        8997
      ]
    },
    {
      "content": "In this example, 2 out of 4 services are unhealthy, below the default target of 0% unhealthy (<bpt id=\"p1\">*</bpt>MaxPercentUnhealthyServices<ept id=\"p1\">*</ept>).",
      "pos": [
        8998,
        9123
      ]
    },
    {
      "pos": [
        9125,
        9338
      ],
      "content": "The upgrade was suspended upon failing by specifying a <bpt id=\"p1\">**</bpt>FailureAction<ept id=\"p1\">**</ept> of manual when starting the upgrade, so we can investigate the live system in the failed state if desired before taking any further actions."
    },
    {
      "content": "Recover from a suspended upgrade",
      "pos": [
        9344,
        9376
      ]
    },
    {
      "content": "With a rollback <bpt id=\"p1\">**</bpt>FailureAction<ept id=\"p1\">**</ept>, there is no recovery needed since the upgrade will automatically rollback upon failing.",
      "pos": [
        9378,
        9500
      ]
    },
    {
      "content": "With a manual <bpt id=\"p1\">**</bpt>FailureAction<ept id=\"p1\">**</ept>, there are several recovery options:",
      "pos": [
        9501,
        9569
      ]
    },
    {
      "content": "Manually trigger a rollback",
      "pos": [
        9574,
        9601
      ]
    },
    {
      "content": "Proceed through the remainder of the upgrade manually",
      "pos": [
        9605,
        9658
      ]
    },
    {
      "content": "Resume the monitored upgrade",
      "pos": [
        9662,
        9690
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Start-ServiceFabricApplicationRollback<ept id=\"p1\">**</ept> command can be used at any time to start rolling back the application.",
      "pos": [
        9692,
        9809
      ]
    },
    {
      "content": "Once the command returns successfully, the rollback request has been registered in the system and will start shortly.",
      "pos": [
        9810,
        9927
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Resume-ServiceFabricApplicationUpgrade<ept id=\"p1\">**</ept> command can be used to proceed through the remainder of the upgrade manually, one upgrade domain at a time.",
      "pos": [
        9929,
        10083
      ]
    },
    {
      "content": "In this mode, only safety checks will be performed by the system - no more health checks will be performed.",
      "pos": [
        10084,
        10191
      ]
    },
    {
      "content": "This command can only be used when the <bpt id=\"p1\">*</bpt>UpgradeState<ept id=\"p1\">*</ept> shows <bpt id=\"p2\">*</bpt>RollingForwardPending<ept id=\"p2\">*</ept>, which means that the current upgrade domain has finished upgrading but the next upgrade domain has not started yet (pending).",
      "pos": [
        10192,
        10402
      ]
    },
    {
      "pos": [
        10404,
        10554
      ],
      "content": "The <bpt id=\"p1\">**</bpt>Update-ServiceFabricApplicationUpgrade<ept id=\"p1\">**</ept> command can be used to resume the monitored upgrade with both safety and health checks being performed."
    },
    {
      "content": "The upgrade will continue from the upgrade domain where it was last suspended and use the same upgrade parameters and health policies as before.",
      "pos": [
        11309,
        11453
      ]
    },
    {
      "content": "If needed, any of the upgrade parameters and health policies shown in the output above can be changed in the same command when resuming the upgrade.",
      "pos": [
        11454,
        11602
      ]
    },
    {
      "content": "In this example, the upgrade was resumed in Monitored mode leaving all other parameters unchanged, thereby using the same parameters and health policies as before.",
      "pos": [
        11603,
        11766
      ]
    },
    {
      "content": "Further troubleshooting",
      "pos": [
        11771,
        11794
      ]
    },
    {
      "content": "Service Fabric is not following the health policies specified",
      "pos": [
        11800,
        11861
      ]
    },
    {
      "content": "Possible Cause 1:",
      "pos": [
        11863,
        11880
      ]
    },
    {
      "content": "Service Fabric translates all percentages into actual numbers of entities (e.g. replicas, partitions, and services) for health evaluation and always rounds up to the nearest number of whole entities.",
      "pos": [
        11882,
        12081
      ]
    },
    {
      "content": "For example, if the maximum <bpt id=\"p1\">_</bpt>MaxPercentUnhealthyReplicasPerPartition<ept id=\"p1\">_</ept> is 21% and there are 5 replicas, then Service Fabric will allow up to 2 replicas (i.e., <ph id=\"ph1\">`Math.Ceiling (5\\*0.21)`</ph>) to be unhealthy when evaluating partition health.",
      "pos": [
        12082,
        12315
      ]
    },
    {
      "content": "Health policies should be set accordingly to account for this.",
      "pos": [
        12316,
        12378
      ]
    },
    {
      "content": "Possible Cause 2:",
      "pos": [
        12380,
        12397
      ]
    },
    {
      "content": "Health policies are specified in terms of percentages of total services and not specific service instances.",
      "pos": [
        12399,
        12506
      ]
    },
    {
      "content": "For example, before an upgrade, assume that an application has four service instances A, B, C, and D, where service D is unhealthy but with no significant impact on the application.",
      "pos": [
        12507,
        12688
      ]
    },
    {
      "content": "We want to ignore the known unhealthy service D during upgrade and set the parameter <bpt id=\"p1\">*</bpt>MaxPercentUnhealthyServices<ept id=\"p1\">*</ept> to be 25% assuming only A, B, and C need to be healthy.",
      "pos": [
        12689,
        12859
      ]
    },
    {
      "content": "However, during the upgrade, D may become healthy while C becomes unhealthy.",
      "pos": [
        12860,
        12936
      ]
    },
    {
      "content": "The upgrade would still complete successfully in this case since only 25% of the services are unhealthy, which might result in unanticipated errors due to C being unexpectedly unhealthy instead of D. In this situation, D should be modeled as a different Service Type from A, B, and C. Since health policies can be specified on a per-Service Type basis, this would allow applying different unhealthy percentage thresholds to different services based on their roles in the application.",
      "pos": [
        12937,
        13420
      ]
    },
    {
      "content": "I did not specify a health policy for application upgrade, but the upgrade still fails for some timeouts which I never specified",
      "pos": [
        13426,
        13554
      ]
    },
    {
      "content": "When health policies aren't provided to the upgrade request, they are taken from the <bpt id=\"p1\">*</bpt>ApplicationManifest.xml<ept id=\"p1\">*</ept> of the current application version (for example, if upgrading Application X from v1 to v2, application health policies specified for Application X in v1 are used).",
      "pos": [
        13556,
        13830
      ]
    },
    {
      "content": "If a different health policy should be used for the upgrade, then the policy needs to be specified as part of the application upgrade API call.",
      "pos": [
        13831,
        13974
      ]
    },
    {
      "content": "Note that the policies specified as part of the API call only apply for the duration of the upgrade.",
      "pos": [
        13975,
        14075
      ]
    },
    {
      "content": "Once the upgrade is completed, the policies specified in the <bpt id=\"p1\">*</bpt>ApplicationManifest.xml<ept id=\"p1\">*</ept> are used.",
      "pos": [
        14076,
        14172
      ]
    },
    {
      "content": "Incorrect Timeouts specified.",
      "pos": [
        14178,
        14207
      ]
    },
    {
      "content": "Users may have wondered about what happens if the timeouts are set inconsistently, for example, having an <bpt id=\"p1\">*</bpt>UpgradeTimeout<ept id=\"p1\">*</ept> less than the <bpt id=\"p2\">*</bpt>UpgradeDomainTimeout<ept id=\"p2\">*</ept>.",
      "pos": [
        14209,
        14369
      ]
    },
    {
      "content": "The answer is that an error is returned.",
      "pos": [
        14370,
        14410
      ]
    },
    {
      "content": "Other cases where this may happen is if <bpt id=\"p1\">*</bpt>UpgradeDomainTimeout<ept id=\"p1\">*</ept> is less than the sum of <bpt id=\"p2\">*</bpt>HealthCheckWaitDuration<ept id=\"p2\">*</ept> and <bpt id=\"p3\">*</bpt>HealthCheckRetryTimeout<ept id=\"p3\">*</ept> or if <bpt id=\"p4\">*</bpt>UpgradeDomainTimeout<ept id=\"p4\">*</ept> is less than the sum of <bpt id=\"p5\">*</bpt>HealthCheckWaitDuration<ept id=\"p5\">*</ept> and <bpt id=\"p6\">*</bpt>HealthCheckStableDuration<ept id=\"p6\">*</ept>.",
      "pos": [
        14411,
        14665
      ]
    },
    {
      "content": "My upgrades are taking too long",
      "pos": [
        14671,
        14702
      ]
    },
    {
      "content": "The time it takes for an upgrade to complete is dependent on the various health checks and timeouts specified, which in turn are dependent on the time it takes for your application to upgrade (including copying the package, deploying, and stabilizing).",
      "pos": [
        14704,
        14956
      ]
    },
    {
      "content": "Being too aggressive with timeouts might mean more failed upgrades, and thus starting conservatively with longer timeouts is recommended.",
      "pos": [
        14957,
        15094
      ]
    },
    {
      "content": "A quick refresher on how the timeouts interact with the upgrade times:",
      "pos": [
        15096,
        15166
      ]
    },
    {
      "pos": [
        15168,
        15281
      ],
      "content": "Upgrade for a upgrade domain cannot complete faster than <bpt id=\"p1\">*</bpt>HealthCheckWaitDuration<ept id=\"p1\">*</ept> + <bpt id=\"p2\">*</bpt>HealthCheckStableDuration<ept id=\"p2\">*</ept>."
    },
    {
      "pos": [
        15283,
        15378
      ],
      "content": "Upgrade failure cannot occur faster than <bpt id=\"p1\">*</bpt>HealthCheckWaitDuration<ept id=\"p1\">*</ept> + <bpt id=\"p2\">*</bpt>HealthCheckRetryTimeout<ept id=\"p2\">*</ept>."
    },
    {
      "content": "The upgrade time for a upgrade domain is limited by <bpt id=\"p1\">*</bpt>UpgradeDomainTimeout<ept id=\"p1\">*</ept>.",
      "pos": [
        15380,
        15455
      ]
    },
    {
      "content": "If <bpt id=\"p1\">*</bpt>HealthCheckRetryTimeout<ept id=\"p1\">*</ept> and <bpt id=\"p2\">*</bpt>HealthCheckStableDuration<ept id=\"p2\">*</ept> are both non-zero and the health of the application keeps switching back and forth, then the upgrade will eventually timeout on <bpt id=\"p3\">*</bpt>UpgradeDomainTimeout<ept id=\"p3\">*</ept>.",
      "pos": [
        15457,
        15669
      ]
    },
    {
      "content": "<bpt id=\"p1\">*</bpt>UpgradeDomainTimeout<ept id=\"p1\">*</ept> starts counting down once the upgrade for the current upgrade domain begins.",
      "pos": [
        15670,
        15769
      ]
    },
    {
      "content": "Next steps",
      "pos": [
        15774,
        15784
      ]
    },
    {
      "content": "Upgrade Tutorial",
      "pos": [
        15787,
        15803
      ]
    },
    {
      "content": "Upgrade Parameters",
      "pos": [
        15855,
        15873
      ]
    },
    {
      "content": "Advanced Topics",
      "pos": [
        15927,
        15942
      ]
    },
    {
      "content": "Data Serialization",
      "pos": [
        15994,
        16012
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Troubleshooting Service Fabric application upgrades\"\n   description=\"This article covers some common issues around upgrading a Service Fabric application and how to resolve them.\"\n   services=\"service-fabric\"\n   documentationCenter=\".net\"\n   authors=\"mani-ramaswamy\"\n   manager=\"samgeo\"\n   editor=\"\"/>\n\n<tags\n   ms.service=\"service-fabric\"\n   ms.devlang=\"dotnet\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"NA\"\n   ms.workload=\"NA\"\n   ms.date=\"08/11/2015\"\n   ms.author=\"subramar\"/>\n\n# Troubleshoot application upgrades\n\nThis article covers some of the common issues around upgrading a Service Fabric application and how to resolve them.\n\n## Troubleshoot a failed application upgrade\n\nWhen an upgrade fails, the output of the **Get-ServiceFabricApplicationUpgrade** command will contain some additional information for debugging the failure. This information can be used to:\n\n1. Identify the failure type\n2. Identify the failure reason\n3. Isolate the failing component(s) for further investigation\n\nThis information will be available as soon as Service Fabric detects the failure regardless of whether the **FailureAction** is to rollback or suspend the upgrade.\n\n### Identify the failure type\n\nIn the output of **Get-ServiceFabricApplicationUpgrade**, **FailureTimestampUtc** identifies the timestamp (in UTC) at which an upgrade failure was detected by Service Fabric and the **FailureAction** was triggered. The **FailureReason** identifies one of three potential high-level causes of the failure:\n\n1. UpgradeDomainTimeout - Indicates that a particular upgrade domain took too long to complete and **UpgradeDomainTimeout** expired.\n2. OverallUpgradeTimeout - Indicates that the overall upgrade took too long to complete and **UpgradeTimeout** expired.\n3. HealthCheck - Indicates that after upgrading an Upgrade Domain, the application remained unhealthy according to the specified health policies and **HealthCheckRetryTimeout** expired.\n\nThese entries will only show up in the output when the upgrade fails and starts rolling back. Further information will be displayed depending on the type of the failure.\n\n### Investigate upgrade timeouts\n\nUpgrade timeout failures are most commonly caused by service availability issues. The output below is typical of upgrades where service replicas or instances fail to start in the new code version. The **UpgradeDomainProgressAtFailure** field captures a snapshot of any pending upgrade work at the time of failure.\n\n~~~\nPS D:\\temp> Get-ServiceFabricApplicationUpgrade fabric:/DemoApp\n\nApplicationName                : fabric:/DemoApp\nApplicationTypeName            : DemoAppType\nTargetApplicationTypeVersion   : v2\nApplicationParameters          : {}\nStartTimestampUtc              : 4/14/2015 9:26:38 PM\nFailureTimestampUtc            : 4/14/2015 9:27:05 PM\nFailureReason                  : UpgradeDomainTimeout\nUpgradeDomainProgressAtFailure : MYUD1\n\n                                 NodeName            : Node4\n                                 UpgradePhase        : PostUpgradeSafetyCheck\n                                 PendingSafetyChecks :\n                                     WaitForPrimaryPlacement - PartitionId: 744c8d9f-1d26-417e-a60e-cd48f5c098f0\n\n                                 NodeName            : Node1\n                                 UpgradePhase        : PostUpgradeSafetyCheck\n                                 PendingSafetyChecks :\n                                     WaitForPrimaryPlacement - PartitionId: 4b43f4d8-b26b-424e-9307-7a7a62e79750\nUpgradeState                   : RollingBackCompleted\nUpgradeDuration                : 00:00:46\nCurrentUpgradeDomainDuration   : 00:00:00\nNextUpgradeDomain              :\nUpgradeDomainsStatus           : { \"MYUD1\" = \"Completed\";\n                                 \"MYUD2\" = \"Completed\";\n                                 \"MYUD3\" = \"Completed\" }\nUpgradeKind                    : Rolling\nRollingUpgradeMode             : UnmonitoredAuto\nForceRestart                   : False\nUpgradeReplicaSetCheckTimeout  : 00:00:00\n~~~\n\nIn this example, we can see that the upgrade failed at upgrade domain *MYUD1* and two partitions (*744c8d9f-1d26-417e-a60e-cd48f5c098f0* and *4b43f4d8-b26b-424e-9307-7a7a62e79750*) were stuck, unable to place primary replicas (*WaitForPrimaryPlacement*) on target nodes *Node1* and *Node4*. The **Get-ServiceFabricNode** command can be used to verify that these two nodes are in upgrade domain *MYUD1*. The *UpgradePhase* says *PostUpgradeSafetyCheck*, which means that these safety checks are occurring after all nodes in the upgrade domain had finished upgrading. All this information combined points to a potential issue with the new version of the application code. The most common issues are service errors in the open or promotion to primary code paths.\n\nAn *UpgradePhase* of *PreUpgradeSafetyCheck* means there were issues preparing the upgrade domain before actually performing the upgrade. The most common issues in this case are service errors in the close or demotion from primary code paths.\n\nThe current **UpgradeState** is *RollingBackCompleted*, so the original upgrade must have been performed with a rollback **FailureAction**, which automatically rolled back the upgrade upon failure. If the original upgrade had been performed with a manual **FailureAction**, then the upgrade would instead be in a suspended state to allow live debugging of the application.\n\n### Investigate health check failures\n\nHealth check failures can be triggered by a variety of additional issues that can happen after all nodes in an upgrade domain finish upgrading, passing all safety checks. The output below is typical of an upgrade failure due to failed health checks. The **UnhealthyEvaluations** field captures a snapshot of all failing health checks at the time of the upgrade failure according the user-specified [Health Policy](service-fabric-health-introduction.md).\n\n~~~\nPS D:\\temp> Get-ServiceFabricApplicationUpgrade fabric:/DemoApp\n\nApplicationName                         : fabric:/DemoApp\nApplicationTypeName                     : DemoAppType\nTargetApplicationTypeVersion            : v4\nApplicationParameters                   : {}\nStartTimestampUtc                       : 4/24/2015 2:42:31 AM\nUpgradeState                            : RollingForwardPending\nUpgradeDuration                         : 00:00:27\nCurrentUpgradeDomainDuration            : 00:00:27\nNextUpgradeDomain                       : MYUD2\nUpgradeDomainsStatus                    : { \"MYUD1\" = \"Completed\";\n                                          \"MYUD2\" = \"Pending\";\n                                          \"MYUD3\" = \"Pending\" }\nUnhealthyEvaluations                    :\n                                          Unhealthy services: 50% (2/4), ServiceType='PersistedServiceType', MaxPercentUnhealthyServices=0%.\n\n                                          Unhealthy service: ServiceName='fabric:/DemoApp/Svc3', AggregatedHealthState='Error'.\n\n                                              Unhealthy partitions: 100% (1/1), MaxPercentUnhealthyPartitionsPerService=0%.\n\n                                              Unhealthy partition: PartitionId='3a9911f6-a2e5-452d-89a8-09271e7e49a8', AggregatedHealthState='Error'.\n\n                                                  Error event: SourceId='Replica', Property='InjectedFault'.\n\n                                          Unhealthy service: ServiceName='fabric:/DemoApp/Svc2', AggregatedHealthState='Error'.\n\n                                              Unhealthy partitions: 100% (1/1), MaxPercentUnhealthyPartitionsPerService=0%.\n\n                                              Unhealthy partition: PartitionId='744c8d9f-1d26-417e-a60e-cd48f5c098f0', AggregatedHealthState='Error'.\n\n                                                  Error event: SourceId='Replica', Property='InjectedFault'.\n\nUpgradeKind                             : Rolling\nRollingUpgradeMode                      : Monitored\nFailureAction                           : Manual\nForceRestart                            : False\nUpgradeReplicaSetCheckTimeout           : 49710.06:28:15\nHealthCheckWaitDuration                 : 00:00:00\nHealthCheckStableDuration               : 00:00:10\nHealthCheckRetryTimeout                 : 00:00:10\nUpgradeDomainTimeout                    : 10675199.02:48:05.4775807\nUpgradeTimeout                          : 10675199.02:48:05.4775807\nConsiderWarningAsError                  :\nMaxPercentUnhealthyPartitionsPerService :\nMaxPercentUnhealthyReplicasPerPartition :\nMaxPercentUnhealthyServices             :\nMaxPercentUnhealthyDeployedApplications :\nServiceTypeHealthPolicyMap              :\n~~~\n\nInvestigating health check failures first requires an understanding of the Service Fabric health model, but even without such an in-depth understanding, we can see that two services are unhealthy: *fabric:/DemoApp/Svc3* and *fabric:/DemoApp/Svc2* along with the error health reports (\"InjectedFault\" in this case). In this example, 2 out of 4 services are unhealthy, below the default target of 0% unhealthy (*MaxPercentUnhealthyServices*).\n\nThe upgrade was suspended upon failing by specifying a **FailureAction** of manual when starting the upgrade, so we can investigate the live system in the failed state if desired before taking any further actions.\n\n### Recover from a suspended upgrade\n\nWith a rollback **FailureAction**, there is no recovery needed since the upgrade will automatically rollback upon failing. With a manual **FailureAction**, there are several recovery options:\n\n1. Manually trigger a rollback\n2. Proceed through the remainder of the upgrade manually\n3. Resume the monitored upgrade\n\nThe **Start-ServiceFabricApplicationRollback** command can be used at any time to start rolling back the application. Once the command returns successfully, the rollback request has been registered in the system and will start shortly.\n\nThe **Resume-ServiceFabricApplicationUpgrade** command can be used to proceed through the remainder of the upgrade manually, one upgrade domain at a time. In this mode, only safety checks will be performed by the system - no more health checks will be performed. This command can only be used when the *UpgradeState* shows *RollingForwardPending*, which means that the current upgrade domain has finished upgrading but the next upgrade domain has not started yet (pending).\n\nThe **Update-ServiceFabricApplicationUpgrade** command can be used to resume the monitored upgrade with both safety and health checks being performed.\n\n~~~\nPS D:\\temp> Update-ServiceFabricApplicationUpgrade fabric:/DemoApp -UpgradeMode Monitored\n\nUpgradeMode                             : Monitored\nForceRestart                            :\nUpgradeReplicaSetCheckTimeout           :\nFailureAction                           :\nHealthCheckWaitDuration                 :\nHealthCheckStableDuration               :\nHealthCheckRetryTimeout                 :\nUpgradeTimeout                          :\nUpgradeDomainTimeout                    :\nConsiderWarningAsError                  :\nMaxPercentUnhealthyPartitionsPerService :\nMaxPercentUnhealthyReplicasPerPartition :\nMaxPercentUnhealthyServices             :\nMaxPercentUnhealthyDeployedApplications :\nServiceTypeHealthPolicyMap              :\n\nPS D:\\temp>\n~~~\n\nThe upgrade will continue from the upgrade domain where it was last suspended and use the same upgrade parameters and health policies as before. If needed, any of the upgrade parameters and health policies shown in the output above can be changed in the same command when resuming the upgrade. In this example, the upgrade was resumed in Monitored mode leaving all other parameters unchanged, thereby using the same parameters and health policies as before.\n\n## Further troubleshooting\n\n### Service Fabric is not following the health policies specified\n\nPossible Cause 1:\n\nService Fabric translates all percentages into actual numbers of entities (e.g. replicas, partitions, and services) for health evaluation and always rounds up to the nearest number of whole entities. For example, if the maximum _MaxPercentUnhealthyReplicasPerPartition_ is 21% and there are 5 replicas, then Service Fabric will allow up to 2 replicas (i.e., `Math.Ceiling (5\\*0.21)`) to be unhealthy when evaluating partition health. Health policies should be set accordingly to account for this.\n\nPossible Cause 2:\n\nHealth policies are specified in terms of percentages of total services and not specific service instances. For example, before an upgrade, assume that an application has four service instances A, B, C, and D, where service D is unhealthy but with no significant impact on the application. We want to ignore the known unhealthy service D during upgrade and set the parameter *MaxPercentUnhealthyServices* to be 25% assuming only A, B, and C need to be healthy. However, during the upgrade, D may become healthy while C becomes unhealthy. The upgrade would still complete successfully in this case since only 25% of the services are unhealthy, which might result in unanticipated errors due to C being unexpectedly unhealthy instead of D. In this situation, D should be modeled as a different Service Type from A, B, and C. Since health policies can be specified on a per-Service Type basis, this would allow applying different unhealthy percentage thresholds to different services based on their roles in the application.\n\n### I did not specify a health policy for application upgrade, but the upgrade still fails for some timeouts which I never specified\n\nWhen health policies aren't provided to the upgrade request, they are taken from the *ApplicationManifest.xml* of the current application version (for example, if upgrading Application X from v1 to v2, application health policies specified for Application X in v1 are used). If a different health policy should be used for the upgrade, then the policy needs to be specified as part of the application upgrade API call. Note that the policies specified as part of the API call only apply for the duration of the upgrade. Once the upgrade is completed, the policies specified in the *ApplicationManifest.xml* are used.\n\n### Incorrect Timeouts specified.\n\nUsers may have wondered about what happens if the timeouts are set inconsistently, for example, having an *UpgradeTimeout* less than the *UpgradeDomainTimeout*. The answer is that an error is returned. Other cases where this may happen is if *UpgradeDomainTimeout* is less than the sum of *HealthCheckWaitDuration* and *HealthCheckRetryTimeout* or if *UpgradeDomainTimeout* is less than the sum of *HealthCheckWaitDuration* and *HealthCheckStableDuration*.\n\n### My upgrades are taking too long\n\nThe time it takes for an upgrade to complete is dependent on the various health checks and timeouts specified, which in turn are dependent on the time it takes for your application to upgrade (including copying the package, deploying, and stabilizing). Being too aggressive with timeouts might mean more failed upgrades, and thus starting conservatively with longer timeouts is recommended.\n\nA quick refresher on how the timeouts interact with the upgrade times:\n\nUpgrade for a upgrade domain cannot complete faster than *HealthCheckWaitDuration* + *HealthCheckStableDuration*.\n\nUpgrade failure cannot occur faster than *HealthCheckWaitDuration* + *HealthCheckRetryTimeout*.\n\nThe upgrade time for a upgrade domain is limited by *UpgradeDomainTimeout*.  If *HealthCheckRetryTimeout* and *HealthCheckStableDuration* are both non-zero and the health of the application keeps switching back and forth, then the upgrade will eventually timeout on *UpgradeDomainTimeout*. *UpgradeDomainTimeout* starts counting down once the upgrade for the current upgrade domain begins.\n\n## Next steps\n\n[Upgrade Tutorial](service-fabric-application-upgrade-tutorial.md)\n\n[Upgrade Parameters](service-fabric-application-upgrade-parameters.md)\n\n[Advanced Topics](service-fabric-application-upgrade-advanced.md)\n\n[Data Serialization](service-fabric-application-upgrade-data-serialization.md)\n \n"
}