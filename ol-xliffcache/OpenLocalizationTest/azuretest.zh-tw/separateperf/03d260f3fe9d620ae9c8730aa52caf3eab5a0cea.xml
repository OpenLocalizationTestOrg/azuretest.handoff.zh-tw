{
  "nodes": [
    {
      "content": "Spark on Ubuntu Resource Manager Template",
      "pos": [
        27,
        68
      ]
    },
    {
      "content": "Learn to easily deploy a new Spark cluster on Ubuntu VMs using Azure PowerShell or the Azure CLI and a Resource Manager template",
      "pos": [
        87,
        215
      ]
    },
    {
      "content": "Spark on Ubuntu with a Resource Manager template",
      "pos": [
        550,
        598
      ]
    },
    {
      "content": "Apache Spark is a fast engine for large-scale data processing.",
      "pos": [
        600,
        662
      ]
    },
    {
      "content": "Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing, and it can access diverse data sources, including HDFS, Spark, HBase, and S3.",
      "pos": [
        663,
        839
      ]
    },
    {
      "content": "In addition to running on the Mesos or YARN cluster managers, Spark provides a simple standalone deployment mode.",
      "pos": [
        841,
        954
      ]
    },
    {
      "content": "This tutorial will walk you through how to use a sample Azure Resource Manager template to deploy a Spark cluster on Ubuntu VMs through <bpt id=\"p1\">[</bpt>Azure PowerShell<ept id=\"p1\">](../powershell-install-configure.md)</ept> or the <bpt id=\"p2\">[</bpt>Azure CLI<ept id=\"p2\">](../xplat-cli.md)</ept>.",
      "pos": [
        955,
        1182
      ]
    },
    {
      "content": "This template deploys a Spark cluster on the Ubuntu virtual machines.",
      "pos": [
        1184,
        1253
      ]
    },
    {
      "content": "This template also provisions a storage account, virtual network, availability sets, public IP addresses and network interfaces required by the installation.",
      "pos": [
        1254,
        1411
      ]
    },
    {
      "content": "The Spark cluster is created behind a subnet, so there is no public IP access to the cluster.",
      "pos": [
        1412,
        1505
      ]
    },
    {
      "content": "As part of the deployment, an optional \"jump box\" can be deployed.",
      "pos": [
        1507,
        1573
      ]
    },
    {
      "content": "This \"jump box\" is an Ubuntu VM deployed in the subnet as well, but it <bpt id=\"p1\">*</bpt>does<ept id=\"p1\">*</ept> expose a public IP address with an open SSH port that you can connect to.",
      "pos": [
        1574,
        1725
      ]
    },
    {
      "content": "Then from the \"jump box\", you can SSH to all the Spark VMs in the subnet.",
      "pos": [
        1726,
        1799
      ]
    },
    {
      "content": "The template utilizes a \"t-shirt size\" concept in order to specify a \"Small\", \"Medium\", or \"Large\" Spark cluster setup.",
      "pos": [
        1801,
        1920
      ]
    },
    {
      "content": "When the template language supports more dynamic template sizing, this could be changed to specify the number of Spark cluster master nodes and slave nodes, VM size, etc. For now, you can see the VM size and the number of masters and slaves defined in the file azuredeploy.json in the variables <bpt id=\"p1\">**</bpt>tshirtSizeS<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>tshirtSizeM<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>tshirtSizeL<ept id=\"p3\">**</ept>:",
      "pos": [
        1922,
        2271
      ]
    },
    {
      "content": "S: 1 master, 1 slave",
      "pos": [
        2275,
        2295
      ]
    },
    {
      "content": "M: 1 master, 4 slaves",
      "pos": [
        2298,
        2319
      ]
    },
    {
      "content": "L: 1 master, 6 slaves",
      "pos": [
        2322,
        2343
      ]
    },
    {
      "content": "Newly deployed clusters based on this template will have the topology described in the following diagram, although other topologies can be easily achieved by customizing the template presented in this article:",
      "pos": [
        2345,
        2554
      ]
    },
    {
      "content": "cluster-architecture",
      "pos": [
        2558,
        2578
      ]
    },
    {
      "content": "As shown in the above image, the deployment topology consists of the following elements:",
      "pos": [
        2645,
        2733
      ]
    },
    {
      "content": "A new storage account hosting the OS disk of newly created virtual machines.",
      "pos": [
        2739,
        2815
      ]
    },
    {
      "content": "A virtual network with a subnet.",
      "pos": [
        2820,
        2852
      ]
    },
    {
      "content": "All the virtual machines created by the template are provisioned inside this virtual network.",
      "pos": [
        2853,
        2946
      ]
    },
    {
      "content": "An availability set for master and slave nodes.",
      "pos": [
        2951,
        2998
      ]
    },
    {
      "content": "A master node in the newly created availability set.",
      "pos": [
        3003,
        3055
      ]
    },
    {
      "content": "Four slave nodes running in the same virtual subnet and availability set as the master node.",
      "pos": [
        3060,
        3152
      ]
    },
    {
      "content": "A jump-box VM located in the same virtual network and subnet that can be used to access the cluster.",
      "pos": [
        3157,
        3257
      ]
    },
    {
      "content": "Spark version 3.0.0 is the default version and can be changed to any pre-built binaries available on the Spark repository.",
      "pos": [
        3259,
        3381
      ]
    },
    {
      "content": "There is also a provision in the script to uncomment the build from source.",
      "pos": [
        3382,
        3457
      ]
    },
    {
      "content": "A static IP address will be assigned to each Spark master node: 10.0.0.10.",
      "pos": [
        3458,
        3532
      ]
    },
    {
      "content": "A static IP address will be assigned to each Spark slave node in order to work around the current limitation of not being able to dynamically compose a list of IP addresses from within the template.",
      "pos": [
        3533,
        3731
      ]
    },
    {
      "content": "(By default, the first node will be assigned the private IP address of 10.0.0.30, the second node will be assigned 10.0.0.31, and so on.) To check deployment errors, go to the new Azure portal and look under <bpt id=\"p1\">**</bpt>Resource Group<ept id=\"p1\">**</ept> &gt; <bpt id=\"p2\">**</bpt>Last deployment<ept id=\"p2\">**</ept> &gt; <bpt id=\"p3\">**</bpt>Check Operation Details<ept id=\"p3\">**</ept>.",
      "pos": [
        3732,
        4011
      ]
    },
    {
      "content": "Before diving into more details related to Azure Resource Manager and the template we will use for this deployment, make sure you have Azure PowerShell or the Azure CLI configured correctly.",
      "pos": [
        4013,
        4203
      ]
    },
    {
      "content": "Create a Spark cluster by using a Resource Manager template",
      "pos": [
        4392,
        4451
      ]
    },
    {
      "content": "Follow these steps to create a Spark cluster by using a Resource Manager template from the GitHub template repository, via either Azure PowerShell or the Azure CLI.",
      "pos": [
        4453,
        4617
      ]
    },
    {
      "content": "Step 1-a: Download the template files by using Azure PowerShell",
      "pos": [
        4623,
        4686
      ]
    },
    {
      "content": "Create a local folder for the JSON template and other associated files (for example, C:\\Azure\\Templates\\Spark).",
      "pos": [
        4688,
        4799
      ]
    },
    {
      "content": "Substitute in the folder name of your local folder and run these commands:",
      "pos": [
        4801,
        4875
      ]
    },
    {
      "content": "Step 1-b: Download the template files by using the Azure CLI",
      "pos": [
        5655,
        5715
      ]
    },
    {
      "content": "Clone the entire template repository by using a Git client of your choice, for example:",
      "pos": [
        5717,
        5804
      ]
    },
    {
      "pos": [
        5892,
        6000
      ],
      "content": "When the cloning is completed, look for the <bpt id=\"p1\">**</bpt>spark-on-ubuntu<ept id=\"p1\">**</ept> folder in your C:\\Azure\\Templates directory."
    },
    {
      "content": "Step 2: (optional) Understand the template parameters",
      "pos": [
        6006,
        6059
      ]
    },
    {
      "content": "When you create a Spark cluster by using a template, you must specify a set of configuration parameters to deal with a number of required settings.",
      "pos": [
        6061,
        6208
      ]
    },
    {
      "content": "By declaring these parameters in template definition, it's possible to specify values during deployment execution through an external file or at a command line.",
      "pos": [
        6209,
        6369
      ]
    },
    {
      "content": "In the \"parameters\" section at the top of the azuredeploy.json file, you'll find the set of parameters that are required by the template to configure a Spark cluster.",
      "pos": [
        6371,
        6537
      ]
    },
    {
      "content": "Here is an example of the \"parameters\" section from this template's azuredeploy.json file:",
      "pos": [
        6538,
        6628
      ]
    },
    {
      "content": "Each parameter has details such as data type and allowed values.",
      "pos": [
        10655,
        10719
      ]
    },
    {
      "content": "This allows for validation of parameters passed during template execution in an interactive mode (e.g., Azure PowerShell or Azure CLI), as well as a self-discovery UI that could be dynamically built by parsing the list of required parameters and their descriptions.",
      "pos": [
        10720,
        10985
      ]
    },
    {
      "content": "Step 3-a: Deploy a Spark cluster by using a template via Azure PowerShell",
      "pos": [
        10991,
        11064
      ]
    },
    {
      "content": "Prepare a parameters file for your deployment by creating a JSON file containing runtime values for all parameters.",
      "pos": [
        11066,
        11181
      ]
    },
    {
      "content": "This file will then be passed as a single entity to the deployment command.",
      "pos": [
        11182,
        11257
      ]
    },
    {
      "content": "If you do not include a parameters file, Azure PowerShell will use any default values specified in the template, and then prompt you to fill in the remaining values.",
      "pos": [
        11258,
        11423
      ]
    },
    {
      "content": "Here is an example set of parameters from the azuredeploy-parameters.json file.",
      "pos": [
        11425,
        11504
      ]
    },
    {
      "content": "Note that you will need to provide valid values for the parameters <bpt id=\"p1\">**</bpt>storageAccountName<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>adminUsername<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>adminPassword<ept id=\"p3\">**</ept>, plus any customizations to the other parameters:",
      "pos": [
        11505,
        11686
      ]
    },
    {
      "content": "Fill in an Azure deployment name, resource group name, Azure location, and the folder of your saved JSON deployment file.",
      "pos": [
        12436,
        12557
      ]
    },
    {
      "content": "Then run these commands:",
      "pos": [
        12558,
        12582
      ]
    },
    {
      "pos": [
        13100,
        13165
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> <bpt id=\"p1\">**</bpt>$RGName<ept id=\"p1\">**</ept> must be unique within your subscription."
    },
    {
      "content": "When you run the <bpt id=\"p1\">**</bpt>New-AzureResourceGroupDeployment<ept id=\"p1\">**</ept> command, this will extract parameter values from the JSON parameters file, and will start executing the template accordingly.",
      "pos": [
        13167,
        13346
      ]
    },
    {
      "content": "Defining and using multiple parameter files with your different environments (test, production, etc.) will promote template reuse and simplify complex multi-environment solutions.",
      "pos": [
        13347,
        13526
      ]
    },
    {
      "content": "When deploying, please keep in mind that a new Azure Storage account needs to be created, so the name you provide as the Storage account parameter must be unique and meet all requirements for an Azure Storage account (lowercase letters and numbers only).",
      "pos": [
        13528,
        13782
      ]
    },
    {
      "content": "During the deployment, you will see something like this:",
      "pos": [
        13784,
        13840
      ]
    },
    {
      "content": "During and after deployment, you can check all the requests that were made during provisioning, including any errors that occurred.",
      "pos": [
        17844,
        17975
      ]
    },
    {
      "pos": [
        17977,
        18061
      ],
      "content": "To do that, go to the <bpt id=\"p1\">[</bpt>Azure portal<ept id=\"p1\">](https://portal.azure.com)</ept> and do the following:"
    },
    {
      "pos": [
        18065,
        18166
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Browse<ept id=\"p1\">**</ept> on the left-hand navigation bar, and then scroll down and click <bpt id=\"p2\">**</bpt>Resource Groups<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Click the resource group that you just created, to bring up the \"Resource Group\" blade.",
      "pos": [
        18169,
        18256
      ]
    },
    {
      "pos": [
        18259,
        18397
      ],
      "content": "By clicking the <bpt id=\"p1\">**</bpt>Events<ept id=\"p1\">**</ept> bar graph in the <bpt id=\"p2\">**</bpt>Monitoring<ept id=\"p2\">**</ept> part of the \"Resource Group\" blade, you can see the events for your deployment."
    },
    {
      "content": "By clicking individual events, you can drill further down into the details of each individual operation made on behalf of the template.",
      "pos": [
        18400,
        18535
      ]
    },
    {
      "content": "portal-events",
      "pos": [
        18539,
        18552
      ]
    },
    {
      "content": "After your tests, if you need to remove this resource group and all of its resources (the storage account, virtual machine, and virtual network), use this single command:",
      "pos": [
        18612,
        18782
      ]
    },
    {
      "content": "Step 3-b: Deploy a Spark cluster by using a template via the Azure CLI",
      "pos": [
        18870,
        18940
      ]
    },
    {
      "content": "To deploy a Spark cluster via the Azure CLI, first create a resource group by specifying a name and a location:",
      "pos": [
        18942,
        19053
      ]
    },
    {
      "content": "Pass this resource group name, the location of the JSON template file, and the location of the parameters file (see the above Azure PowerShell section for details) into the following command:",
      "pos": [
        19108,
        19299
      ]
    },
    {
      "content": "You can check the status of individual resources deployments by using the following command:",
      "pos": [
        19410,
        19502
      ]
    },
    {
      "content": "A tour of the Spark template structure and file organization",
      "pos": [
        19559,
        19619
      ]
    },
    {
      "content": "In order to design a robust and reusable Resource Manager template, additional thinking is needed to organize the series of complex and interrelated tasks required during the deployment of a complex solution like Spark.",
      "pos": [
        19621,
        19840
      ]
    },
    {
      "content": "Leveraging Resource Manager template linking and resource looping in addition to script execution through related extensions, it's possible to implement a modular approach that can be reused with virtually any complex template-based deployment.",
      "pos": [
        19841,
        20085
      ]
    },
    {
      "content": "This diagram describes the relationships between all the files downloaded from GitHub for this deployment:",
      "pos": [
        20087,
        20193
      ]
    },
    {
      "content": "spark-files",
      "pos": [
        20197,
        20208
      ]
    },
    {
      "content": "This section steps you through the structure of the azuredeploy.json file for the Spark cluster.",
      "pos": [
        20266,
        20362
      ]
    },
    {
      "content": "If you have not already downloaded a copy of the template file, designate a local folder as the location for the file and create it (for example, C:\\Azure\\Templates\\Spark).",
      "pos": [
        20364,
        20536
      ]
    },
    {
      "content": "Fill in the folder name and run these commands:",
      "pos": [
        20537,
        20584
      ]
    },
    {
      "content": "Open the azuredeploy.json template in a text editor or tool of your choice.",
      "pos": [
        20914,
        20989
      ]
    },
    {
      "content": "The following information describes the structure of the template file and the purpose of each section.",
      "pos": [
        20990,
        21093
      ]
    },
    {
      "content": "Alternately, you can see the contents of this template in your browser from <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/spark-on-ubuntu/azuredeploy.json)</ept>.",
      "pos": [
        21094,
        21285
      ]
    },
    {
      "content": "\"parameters\" section",
      "pos": [
        21291,
        21311
      ]
    },
    {
      "content": "The \"parameters\" section of azuredeploy.json specifies modifiable parameters that are used in this template.",
      "pos": [
        21313,
        21421
      ]
    },
    {
      "content": "The aforementioned azuredeploy-parameters.json file is used to pass values into the \"parameters\" section of azuredeploy.json during template execution.",
      "pos": [
        21422,
        21573
      ]
    },
    {
      "content": "Here is an example of a parameter for the \"t-shirt size\":",
      "pos": [
        21575,
        21632
      ]
    },
    {
      "pos": [
        21869,
        21958
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Notice that <bpt id=\"p1\">**</bpt>defaultValue<ept id=\"p1\">**</ept> may be specified, as well as <bpt id=\"p2\">**</bpt>allowedValues<ept id=\"p2\">**</ept>."
    },
    {
      "content": "\"variables\" section",
      "pos": [
        21964,
        21983
      ]
    },
    {
      "content": "The \"variables\" section specifies variables that can be used throughout this template.",
      "pos": [
        21985,
        22071
      ]
    },
    {
      "content": "This contains a number of fields (JSON data types or fragments) that will be set to constants or calculated values at execution time.",
      "pos": [
        22072,
        22205
      ]
    },
    {
      "content": "Here are some examples that range from simple to more complex:",
      "pos": [
        22206,
        22268
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>vmStorageAccountContainerName<ept id=\"p1\">**</ept> variable is an example of a simple name/value variable.",
      "pos": [
        23554,
        23647
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>vnetID<ept id=\"p1\">**</ept> is an example of a variable that is calculated at run time using the functions <bpt id=\"p2\">**</bpt>resourceId<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>parameters<ept id=\"p3\">**</ept>.",
      "pos": [
        23649,
        23773
      ]
    },
    {
      "content": "The value of the <bpt id=\"p1\">**</bpt>numberOfMasterInstances<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>vmSize<ept id=\"p2\">**</ept> variables are calculated at run time using the <bpt id=\"p3\">**</bpt>concat<ept id=\"p3\">**</ept>, <bpt id=\"p4\">**</bpt>variables<ept id=\"p4\">**</ept>, and <bpt id=\"p5\">**</bpt>parameters<ept id=\"p5\">**</ept> functions.",
      "pos": [
        23775,
        23938
      ]
    },
    {
      "pos": [
        23942,
        24148
      ],
      "content": "If you want to customize the size of the Spark cluster deployment, then you can change the properties of the variables <bpt id=\"p1\">**</bpt>tshirtSizeS<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>tshirtSizeM<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>tshirtSizeL<ept id=\"p3\">**</ept> in the azuredeploy.json template."
    },
    {
      "pos": [
        24152,
        24310
      ],
      "content": "More information regarding the template language can be found on MSDN at <bpt id=\"p1\">[</bpt>Azure Resource Manager Template Language<ept id=\"p1\">](../resource-group-authoring-templates.md)</ept>."
    },
    {
      "content": "\"resources\" section",
      "pos": [
        24317,
        24336
      ]
    },
    {
      "content": "The \"resources\" section is where most of the action happens.",
      "pos": [
        24338,
        24398
      ]
    },
    {
      "content": "Looking carefully inside this section, you can immediately identify two different cases.",
      "pos": [
        24399,
        24487
      ]
    },
    {
      "content": "The first is an element defined of type <ph id=\"ph1\">`Microsoft.Resources/deployments`</ph> that essentially invokes a nested deployment within the main one.",
      "pos": [
        24488,
        24627
      ]
    },
    {
      "content": "The second is the <bpt id=\"p1\">**</bpt>templateLink<ept id=\"p1\">**</ept> property (and related <bpt id=\"p2\">**</bpt>contentVersion<ept id=\"p2\">**</ept> property) that makes it possible to specify a linked template file that will be invoked, passing a set of parameters as input.",
      "pos": [
        24628,
        24830
      ]
    },
    {
      "content": "These can be seen in this template fragment:",
      "pos": [
        24831,
        24875
      ]
    },
    {
      "content": "From this first example, it is clear how azuredeploy.json in this scenario has been organized as a sort of orchestration mechanism, invoking a number of other template files.",
      "pos": [
        25903,
        26077
      ]
    },
    {
      "content": "Each file is responsible for part of the required deployment activities.",
      "pos": [
        26078,
        26150
      ]
    },
    {
      "content": "In particular, the following linked templates will be used for this deployment:",
      "pos": [
        26152,
        26231
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>shared-resource.json<ept id=\"p1\">**</ept>: contains the definition of all resources that will be shared across the deployment.",
      "pos": [
        26237,
        26346
      ]
    },
    {
      "content": "Examples are Storage accounts used to store a VM's OS disks and virtual networks.",
      "pos": [
        26347,
        26428
      ]
    },
    {
      "pos": [
        26433,
        26624
      ],
      "content": "<bpt id=\"p1\">**</bpt>jumpbox-resources-enabled.json<ept id=\"p1\">**</ept>: deploys the \"jump box\" VM and all related resources, such as network interface, public IP address, and the input endpoint used to SSH into the environment."
    },
    {
      "content": "After invoking these two templates, azuredeploy.json provisions all Spark cluster node VMs and connected resources (network adapters, private IPs, etc.).",
      "pos": [
        26626,
        26779
      ]
    },
    {
      "content": "This template will also deploy VM extensions (custom scripts for Linux) and invoke a bash script (spark-cluster-install.sh) to physically install and set up Spark on each node.",
      "pos": [
        26780,
        26956
      ]
    },
    {
      "content": "Let's drill down into <bpt id=\"p1\">*</bpt>how<ept id=\"p1\">*</ept> this last template, azuredeploy.json, is used, as it is one of the most interesting from a template development perspective.",
      "pos": [
        26960,
        27112
      ]
    },
    {
      "content": "One important concept to highlight is how a single template file can deploy multiple copies of a single resource type, and for each instance, it can set unique values for required settings.",
      "pos": [
        27113,
        27302
      ]
    },
    {
      "content": "This concept is known as <bpt id=\"p1\">**</bpt>resource looping<ept id=\"p1\">**</ept>.",
      "pos": [
        27303,
        27349
      ]
    },
    {
      "content": "A resource that uses the <bpt id=\"p1\">**</bpt>copy<ept id=\"p1\">**</ept> element will \"copy\" itself for the number of times specified in the <bpt id=\"p2\">**</bpt>count<ept id=\"p2\">**</ept> parameter of the <bpt id=\"p3\">**</bpt>copy<ept id=\"p3\">**</ept> element.",
      "pos": [
        27351,
        27497
      ]
    },
    {
      "content": "For all settings where it is necessary to specify unique values between different instances of the deployed resource, the <bpt id=\"p1\">**</bpt>copyindex()<ept id=\"p1\">**</ept> function can be used to obtain a numeric value indicating the current index in that particular resource loop creation.",
      "pos": [
        27499,
        27755
      ]
    },
    {
      "content": "In the following fragment from azuredeploy.json, you can see this concept applied to multiple network adapters, VMs, and VM extensions being created for the Spark cluster:",
      "pos": [
        27756,
        27927
      ]
    },
    {
      "content": "Another important concept in resource creation is the ability to specify dependencies and precedencies between resources, as you can see in the <bpt id=\"p1\">**</bpt>dependsOn<ept id=\"p1\">**</ept> JSON array.",
      "pos": [
        36020,
        36189
      ]
    },
    {
      "content": "In this particular template, you can see that the Spark cluster nodes are dependent on the shared resources and <bpt id=\"p1\">**</bpt>networkInterfaces<ept id=\"p1\">**</ept> resources being created first.",
      "pos": [
        36190,
        36354
      ]
    },
    {
      "content": "Another interesting fragment to explore is the one related to <bpt id=\"p1\">**</bpt>CustomScriptForLinux<ept id=\"p1\">**</ept> VM extensions.",
      "pos": [
        36356,
        36457
      ]
    },
    {
      "content": "These are installed as a separate type of resource, with a dependency on each cluster node.",
      "pos": [
        36458,
        36549
      ]
    },
    {
      "content": "In this case, this is used to install and set up Spark on each VM node.",
      "pos": [
        36551,
        36622
      ]
    },
    {
      "content": "Let's look at a snippet from the azuredeploy.json template that uses these:",
      "pos": [
        36624,
        36699
      ]
    },
    {
      "pos": [
        38759,
        38936
      ],
      "content": "Notice that the extension for the master and slave node resources executes different commands, defined in the <bpt id=\"p1\">**</bpt>commandToExecute<ept id=\"p1\">**</ept> property, as part of the provisioning process."
    },
    {
      "content": "If you look at the JSON snippet of the latest virtual machine extension, you can see that this resource depends on the virtual manchine resource and its network interface.",
      "pos": [
        38940,
        39111
      ]
    },
    {
      "content": "This indicates that these two resources need to be already deployed before provisioning and running this VM extension.",
      "pos": [
        39112,
        39230
      ]
    },
    {
      "content": "Also note the use of the <bpt id=\"p1\">**</bpt>copyindex()<ept id=\"p1\">**</ept> function to repeat this step for each slave virtual machine.",
      "pos": [
        39231,
        39332
      ]
    },
    {
      "content": "By familiarizing yourself with the other files included in this deployment, you will be able to understand all the details and best practices required to organize and orchestrate complex deployment strategies for multi-node solutions, based on any technology, leveraging Azure Resource Manager templates.",
      "pos": [
        39334,
        39638
      ]
    },
    {
      "content": "While not mandatory, a recommended approach is to structure your template files as highlighted by the following diagram:",
      "pos": [
        39639,
        39759
      ]
    },
    {
      "content": "spark-template-structure",
      "pos": [
        39763,
        39787
      ]
    },
    {
      "content": "In essence, this approach suggests to:",
      "pos": [
        39858,
        39896
      ]
    },
    {
      "content": "Define your core template file as a central orchestration point for all specific deployment activities, leveraging template linking to invoke sub-template executions.",
      "pos": [
        39902,
        40068
      ]
    },
    {
      "content": "Create a specific template file that will deploy all resources shared across all other specific deployment tasks (storage accounts, virtual network configuration, etc.).",
      "pos": [
        40073,
        40242
      ]
    },
    {
      "content": "This can be heavily reused between deployments that have similar requirements in terms of common infrastructure.",
      "pos": [
        40243,
        40355
      ]
    },
    {
      "content": "Include optional resource templates for spot requirements specific to a given resource.",
      "pos": [
        40360,
        40447
      ]
    },
    {
      "content": "For identical members of a group of resources (nodes in a cluster, etc.), create specific templates that leverage resource looping in order to deploy multiple instances with unique properties.",
      "pos": [
        40452,
        40644
      ]
    },
    {
      "content": "For all post-deployment tasks (product installation, configurations, etc.), leverage script deployment extensions and create scripts specific to each technology.",
      "pos": [
        40649,
        40810
      ]
    },
    {
      "pos": [
        40812,
        40923
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Azure Resource Manager Template Language<ept id=\"p1\">](../resource-group-authoring-templates.md)</ept>."
    },
    {
      "content": "Next steps",
      "pos": [
        40928,
        40938
      ]
    },
    {
      "pos": [
        40940,
        41022
      ],
      "content": "Read more details on <bpt id=\"p1\">[</bpt>deploying a template<ept id=\"p1\">](../resource-group-template-deploy.md)</ept>."
    },
    {
      "pos": [
        41024,
        41099
      ],
      "content": "Discover more <bpt id=\"p1\">[</bpt>application frameworks<ept id=\"p1\">](virtual-machines-app-frameworks.md)</ept>."
    },
    {
      "pos": [
        41101,
        41169
      ],
      "content": "<bpt id=\"p1\">[</bpt>Troubleshoot template deployments<ept id=\"p1\">](resource-group-deploy-debug.md)</ept>."
    }
  ],
  "content": "<properties\n    pageTitle=\"Spark on Ubuntu Resource Manager Template\"\n    description=\"Learn to easily deploy a new Spark cluster on Ubuntu VMs using Azure PowerShell or the Azure CLI and a Resource Manager template\"\n    services=\"virtual-machines\"\n    documentationCenter=\"\"\n    authors=\"paolosalvatori\"\n    manager=\"timlt\"\n    editor=\"tysonn\"/>\n\n<tags\n    ms.service=\"virtual-machines\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.tgt_pltfrm=\"vm-windows\"\n    ms.workload=\"multiple\"\n    ms.date=\"05/16/2015\"\n    ms.author=\"paolosalvatori\"/>\n\n# Spark on Ubuntu with a Resource Manager template\n\nApache Spark is a fast engine for large-scale data processing. Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing, and it can access diverse data sources, including HDFS, Spark, HBase, and S3.\n\nIn addition to running on the Mesos or YARN cluster managers, Spark provides a simple standalone deployment mode. This tutorial will walk you through how to use a sample Azure Resource Manager template to deploy a Spark cluster on Ubuntu VMs through [Azure PowerShell](../powershell-install-configure.md) or the [Azure CLI](../xplat-cli.md).\n\nThis template deploys a Spark cluster on the Ubuntu virtual machines. This template also provisions a storage account, virtual network, availability sets, public IP addresses and network interfaces required by the installation. The Spark cluster is created behind a subnet, so there is no public IP access to the cluster.  As part of the deployment, an optional \"jump box\" can be deployed. This \"jump box\" is an Ubuntu VM deployed in the subnet as well, but it *does* expose a public IP address with an open SSH port that you can connect to. Then from the \"jump box\", you can SSH to all the Spark VMs in the subnet.\n\nThe template utilizes a \"t-shirt size\" concept in order to specify a \"Small\", \"Medium\", or \"Large\" Spark cluster setup.  When the template language supports more dynamic template sizing, this could be changed to specify the number of Spark cluster master nodes and slave nodes, VM size, etc. For now, you can see the VM size and the number of masters and slaves defined in the file azuredeploy.json in the variables **tshirtSizeS**, **tshirtSizeM**, and **tshirtSizeL**:\n\n- S: 1 master, 1 slave\n- M: 1 master, 4 slaves\n- L: 1 master, 6 slaves\n\nNewly deployed clusters based on this template will have the topology described in the following diagram, although other topologies can be easily achieved by customizing the template presented in this article:\n\n![cluster-architecture](media/virtual-machines-spark-template/cluster-architecture.png)\n\nAs shown in the above image, the deployment topology consists of the following elements:\n\n-   A new storage account hosting the OS disk of newly created virtual machines.\n-   A virtual network with a subnet. All the virtual machines created by the template are provisioned inside this virtual network.\n-   An availability set for master and slave nodes.\n-   A master node in the newly created availability set.\n-   Four slave nodes running in the same virtual subnet and availability set as the master node.\n-   A jump-box VM located in the same virtual network and subnet that can be used to access the cluster.\n\nSpark version 3.0.0 is the default version and can be changed to any pre-built binaries available on the Spark repository. There is also a provision in the script to uncomment the build from source. A static IP address will be assigned to each Spark master node: 10.0.0.10. A static IP address will be assigned to each Spark slave node in order to work around the current limitation of not being able to dynamically compose a list of IP addresses from within the template. (By default, the first node will be assigned the private IP address of 10.0.0.30, the second node will be assigned 10.0.0.31, and so on.) To check deployment errors, go to the new Azure portal and look under **Resource Group** > **Last deployment** > **Check Operation Details**.\n\nBefore diving into more details related to Azure Resource Manager and the template we will use for this deployment, make sure you have Azure PowerShell or the Azure CLI configured correctly.\n\n[AZURE.INCLUDE [arm-getting-setup-powershell](../../includes/arm-getting-setup-powershell.md)]\n\n[AZURE.INCLUDE [xplat-getting-set-up-arm](../../includes/xplat-getting-set-up-arm.md)]\n\n## Create a Spark cluster by using a Resource Manager template\n\nFollow these steps to create a Spark cluster by using a Resource Manager template from the GitHub template repository, via either Azure PowerShell or the Azure CLI.\n\n### Step 1-a: Download the template files by using Azure PowerShell\n\nCreate a local folder for the JSON template and other associated files (for example, C:\\Azure\\Templates\\Spark).\n\nSubstitute in the folder name of your local folder and run these commands:\n\n```powershell\n# Define variables\n$folderName=\"C:\\Azure\\Templates\\Spark\\\"\n$baseAddress = \"https://raw.githubusercontent.com/azure/azure-quickstart-templates/master/spark-on-ubuntu/\"\n$webclient = New-Object System.Net.WebClient\n$files = $(\"azuredeploy.json\", `\n         \"azuredeploy-parameters.json\", `\n         \"jumpbox-resources-disabled.json\", `\n         \"jumpbox-resources-enabled.json\",\n         \"metadata.json\", `\n         \"shared-resources.json\", `\n         \"spark-cluster-install.sh\", `\n         \"jumpbox-resources-enabled.json\")\n\n# Download files\nforeach ($file in $files)\n{\n    $url = $baseAddress + $file\n    $filePath = $folderName + $file\n    $webclient.DownloadFile($baseAddress + $file, $folderName + $file)\n    Write-Host $url \"downloaded to\" $filePath\n}\n```\n\n### Step 1-b: Download the template files by using the Azure CLI\n\nClone the entire template repository by using a Git client of your choice, for example:\n\n    git clone https://github.com/Azure/azure-quickstart-templates C:\\Azure\\Templates\n\nWhen the cloning is completed, look for the **spark-on-ubuntu** folder in your C:\\Azure\\Templates directory.\n\n### Step 2: (optional) Understand the template parameters\n\nWhen you create a Spark cluster by using a template, you must specify a set of configuration parameters to deal with a number of required settings. By declaring these parameters in template definition, it's possible to specify values during deployment execution through an external file or at a command line.\n\nIn the \"parameters\" section at the top of the azuredeploy.json file, you'll find the set of parameters that are required by the template to configure a Spark cluster. Here is an example of the \"parameters\" section from this template's azuredeploy.json file:\n\n```json\n\"parameters\": {\n    \"adminUsername\": {\n        \"type\": \"string\",\n        \"metadata\": {\n            \"Description\": \"Administrator user name used when provisioning virtual machines\"\n        }\n    },\n    \"adminPassword\": {\n        \"type\": \"securestring\",\n        \"metadata\": {\n            \"Description\": \"Administrator password used when provisioning virtual machines\"\n        }\n    },\n    \"imagePublisher\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"Canonical\",\n        \"metadata\": {\n            \"Description\": \"Image publisher\"\n        }\n    },\n    \"imageOffer\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"UbuntuServer\",\n        \"metadata\": {\n            \"Description\": \"Image offer\"\n        }\n    },\n    \"imageSKU\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"14.04.2-LTS\",\n        \"metadata\": {\n            \"Description\": \"Image SKU\"\n        }\n    },\n    \"storageAccountName\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"uniquesparkstoragev12\",\n        \"metadata\": {\n            \"Description\": \"Unique namespace for the Storage account where the virtual machine's disks will be placed\"\n        }\n    },\n    \"region\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"West US\",\n        \"metadata\": {\n            \"Description\": \"Location where resources will be provisioned\"\n        }\n    },\n    \"virtualNetworkName\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"myVNETspark\",\n        \"metadata\": {\n            \"Description\": \"The arbitrary name of the virtual network provisioned for the cluster\"\n        }\n    },\n    \"dataDiskSize\": {\n        \"type\": \"int\",\n        \"defaultValue\": 100,\n        \"metadata\": {\n            \"Description\": \"Size of the data disk attached to Spark nodes (in GB)\"\n        }\n    },\n    \"addressPrefix\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"10.0.0.0/16\",\n        \"metadata\": {\n            \"Description\": \"The network address space for the virtual network\"\n        }\n    },\n    \"subnetName\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"Subnet-1\",\n        \"metadata\": {\n            \"Description\": \"Subnet name for the virtual network that resources will be provisioned into\"\n        }\n    },\n    \"subnetPrefix\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"10.0.0.0/24\",\n        \"metadata\": {\n            \"Description\": \"Address space for the virtual network subnet\"\n        }\n    },\n    \"sparkVersion\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"stable\",\n        \"metadata\": {\n            \"Description\": \"The version of the Spark package to be deployed on the cluster (or use 'stable' to pull in the latest and greatest)\"\n        }\n    },\n    \"sparkClusterName\": {\n        \"type\": \"string\",\n        \"metadata\": {\n            \"Description\": \"The arbitrary name of the Spark cluster (maps to cluster's configuration file name)\"\n        }\n    },\n    \"sparkNodeIPAddressPrefix\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"10.0.0.1\",\n        \"metadata\": {\n            \"Description\": \"The IP address prefix that will be used for constructing a static private IP address for each node in the cluster\"\n        }\n    },\n    \"sparkSlaveNodeIPAddressPrefix\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"10.0.0.3\",\n        \"metadata\": {\n            \"Description\": \"The IP address prefix that will be used for constructing a static private IP address for each node in the cluster\"\n        }\n    },\n    \"jumpbox\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"enabled\",\n        \"allowedValues\": [\n        \"enabled\",\n        \"disabled\"\n        ],\n        \"metadata\": {\n            \"Description\": \"The flag allowing to enable or disable provisioning of the jump-box VM that can be used to access the Spark nodes\"\n        }\n    },\n    \"tshirtSize\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"S\",\n        \"allowedValues\": [\n        \"S\",\n        \"M\",\n        \"L\"\n        ],\n        \"metadata\": {\n            \"Description\": \"T-shirt size of the Spark cluster\"\n        }\n    }\n}\n```\n\nEach parameter has details such as data type and allowed values. This allows for validation of parameters passed during template execution in an interactive mode (e.g., Azure PowerShell or Azure CLI), as well as a self-discovery UI that could be dynamically built by parsing the list of required parameters and their descriptions.\n\n### Step 3-a: Deploy a Spark cluster by using a template via Azure PowerShell\n\nPrepare a parameters file for your deployment by creating a JSON file containing runtime values for all parameters. This file will then be passed as a single entity to the deployment command. If you do not include a parameters file, Azure PowerShell will use any default values specified in the template, and then prompt you to fill in the remaining values.\n\nHere is an example set of parameters from the azuredeploy-parameters.json file. Note that you will need to provide valid values for the parameters **storageAccountName**, **adminUsername**, and **adminPassword**, plus any customizations to the other parameters:\n\n```json\n{\n  \"storageAccountName\": {\n    \"value\": \"paolosspark\"\n  },\n  \"adminUsername\": {\n    \"value\": \"paolos\"\n  },\n  \"adminPassword\": {\n    \"value\": \"Passw0rd!\"\n  },\n  \"region\": {\n    \"value\": \"West US\"\n  },\n  \"virtualNetworkName\": {\n    \"value\": \"sparkClustVnet\"\n  },\n  \"subnetName\": {\n    \"value\": \"Subnet1\"\n  },\n  \"addressPrefix\": {\n    \"value\": \"10.0.0.0/16\"\n  },\n  \"subnetPrefix\": {\n    \"value\": \"10.0.0.0/24\"\n  },\n  \"sparkVersion\": {\n    \"value\": \"3.0.0\"\n  },\n  \"sparkClusterName\": {\n    \"value\": \"spark-arm-cluster\"\n  },\n  \"sparkNodeIPAddressPrefix\": {\n        \"value\": \"10.0.0.1\"\n  },\n  \"sparkSlaveNodeIPAddressPrefix\": {\n    \"value\": \"10.0.0.3\"\n  },\n  \"jumpbox\": {\n    \"value\": \"enabled\"\n  },\n  \"tshirtSize\": {\n    \"value\": \"M\"\n  }\n}\n```\nFill in an Azure deployment name, resource group name, Azure location, and the folder of your saved JSON deployment file. Then run these commands:\n\n```powershell\n$deployName=\"<deployment name>\"\n$RGName=\"<resource group name>\"\n$locName=\"<Azure location, such as West US>\"\n$folderName=\"<folder name, such as C:\\Azure\\Templates\\Spark>\"\n$templateFile= $folderName + \"\\azuredeploy.json\"\n$templateParameterFile= $folderName + \"\\azuredeploy-parameters.json\"\nNew-AzureResourceGroup -Name $RGName -Location $locName\nNew-AzureResourceGroupDeployment -Name $deployName -ResourceGroupName $RGName -TemplateParamterFile $templateParameterFile -TemplateFile $templateFile\n```\n> [AZURE.NOTE] **$RGName** must be unique within your subscription.\n\nWhen you run the **New-AzureResourceGroupDeployment** command, this will extract parameter values from the JSON parameters file, and will start executing the template accordingly. Defining and using multiple parameter files with your different environments (test, production, etc.) will promote template reuse and simplify complex multi-environment solutions.\n\nWhen deploying, please keep in mind that a new Azure Storage account needs to be created, so the name you provide as the Storage account parameter must be unique and meet all requirements for an Azure Storage account (lowercase letters and numbers only).\n\nDuring the deployment, you will see something like this:\n\n```powershell\nPS C:\\> New-AzureResourceGroup -Name $RGName -Location $locName\n\nResourceGroupName : SparkResourceGroup\nLocation          : westus\nProvisioningState : Succeeded\nTags              :\nPermissions       :\n                    Actions  NotActions\n                    =======  ==========\n                    *\n\nResourceId        : /subscriptions/2018abc3-dbd9-4437-81a8-bb3cf56138ed/resourceGroups/sparkresourcegroup\n\nPS C:\\> New-AzureResourceGroupDeployment -Name $deployName -ResourceGroupName $RGName -TemplateParameterFile $templateParameterFile -TemplateFile $templateFile\nVERBOSE: 10:08:28 AM - Template is valid.\nVERBOSE: 10:08:28 AM - Create template deployment 'SparkTestDeployment'.\nVERBOSE: 10:08:37 AM - Resource Microsoft.Resources/deployments 'shared-resources' provisioning status is running\nVERBOSE: 10:09:11 AM - Resource Microsoft.Resources/deployments 'shared-resources' provisioning status is succeeded\nVERBOSE: 10:09:13 AM - Resource Microsoft.Network/networkInterfaces 'nicsl0' provisioning status is succeeded\nVERBOSE: 10:09:16 AM - Resource Microsoft.Network/networkInterfaces 'nic0' provisioning status is succeeded\nVERBOSE: 10:09:16 AM - Resource Microsoft.Resources/deployments 'jumpbox-resources' provisioning status is running\nVERBOSE: 10:09:24 AM - Resource Microsoft.Compute/virtualMachines 'mastervm0' provisioning status is running\nVERBOSE: 10:11:04 AM - Resource Microsoft.Compute/virtualMachines/extensions 'mastervm0/installsparkmaster'\nprovisioning status is running\nVERBOSE: 10:11:04 AM - Resource Microsoft.Compute/virtualMachines 'mastervm0' provisioning status is succeeded\nVERBOSE: 10:11:11 AM - Resource Microsoft.Compute/virtualMachines 'slavevm0' provisioning status is running\nVERBOSE: 10:11:42 AM - Resource Microsoft.Resources/deployments 'jumpbox-resources' provisioning status is succeeded\nVERBOSE: 10:13:10 AM - Resource Microsoft.Compute/virtualMachines 'slavevm0' provisioning status is succeeded\nVERBOSE: 10:13:15 AM - Resource Microsoft.Compute/virtualMachines/extensions 'slavevm0/installsparkslave' provisioning\nstatus is running\nVERBOSE: 10:16:58 AM - Resource Microsoft.Compute/virtualMachines/extensions 'mastervm0/installsparkmaster'\nprovisioning status is succeeded\nVERBOSE: 10:19:05 AM - Resource Microsoft.Compute/virtualMachines/extensions 'slavevm0/installsparkslave' provisioning\nstatus is succeeded\n\n\nDeploymentName    : SparkTestDeployment\nResourceGroupName : SparkResourceGroup\nProvisioningState : Succeeded\nTimestamp         : 5/5/2015 5:19:05 PM\nMode              : Incremental\nTemplateLink      :\nParameters        :\n                    Name             Type                       Value\n                    ===============  =========================  ==========\n                    adminUsername    String                     Paolos\n                    adminPassword    SecureString\n                    imagePublisher   String                     Canonical\n                    imageOffer       String                     UbuntuServer\n                    imageSKU         String                     14.04.2-LTS\n                    storageAccountName  String                  paolosspark\n                    region           String                     West US\n                    virtualNetworkName  String                  sparkClustVnet\n                    addressPrefix    String                     10.0.0.0/16\n                                        subnetName       String                     Subnet1\n                    subnetPrefix     String                     10.0.0.0/24\n                    sparkVersion     String                     3.0.0\n                    sparkClusterName  String                    spark-arm-cluster\n                    sparkNodeIPAddressPrefix  String            10.0.0.1\n                    sparkSlaveNodeIPAddressPrefix  String       10.0.0.3\n                    jumpbox          String                     enabled\n                    tshirtSize       String                     M\n```\n\nDuring and after deployment, you can check all the requests that were made during provisioning, including any errors that occurred.\n\nTo do that, go to the [Azure portal](https://portal.azure.com) and do the following:\n\n- Click **Browse** on the left-hand navigation bar, and then scroll down and click **Resource Groups**.\n- Click the resource group that you just created, to bring up the \"Resource Group\" blade.\n- By clicking the **Events** bar graph in the **Monitoring** part of the \"Resource Group\" blade, you can see the events for your deployment.\n- By clicking individual events, you can drill further down into the details of each individual operation made on behalf of the template.\n\n![portal-events](media/virtual-machines-spark-template/portal-events.png)\n\nAfter your tests, if you need to remove this resource group and all of its resources (the storage account, virtual machine, and virtual network), use this single command:\n\n```powershell\nRemove-AzureResourceGroup -Name \"<resource group name>\" -Force\n```\n\n### Step 3-b: Deploy a Spark cluster by using a template via the Azure CLI\n\nTo deploy a Spark cluster via the Azure CLI, first create a resource group by specifying a name and a location:\n\n    azure group create SparkResourceGroup \"West US\"\n\nPass this resource group name, the location of the JSON template file, and the location of the parameters file (see the above Azure PowerShell section for details) into the following command:\n\n    azure group deployment create SparkResourceGroup -f .\\azuredeploy.json -e .\\azuredeploy-parameters.json\n\nYou can check the status of individual resources deployments by using the following command:\n\n    azure group deployment list SparkResourceGroup\n\n## A tour of the Spark template structure and file organization\n\nIn order to design a robust and reusable Resource Manager template, additional thinking is needed to organize the series of complex and interrelated tasks required during the deployment of a complex solution like Spark. Leveraging Resource Manager template linking and resource looping in addition to script execution through related extensions, it's possible to implement a modular approach that can be reused with virtually any complex template-based deployment.\n\nThis diagram describes the relationships between all the files downloaded from GitHub for this deployment:\n\n![spark-files](media/virtual-machines-spark-template/spark-files.png)\n\nThis section steps you through the structure of the azuredeploy.json file for the Spark cluster.\n\nIf you have not already downloaded a copy of the template file, designate a local folder as the location for the file and create it (for example, C:\\Azure\\Templates\\Spark). Fill in the folder name and run these commands:\n\n```powershell\n$folderName=\"<folder name, such as C:\\Azure\\Templates\\Spark>\"\n$webclient = New-Object System.Net.WebClient\n$url = \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/spark-on-ubuntu/azuredeploy.json\"\n$filePath = $folderName + \"\\azuredeploy.json\"\n$webclient.DownloadFile($url,$filePath)\n```\n\nOpen the azuredeploy.json template in a text editor or tool of your choice. The following information describes the structure of the template file and the purpose of each section. Alternately, you can see the contents of this template in your browser from [here](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/spark-on-ubuntu/azuredeploy.json).\n\n### \"parameters\" section\n\nThe \"parameters\" section of azuredeploy.json specifies modifiable parameters that are used in this template. The aforementioned azuredeploy-parameters.json file is used to pass values into the \"parameters\" section of azuredeploy.json during template execution.\n\nHere is an example of a parameter for the \"t-shirt size\":\n\n```json\n\"tshirtSize\": {\n    \"type\": \"string\",\n    \"defaultValue\": \"S\",\n    \"allowedValues\": [\n        \"S\",\n        \"M\",\n        \"L\"\n    ],\n    \"metadata\": {\n        \"Description\": \"T-shirt size of the Spark deployment\"\n    }\n},\n```\n\n> [AZURE.NOTE] Notice that **defaultValue** may be specified, as well as **allowedValues**.\n\n### \"variables\" section\n\nThe \"variables\" section specifies variables that can be used throughout this template. This contains a number of fields (JSON data types or fragments) that will be set to constants or calculated values at execution time. Here are some examples that range from simple to more complex:\n\n```json\n\"variables\": {\n    \"vmStorageAccountContainerName\": \"vhd\",\n    \"vnetID\": \"[resourceId('Microsoft.Network/virtualNetworks',parameters('virtualNetworkName'))]\",\n    \"subnetRef\": \"[concat(variables('vnetID'),'/subnets/',parameters('subnetName'))]\",\n    \"computerNamePrefix\": \"sparknode\",\n    \"scriptUrl\": \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/spark-on-ubuntu/\",\n    \"templateUrl\": \"[variables('scriptUrl')]\",\n    \"sharedTemplateName\": \"shared-resources\",\n    \"jumpboxTemplateName\": \"jumpbox-resources-\",\n    \"tshirtSizeS\": {\n        \"numberOfMasterInstances\": 1,\n        \"numberOfSlavesInstances\" : 1,\n        \"vmSize\": \"Standard_A2\"\n    },\n    \"tshirtSizeM\": {\n        \"numberOfMasterInstances\": 1,\n        \"numberOfSlavesInstances\" : 4,\n        \"vmSize\": \"Standard_A4\"\n    },\n    \"tshirtSizeL\": {\n        \"numberOfMasterInstances\": 1,\n        \"numberOfSlavesInstances\" : 6,\n        \"vmSize\": \"Standard_A7\"\n    },\n    \"numberOfMasterInstances\": \"[variables(concat('tshirtSize', parameters('tshirtSize'))).numberOfMasterInstances]\",\n    \"numberOfSlavesInstances\": \"[variables(concat('tshirtSize', parameters('tshirtSize'))).numberOfSlavesInstances]\",\n    \"vmSize\": \"[variables(concat('tshirtSize', parameters('tshirtSize'))).vmSize]\"\n},\n```\n\nThe **vmStorageAccountContainerName** variable is an example of a simple name/value variable.  **vnetID** is an example of a variable that is calculated at run time using the functions **resourceId** and **parameters**.  The value of the **numberOfMasterInstances** and **vmSize** variables are calculated at run time using the **concat**, **variables**, and **parameters** functions.  \n\nIf you want to customize the size of the Spark cluster deployment, then you can change the properties of the variables **tshirtSizeS**, **tshirtSizeM**, and **tshirtSizeL** in the azuredeploy.json template.  \n\nMore information regarding the template language can be found on MSDN at [Azure Resource Manager Template Language](../resource-group-authoring-templates.md).\n\n\n### \"resources\" section\n\nThe \"resources\" section is where most of the action happens. Looking carefully inside this section, you can immediately identify two different cases. The first is an element defined of type `Microsoft.Resources/deployments` that essentially invokes a nested deployment within the main one. The second is the **templateLink** property (and related **contentVersion** property) that makes it possible to specify a linked template file that will be invoked, passing a set of parameters as input. These can be seen in this template fragment:\n\n```json\n\"resources\": [\n{\n    \"name\": \"shared-resources\",\n    \"type\": \"Microsoft.Resources/deployments\",\n    \"apiVersion\": \"2015-01-01\",\n    \"properties\": {\n        \"mode\": \"Incremental\",\n        \"templateLink\": {\n            \"uri\": \"[concat(variables('templateUrl'), variables('sharedTemplateName'), '.json')]\",\n            \"contentVersion\": \"1.0.0.0\"\n        },\n        \"parameters\": {\n            \"region\": {\n                \"value\": \"[parameters('region')]\"\n            },\n            \"storageAccountName\": {\n                \"value\": \"[parameters('storageAccountName')]\"\n            },\n            \"virtualNetworkName\": {\n                \"value\": \"[parameters('virtualNetworkName')]\"\n            },\n            \"addressPrefix\": {\n                \"value\": \"[parameters('addressPrefix')]\"\n            },\n            \"subnetName\": {\n                \"value\": \"[parameters('subnetName')]\"\n            },\n            \"subnetPrefix\": {\n                \"value\": \"[parameters('subnetPrefix')]\"\n            }\n        }\n    }\n},\n```\n\nFrom this first example, it is clear how azuredeploy.json in this scenario has been organized as a sort of orchestration mechanism, invoking a number of other template files. Each file is responsible for part of the required deployment activities.\n\nIn particular, the following linked templates will be used for this deployment:\n\n-   **shared-resource.json**: contains the definition of all resources that will be shared across the deployment. Examples are Storage accounts used to store a VM's OS disks and virtual networks.\n-   **jumpbox-resources-enabled.json**: deploys the \"jump box\" VM and all related resources, such as network interface, public IP address, and the input endpoint used to SSH into the environment.\n\nAfter invoking these two templates, azuredeploy.json provisions all Spark cluster node VMs and connected resources (network adapters, private IPs, etc.). This template will also deploy VM extensions (custom scripts for Linux) and invoke a bash script (spark-cluster-install.sh) to physically install and set up Spark on each node.  \n\nLet's drill down into *how* this last template, azuredeploy.json, is used, as it is one of the most interesting from a template development perspective. One important concept to highlight is how a single template file can deploy multiple copies of a single resource type, and for each instance, it can set unique values for required settings. This concept is known as **resource looping**.\n\nA resource that uses the **copy** element will \"copy\" itself for the number of times specified in the **count** parameter of the **copy** element.  For all settings where it is necessary to specify unique values between different instances of the deployed resource, the **copyindex()** function can be used to obtain a numeric value indicating the current index in that particular resource loop creation. In the following fragment from azuredeploy.json, you can see this concept applied to multiple network adapters, VMs, and VM extensions being created for the Spark cluster:\n\n```json\n{\n    \"apiVersion\": \"2015-05-01-preview\",\n    \"type\": \"Microsoft.Network/networkInterfaces\",\n    \"name\": \"[concat('nic', copyindex())]\",\n    \"location\": \"[parameters('region')]\",\n    \"copy\": {\n        \"name\": \"nicLoop\",\n        \"count\": \"[variables('numberOfMasterInstances')]\"\n    },\n    \"dependsOn\": [\n    \"[concat('Microsoft.Resources/deployments/', 'shared-resources')]\"\n    ],\n    \"properties\": {\n        \"ipConfigurations\": [\n        {\n            \"name\": \"ipconfig1\",\n            \"properties\": {\n                \"privateIPAllocationMethod\": \"Static\",\n                \"privateIPAddress\": \"[concat(parameters('sparkNodeIPAddressPrefix'), copyindex())]\",\n                \"subnet\": {\n                    \"id\": \"[variables('subnetRef')]\"\n                }\n            }\n        }\n        ]\n    }\n},\n{\n    \"apiVersion\": \"2015-05-01-preview\",\n    \"type\": \"Microsoft.Network/networkInterfaces\",\n    \"name\": \"[concat('nicsl', copyindex())]\",\n    \"location\": \"[parameters('region')]\",\n    \"copy\": {\n        \"name\": \"nicslLoop\",\n        \"count\": \"[variables('numberOfSlavesInstances')]\"\n    },\n    \"dependsOn\": [\n    \"[concat('Microsoft.Resources/deployments/', 'shared-resources')]\"\n    ],\n    \"properties\": {\n        \"ipConfigurations\": [\n        {\n            \"name\": \"ipconfig1\",\n            \"properties\": {\n                \"privateIPAllocationMethod\": \"Static\",\n                \"privateIPAddress\": \"[concat(parameters('sparkSlaveNodeIPAddressPrefix'), copyindex())]\",\n                \"subnet\": {\n                    \"id\": \"[variables('subnetRef')]\"\n                }\n            }\n        }\n        ]\n    }\n},\n{\n    \"apiVersion\": \"2015-05-01-preview\",\n    \"type\": \"Microsoft.Compute/virtualMachines\",\n    \"name\": \"[concat('mastervm', copyindex())]\",\n    \"location\": \"[parameters('region')]\",\n    \"copy\": {\n        \"name\": \"virtualMachineLoopMaster\",\n        \"count\": \"[variables('numberOfMasterInstances')]\"\n    },\n    \"dependsOn\": [\n    \"[concat('Microsoft.Resources/deployments/', 'shared-resources')]\",\n    \"[concat('Microsoft.Network/networkInterfaces/', 'nic', copyindex())]\"\n    ],\n    \"properties\": {\n        \"availabilitySet\": {\n            \"id\": \"[resourceId('Microsoft.Compute/availabilitySets', 'sparkCluserAS')]\"\n        },\n        \"hardwareProfile\": {\n            \"vmSize\": \"[variables('vmSize')]\"\n        },\n        \"osProfile\": {\n            \"computername\": \"[concat(variables('computerNamePrefix'), copyIndex())]\",\n            \"adminUsername\": \"[parameters('adminUsername')]\",\n            \"adminPassword\": \"[parameters('adminPassword')]\",\n            \"linuxConfiguration\": {\n                \"disablePasswordAuthentication\": \"false\"\n            }\n        },\n        \"storageProfile\": {\n            \"imageReference\": {\n                \"publisher\": \"[parameters('imagePublisher')]\",\n                \"offer\": \"[parameters('imageOffer')]\",\n                \"sku\" : \"[parameters('imageSKU')]\",\n                \"version\":\"latest\"\n            },\n            \"osDisk\" : {\n                \"name\": \"osdisk\",\n                \"vhd\": {\n                    \"uri\": \"[concat('https://',parameters('storageAccountName'),'.blob.core.windows.net/',variables('vmStorageAccountContainerName'),'/','osdisk', copyindex() ,'.vhd')]\"\n                    },\n                    \"caching\": \"ReadWrite\",\n                    \"createOption\": \"FromImage\"\n                }\n            },\n            \"networkProfile\": {\n                \"networkInterfaces\": [\n                {\n                    \"id\": \"[resourceId('Microsoft.Network/networkInterfaces',concat('nic', copyindex()))]\"\n                }\n                ]\n            }\n        }\n    },\n    {\n        \"apiVersion\": \"2015-05-01-preview\",\n        \"type\": \"Microsoft.Compute/virtualMachines\",\n        \"name\": \"[concat('slavevm', copyindex())]\",\n        \"location\": \"[parameters('region')]\",\n        \"copy\": {\n            \"name\": \"virtualMachineLoop\",\n            \"count\": \"[variables('numberOfSlavesInstances')]\"\n        },\n        \"dependsOn\": [\n        \"[concat('Microsoft.Resources/deployments/', 'shared-resources')]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', 'nicsl', copyindex())]\",\n        \"virtualMachineLoopMaster\"\n        ],\n        \"properties\": {\n            \"availabilitySet\": {\n                \"id\": \"[resourceId('Microsoft.Compute/availabilitySets', 'sparkCluserAS')]\"\n            },\n            \"hardwareProfile\": {\n                \"vmSize\": \"[variables('vmSize')]\"\n            },\n            \"osProfile\": {\n                \"computername\": \"[concat(variables('computerNamePrefix'),'sl', copyIndex())]\",\n                \"adminUsername\": \"[parameters('adminUsername')]\",\n                \"adminPassword\": \"[parameters('adminPassword')]\",\n                \"linuxConfiguration\": {\n                    \"disablePasswordAuthentication\": \"false\"\n                }\n            },\n            \"storageProfile\": {\n                \"imageReference\": {\n                    \"publisher\": \"[parameters('imagePublisher')]\",\n                    \"offer\": \"[parameters('imageOffer')]\",\n                    \"sku\" : \"[parameters('imageSKU')]\",\n                    \"version\":\"latest\"\n                },\n                \"osDisk\" : {\n                    \"name\": \"osdisksl\",\n                    \"vhd\": {\n                        \"uri\": \"[concat('https://',parameters('storageAccountName'),'.blob.core.windows.net/',variables('vmStorageAccountContainerName'),'/','osdisksl', copyindex() ,'.vhd')]\"\n                    },\n                    \"caching\": \"ReadWrite\",\n                    \"createOption\": \"FromImage\"\n                }\n            },\n            \"networkProfile\": {\n                \"networkInterfaces\": [\n                {\n                    \"id\": \"[resourceId('Microsoft.Network/networkInterfaces',concat('nicsl', copyindex()))]\"\n                }\n                ]\n            }\n        }\n    },\n    {\n        \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n        \"name\": \"[concat('mastervm', copyindex(), '/installsparkmaster')]\",\n        \"apiVersion\": \"2015-05-01-preview\",\n        \"location\": \"[parameters('region')]\",\n        \"copy\": {\n            \"name\": \"virtualMachineExtensionsLoopMaster\",\n            \"count\": \"[variables('numberOfMasterInstances')]\"\n        },\n        \"dependsOn\": [\n        \"[concat('Microsoft.Compute/virtualMachines/', 'mastervm', copyindex())]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', 'nic', copyindex())]\"\n        ],\n        \"properties\": {\n            \"publisher\": \"Microsoft.OSTCExtensions\",\n            \"type\": \"CustomScriptForLinux\",\n            \"typeHandlerVersion\": \"1.2\",\n            \"settings\": {\n                \"fileUris\": [\n                \"[concat(variables('scriptUrl'), 'spark-cluster-install.sh')]\"\n                ],\n                \"commandToExecute\": \"[concat('bash spark-cluster-install.sh -k ',parameters('sparkVersion'),' -d ', reference('nic0').ipConfigurations[0].properties.privateIPAddress,' -s ',variables('numberOfSlavesInstances'),' -m ', ' 1 ')]\"\n            }\n        }\n    },\n    {\n        \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n        \"name\": \"[concat('slavevm', copyindex(), '/installsparkslave')]\",\n        \"apiVersion\": \"2015-05-01-preview\",\n        \"location\": \"[parameters('region')]\",\n        \"copy\": {\n            \"name\": \"virtualMachineExtensionsLoopSlave\",\n            \"count\": \"[variables('numberOfSlavesInstances')]\"\n        },\n        \"dependsOn\": [\n        \"[concat('Microsoft.Compute/virtualMachines/', 'slavevm', copyindex())]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', 'nicsl', copyindex())]\"\n        ],\n        \"properties\": {\n            \"publisher\": \"Microsoft.OSTCExtensions\",\n            \"type\": \"CustomScriptForLinux\",\n            \"typeHandlerVersion\": \"1.2\",\n            \"settings\": {\n                \"fileUris\": [\n                \"[concat(variables('scriptUrl'), 'spark-cluster-install.sh')]\"\n                ],\n                \"commandToExecute\": \"[concat('bash spark-cluster-install.sh -k ',parameters('sparkVersion'),' -m ', ' 0 ')]\"\n            }\n        }\n    }\n```\n\nAnother important concept in resource creation is the ability to specify dependencies and precedencies between resources, as you can see in the **dependsOn** JSON array. In this particular template, you can see that the Spark cluster nodes are dependent on the shared resources and **networkInterfaces** resources being created first.\n\nAnother interesting fragment to explore is the one related to **CustomScriptForLinux** VM extensions. These are installed as a separate type of resource, with a dependency on each cluster node.  In this case, this is used to install and set up Spark on each VM node.  Let's look at a snippet from the azuredeploy.json template that uses these:\n\n```json\n{\n    \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n    \"name\": \"[concat('mastervm', copyindex(), '/installsparkmaster')]\",\n    \"apiVersion\": \"2015-05-01-preview\",\n    \"location\": \"[parameters('region')]\",\n    \"copy\": {\n        \"name\": \"virtualMachineExtensionsLoopMaster\",\n        \"count\": \"[variables('numberOfMasterInstances')]\"\n    },\n    \"dependsOn\": [\n        \"[concat('Microsoft.Compute/virtualMachines/', 'mastervm', copyindex())]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', 'nic', copyindex())]\"\n    ],\n    \"properties\": {\n        \"publisher\": \"Microsoft.OSTCExtensions\",\n        \"type\": \"CustomScriptForLinux\",\n        \"typeHandlerVersion\": \"1.2\",\n        \"settings\": {\n            \"fileUris\": [\n                \"[concat(variables('scriptUrl'), 'spark-cluster-install.sh')]\"\n            ],\n            \"commandToExecute\": \"[concat('bash spark-cluster-install.sh -k ',parameters('sparkVersion'),' -d ', reference('nic0').ipConfigurations[0].properties.privateIPAddress,' -s ',variables('numberOfSlavesInstances'),' -m ', ' 1 ')]\"\n        }\n    }\n},\n{\n    \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n    \"name\": \"[concat('slavevm', copyindex(), '/installsparkslave')]\",\n    \"apiVersion\": \"2015-05-01-preview\",\n    \"location\": \"[parameters('region')]\",\n    \"copy\": {\n        \"name\": \"virtualMachineExtensionsLoopSlave\",\n        \"count\": \"[variables('numberOfSlavesInstances')]\"\n    },\n    \"dependsOn\": [\n        \"[concat('Microsoft.Compute/virtualMachines/', 'slavevm', copyindex())]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', 'nicsl', copyindex())]\"\n    ],\n    \"properties\": {\n        \"publisher\": \"Microsoft.OSTCExtensions\",\n        \"type\": \"CustomScriptForLinux\",\n        \"typeHandlerVersion\": \"1.2\",\n        \"settings\": {\n            \"fileUris\": [\n                \"[concat(variables('scriptUrl'), 'spark-cluster-install.sh')]\"\n            ],\n            \"commandToExecute\": \"[concat('bash spark-cluster-install.sh -k ',parameters('sparkVersion'),' -m ', ' 0 ')]\"\n        }\n    }\n}\n```\n\nNotice that the extension for the master and slave node resources executes different commands, defined in the **commandToExecute** property, as part of the provisioning process.  \n\nIf you look at the JSON snippet of the latest virtual machine extension, you can see that this resource depends on the virtual manchine resource and its network interface. This indicates that these two resources need to be already deployed before provisioning and running this VM extension. Also note the use of the **copyindex()** function to repeat this step for each slave virtual machine.\n\nBy familiarizing yourself with the other files included in this deployment, you will be able to understand all the details and best practices required to organize and orchestrate complex deployment strategies for multi-node solutions, based on any technology, leveraging Azure Resource Manager templates. While not mandatory, a recommended approach is to structure your template files as highlighted by the following diagram:\n\n![spark-template-structure](media/virtual-machines-spark-template/spark-template-structure.png)\n\nIn essence, this approach suggests to:\n\n-   Define your core template file as a central orchestration point for all specific deployment activities, leveraging template linking to invoke sub-template executions.\n-   Create a specific template file that will deploy all resources shared across all other specific deployment tasks (storage accounts, virtual network configuration, etc.). This can be heavily reused between deployments that have similar requirements in terms of common infrastructure.\n-   Include optional resource templates for spot requirements specific to a given resource.\n-   For identical members of a group of resources (nodes in a cluster, etc.), create specific templates that leverage resource looping in order to deploy multiple instances with unique properties.\n-   For all post-deployment tasks (product installation, configurations, etc.), leverage script deployment extensions and create scripts specific to each technology.\n\nFor more information, see [Azure Resource Manager Template Language](../resource-group-authoring-templates.md).\n\n## Next steps\n\nRead more details on [deploying a template](../resource-group-template-deploy.md).\n\nDiscover more [application frameworks](virtual-machines-app-frameworks.md).\n\n[Troubleshoot template deployments](resource-group-deploy-debug.md).\n"
}