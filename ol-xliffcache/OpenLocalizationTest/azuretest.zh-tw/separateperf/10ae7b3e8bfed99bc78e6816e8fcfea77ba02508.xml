{
  "nodes": [
    {
      "content": "Import data to DocumentDB | Microsoft Azure",
      "pos": [
        28,
        71
      ]
    },
    {
      "content": "Learn how to use the open source DocumentDB data migration tool to import data to DocumentDB from various sources, including JSON files, CSV files, SQL, MongoDB, Azure Table storage, Amazon DynamoDB, and DocumentDB collections.",
      "pos": [
        91,
        318
      ]
    },
    {
      "content": "Import data to DocumentDB",
      "pos": [
        642,
        667
      ]
    },
    {
      "pos": [
        671,
        981
      ],
      "content": "This article shows you how to use the open source DocumentDB data migration tool to import data to <bpt id=\"p1\">[</bpt>Microsoft Azure DocumentDB<ept id=\"p1\">](http://azure.microsoft.com/services/documentdb/)</ept> from various sources, including JSON files, CSV files, SQL, MongoDB, Azure Table storage, Amazon DynamoDB and DocumentDB collections."
    },
    {
      "content": "After reading this article, you'll be able to answer the following questions:",
      "pos": [
        983,
        1060
      ]
    },
    {
      "content": "How can I import JSON file data to DocumentDB?",
      "pos": [
        1068,
        1114
      ]
    },
    {
      "content": "How can I import CSV file data to DocumentDB?",
      "pos": [
        1119,
        1164
      ]
    },
    {
      "content": "How can I import SQL Server data to DocumentDB?",
      "pos": [
        1169,
        1216
      ]
    },
    {
      "content": "How can I import MongoDB data to DocumentDB?",
      "pos": [
        1221,
        1265
      ]
    },
    {
      "content": "How can I import data from Azure Table storage to DocumentDB?",
      "pos": [
        1270,
        1331
      ]
    },
    {
      "content": "How can I import data from Amazon DynamoDB to DocumentDB?",
      "pos": [
        1336,
        1393
      ]
    },
    {
      "content": "How can I import data from HBase to DocumentDB",
      "pos": [
        1398,
        1444
      ]
    },
    {
      "content": "How can I migrate data between DocumentDB collections?",
      "pos": [
        1449,
        1503
      ]
    },
    {
      "pos": [
        1507,
        1546
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"Prerequisites\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "Before following the instructions in this article, ensure that you have the following installed:",
      "pos": [
        1551,
        1647
      ]
    },
    {
      "pos": [
        1651,
        1733
      ],
      "content": "<bpt id=\"p1\">[</bpt>Microsoft .NET Framework 4.51<ept id=\"p1\">](http://www.microsoft.com/net/downloads)</ept> or higher."
    },
    {
      "pos": [
        1737,
        1805
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"Overviewl\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Overview of the DocumentDB Data Migration Tool"
    },
    {
      "content": "The DocumentDB Data Migration tool is an open source solution that imports data to DocumentDB from a variety of sources, including:",
      "pos": [
        1810,
        1941
      ]
    },
    {
      "content": "JSON files",
      "pos": [
        1945,
        1955
      ]
    },
    {
      "content": "MongoDB",
      "pos": [
        1958,
        1965
      ]
    },
    {
      "content": "SQL Server",
      "pos": [
        1968,
        1978
      ]
    },
    {
      "content": "CSV files",
      "pos": [
        1981,
        1990
      ]
    },
    {
      "content": "Azure Table storage",
      "pos": [
        1993,
        2012
      ]
    },
    {
      "content": "Amazon DynamoDB",
      "pos": [
        2015,
        2030
      ]
    },
    {
      "content": "HBase",
      "pos": [
        2033,
        2038
      ]
    },
    {
      "content": "DocumentDB collections",
      "pos": [
        2041,
        2063
      ]
    },
    {
      "content": "While the import tool includes a graphical user interface (dtui.exe), it can also be driven from the command line (dt.exe).",
      "pos": [
        2065,
        2188
      ]
    },
    {
      "content": "In fact, there is an option to output the associated command after setting up an import through the UI.",
      "pos": [
        2190,
        2293
      ]
    },
    {
      "content": "Tabular source data (e.g. SQL Server or CSV files) can be transformed such that hierarchical relationships (subdocuments) can be created during import.",
      "pos": [
        2295,
        2446
      ]
    },
    {
      "content": "Keep reading to learn more about source options, sample command lines to import from each source, target options, and viewing import results.",
      "pos": [
        2448,
        2589
      ]
    },
    {
      "pos": [
        2594,
        2659
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"Install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Installing the DocumentDB Data Migration Tool"
    },
    {
      "content": "The migration tool source code is available on GitHub in <bpt id=\"p1\">[</bpt>this repository<ept id=\"p1\">](https://github.com/azure/azure-documentdb-datamigrationtool)</ept> and a compiled version is available from <bpt id=\"p2\">[</bpt>Microsoft Download Center<ept id=\"p2\">](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d)</ept>.",
      "pos": [
        2664,
        2964
      ]
    },
    {
      "content": "You may either compile the solution or simply download and extract the compiled version to a directory of your choice.",
      "pos": [
        2966,
        3084
      ]
    },
    {
      "content": "Then run either:",
      "pos": [
        3086,
        3102
      ]
    },
    {
      "content": "Dtui.exe: Graphical interface version of the tool",
      "pos": [
        3106,
        3155
      ]
    },
    {
      "content": "Dt.exe: Command-line version of the tool",
      "pos": [
        3158,
        3198
      ]
    },
    {
      "pos": [
        3202,
        3236
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"JSON\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import JSON files"
    },
    {
      "content": "The JSON file source importer option allows you to import one or more single document JSON files or JSON files that each contain an array of JSON documents.",
      "pos": [
        3241,
        3397
      ]
    },
    {
      "content": "When adding folders that contain JSON files to import, you have the option of recursively searching for files in subfolders.",
      "pos": [
        3399,
        3523
      ]
    },
    {
      "content": "Screenshot of JSON file source options",
      "pos": [
        3527,
        3565
      ]
    },
    {
      "content": "Here are some command line samples to import JSON files:",
      "pos": [
        3615,
        3671
      ]
    },
    {
      "pos": [
        5350,
        5389
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"MongoDB\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from MongoDB"
    },
    {
      "content": "The MongoDB source importer option allows you to import from an individual MongoDB collection and optionally filter documents using a query and/or modify the document structure by using a projection.",
      "pos": [
        5394,
        5593
      ]
    },
    {
      "content": "Screenshot of MongoDB source options",
      "pos": [
        5599,
        5635
      ]
    },
    {
      "content": "The connection string is in the standard MongoDB format:",
      "pos": [
        5688,
        5744
      ]
    },
    {
      "pos": [
        5810,
        5939
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the MongoDB instance specified in the connection string field can be accessed."
    },
    {
      "content": "Enter the name of the collection from which data will be imported.",
      "pos": [
        5942,
        6008
      ]
    },
    {
      "content": "You may optionally specify or provide a file for a query (e.g. {pop: {$gt:5000}}) and/or projection (e.g. {loc:0}) to both filter and shape the data to be imported.",
      "pos": [
        6010,
        6174
      ]
    },
    {
      "content": "Here are some command line samples to import from MongoDB:",
      "pos": [
        6176,
        6234
      ]
    },
    {
      "pos": [
        7075,
        7128
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"MongoDBExport\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import MongoDB export files"
    },
    {
      "content": "The MongoDB export JSON file source importer option allows you to import one or more JSON files produced from the mongoexport utility.",
      "pos": [
        7133,
        7267
      ]
    },
    {
      "content": "Screenshot of MongoDB export source options",
      "pos": [
        7273,
        7316
      ]
    },
    {
      "content": "When adding folders that contain MongoDB export JSON files for import, you have the option of recursively searching for files in subfolders.",
      "pos": [
        7375,
        7515
      ]
    },
    {
      "content": "Here is a command line sample to import from MongoDB export JSON files:",
      "pos": [
        7517,
        7588
      ]
    },
    {
      "pos": [
        7866,
        7904
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"SQL\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from SQL Server"
    },
    {
      "content": "The SQL source importer option allows you to import from an individual SQL Server database and optionally filter the records to be imported using a query.",
      "pos": [
        7909,
        8063
      ]
    },
    {
      "content": "In addition, you can modify the document structure by specifying a nesting separator (more on that in a moment).",
      "pos": [
        8065,
        8177
      ]
    },
    {
      "content": "Screenshot of SQL source options",
      "pos": [
        8183,
        8215
      ]
    },
    {
      "content": "The format of the connection string is the standard SQL connection string format.",
      "pos": [
        8270,
        8351
      ]
    },
    {
      "pos": [
        8355,
        8487
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the SQL Server instance specified in the connection string field can be accessed."
    },
    {
      "content": "The nesting separator property is used to create hierarchical relationships (sub-documents) during import.",
      "pos": [
        8490,
        8596
      ]
    },
    {
      "content": "Consider the following SQL query:",
      "pos": [
        8598,
        8631
      ]
    },
    {
      "content": "select CAST(BusinessEntityID AS varchar) as Id, Name, AddressType as [Address.AddressType], AddressLine1 as [Address.AddressLine1], City as [Address.Location.City], StateProvinceName as [Address.Location.StateProvinceName], PostalCode as [Address.PostalCode], CountryRegionName as [Address.CountryRegionName] from Sales.vStoreWithAddresses WHERE AddressType='Main Office'",
      "pos": [
        8634,
        9005
      ]
    },
    {
      "content": "Which returns the following (partial) results:",
      "pos": [
        9008,
        9054
      ]
    },
    {
      "content": "Screenshot of SQL query results",
      "pos": [
        9058,
        9089
      ]
    },
    {
      "content": "Note the aliases such as Address.AddressType and Address.Location.StateProvinceName.",
      "pos": [
        9144,
        9228
      ]
    },
    {
      "content": "By specifying a nesting separator of ‘.’, the import tool creates Address and Address.Location subdocuments during the import.",
      "pos": [
        9230,
        9356
      ]
    },
    {
      "content": "Here is an example of a resulting document in DocumentDB:",
      "pos": [
        9358,
        9415
      ]
    },
    {
      "content": "{",
      "pos": [
        9418,
        9419
      ]
    },
    {
      "content": "\"id\": \"956\",",
      "pos": [
        9422,
        9434
      ]
    },
    {
      "content": "\"Name\": \"Finer Sales and Service\",",
      "pos": [
        9437,
        9471
      ]
    },
    {
      "content": "\"Address\": {",
      "pos": [
        9474,
        9486
      ]
    },
    {
      "content": "\"AddressType\": \"Main Office\",",
      "pos": [
        9491,
        9520
      ]
    },
    {
      "content": "\"AddressLine1\": \"#500-75 O'Connor Street\",",
      "pos": [
        9525,
        9567
      ]
    },
    {
      "content": "\"Location\": {",
      "pos": [
        9572,
        9585
      ]
    },
    {
      "content": "\"City\": \"Ottawa\",",
      "pos": [
        9592,
        9609
      ]
    },
    {
      "content": "\"StateProvinceName\": \"Ontario\"",
      "pos": [
        9616,
        9646
      ]
    },
    {
      "content": "},",
      "pos": [
        9651,
        9653
      ]
    },
    {
      "content": "\"PostalCode\": \"K4B 1S2\",",
      "pos": [
        9658,
        9682
      ]
    },
    {
      "content": "\"CountryRegionName\": \"Canada\"",
      "pos": [
        9687,
        9716
      ]
    },
    {
      "content": "}",
      "pos": [
        9719,
        9720
      ]
    },
    {
      "content": "}",
      "pos": [
        9721,
        9722
      ]
    },
    {
      "content": "Here are some command line samples to import from SQL Server:",
      "pos": [
        9726,
        9787
      ]
    },
    {
      "pos": [
        11115,
        11147
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"CSV\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import CSV files"
    },
    {
      "content": "The CSV file source importer option enables you to import one or more CSV files.",
      "pos": [
        11152,
        11232
      ]
    },
    {
      "content": "When adding folders that contain CSV files for import, you have the option of recursively searching for files in subfolders.",
      "pos": [
        11234,
        11358
      ]
    },
    {
      "content": "Screenshot of CSV source options",
      "pos": [
        11362,
        11394
      ]
    },
    {
      "content": "Similar to the SQL source, the nesting separator property may be used to create hierarchical relationships (sub-documents) during import.",
      "pos": [
        11441,
        11578
      ]
    },
    {
      "content": "Consider the following CSV header row and data rows:",
      "pos": [
        11580,
        11632
      ]
    },
    {
      "content": "Screenshot of CSV sample records",
      "pos": [
        11636,
        11668
      ]
    },
    {
      "content": "Note the aliases such as DomainInfo.Domain_Name and RedirectInfo.Redirecting.",
      "pos": [
        11717,
        11794
      ]
    },
    {
      "content": "By specifying a nesting separator of ‘.’, the import tool will create DomainInfo and RedirectInfo subdocuments during the import.",
      "pos": [
        11796,
        11925
      ]
    },
    {
      "content": "Here is an example of a resulting document in DocumentDB:",
      "pos": [
        11927,
        11984
      ]
    },
    {
      "content": "{",
      "pos": [
        11987,
        11988
      ]
    },
    {
      "content": "\"DomainInfo\": {",
      "pos": [
        11991,
        12006
      ]
    },
    {
      "content": "\"Domain_Name\": \"ACUS.GOV\",",
      "pos": [
        12011,
        12037
      ]
    },
    {
      "content": "\"Domain_Name_Address\": \"http://www.ACUS.GOV\"",
      "pos": [
        12042,
        12086
      ]
    },
    {
      "content": "},",
      "pos": [
        12089,
        12091
      ]
    },
    {
      "content": "\"Federal Agency\": \"Administrative Conference of the United States\",",
      "pos": [
        12094,
        12161
      ]
    },
    {
      "content": "\"RedirectInfo\": {",
      "pos": [
        12164,
        12181
      ]
    },
    {
      "content": "\"Redirecting\": \"0\",",
      "pos": [
        12186,
        12205
      ]
    },
    {
      "content": "\"Redirect_Destination\": \"\"",
      "pos": [
        12210,
        12236
      ]
    },
    {
      "content": "},",
      "pos": [
        12239,
        12241
      ]
    },
    {
      "content": "\"id\": \"9cc565c5-ebcd-1c03-ebd3-cc3e2ecd814d\"",
      "pos": [
        12244,
        12288
      ]
    },
    {
      "content": "}",
      "pos": [
        12289,
        12290
      ]
    },
    {
      "content": "The import tool will attempt to infer type information for unquoted values in CSV files (quoted values are always treated as strings).",
      "pos": [
        12293,
        12427
      ]
    },
    {
      "content": "Types are identified in the following order: number, datetime, boolean.",
      "pos": [
        12429,
        12500
      ]
    },
    {
      "content": "There are two other things to note about CSV import:",
      "pos": [
        12504,
        12556
      ]
    },
    {
      "content": "By default, unquoted values are always trimmed for tabs and spaces, while quoted values are preserved as-is.",
      "pos": [
        12563,
        12671
      ]
    },
    {
      "content": "This behavior can be overridden with the Trim quoted values checkbox or the /s.TrimQuoted command line option.",
      "pos": [
        12673,
        12783
      ]
    },
    {
      "content": "By default, an unquoted null is treated as a null value.",
      "pos": [
        12789,
        12845
      ]
    },
    {
      "content": "This behavior can be overridden (i.e. treat an unquoted null as a “null” string) with the Treat unquoted NULL as string checkbox or the /s.NoUnquotedNulls command line option.",
      "pos": [
        12847,
        13022
      ]
    },
    {
      "content": "Here is a command line sample for CSV import:",
      "pos": [
        13025,
        13070
      ]
    },
    {
      "pos": [
        13325,
        13385
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"AzureTableSource\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from Azure Table storage"
    },
    {
      "content": "The Azure Table storage source importer option allows you to import from an individual Azure Table storage table and optionally filter the table entities to be imported.",
      "pos": [
        13390,
        13559
      ]
    },
    {
      "content": "Screenshot of Azure Table storage source options",
      "pos": [
        13565,
        13613
      ]
    },
    {
      "content": "The format of the Azure Table storage connection string is:",
      "pos": [
        13669,
        13728
      ]
    },
    {
      "pos": [
        13826,
        13967
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the Azure Table storage instance specified in the connection string field can be accessed."
    },
    {
      "content": "Enter the name of the Azure table from which data will be imported.",
      "pos": [
        13970,
        14037
      ]
    },
    {
      "content": "You may optionally specify a <bpt id=\"p1\">[</bpt>filter<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/ff683669.aspx)</ept>.",
      "pos": [
        14039,
        14133
      ]
    },
    {
      "content": "The Azure Table storage source importer option has the following additional options:",
      "pos": [
        14135,
        14219
      ]
    },
    {
      "content": "Include Internal Fields",
      "pos": [
        14224,
        14247
      ]
    },
    {
      "content": "All - Include all internal fields (PartitionKey, RowKey, and Timestamp)",
      "pos": [
        14256,
        14327
      ]
    },
    {
      "content": "None - Exclude all internal fields",
      "pos": [
        14335,
        14369
      ]
    },
    {
      "content": "RowKey - Only include the RowKey field",
      "pos": [
        14377,
        14415
      ]
    },
    {
      "content": "Select Columns",
      "pos": [
        14419,
        14433
      ]
    },
    {
      "content": "Azure Table storage filters do not support projections.",
      "pos": [
        14441,
        14496
      ]
    },
    {
      "content": "If you want to only import specific Azure Table entity properties, add them to the Select Columns list.",
      "pos": [
        14498,
        14601
      ]
    },
    {
      "content": "All other entity properties will be ignored.",
      "pos": [
        14603,
        14647
      ]
    },
    {
      "content": "Here is a command line sample to import from Azure Table storage:",
      "pos": [
        14649,
        14714
      ]
    },
    {
      "pos": [
        15171,
        15225
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"DynamoDBSource\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from Amazon DynamoDB"
    },
    {
      "content": "The Amazon DynamoDB source importer option allows you to import from an individual Amazon DynamoDB table and optionally filter the entities to be imported.",
      "pos": [
        15230,
        15385
      ]
    },
    {
      "content": "Several templates are provided so that setting up an import is as easy as possible.",
      "pos": [
        15387,
        15470
      ]
    },
    {
      "content": "Screenshot of Amazon DynamoDB source options",
      "pos": [
        15474,
        15518
      ]
    },
    {
      "content": "Screenshot of Amazon DynamoDB source options",
      "pos": [
        15575,
        15619
      ]
    },
    {
      "content": "The format of the Amazon DynamoDB connection string is:",
      "pos": [
        15674,
        15729
      ]
    },
    {
      "pos": [
        15814,
        15951
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the Amazon DynamoDB instance specified in the connection string field can be accessed."
    },
    {
      "content": "Here is a command line sample to import from Amazon DynamoDB:",
      "pos": [
        15954,
        16015
      ]
    },
    {
      "pos": [
        16407,
        16466
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"BlobImport\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import files from Azure Blob storage"
    },
    {
      "content": "The JSON file, MongoDB export file, and CSV file source importer options allow you to import one or more files from Azure Blob storage.",
      "pos": [
        16470,
        16605
      ]
    },
    {
      "content": "After specifying a Blob container URL and Account Key, simply provide a regular expression to select the file(s) to import.",
      "pos": [
        16607,
        16730
      ]
    },
    {
      "content": "Screenshot of Blob file source options",
      "pos": [
        16734,
        16772
      ]
    },
    {
      "content": "Here is command line sample to import JSON files from Azure Blob storage:",
      "pos": [
        16822,
        16895
      ]
    },
    {
      "pos": [
        17169,
        17220
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"DocumentDBSource\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from DocumentDB"
    },
    {
      "content": "The DocumentDB source importer option allows you to import data from one or more DocumentDB collections and optionally filter documents using a query.",
      "pos": [
        17225,
        17375
      ]
    },
    {
      "content": "Screenshot of DocumentDB source options",
      "pos": [
        17381,
        17420
      ]
    },
    {
      "content": "The format of the DocumentDB connection string is:",
      "pos": [
        17476,
        17526
      ]
    },
    {
      "pos": [
        17633,
        17765
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed."
    },
    {
      "content": "To import from a single DocumentDB collection, enter the name of the collection from which data will be imported.",
      "pos": [
        17768,
        17881
      ]
    },
    {
      "content": "To import from multiple DocumentDB collections, provide a regular expression to match one or more collection names (e.g. collection01 | collection02 | collection03).",
      "pos": [
        17883,
        18048
      ]
    },
    {
      "content": "You may optionally specify, or provide a file for, a query to both filter and shape the data to be imported.",
      "pos": [
        18050,
        18158
      ]
    },
    {
      "pos": [
        18162,
        18378
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Since the collection field accepts regular expressions, if you are importing from a single collection whose name contains regular expression characters, then those characters must be escaped accordingly."
    },
    {
      "content": "The DocumentDB source importer option has the following advanced options:",
      "pos": [
        18380,
        18453
      ]
    },
    {
      "content": "Include Internal Fields: Specifies whether or not to include DocumentDB document system properties in the export (e.g. _rid, _ts).",
      "pos": [
        18458,
        18588
      ]
    },
    {
      "content": "Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        18592,
        18761
      ]
    },
    {
      "content": "Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        18765,
        18925
      ]
    },
    {
      "content": "Connection Mode: Specifies the connection mode to use with DocumentDB.",
      "pos": [
        18929,
        18999
      ]
    },
    {
      "content": "The available choices are DirectTcp, DirectHttps, and Gateway.",
      "pos": [
        19001,
        19063
      ]
    },
    {
      "content": "The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.",
      "pos": [
        19065,
        19179
      ]
    },
    {
      "content": "Screenshot of DocumentDB source advanced options",
      "pos": [
        19183,
        19231
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> The import tool defaults to connection mode DirectTcp.",
      "pos": [
        19296,
        19362
      ]
    },
    {
      "content": "If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.",
      "pos": [
        19364,
        19463
      ]
    },
    {
      "content": "Here are some command line samples to import from DocumentDB:",
      "pos": [
        19466,
        19527
      ]
    },
    {
      "pos": [
        20722,
        20763
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"HBaseSource\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import from HBase"
    },
    {
      "content": "The HBase source importer option allows you to import data from an HBase table and optionally filter the data.",
      "pos": [
        20768,
        20878
      ]
    },
    {
      "content": "Several templates are provided so that setting up an import is as easy as possible.",
      "pos": [
        20879,
        20962
      ]
    },
    {
      "content": "Screenshot of HBase source options",
      "pos": [
        20967,
        21001
      ]
    },
    {
      "content": "Screenshot of HBase source options",
      "pos": [
        21055,
        21089
      ]
    },
    {
      "content": "The format of the HBase Stargate connection string is:",
      "pos": [
        21141,
        21195
      ]
    },
    {
      "pos": [
        21272,
        21399
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the HBase instance specified in the connection string field can be accessed."
    },
    {
      "content": "Here is a command line sample to import from HBase:",
      "pos": [
        21402,
        21453
      ]
    },
    {
      "pos": [
        21748,
        21815
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"DocumentDBBulkTarget\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import to DocumentDB (Bulk Import)"
    },
    {
      "content": "The DocumentDB Bulk importer allows you to import from any of the available source options, using a DocumentDB stored procedure for efficiency.",
      "pos": [
        21820,
        21963
      ]
    },
    {
      "content": "The tool supports import to a single DocumentDB collection, as well as sharded import whereby data is partitioned across multiple DocumentDB collections.",
      "pos": [
        21965,
        22118
      ]
    },
    {
      "content": "Read more about partitioning data in DocumentDB <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](documentdb-partition-data.md)</ept>.",
      "pos": [
        22120,
        22205
      ]
    },
    {
      "content": "The tool will create, execute, and then delete the stored procedure from the target collection(s).",
      "pos": [
        22207,
        22305
      ]
    },
    {
      "content": "Screenshot of DocumentDB bulk options",
      "pos": [
        22311,
        22348
      ]
    },
    {
      "content": "The format of the DocumentDB connection string is:",
      "pos": [
        22402,
        22452
      ]
    },
    {
      "pos": [
        22559,
        22691
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed."
    },
    {
      "content": "To import to a single collection, enter the name of the collection to which data will be imported and click the Add button.",
      "pos": [
        22694,
        22817
      ]
    },
    {
      "content": "To import to multiple collections, either enter each collection name individually or use the following syntax to specify multiple collections: <bpt id=\"p1\">*</bpt>collection_prefix<ept id=\"p1\">*</ept>[start index - end index].",
      "pos": [
        22819,
        23007
      ]
    },
    {
      "content": "When specifying multiple collections via the aforementioned syntax, keep the following in mind:",
      "pos": [
        23009,
        23104
      ]
    },
    {
      "content": "Only integer range name patterns are supported.",
      "pos": [
        23109,
        23156
      ]
    },
    {
      "content": "For example, specifying collection[0-3] will produce the following collections: collection0, collection1, collection2, collection3.",
      "pos": [
        23158,
        23289
      ]
    },
    {
      "content": "You can use an abbreviated syntax: collection[3] will emit same set of collections mentioned in step 1.",
      "pos": [
        23293,
        23396
      ]
    },
    {
      "content": "More than one substitution can be provided.",
      "pos": [
        23400,
        23443
      ]
    },
    {
      "content": "For example, collection[0-1] [0-9] will generate 20 collection names with leading zeros (collection01, ..02, ..03).",
      "pos": [
        23445,
        23560
      ]
    },
    {
      "content": "Once the collection name(s) have been specified, choose the desired pricing tier of the collection(s) (S1, S2, or S3).",
      "pos": [
        23562,
        23680
      ]
    },
    {
      "content": "For best import performance, choose S3.",
      "pos": [
        23682,
        23721
      ]
    },
    {
      "content": "Read more about DocumentDB performance levels <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](documentdb-performance-levels.md)</ept>.",
      "pos": [
        23723,
        23810
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The performance tier setting only applies to collection creation.",
      "pos": [
        23814,
        23892
      ]
    },
    {
      "content": "If the specified collection already exists, its pricing tier will not be modified.",
      "pos": [
        23894,
        23976
      ]
    },
    {
      "content": "When importing to multiple collections, the import tool supports hash based sharding.",
      "pos": [
        23978,
        24063
      ]
    },
    {
      "content": "In this scenario, specify the document property you wish to use as the Partition Key (if Partition Key is left blank, documents will be sharded randomly across the target collections).",
      "pos": [
        24065,
        24249
      ]
    },
    {
      "content": "You may optionally specify which field in the import source should be used as the DocumentDB document id property during the import (note that if documents do not contain this property, then the import tool will generate a GUID as the id property value).",
      "pos": [
        24252,
        24506
      ]
    },
    {
      "content": "There are a number of advanced options available during import.",
      "pos": [
        24508,
        24571
      ]
    },
    {
      "content": "First, while the tool includes a default bulk import stored procedure (BulkInsert.js), you may choose to specify your own import stored procedure:",
      "pos": [
        24573,
        24719
      ]
    },
    {
      "content": "Screenshot of DocumentDB bulk insert sproc option",
      "pos": [
        24724,
        24773
      ]
    },
    {
      "content": "Additionally, when importing date types (e.g. from SQL Server or MongoDB), you can choose between three import options:",
      "pos": [
        24825,
        24944
      ]
    },
    {
      "content": "Screenshot of DocumentDB date time import options",
      "pos": [
        24949,
        24998
      ]
    },
    {
      "content": "String: Persist as a string value",
      "pos": [
        25057,
        25090
      ]
    },
    {
      "content": "Epoch: Persist as an Epoch number value",
      "pos": [
        25095,
        25134
      ]
    },
    {
      "pos": [
        25139,
        25332
      ],
      "content": "Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:\n\"date_joined\": {\n\"Value\": \"2013-10-21T21:17:25.2410000Z\",\n\"Epoch\": 1382390245\n}",
      "leadings": [
        "",
        "",
        "    ",
        "    ",
        "  "
      ],
      "nodes": [
        {
          "content": "Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:",
          "pos": [
            0,
            103
          ],
          "nodes": [
            {
              "content": "Both: Persist both string and Epoch number values.",
              "pos": [
                0,
                50
              ]
            },
            {
              "content": "This option will create a subdocument, for example:",
              "pos": [
                52,
                103
              ]
            }
          ]
        },
        {
          "content": "\"date_joined\": {",
          "pos": [
            104,
            120
          ]
        },
        {
          "content": "\"Value\": \"2013-10-21T21:17:25.2410000Z\",",
          "pos": [
            121,
            161
          ]
        },
        {
          "content": "\"Epoch\": 1382390245",
          "pos": [
            162,
            181
          ]
        },
        {
          "content": "}",
          "pos": [
            182,
            183
          ]
        }
      ]
    },
    {
      "content": "The DocumentDB Bulk importer has the following additional advanced options:",
      "pos": [
        25336,
        25411
      ]
    },
    {
      "content": "Batch Size: The tool defaults to a batch size of 50.",
      "pos": [
        25416,
        25468
      ]
    },
    {
      "content": "If the documents to be imported are large, consider lowering the batch size.",
      "pos": [
        25470,
        25546
      ]
    },
    {
      "content": "Conversely, if the documents to be imported are small, consider raising the batch size.",
      "pos": [
        25548,
        25635
      ]
    },
    {
      "content": "Max Script Size (bytes): The tool defaults to a max script size of 960KB",
      "pos": [
        25639,
        25711
      ]
    },
    {
      "content": "Disable Sutomatic Id Generation: If every document to be imported contains an id field, then selecting this option can increase performance.",
      "pos": [
        25715,
        25855
      ]
    },
    {
      "content": "Documents missing a unique id field will not be imported.",
      "pos": [
        25857,
        25914
      ]
    },
    {
      "content": "Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        25918,
        26087
      ]
    },
    {
      "content": "Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        26091,
        26251
      ]
    },
    {
      "content": "Connection Mode: Specifies the connection mode to use with DocumentDB.",
      "pos": [
        26255,
        26325
      ]
    },
    {
      "content": "The available choices are DirectTcp, DirectHttps, and Gateway.",
      "pos": [
        26327,
        26389
      ]
    },
    {
      "content": "The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.",
      "pos": [
        26391,
        26505
      ]
    },
    {
      "content": "Screenshot of DocumentDB bulk import advanced options",
      "pos": [
        26509,
        26562
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> The import tool defaults to connection mode DirectTcp.",
      "pos": [
        26620,
        26686
      ]
    },
    {
      "content": "If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.",
      "pos": [
        26688,
        26787
      ]
    },
    {
      "pos": [
        26791,
        26870
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"DocumentDBSeqTarget\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Import to DocumentDB (Sequential Record Import)"
    },
    {
      "content": "The DocumentDB sequential record importer allows you to import from any of the available source options on a record by record basis.",
      "pos": [
        26875,
        27007
      ]
    },
    {
      "content": "You might choose this option if you’re importing to an existing collection that has reached its quota of stored procedures.",
      "pos": [
        27009,
        27132
      ]
    },
    {
      "content": "The tool supports import to a single DocumentDB collection, as well as sharded import whereby data is partitioned across multiple DocumentDB collections.",
      "pos": [
        27134,
        27287
      ]
    },
    {
      "content": "Read more about partitioning data in DocumentDB <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](documentdb-partition-data.md)</ept>.",
      "pos": [
        27289,
        27374
      ]
    },
    {
      "content": "Screenshot of DocumentDB sequential record import options",
      "pos": [
        27379,
        27436
      ]
    },
    {
      "content": "The format of the DocumentDB connection string is:",
      "pos": [
        27496,
        27546
      ]
    },
    {
      "pos": [
        27653,
        27785
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed."
    },
    {
      "content": "To import to a single collection, enter the name of the collection to which data will be imported and click the Add button.",
      "pos": [
        27788,
        27911
      ]
    },
    {
      "content": "To import to multiple collections, either enter each collection name individually or use the following syntax to specify multiple collections: <bpt id=\"p1\">*</bpt>collection_prefix<ept id=\"p1\">*</ept>[start index - end index].",
      "pos": [
        27913,
        28101
      ]
    },
    {
      "content": "When specifying multiple collections via the aforementioned syntax, keep the following in mind:",
      "pos": [
        28103,
        28198
      ]
    },
    {
      "content": "Only integer range name patterns are supported.",
      "pos": [
        28203,
        28250
      ]
    },
    {
      "content": "For example, specifying collection[0-3] will produce the following collections: collection0, collection1, collection2, collection3.",
      "pos": [
        28252,
        28383
      ]
    },
    {
      "content": "You can use an abbreviated syntax: collection[3] will emit same set of collections mentioned in step 1.",
      "pos": [
        28387,
        28490
      ]
    },
    {
      "content": "More than one substitution can be provided.",
      "pos": [
        28494,
        28537
      ]
    },
    {
      "content": "For example, collection[0-1] [0-9] will generate 20 collection names with leading zeros (collection01, ..02, ..03).",
      "pos": [
        28539,
        28654
      ]
    },
    {
      "content": "Once the collection name(s) have been specified, choose the desired pricing tier of the collection(s) (S1, S2, or S3).",
      "pos": [
        28656,
        28774
      ]
    },
    {
      "content": "For best import performance, choose S3.",
      "pos": [
        28776,
        28815
      ]
    },
    {
      "content": "Read more about DocumentDB performance levels <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](documentdb-performance-levels.md)</ept>.",
      "pos": [
        28817,
        28904
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The performance tier setting only applies to collection creation.",
      "pos": [
        28908,
        28986
      ]
    },
    {
      "content": "If the specified collection already exists, its pricing tier will not be modified.",
      "pos": [
        28988,
        29070
      ]
    },
    {
      "content": "When importing to multiple collections, the import tool supports hash based sharding.",
      "pos": [
        29072,
        29157
      ]
    },
    {
      "content": "In this scenario, specify the document property you wish to use as the Partition Key (if Partition Key is left blank, documents will be sharded randomly across the target collections).",
      "pos": [
        29159,
        29343
      ]
    },
    {
      "content": "You may optionally specify which field in the import source should be used as the DocumentDB document id property during the import (note that if documents do not contain this property, then the import tool will generate a GUID as the id property value).",
      "pos": [
        29346,
        29600
      ]
    },
    {
      "content": "There are a number of advanced options available during import.",
      "pos": [
        29602,
        29665
      ]
    },
    {
      "content": "First, when importing date types (e.g. from SQL Server or MongoDB), you can choose between three import options:",
      "pos": [
        29667,
        29779
      ]
    },
    {
      "content": "Screenshot of DocumentDB date time import options",
      "pos": [
        29784,
        29833
      ]
    },
    {
      "content": "String: Persist as a string value",
      "pos": [
        29892,
        29925
      ]
    },
    {
      "content": "Epoch: Persist as an Epoch number value",
      "pos": [
        29930,
        29969
      ]
    },
    {
      "pos": [
        29974,
        30167
      ],
      "content": "Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:\n\"date_joined\": {\n\"Value\": \"2013-10-21T21:17:25.2410000Z\",\n\"Epoch\": 1382390245\n}",
      "leadings": [
        "",
        "",
        "    ",
        "    ",
        "  "
      ],
      "nodes": [
        {
          "content": "Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:",
          "pos": [
            0,
            103
          ],
          "nodes": [
            {
              "content": "Both: Persist both string and Epoch number values.",
              "pos": [
                0,
                50
              ]
            },
            {
              "content": "This option will create a subdocument, for example:",
              "pos": [
                52,
                103
              ]
            }
          ]
        },
        {
          "content": "\"date_joined\": {",
          "pos": [
            104,
            120
          ]
        },
        {
          "content": "\"Value\": \"2013-10-21T21:17:25.2410000Z\",",
          "pos": [
            121,
            161
          ]
        },
        {
          "content": "\"Epoch\": 1382390245",
          "pos": [
            162,
            181
          ]
        },
        {
          "content": "}",
          "pos": [
            182,
            183
          ]
        }
      ]
    },
    {
      "content": "The DocumentDB - Sequential record importer has the following additional advanced options:",
      "pos": [
        30170,
        30260
      ]
    },
    {
      "content": "Number of Parallel Requests: The tool defaults to 2 parallel requests.",
      "pos": [
        30265,
        30335
      ]
    },
    {
      "content": "If the documents to be imported are small, consider raising the number of parallel requests.",
      "pos": [
        30337,
        30429
      ]
    },
    {
      "content": "Note that if this number is raised too much, the import may experience throttling.",
      "pos": [
        30431,
        30513
      ]
    },
    {
      "content": "Disable Automatic Id Generation: If every document to be imported contains an id field, then selecting this option can increase performance.",
      "pos": [
        30517,
        30657
      ]
    },
    {
      "content": "Documents missing a unique id field will not be imported.",
      "pos": [
        30659,
        30716
      ]
    },
    {
      "content": "Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        30720,
        30889
      ]
    },
    {
      "content": "Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).",
      "pos": [
        30893,
        31053
      ]
    },
    {
      "content": "Connection Mode: Specifies the connection mode to use with DocumentDB.",
      "pos": [
        31057,
        31127
      ]
    },
    {
      "content": "The available choices are DirectTcp, DirectHttps, and Gateway.",
      "pos": [
        31129,
        31191
      ]
    },
    {
      "content": "The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.",
      "pos": [
        31193,
        31307
      ]
    },
    {
      "content": "Screenshot of DocumentDB sequential record import advanced options",
      "pos": [
        31311,
        31377
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.TIP]</ph> The import tool defaults to connection mode DirectTcp.",
      "pos": [
        31446,
        31512
      ]
    },
    {
      "content": "If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.",
      "pos": [
        31514,
        31613
      ]
    },
    {
      "pos": [
        31617,
        31707
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"IndexingPolicy\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Specify an indexing policy when creating DocumentDB collections"
    },
    {
      "content": "When you allow the migration tool to create collections during import, you can specify the indexing policy of the collections.",
      "pos": [
        31712,
        31838
      ]
    },
    {
      "content": "In the advanced options section of the DocumentDB Bulk import and DocumentDB Sequential record options, navigate to the Indexing Policy section.",
      "pos": [
        31840,
        31984
      ]
    },
    {
      "content": "Screenshot of DocumentDB Indexing Policy advanced options",
      "pos": [
        31988,
        32045
      ]
    },
    {
      "content": "Using the Indexing Policy advanced option, you can select an indexing policy file, manually enter an indexing policy, or select from a set of default templates (by right clicking in the indexing policy textbox).",
      "pos": [
        32100,
        32311
      ]
    },
    {
      "content": "The policy templates the tool provides are:",
      "pos": [
        32313,
        32356
      ]
    },
    {
      "content": "Default.",
      "pos": [
        32360,
        32368
      ]
    },
    {
      "content": "This policy is best when you’re performing equality queries against strings and using ORDER BY, range, and equality queries for numbers.",
      "pos": [
        32370,
        32506
      ]
    },
    {
      "content": "This policy has a lower index storage overhead than Range.",
      "pos": [
        32508,
        32566
      ]
    },
    {
      "content": "Hash.",
      "pos": [
        32569,
        32574
      ]
    },
    {
      "content": "This policy is best when you’re performing equality queries for both numbers and strings.",
      "pos": [
        32575,
        32664
      ]
    },
    {
      "content": "This policy has the lowest index storage overhead.",
      "pos": [
        32666,
        32716
      ]
    },
    {
      "content": "Range.",
      "pos": [
        32719,
        32725
      ]
    },
    {
      "content": "This policy is best you’re using ORDER BY, range and equality queries on both numbers and strings.",
      "pos": [
        32726,
        32824
      ]
    },
    {
      "content": "This policy has a higher index storage overhead than Default or Hash.",
      "pos": [
        32826,
        32895
      ]
    },
    {
      "content": "Screenshot of DocumentDB Indexing Policy advanced options",
      "pos": [
        32900,
        32957
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you do not specify an indexing policy, then the default policy will be applied.",
      "pos": [
        33014,
        33109
      ]
    },
    {
      "content": "Read more about DocumentDB indexing policies <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](documentdb-indexing-policies.md)</ept>.",
      "pos": [
        33111,
        33196
      ]
    },
    {
      "content": "Export to JSON file",
      "pos": [
        33203,
        33222
      ]
    },
    {
      "content": "The DocumentDB JSON exporter allows you to export any of the available source options to a JSON file that contains an array of JSON documents.",
      "pos": [
        33224,
        33366
      ]
    },
    {
      "content": "The tool will handle the export for you, or you can choose to view the resulting migration command and run the command yourself.",
      "pos": [
        33368,
        33496
      ]
    },
    {
      "content": "The resulting JSON file may be stored locally or in Azure Blob storage.",
      "pos": [
        33498,
        33569
      ]
    },
    {
      "content": "Screenshot of DocumentDB JSON local file export option",
      "pos": [
        33573,
        33627
      ]
    },
    {
      "content": "Screenshot of DocumentDB JSON Azure Blob storage export option",
      "pos": [
        33679,
        33741
      ]
    },
    {
      "content": "You may optionally choose to prettify the resulting JSON, which will increase the size of the resulting document while making the contents more human readable.",
      "pos": [
        33792,
        33951
      ]
    },
    {
      "content": "Advanced Configuration",
      "pos": [
        34850,
        34872
      ]
    },
    {
      "content": "In the Advanced configuration screen, specify the location of the log file to which you would like any errors written.",
      "pos": [
        34874,
        34992
      ]
    },
    {
      "content": "The following rules apply to this page:",
      "pos": [
        34993,
        35032
      ]
    },
    {
      "content": "If a file name is not provided, then all errors will be returned on the Results page.",
      "pos": [
        35038,
        35123
      ]
    },
    {
      "content": "If a file name is provided without a directory, then the file will be created (or overwritten) in the current environment directory.",
      "pos": [
        35128,
        35260
      ]
    },
    {
      "content": "If you select an existing file, then the file will be overwritten, there is no append option.",
      "pos": [
        35265,
        35358
      ]
    },
    {
      "content": "Screenshot of Advanced configuration screen",
      "pos": [
        35366,
        35409
      ]
    },
    {
      "content": "Confirm Import Settings and View Command Line",
      "pos": [
        35473,
        35518
      ]
    },
    {
      "content": "After specifying source information, target information, and advanced configuration, review the migration summary and, optionally, view/copy the resulting migration command (copying the command is useful to automate import operations):",
      "pos": [
        35523,
        35758
      ]
    },
    {
      "content": "Screenshot of summary screen",
      "pos": [
        35766,
        35794
      ]
    },
    {
      "content": "Screenshot of summary screen",
      "pos": [
        35847,
        35875
      ]
    },
    {
      "content": "Once you’re satisfied with your source and target options, click <bpt id=\"p1\">**</bpt>Import<ept id=\"p1\">**</ept>.",
      "pos": [
        35932,
        36008
      ]
    },
    {
      "content": "The elapsed time, transferred count, and failure information (if you didn't provide a file name in the Advanced configuration) will update as the import is in process.",
      "pos": [
        36010,
        36177
      ]
    },
    {
      "content": "Once complete, you can export the results (e.g. to deal with any import failures).",
      "pos": [
        36179,
        36261
      ]
    },
    {
      "content": "Screenshot of DocumentDB JSON export option",
      "pos": [
        36270,
        36313
      ]
    },
    {
      "content": "You may also start a new import, either keeping the existing settings (e.g. connection string information, source and target choice, etc.) or resetting all values.",
      "pos": [
        36367,
        36530
      ]
    },
    {
      "content": "Screenshot of DocumentDB JSON export option",
      "pos": [
        36539,
        36582
      ]
    },
    {
      "content": "Next steps",
      "pos": [
        36634,
        36644
      ]
    },
    {
      "pos": [
        36648,
        36717
      ],
      "content": "To learn more about DocumentDB, click <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](http://azure.com/docdb)</ept>."
    },
    {
      "content": "test",
      "pos": [
        36722,
        36726
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Import data to DocumentDB | Microsoft Azure\" \n    description=\"Learn how to use the open source DocumentDB data migration tool to import data to DocumentDB from various sources, including JSON files, CSV files, SQL, MongoDB, Azure Table storage, Amazon DynamoDB, and DocumentDB collections.\" \n    services=\"documentdb\" \n    authors=\"stephbaron\" \n    manager=\"jhubbard\" \n    editor=\"monicar\" \n    documentationCenter=\"\"/>\n\n<tags \n    ms.service=\"documentdb\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/10/2015\" \n    ms.author=\"stbaro\"/>\n\n# Import data to DocumentDB #\n\nThis article shows you how to use the open source DocumentDB data migration tool to import data to [Microsoft Azure DocumentDB](http://azure.microsoft.com/services/documentdb/) from various sources, including JSON files, CSV files, SQL, MongoDB, Azure Table storage, Amazon DynamoDB and DocumentDB collections.\n\nAfter reading this article, you'll be able to answer the following questions:  \n\n-   How can I import JSON file data to DocumentDB?\n-   How can I import CSV file data to DocumentDB?\n-   How can I import SQL Server data to DocumentDB?\n-   How can I import MongoDB data to DocumentDB?\n-   How can I import data from Azure Table storage to DocumentDB?\n-   How can I import data from Amazon DynamoDB to DocumentDB?\n-   How can I import data from HBase to DocumentDB\n-   How can I migrate data between DocumentDB collections?\n\n##<a id=\"Prerequisites\"></a>Prerequisites ##\n\nBefore following the instructions in this article, ensure that you have the following installed:\n\n- [Microsoft .NET Framework 4.51](http://www.microsoft.com/net/downloads) or higher.\n\n##<a id=\"Overviewl\"></a>Overview of the DocumentDB Data Migration Tool ##\n\nThe DocumentDB Data Migration tool is an open source solution that imports data to DocumentDB from a variety of sources, including:\n\n- JSON files\n- MongoDB\n- SQL Server\n- CSV files\n- Azure Table storage\n- Amazon DynamoDB\n- HBase\n- DocumentDB collections\n\nWhile the import tool includes a graphical user interface (dtui.exe), it can also be driven from the command line (dt.exe).  In fact, there is an option to output the associated command after setting up an import through the UI.  Tabular source data (e.g. SQL Server or CSV files) can be transformed such that hierarchical relationships (subdocuments) can be created during import.  Keep reading to learn more about source options, sample command lines to import from each source, target options, and viewing import results.\n\n\n##<a id=\"Install\"></a>Installing the DocumentDB Data Migration Tool ##\n\nThe migration tool source code is available on GitHub in [this repository](https://github.com/azure/azure-documentdb-datamigrationtool) and a compiled version is available from [Microsoft Download Center](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d).  You may either compile the solution or simply download and extract the compiled version to a directory of your choice.  Then run either:\n\n- Dtui.exe: Graphical interface version of the tool\n- Dt.exe: Command-line version of the tool\n\n##<a id=\"JSON\"></a>Import JSON files ##\n\nThe JSON file source importer option allows you to import one or more single document JSON files or JSON files that each contain an array of JSON documents.  When adding folders that contain JSON files to import, you have the option of recursively searching for files in subfolders.\n\n![Screenshot of JSON file source options](./media/documentdb-import-data/jsonsource.png)\n\nHere are some command line samples to import JSON files:\n\n    #Import a single JSON file  \n    dt.exe /s:JsonFile /s.Files:.\\Sessions.json /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:Sessions /t.CollectionTier:S3\n\n    #Import a directory of JSON files\n    dt.exe /s:JsonFile /s.Files:C:\\TESessions\\*.json /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:Sessions /t.CollectionTier:S3\n\n    #Import a directory (including sub-directories) of JSON files\n    dt.exe /s:JsonFile /s.Files:C:\\LastFMMusic\\**\\*.json /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:Music /t.CollectionTier:S3\n\n    #Import a directory (single), directory (recursive), and individual JSON files\n    dt.exe /s:JsonFile /s.Files:C:\\Tweets\\*.*;C:\\LargeDocs\\**\\*.*;C:\\TESessions\\Session48172.json;C:\\TESessions\\Session48173.json;C:\\TESessions\\Session48174.json;C:\\TESessions\\Session48175.json;C:\\TESessions\\Session48177.json /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:subs /t.CollectionTier:S3\n\n    #Import a single JSON file and partition the data across 4 collections\n    dt.exe /s:JsonFile /s.Files:D:\\\\CompanyData\\\\Companies.json /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:comp[1-4] /t.PartitionKey:name /t.CollectionTier:S3\n\n##<a id=\"MongoDB\"></a>Import from MongoDB ##\n\nThe MongoDB source importer option allows you to import from an individual MongoDB collection and optionally filter documents using a query and/or modify the document structure by using a projection.  \n\n![Screenshot of MongoDB source options](./media/documentdb-import-data/mongodbsource.png)\n\nThe connection string is in the standard MongoDB format:\n\n    mongodb://<dbuser>:<dbpassword>@<host>:<port>/<database>\n\n> [AZURE.NOTE] Use the Verify command to ensure that the MongoDB instance specified in the connection string field can be accessed. \n\nEnter the name of the collection from which data will be imported.  You may optionally specify or provide a file for a query (e.g. {pop: {$gt:5000}}) and/or projection (e.g. {loc:0}) to both filter and shape the data to be imported.\n\nHere are some command line samples to import from MongoDB:\n\n    #Import all documents from a MongoDB collection\n    dt.exe /s:MongoDB /s.ConnectionString:mongodb://<dbuser>:<dbpassword>@<host>:<port>/<database> /s.Collection:zips /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:BulkZips /t.IdField:_id /t.CollectionTier:S3\n\n    #Import documents from a MongoDB collection which match the query and exclude the loc field\n    dt.exe /s:MongoDB /s.ConnectionString:mongodb://<dbuser>:<dbpassword>@<host>:<port>/<database> /s.Collection:zips /s.Query:{pop:{$gt:50000}} /s.Projection:{loc:0} /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:BulkZipsTransform /t.IdField:_id/t.CollectionTier:S3\n\n##<a id=\"MongoDBExport\"></a>Import MongoDB export files ##\n\nThe MongoDB export JSON file source importer option allows you to import one or more JSON files produced from the mongoexport utility.  \n\n![Screenshot of MongoDB export source options](./media/documentdb-import-data/mongodbexportsource.png)\n\nWhen adding folders that contain MongoDB export JSON files for import, you have the option of recursively searching for files in subfolders.\n\nHere is a command line sample to import from MongoDB export JSON files:\n\n    dt.exe /s:MongoDBExport /s.Files:D:\\mongoemployees.json /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:employees /t.IdField:_id /t.Dates:Epoch /t.CollectionTier:S3\n\n##<a id=\"SQL\"></a>Import from SQL Server ##\n\nThe SQL source importer option allows you to import from an individual SQL Server database and optionally filter the records to be imported using a query.  In addition, you can modify the document structure by specifying a nesting separator (more on that in a moment).  \n\n![Screenshot of SQL source options](./media/documentdb-import-data/sqlexportsource.png)\n\nThe format of the connection string is the standard SQL connection string format.\n\n> [AZURE.NOTE] Use the Verify command to ensure that the SQL Server instance specified in the connection string field can be accessed. \n\nThe nesting separator property is used to create hierarchical relationships (sub-documents) during import.  Consider the following SQL query:\n\n*select CAST(BusinessEntityID AS varchar) as Id, Name, AddressType as [Address.AddressType], AddressLine1 as [Address.AddressLine1], City as [Address.Location.City], StateProvinceName as [Address.Location.StateProvinceName], PostalCode as [Address.PostalCode], CountryRegionName as [Address.CountryRegionName] from Sales.vStoreWithAddresses WHERE AddressType='Main Office'*\n\nWhich returns the following (partial) results:\n\n![Screenshot of SQL query results](./media/documentdb-import-data/sqlqueryresults.png)\n\nNote the aliases such as Address.AddressType and Address.Location.StateProvinceName.  By specifying a nesting separator of ‘.’, the import tool creates Address and Address.Location subdocuments during the import.  Here is an example of a resulting document in DocumentDB:\n\n*{\n  \"id\": \"956\",\n  \"Name\": \"Finer Sales and Service\",\n  \"Address\": {\n    \"AddressType\": \"Main Office\",\n    \"AddressLine1\": \"#500-75 O'Connor Street\",\n    \"Location\": {\n      \"City\": \"Ottawa\",\n      \"StateProvinceName\": \"Ontario\"\n    },\n    \"PostalCode\": \"K4B 1S2\",\n    \"CountryRegionName\": \"Canada\"\n  }\n}*\n \nHere are some command line samples to import from SQL Server:\n\n    #Import records from SQL which match a query    \n    dt.exe /s:SQL /s.ConnectionString:\"Data Source=<server>;Initial Catalog=AdventureWorks;User Id=advworks;Password=<password>;\" /s.Query:\"select CAST(BusinessEntityID AS varchar) as Id, * from Sales.vStoreWithAddresses WHERE AddressType='Main Office'\" /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:Stores /t.IdField:Id /t.CollectionTier:S3\n\n    #Import records from sql which match a query and create hierarchical relationships\n    dt.exe /s:SQL /s.ConnectionString:\"Data Source=<server>;Initial Catalog=AdventureWorks;User Id=advworks;Password=<password>;\" /s.Query:\"select CAST(BusinessEntityID AS varchar) as Id, Name, AddressType as [Address.AddressType], AddressLine1 as [Address.AddressLine1], City as [Address.Location.City], StateProvinceName as [Address.Location.StateProvinceName], PostalCode as [Address.PostalCode], CountryRegionName as [Address.CountryRegionName] from Sales.vStoreWithAddresses WHERE AddressType='Main Office'\" /s.NestingSeparator:. /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:StoresSub /t.IdField:Id /t.CollectionTier:S3\n\n##<a id=\"CSV\"></a>Import CSV files ##\n\nThe CSV file source importer option enables you to import one or more CSV files.  When adding folders that contain CSV files for import, you have the option of recursively searching for files in subfolders.\n\n![Screenshot of CSV source options](media/documentdb-import-data/csvsource.png)\n\nSimilar to the SQL source, the nesting separator property may be used to create hierarchical relationships (sub-documents) during import.  Consider the following CSV header row and data rows:\n\n![Screenshot of CSV sample records](./media/documentdb-import-data/csvsample.png)\n\nNote the aliases such as DomainInfo.Domain_Name and RedirectInfo.Redirecting.  By specifying a nesting separator of ‘.’, the import tool will create DomainInfo and RedirectInfo subdocuments during the import.  Here is an example of a resulting document in DocumentDB:\n\n*{\n  \"DomainInfo\": {\n    \"Domain_Name\": \"ACUS.GOV\",\n    \"Domain_Name_Address\": \"http://www.ACUS.GOV\"\n  },\n  \"Federal Agency\": \"Administrative Conference of the United States\",\n  \"RedirectInfo\": {\n    \"Redirecting\": \"0\",\n    \"Redirect_Destination\": \"\"\n  },\n  \"id\": \"9cc565c5-ebcd-1c03-ebd3-cc3e2ecd814d\"\n}*\n\nThe import tool will attempt to infer type information for unquoted values in CSV files (quoted values are always treated as strings).  Types are identified in the following order: number, datetime, boolean.  \n\nThere are two other things to note about CSV import: \n\n1.  By default, unquoted values are always trimmed for tabs and spaces, while quoted values are preserved as-is.  This behavior can be overridden with the Trim quoted values checkbox or the /s.TrimQuoted command line option.\n\n2.  By default, an unquoted null is treated as a null value.  This behavior can be overridden (i.e. treat an unquoted null as a “null” string) with the Treat unquoted NULL as string checkbox or the /s.NoUnquotedNulls command line option.\n\n\nHere is a command line sample for CSV import:\n\n    dt.exe /s:CsvFile /s.Files:.\\Employees.csv /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:Employees /t.IdField:EntityID /t.CollectionTier:S3\n\n##<a id=\"AzureTableSource\"></a>Import from Azure Table storage ##\n\nThe Azure Table storage source importer option allows you to import from an individual Azure Table storage table and optionally filter the table entities to be imported.  \n\n![Screenshot of Azure Table storage source options](./media/documentdb-import-data/azuretablesource.png)\n\nThe format of the Azure Table storage connection string is:\n\n    DefaultEndpointsProtocol=<protocol>;AccountName=<Account Name>;AccountKey=<Account Key>;\n\n> [AZURE.NOTE] Use the Verify command to ensure that the Azure Table storage instance specified in the connection string field can be accessed. \n\nEnter the name of the Azure table from which data will be imported.  You may optionally specify a [filter](https://msdn.microsoft.com/library/azure/ff683669.aspx).\n\nThe Azure Table storage source importer option has the following additional options:\n\n1. Include Internal Fields \n    2. All - Include all internal fields (PartitionKey, RowKey, and Timestamp)\n    3. None - Exclude all internal fields\n    4. RowKey - Only include the RowKey field\n3. Select Columns\n    1. Azure Table storage filters do not support projections.  If you want to only import specific Azure Table entity properties, add them to the Select Columns list.  All other entity properties will be ignored.\n\nHere is a command line sample to import from Azure Table storage:\n\n    dt.exe /s:AzureTable /s.ConnectionString:\"DefaultEndpointsProtocol=https;AccountName=<Account Name>;AccountKey=<Account Key>\" /s.Table:metrics /s.InternalFields:All /s.Filter:\"PartitionKey eq 'Partition1' and RowKey gt '00001'\" /s.Projection:ObjectCount;ObjectSize  /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:metrics /t.CollectionTier:S3\n\n##<a id=\"DynamoDBSource\"></a>Import from Amazon DynamoDB ##\n\nThe Amazon DynamoDB source importer option allows you to import from an individual Amazon DynamoDB table and optionally filter the entities to be imported.  Several templates are provided so that setting up an import is as easy as possible.\n\n![Screenshot of Amazon DynamoDB source options](./media/documentdb-import-data/dynamodbsource1.png)\n\n![Screenshot of Amazon DynamoDB source options](./media/documentdb-import-data/dynamodbsource2.png)\n\nThe format of the Amazon DynamoDB connection string is:\n\n    ServiceURL=<Service Address>;AccessKey=<Access Key>;SecretKey=<Secret Key>;\n\n> [AZURE.NOTE] Use the Verify command to ensure that the Amazon DynamoDB instance specified in the connection string field can be accessed. \n\nHere is a command line sample to import from Amazon DynamoDB:\n\n    dt.exe /s:DynamoDB /s.ConnectionString:ServiceURL=https://dynamodb.us-east-1.amazonaws.com;AccessKey=<accessKey>;SecretKey=<secretKey> /s.Request:\"{   \"\"\"TableName\"\"\": \"\"\"ProductCatalog\"\"\" }\" /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:catalogCollection /t.CollectionTier:S3\n\n##<a id=\"BlobImport\"></a>Import files from Azure Blob storage##\n\nThe JSON file, MongoDB export file, and CSV file source importer options allow you to import one or more files from Azure Blob storage.  After specifying a Blob container URL and Account Key, simply provide a regular expression to select the file(s) to import.\n\n![Screenshot of Blob file source options](./media/documentdb-import-data/blobsource.png)\n\nHere is command line sample to import JSON files from Azure Blob storage:\n\n    dt.exe /s:JsonFile /s.Files:\"blobs://<account key>@account.blob.core.windows.net:443/importcontainer/.*\" /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:doctest\n\n##<a id=\"DocumentDBSource\"></a>Import from DocumentDB ##\n\nThe DocumentDB source importer option allows you to import data from one or more DocumentDB collections and optionally filter documents using a query.  \n\n![Screenshot of DocumentDB source options](./media/documentdb-import-data/documentdbsource.png)\n\nThe format of the DocumentDB connection string is:\n\n    AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\n\n> [AZURE.NOTE] Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed. \n\nTo import from a single DocumentDB collection, enter the name of the collection from which data will be imported.  To import from multiple DocumentDB collections, provide a regular expression to match one or more collection names (e.g. collection01 | collection02 | collection03).  You may optionally specify, or provide a file for, a query to both filter and shape the data to be imported.\n\n> [AZURE.NOTE] Since the collection field accepts regular expressions, if you are importing from a single collection whose name contains regular expression characters, then those characters must be escaped accordingly.\n\nThe DocumentDB source importer option has the following advanced options:\n\n1. Include Internal Fields: Specifies whether or not to include DocumentDB document system properties in the export (e.g. _rid, _ts).\n2. Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n3. Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n4. Connection Mode: Specifies the connection mode to use with DocumentDB.  The available choices are DirectTcp, DirectHttps, and Gateway.  The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.\n\n![Screenshot of DocumentDB source advanced options](./media/documentdb-import-data/documentdbsourceoptions.png)\n\n> [AZURE.TIP] The import tool defaults to connection mode DirectTcp.  If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.\n\n\nHere are some command line samples to import from DocumentDB:\n\n    #Migrate data from one DocumentDB collection to another DocumentDB collections\n    dt.exe /s:DocumentDB /s.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /s.Collection:TEColl /t:DocumentDBBulk /t.ConnectionString:\" AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:TESessions /t.CollectionTier:S3\n\n    #Migrate data from multiple DocumentDB collections to a single DocumentDB collection\n    dt.exe /s:DocumentDB /s.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /s.Collection:comp1|comp2|comp3|comp4 /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:singleCollection /t.CollectionTier:S3\n\n    #Export a DocumentDB collection to a JSON file\n    dt.exe /s:DocumentDB /s.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /s.Collection:StoresSub /t:JsonFile /t.File:StoresExport.json /t.Overwrite /t.CollectionTier:S3\n\n##<a id=\"HBaseSource\"></a>Import from HBase ##\n\nThe HBase source importer option allows you to import data from an HBase table and optionally filter the data. Several templates are provided so that setting up an import is as easy as possible. \n\n![Screenshot of HBase source options](./media/documentdb-import-data/hbasesource1.png)\n\n![Screenshot of HBase source options](./media/documentdb-import-data/hbasesource2.png)\n\nThe format of the HBase Stargate connection string is:\n\n    ServiceURL=<server-address>;Username=<username>;Password=<password>\n\n> [AZURE.NOTE] Use the Verify command to ensure that the HBase instance specified in the connection string field can be accessed. \n\nHere is a command line sample to import from HBase:\n\n    dt.exe /s:HBase /s.ConnectionString:ServiceURL=<server-address>;Username=<username>;Password=<password> /s.Table:Contacts /t:DocumentDBBulk /t.ConnectionString:\"AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\" /t.Collection:hbaseimport\n\n##<a id=\"DocumentDBBulkTarget\"></a>Import to DocumentDB (Bulk Import) ##\n\nThe DocumentDB Bulk importer allows you to import from any of the available source options, using a DocumentDB stored procedure for efficiency.  The tool supports import to a single DocumentDB collection, as well as sharded import whereby data is partitioned across multiple DocumentDB collections.  Read more about partitioning data in DocumentDB [here](documentdb-partition-data.md).  The tool will create, execute, and then delete the stored procedure from the target collection(s).  \n\n![Screenshot of DocumentDB bulk options](./media/documentdb-import-data/documentdbbulk.png)\n\nThe format of the DocumentDB connection string is:\n\n    AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\n\n> [AZURE.NOTE] Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed. \n\nTo import to a single collection, enter the name of the collection to which data will be imported and click the Add button.  To import to multiple collections, either enter each collection name individually or use the following syntax to specify multiple collections: *collection_prefix*[start index - end index].  When specifying multiple collections via the aforementioned syntax, keep the following in mind:\n\n1. Only integer range name patterns are supported.  For example, specifying collection[0-3] will produce the following collections: collection0, collection1, collection2, collection3.\n2. You can use an abbreviated syntax: collection[3] will emit same set of collections mentioned in step 1.\n3. More than one substitution can be provided.  For example, collection[0-1] [0-9] will generate 20 collection names with leading zeros (collection01, ..02, ..03).\n\nOnce the collection name(s) have been specified, choose the desired pricing tier of the collection(s) (S1, S2, or S3).  For best import performance, choose S3.  Read more about DocumentDB performance levels [here](documentdb-performance-levels.md).\n\n> [AZURE.NOTE] The performance tier setting only applies to collection creation.  If the specified collection already exists, its pricing tier will not be modified.\n\nWhen importing to multiple collections, the import tool supports hash based sharding.  In this scenario, specify the document property you wish to use as the Partition Key (if Partition Key is left blank, documents will be sharded randomly across the target collections). \n\nYou may optionally specify which field in the import source should be used as the DocumentDB document id property during the import (note that if documents do not contain this property, then the import tool will generate a GUID as the id property value).\n\nThere are a number of advanced options available during import.  First, while the tool includes a default bulk import stored procedure (BulkInsert.js), you may choose to specify your own import stored procedure:\n\n ![Screenshot of DocumentDB bulk insert sproc option](./media/documentdb-import-data/bulkinsertsp.png)\n\nAdditionally, when importing date types (e.g. from SQL Server or MongoDB), you can choose between three import options:\n\n ![Screenshot of DocumentDB date time import options](./media/documentdb-import-data/datetimeoptions.png)\n\n-   String: Persist as a string value\n-   Epoch: Persist as an Epoch number value\n-   Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:\n\"date_joined\": {\n    \"Value\": \"2013-10-21T21:17:25.2410000Z\",\n    \"Epoch\": 1382390245\n  } \n\n\nThe DocumentDB Bulk importer has the following additional advanced options:\n\n1. Batch Size: The tool defaults to a batch size of 50.  If the documents to be imported are large, consider lowering the batch size.  Conversely, if the documents to be imported are small, consider raising the batch size.\n2. Max Script Size (bytes): The tool defaults to a max script size of 960KB\n3. Disable Sutomatic Id Generation: If every document to be imported contains an id field, then selecting this option can increase performance.  Documents missing a unique id field will not be imported.\n4. Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n5. Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n6. Connection Mode: Specifies the connection mode to use with DocumentDB.  The available choices are DirectTcp, DirectHttps, and Gateway.  The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.\n\n![Screenshot of DocumentDB bulk import advanced options](./media/documentdb-import-data/docdbbulkoptions.png)\n\n> [AZURE.TIP] The import tool defaults to connection mode DirectTcp.  If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.\n\n##<a id=\"DocumentDBSeqTarget\"></a>Import to DocumentDB (Sequential Record Import) ##\n\nThe DocumentDB sequential record importer allows you to import from any of the available source options on a record by record basis.  You might choose this option if you’re importing to an existing collection that has reached its quota of stored procedures.  The tool supports import to a single DocumentDB collection, as well as sharded import whereby data is partitioned across multiple DocumentDB collections.  Read more about partitioning data in DocumentDB [here](documentdb-partition-data.md). \n\n![Screenshot of DocumentDB sequential record import options](./media/documentdb-import-data/documentdbsequential.png)\n\nThe format of the DocumentDB connection string is:\n\n    AccountEndpoint=<DocumentDB Endpoint>;AccountKey=<DocumentDB Key>;Database=<DocumentDB Database>;\n\n> [AZURE.NOTE] Use the Verify command to ensure that the DocumentDB instance specified in the connection string field can be accessed. \n\nTo import to a single collection, enter the name of the collection to which data will be imported and click the Add button.  To import to multiple collections, either enter each collection name individually or use the following syntax to specify multiple collections: *collection_prefix*[start index - end index].  When specifying multiple collections via the aforementioned syntax, keep the following in mind:\n\n1. Only integer range name patterns are supported.  For example, specifying collection[0-3] will produce the following collections: collection0, collection1, collection2, collection3.\n2. You can use an abbreviated syntax: collection[3] will emit same set of collections mentioned in step 1.\n3. More than one substitution can be provided.  For example, collection[0-1] [0-9] will generate 20 collection names with leading zeros (collection01, ..02, ..03).\n\nOnce the collection name(s) have been specified, choose the desired pricing tier of the collection(s) (S1, S2, or S3).  For best import performance, choose S3.  Read more about DocumentDB performance levels [here](documentdb-performance-levels.md).\n\n> [AZURE.NOTE] The performance tier setting only applies to collection creation.  If the specified collection already exists, its pricing tier will not be modified.\n\nWhen importing to multiple collections, the import tool supports hash based sharding.  In this scenario, specify the document property you wish to use as the Partition Key (if Partition Key is left blank, documents will be sharded randomly across the target collections). \n\nYou may optionally specify which field in the import source should be used as the DocumentDB document id property during the import (note that if documents do not contain this property, then the import tool will generate a GUID as the id property value).\n\nThere are a number of advanced options available during import.  First, when importing date types (e.g. from SQL Server or MongoDB), you can choose between three import options:\n\n ![Screenshot of DocumentDB date time import options](./media/documentdb-import-data/datetimeoptions.png)\n\n-   String: Persist as a string value\n-   Epoch: Persist as an Epoch number value\n-   Both: Persist both string and Epoch number values.  This option will create a subdocument, for example:\n\"date_joined\": {\n    \"Value\": \"2013-10-21T21:17:25.2410000Z\",\n    \"Epoch\": 1382390245\n  } \n\nThe DocumentDB - Sequential record importer has the following additional advanced options:\n\n1. Number of Parallel Requests: The tool defaults to 2 parallel requests.  If the documents to be imported are small, consider raising the number of parallel requests.  Note that if this number is raised too much, the import may experience throttling.\n2. Disable Automatic Id Generation: If every document to be imported contains an id field, then selecting this option can increase performance.  Documents missing a unique id field will not be imported.\n3. Number of Retries on Failure: Specifies the number of times to retry the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n4. Retry Interval: Specifies how long to wait between retrying the connection to DocumentDB in case of transient failures (e.g. network connectivity interruption).\n5. Connection Mode: Specifies the connection mode to use with DocumentDB.  The available choices are DirectTcp, DirectHttps, and Gateway.  The direct connection modes are faster, while the gateway mode is more firewall friendly as it only uses port 443.\n\n![Screenshot of DocumentDB sequential record import advanced options](./media/documentdb-import-data/documentdbsequentialoptions.png)\n\n> [AZURE.TIP] The import tool defaults to connection mode DirectTcp.  If you experience firewall issues, switch to connection mode Gateway, as it only requires port 443.\n\n##<a id=\"IndexingPolicy\"></a>Specify an indexing policy when creating DocumentDB collections ##\n\nWhen you allow the migration tool to create collections during import, you can specify the indexing policy of the collections.  In the advanced options section of the DocumentDB Bulk import and DocumentDB Sequential record options, navigate to the Indexing Policy section.\n\n![Screenshot of DocumentDB Indexing Policy advanced options](./media/documentdb-import-data/indexingpolicy1.png)\n\nUsing the Indexing Policy advanced option, you can select an indexing policy file, manually enter an indexing policy, or select from a set of default templates (by right clicking in the indexing policy textbox).\n\nThe policy templates the tool provides are:\n\n- Default.  This policy is best when you’re performing equality queries against strings and using ORDER BY, range, and equality queries for numbers.  This policy has a lower index storage overhead than Range.\n- Hash. This policy is best when you’re performing equality queries for both numbers and strings.  This policy has the lowest index storage overhead.\n- Range. This policy is best you’re using ORDER BY, range and equality queries on both numbers and strings.  This policy has a higher index storage overhead than Default or Hash.\n\n\n![Screenshot of DocumentDB Indexing Policy advanced options](./media/documentdb-import-data/indexingpolicy2.png)\n\n> [AZURE.NOTE] If you do not specify an indexing policy, then the default policy will be applied.  Read more about DocumentDB indexing policies [here](documentdb-indexing-policies.md). \n\n\n## Export to JSON file\n\nThe DocumentDB JSON exporter allows you to export any of the available source options to a JSON file that contains an array of JSON documents.  The tool will handle the export for you, or you can choose to view the resulting migration command and run the command yourself.  The resulting JSON file may be stored locally or in Azure Blob storage.\n\n![Screenshot of DocumentDB JSON local file export option](./media/documentdb-import-data/jsontarget.png)\n\n![Screenshot of DocumentDB JSON Azure Blob storage export option](./media/documentdb-import-data/jsontarget2.png)\n\nYou may optionally choose to prettify the resulting JSON, which will increase the size of the resulting document while making the contents more human readable.\n\n    Standard JSON export\n    [{\"id\":\"Sample\",\"Title\":\"About Paris\",\"Language\":{\"Name\":\"English\"},\"Author\":{\"Name\":\"Don\",\"Location\":{\"City\":\"Paris\",\"Country\":\"France\"}},\"Content\":\"Don's document in DocumentDB is a valid JSON document as defined by the JSON spec.\",\"PageViews\":10000,\"Topics\":[{\"Title\":\"History of Paris\"},{\"Title\":\"Places to see in Paris\"}]}]\n    \n    Prettified JSON export\n    [\n    {\n    \"id\": \"Sample\",\n    \"Title\": \"About Paris\",\n    \"Language\": {\n      \"Name\": \"English\"\n    },\n    \"Author\": {\n      \"Name\": \"Don\",\n      \"Location\": {\n        \"City\": \"Paris\",\n        \"Country\": \"France\"\n      }\n    },\n    \"Content\": \"Don's document in DocumentDB is a valid JSON document as defined by the JSON spec.\",\n    \"PageViews\": 10000,\n    \"Topics\": [\n      {\n        \"Title\": \"History of Paris\"\n      },\n      {\n        \"Title\": \"Places to see in Paris\"\n      }\n    ]\n    }]\n    \n## Advanced Configuration\n\nIn the Advanced configuration screen, specify the location of the log file to which you would like any errors written. The following rules apply to this page:\n\n1.  If a file name is not provided, then all errors will be returned on the Results page.\n2.  If a file name is provided without a directory, then the file will be created (or overwritten) in the current environment directory.\n3.  If you select an existing file, then the file will be overwritten, there is no append option.\n\n    ![Screenshot of Advanced configuration screen](./media/documentdb-import-data/AdvancedConfiguration.png)\n\n## Confirm Import Settings and View Command Line\n\n1. After specifying source information, target information, and advanced configuration, review the migration summary and, optionally, view/copy the resulting migration command (copying the command is useful to automate import operations):\n\n    ![Screenshot of summary screen](./media/documentdb-import-data/summary.png)\n\n    ![Screenshot of summary screen](./media/documentdb-import-data/summarycommand.png)\n\n2. Once you’re satisfied with your source and target options, click **Import**.  The elapsed time, transferred count, and failure information (if you didn't provide a file name in the Advanced configuration) will update as the import is in process.  Once complete, you can export the results (e.g. to deal with any import failures). \n\n    ![Screenshot of DocumentDB JSON export option](./media/documentdb-import-data/viewresults.png)\n\n3. You may also start a new import, either keeping the existing settings (e.g. connection string information, source and target choice, etc.) or resetting all values. \n\n    ![Screenshot of DocumentDB JSON export option](./media/documentdb-import-data/newimport.png)\n\n## Next steps\n\n- To learn more about DocumentDB, click [here](http://azure.com/docdb).\n\n\n \ntest\n"
}