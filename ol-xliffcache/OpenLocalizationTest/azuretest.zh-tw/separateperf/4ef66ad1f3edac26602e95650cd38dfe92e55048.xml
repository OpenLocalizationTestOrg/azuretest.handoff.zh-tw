{
  "nodes": [
    {
      "content": "Hive Activity",
      "pos": [
        28,
        41
      ]
    },
    {
      "content": "Learn how you can use the Hive Activity in an Azure data factory to run Hive queries on an on-demand/your own HDInsight cluster.",
      "pos": [
        61,
        189
      ]
    },
    {
      "content": "Hive Activity",
      "pos": [
        517,
        530
      ]
    },
    {
      "content": "The HDInsight Hive activity in a Data Factory <bpt id=\"p1\">[</bpt>pipeline<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> executes Hive queries on <bpt id=\"p2\">[</bpt>your own<ept id=\"p2\">](data-factory-compute-linked-services.md#azure-hdinsight-linked-service)</ept> or <bpt id=\"p3\">[</bpt>on-demand<ept id=\"p3\">](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service)</ept> HDInsight cluster.",
      "pos": [
        532,
        846
      ]
    },
    {
      "content": "This article builds on the <bpt id=\"p1\">[</bpt>data transformation activities<ept id=\"p1\">](data-factory-data-transformation-activities.md)</ept> article which presents a general overview of data transformation and the supported transformation activities.",
      "pos": [
        847,
        1064
      ]
    },
    {
      "content": "Syntax",
      "pos": [
        1069,
        1075
      ]
    },
    {
      "content": "Syntax details",
      "pos": [
        1742,
        1756
      ]
    },
    {
      "content": "Property",
      "pos": [
        1758,
        1766
      ]
    },
    {
      "content": "Description",
      "pos": [
        1769,
        1780
      ]
    },
    {
      "content": "Required",
      "pos": [
        1783,
        1791
      ]
    },
    {
      "content": "name",
      "pos": [
        1826,
        1830
      ]
    },
    {
      "content": "Name of the activity",
      "pos": [
        1833,
        1853
      ]
    },
    {
      "content": "Yes",
      "pos": [
        1856,
        1859
      ]
    },
    {
      "content": "description",
      "pos": [
        1860,
        1871
      ]
    },
    {
      "content": "Text describing what the activity is used for",
      "pos": [
        1874,
        1919
      ]
    },
    {
      "content": "No",
      "pos": [
        1922,
        1924
      ]
    },
    {
      "content": "type",
      "pos": [
        1925,
        1929
      ]
    },
    {
      "content": "HDinsightHive",
      "pos": [
        1932,
        1945
      ]
    },
    {
      "content": "Yes",
      "pos": [
        1948,
        1951
      ]
    },
    {
      "content": "inputs",
      "pos": [
        1952,
        1958
      ]
    },
    {
      "content": "Input(s) consumed by the Hive activity",
      "pos": [
        1961,
        1999
      ]
    },
    {
      "content": "No",
      "pos": [
        2002,
        2004
      ]
    },
    {
      "content": "outputs",
      "pos": [
        2005,
        2012
      ]
    },
    {
      "content": "Output(s) produced by the Hive activity",
      "pos": [
        2015,
        2054
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2057,
        2060
      ]
    },
    {
      "content": "linkedServiceName",
      "pos": [
        2062,
        2079
      ]
    },
    {
      "content": "Reference to the HDInsight cluster registered as a linked service in Data Factory",
      "pos": [
        2082,
        2163
      ]
    },
    {
      "content": "Yes",
      "pos": [
        2166,
        2169
      ]
    },
    {
      "content": "script",
      "pos": [
        2171,
        2177
      ]
    },
    {
      "content": "Specify the Hive script inline",
      "pos": [
        2180,
        2210
      ]
    },
    {
      "content": "No",
      "pos": [
        2213,
        2215
      ]
    },
    {
      "content": "script path",
      "pos": [
        2216,
        2227
      ]
    },
    {
      "content": "Store the Hive script in an Azure blob storage and provide the path to the file.",
      "pos": [
        2230,
        2310
      ]
    },
    {
      "content": "Use 'script' or 'scriptPath' property.",
      "pos": [
        2311,
        2349
      ]
    },
    {
      "content": "Both cannot be used together",
      "pos": [
        2350,
        2378
      ]
    },
    {
      "content": "No",
      "pos": [
        2381,
        2383
      ]
    },
    {
      "content": "defines",
      "pos": [
        2385,
        2392
      ]
    },
    {
      "content": "Specify parameters as key/value pairs for referencing within the Hive script using 'hiveconf'",
      "pos": [
        2395,
        2488
      ]
    },
    {
      "content": "No",
      "pos": [
        2492,
        2494
      ]
    },
    {
      "content": "Example",
      "pos": [
        2499,
        2506
      ]
    },
    {
      "content": "Let’s consider an example of game logs analytics where you want to identify the time spent by users playing games launched by your company.",
      "pos": [
        2508,
        2647
      ]
    },
    {
      "content": "Below is a sample game log which is comma (,) separated and contains the following fields – ProfileID, SessionStart, Duration, SrcIPAddress, and GameType.",
      "pos": [
        2650,
        2804
      ]
    },
    {
      "pos": [
        3077,
        3134
      ],
      "content": "The <bpt id=\"p1\">**</bpt>Hive script<ept id=\"p1\">**</ept> to process this data looks like this:"
    },
    {
      "content": "To execute this Hive script in a Data Factory pipeline, you need to the do the following",
      "pos": [
        4032,
        4120
      ]
    },
    {
      "content": "Create a linked service to register <bpt id=\"p1\">[</bpt>your own HDInsight compute cluster<ept id=\"p1\">](data-factory-compute-linked-services.md#azure-hdinsight-linked-service)</ept> or configure <bpt id=\"p2\">[</bpt>on-demand HDInsight compute cluster<ept id=\"p2\">](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service)</ept>.",
      "pos": [
        4125,
        4403
      ]
    },
    {
      "content": "Let’s call this linked service “HDInsightLinkedService”.",
      "pos": [
        4404,
        4460
      ]
    },
    {
      "content": "Create a <bpt id=\"p1\">[</bpt>linked service<ept id=\"p1\">](data-factory-azure-storage-connector.md)</ept> to configure the connection to Azure Blob storage hosting the data.",
      "pos": [
        4464,
        4598
      ]
    },
    {
      "content": "Let’s call this linked service “StorageLinkedService”",
      "pos": [
        4599,
        4652
      ]
    },
    {
      "content": "Create <bpt id=\"p1\">[</bpt>datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept> pointing to the input and the output data.",
      "pos": [
        4656,
        4749
      ]
    },
    {
      "content": "Let’s call the input dataset “HiveSampleIn” and the output dataset “HiveSampleOut”",
      "pos": [
        4750,
        4832
      ]
    },
    {
      "content": "Copy the Hive query as a file to Azure Blob Storage configured in step #2 above.",
      "pos": [
        4836,
        4916
      ]
    },
    {
      "content": "if the linked service for hosting the data is different from the one hosting this query file, create a separate Azure Storage linked service and refer to it in the activity configuration.",
      "pos": [
        4917,
        5104
      ]
    },
    {
      "content": "Use **scriptPath **to specify the path to hive query file and <bpt id=\"p1\">**</bpt>scriptLinkedService<ept id=\"p1\">**</ept> to specify the Azure storage that contains the script file.",
      "pos": [
        5105,
        5250
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You can also provide the Hive script inline in the activity definition by using the <bpt id=\"p1\">**</bpt>script<ept id=\"p1\">**</ept> property but this is not recommended as all special characters in the script within the JSON document needs to be escaped and may cause debugging issues.",
      "pos": [
        5259,
        5520
      ]
    },
    {
      "content": "The best practice is to follow step #4.",
      "pos": [
        5521,
        5560
      ]
    },
    {
      "content": "Create the below pipeline with the HDInsightHive activity to process the data.",
      "pos": [
        5565,
        5643
      ]
    },
    {
      "content": "Deploy the pipeline.",
      "pos": [
        6560,
        6580
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Creating pipelines<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> article for details.",
      "pos": [
        6581,
        6660
      ]
    },
    {
      "content": "Monitor the pipeline using the data factory monitoring and management views.",
      "pos": [
        6666,
        6742
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Monitoring and manage Data Factory pipelines<ept id=\"p1\">](data-factory-monitor-manage-pipelines.md)</ept> article for details.",
      "pos": [
        6743,
        6856
      ]
    },
    {
      "content": "Specifying parameters for a Hive script using the defines element",
      "pos": [
        6863,
        6928
      ]
    },
    {
      "content": "Consider the example where the game logs are ingested daily into Azure Blob Storage and stored in a folder partitioned with date and time.",
      "pos": [
        6931,
        7069
      ]
    },
    {
      "content": "You want to parameterize the Hive script and pass the input folder location dynamically during runtime and also produce the output partitioned with date and time.",
      "pos": [
        7070,
        7232
      ]
    },
    {
      "content": "To use parameterize Hive script, do the following",
      "pos": [
        7234,
        7283
      ]
    },
    {
      "pos": [
        7287,
        7324
      ],
      "content": "Define the parameters in <bpt id=\"p1\">**</bpt>defines<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        8810,
        8889
      ],
      "content": "In the Hive Script, refer to the parameter using <bpt id=\"p1\">**</bpt>${hiveconf:parameterName}<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Send Feedback",
      "pos": [
        9760,
        9773
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        9774,
        9831
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-hive-activity.md)</ept>.",
      "pos": [
        9832,
        9969
      ]
    },
    {
      "content": "test",
      "pos": [
        9980,
        9984
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Hive Activity\" \n    description=\"Learn how you can use the Hive Activity in an Azure data factory to run Hive queries on an on-demand/your own HDInsight cluster.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/26/2015\" \n    ms.author=\"spelluru\"/>\n\n# Hive Activity\n\nThe HDInsight Hive activity in a Data Factory [pipeline](data-factory-create-pipelines.md) executes Hive queries on [your own](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) or [on-demand](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service) HDInsight cluster. This article builds on the [data transformation activities](data-factory-data-transformation-activities.md) article which presents a general overview of data transformation and the supported transformation activities.\n\n## Syntax\n\n    {\n        \"name\": \"Hive Activity\",\n        \"description\": \"description\",\n        \"type\": \"HDInsightHive\",\n        \"inputs\": [\n          {\n            \"name\": \"input tables\"\n          }\n        ],\n        \"outputs\": [\n          {\n            \"name\": \"output tables\"\n          }\n        ],\n        \"linkedServiceName\": \"MyHDInsightLinkedService\",\n        \"typeProperties\": {\n          \"script\": \"Hive script\",\n          \"scriptPath\": \"<pathtotheHivescriptfileinAzureblobstorage>\",\n          \"defines\": {\n            \"param1\": \"param1Value\"\n          }\n        },\n       \"scheduler\": {\n          \"frequency\": \"Day\",\n          \"interval\": 1\n        }\n    }\n    \n## Syntax details\n\nProperty | Description | Required\n-------- | ----------- | --------\nname | Name of the activity | Yes\ndescription | Text describing what the activity is used for | No\ntype | HDinsightHive | Yes\ninputs | Input(s) consumed by the Hive activity | No\noutputs | Output(s) produced by the Hive activity | Yes \nlinkedServiceName | Reference to the HDInsight cluster registered as a linked service in Data Factory | Yes \nscript | Specify the Hive script inline | No\nscript path | Store the Hive script in an Azure blob storage and provide the path to the file. Use 'script' or 'scriptPath' property. Both cannot be used together | No \ndefines | Specify parameters as key/value pairs for referencing within the Hive script using 'hiveconf'  | No\n\n## Example\n\nLet’s consider an example of game logs analytics where you want to identify the time spent by users playing games launched by your company. \n\nBelow is a sample game log which is comma (,) separated and contains the following fields – ProfileID, SessionStart, Duration, SrcIPAddress, and GameType.\n\n    1809,2014-05-04 12:04:25.3470000,14,221.117.223.75,CaptureFlag\n    1703,2014-05-04 06:05:06.0090000,16,12.49.178.247,KingHill\n    1703,2014-05-04 10:21:57.3290000,10,199.118.18.179,CaptureFlag\n    1809,2014-05-04 05:24:22.2100000,23,192.84.66.141,KingHill\n    .....\n\nThe **Hive script** to process this data looks like this:\n\n    DROP TABLE IF EXISTS HiveSampleIn; \n    CREATE EXTERNAL TABLE HiveSampleIn \n    (\n        ProfileID       string, \n        SessionStart    string, \n        Duration        int, \n        SrcIPAddress    string, \n        GameType        string\n    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '10' STORED AS TEXTFILE LOCATION 'wasb://adfwalkthrough@<storageaccount>.blob.core.windows.net/samplein/'; \n    \n    DROP TABLE IF EXISTS HiveSampleOut; \n    CREATE EXTERNAL TABLE HiveSampleOut \n    (   \n        ProfileID   string, \n        Duration    int\n    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '10' STORED AS TEXTFILE LOCATION 'wasb://adfwalkthrough@<storageaccount>.blob.core.windows.net/sampleout/';\n    \n    INSERT OVERWRITE TABLE HiveSampleOut\n    Select \n        ProfileID,\n        SUM(Duration)\n    FROM HiveSampleIn Group by ProfileID\n\nTo execute this Hive script in a Data Factory pipeline, you need to the do the following\n\n1. Create a linked service to register [your own HDInsight compute cluster](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) or configure [on-demand HDInsight compute cluster](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service). Let’s call this linked service “HDInsightLinkedService”.\n2. Create a [linked service](data-factory-azure-storage-connector.md) to configure the connection to Azure Blob storage hosting the data. Let’s call this linked service “StorageLinkedService”\n3. Create [datasets](data-factory-create-datasets.md) pointing to the input and the output data. Let’s call the input dataset “HiveSampleIn” and the output dataset “HiveSampleOut”\n4. Copy the Hive query as a file to Azure Blob Storage configured in step #2 above. if the linked service for hosting the data is different from the one hosting this query file, create a separate Azure Storage linked service and refer to it in the activity configuration. Use **scriptPath **to specify the path to hive query file and **scriptLinkedService** to specify the Azure storage that contains the script file. \n\n    > [AZURE.NOTE] You can also provide the Hive script inline in the activity definition by using the **script** property but this is not recommended as all special characters in the script within the JSON document needs to be escaped and may cause debugging issues. The best practice is to follow step #4.\n5.  Create the below pipeline with the HDInsightHive activity to process the data.\n\n        {\n          \"name\": \"HiveActivitySamplePipeline\",\n          \"properties\": {\n            \"activities\": [\n              {\n                \"name\": \"HiveActivitySample\",\n                \"type\": \"HDInsightHive\",\n                \"inputs\": [\n                  {\n                    \"name\": \"HiveSampleIn\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"HiveSampleOut\"\n                  }\n                ],\n                \"linkedServiceName\": \"HDInsightLinkedService\",\n                \"typeproperties\": {\n                  \"scriptPath\": \"adfwalkthrough\\\\scripts\\\\samplehive.hql\",\n                  \"scriptLinkedService\": \"StorageLinkedService\"\n                },\n                \"scheduler\": {\n                    \"frequency\": \"Hour\",\n                    \"interval\": 1\n                }\n              }\n            ]\n          }\n        }\n\n6.  Deploy the pipeline. See [Creating pipelines](data-factory-create-pipelines.md) article for details. \n7.  Monitor the pipeline using the data factory monitoring and management views. See [Monitoring and manage Data Factory pipelines](data-factory-monitor-manage-pipelines.md) article for details. \n\n\n## Specifying parameters for a Hive script using the defines element \n\nConsider the example where the game logs are ingested daily into Azure Blob Storage and stored in a folder partitioned with date and time. You want to parameterize the Hive script and pass the input folder location dynamically during runtime and also produce the output partitioned with date and time.\n\nTo use parameterize Hive script, do the following\n\n- Define the parameters in **defines**.\n\n        {\n            \"name\": \"HiveActivitySamplePipeline\",\n            \"properties\": {\n            \"activities\": [\n                {\n                    \"name\": \"HiveActivitySample\",\n                    \"type\": \"HDInsightHive\",\n                    \"inputs\": [\n                        {\n                            \"name\": \"HiveSampleIn\"\n                          }\n                    ],\n                    \"outputs\": [\n                        {\n                            \"name\": \"HiveSampleOut\"\n                        }\n                    ],\n                    \"linkedServiceName\": \"HDInsightLinkedService\",\n                    \"typeproperties\": {\n                        \"scriptPath\": \"adfwalkthrough\\\\scripts\\\\samplehive.hql\",\n                        \"scriptLinkedService\": \"StorageLinkedService\",\n                        \"defines\": {\n                            \"Input\": \"$$Text.Format('wasb://adfwalkthrough@<storageaccountname>.blob.core.windows.net/samplein/yearno={0:yyyy}/monthno={0:%M}/dayno={0:%d}/', SliceStart)\",\n                            \"Output\": \"$$Text.Format('wasb://adfwalkthrough@<storageaccountname>.blob.core.windows.net/sampleout/yearno={0:yyyy}/monthno={0:%M}/dayno={0:%d}/', SliceStart)\"\n                        },\n                        \"scheduler\": {\n                            \"frequency\": \"Hour\",\n                            \"interval\": 1\n                        }\n                    }\n                }\n            ]\n          }\n        }\n\n- In the Hive Script, refer to the parameter using **${hiveconf:parameterName}**. \n\n        DROP TABLE IF EXISTS HiveSampleIn; \n        CREATE EXTERNAL TABLE HiveSampleIn \n        (\n            ProfileID   string, \n            SessionStart    string, \n            Duration    int, \n            SrcIPAddress    string, \n            GameType    string\n        ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '10' STORED AS TEXTFILE LOCATION '${hiveconf:Input}'; \n        \n        DROP TABLE IF EXISTS HiveSampleOut; \n        CREATE EXTERNAL TABLE HiveSampleOut \n        (\n            ProfileID   string, \n            Duration    int\n        ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '10' STORED AS TEXTFILE LOCATION '${hiveconf:Output}';\n        \n        INSERT OVERWRITE TABLE HiveSampleOut\n        Select \n            ProfileID,\n            SUM(Duration)\n        FROM HiveSampleIn Group by ProfileID\n\n\n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-hive-activity.md).\n\n\n\n\n\n\n\n\n\n\ntest\n"
}