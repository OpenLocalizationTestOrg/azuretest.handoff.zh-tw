{
  "nodes": [
    {
      "content": "Azure Search Developer Case Study: How WhatToPedia built an infomedia portal on Microsoft Azure",
      "pos": [
        28,
        123
      ]
    },
    {
      "content": "Learn how to build an information portal and meta search engine using Search service on Microsoft Azure",
      "pos": [
        143,
        246
      ]
    },
    {
      "content": "Azure Search Developer Case Study",
      "pos": [
        568,
        601
      ]
    },
    {
      "pos": [
        606,
        697
      ],
      "content": "How <bpt id=\"p1\">[</bpt>WhatToPedia.com<ept id=\"p1\">](http://whattopedia.com/)</ept> built an infomedia portal on Microsoft Azure"
    },
    {
      "content": "<ph id=\"ph1\">![][6]</ph>  &amp;nbsp;&amp;nbsp;&amp;nbsp;",
      "pos": [
        700,
        726
      ]
    },
    {
      "content": "The big idea",
      "pos": [
        743,
        755
      ]
    },
    {
      "content": "Our idea is to build an information portal that helps shoppers connect with retailers through a highly-relevant, scoped-search experience, similar to how travel portals match tourists up with the hotels, restaurants, and entertainment when in uncharted territory.",
      "pos": [
        766,
        1029
      ]
    },
    {
      "content": "The portal we envision will deliver an exceptionally high-quality search experience over retailer data in a given market, helping shoppers find stores based on location and other amenities the retailer provides.",
      "pos": [
        1032,
        1243
      ]
    },
    {
      "content": "We will seed the search engine with an initial dataset, but deeper value will be built over time, with the help of retailer subscribers who post information about their business.",
      "pos": [
        1244,
        1422
      ]
    },
    {
      "content": "Promotions, new merchandise, popular brands, in-house specialty services -– all are examples of data that adds value to our site.",
      "pos": [
        1423,
        1552
      ]
    },
    {
      "content": "This data is self-reported and integrated into the search corpus, once the retailer signs up as a subscriber.",
      "pos": [
        1553,
        1662
      ]
    },
    {
      "content": "Advertising, plus the subscription model, provide the revenue stream for our new business.",
      "pos": [
        1663,
        1753
      ]
    },
    {
      "content": "Search will be the predominant user interaction model, on a pure cloud platform.",
      "pos": [
        1755,
        1835
      ]
    },
    {
      "content": "For purposes of scale and low-costs, almost everything we do, from the portal experience to source control, will be through an online service.",
      "pos": [
        1836,
        1978
      ]
    },
    {
      "content": "Using a search engine that provides the features we need out of the box, we can create a search application quickly, without having to build and manage a search engine ourselves.",
      "pos": [
        1979,
        2157
      ]
    },
    {
      "content": "What we built",
      "pos": [
        2162,
        2175
      ]
    },
    {
      "content": "WhatToPedia is a start-up infomedia company.",
      "pos": [
        2177,
        2221
      ]
    },
    {
      "content": "We built <bpt id=\"p1\">[</bpt>WhatToPedia.com<ept id=\"p1\">](http://whattopedia.com/)</ept> –- currently in test-market in northern Europe with a go-live date of February 2, 2015.",
      "pos": [
        2222,
        2361
      ]
    },
    {
      "content": "Our customer base is primarily brick-and-mortar shops who need an affordable online presence that is easy to manage and maintain.",
      "pos": [
        2362,
        2491
      ]
    },
    {
      "content": "Our task is to attract shoppers through a great online search experience, boosting results based on city or neighborhood, brands, store names, or store types.",
      "pos": [
        2493,
        2651
      ]
    },
    {
      "content": "Attracting shoppers has a ripple effect, motivating retailers to subscribe to our portal site.",
      "pos": [
        2652,
        2746
      ]
    },
    {
      "content": "Subscriptions are fee-based, at an affordable rate.",
      "pos": [
        2747,
        2798
      ]
    },
    {
      "content": "After signing up for a subscription, a retailer takes over their existing profile (created initially by us from purchased data), updating it with additional data about promotions, featured brands, or announcements.",
      "pos": [
        2810,
        3024
      ]
    },
    {
      "content": "In-house capabilities, such as languages spoken, currencies accepted, tax-free shopping, can be self-reported to better attract shoppers who are looking for those amenities.",
      "pos": [
        3025,
        3198
      ]
    },
    {
      "content": "Who we are",
      "pos": [
        3203,
        3213
      ]
    },
    {
      "content": "My name is Thomas Segato (Microsoft Consulting) and I worked with Jesper Boelling, Lead Developer at WhatToPedia, to design the solution.",
      "pos": [
        3215,
        3352
      ]
    },
    {
      "content": "WhatToPedia is a start-up, test marketing its new portal business in Sweden, where most of the 60,000 retailers are brick-and-mortar SMEs (small and medium sized enterprises).",
      "pos": [
        3355,
        3530
      ]
    },
    {
      "content": "Because we know that a person shopping in Europe speaks multiple languages and carries multiple currencies, we build solutions that accommodate a multilingual shopper.",
      "pos": [
        3531,
        3698
      ]
    },
    {
      "content": "We needed, and found, a search engine that supports our multilingual requirements in Azure Search.",
      "pos": [
        3699,
        3797
      ]
    },
    {
      "content": "Azure Search was a game-changer for our project.",
      "pos": [
        3799,
        3847
      ]
    },
    {
      "content": "Prior to the availability of Azure Search, we expended considerable energy working through the kinks of building our own search engine.",
      "pos": [
        3848,
        3983
      ]
    },
    {
      "content": "Having Azure Search as an online service removed the biggest technical and administrative hurdle from our solution, which meant getting to market faster, and with a more robust search experience.",
      "pos": [
        3984,
        4179
      ]
    },
    {
      "content": "How we did it",
      "pos": [
        4186,
        4199
      ]
    },
    {
      "content": "Our vision was to build a complete infrastructure based on cloud services only.",
      "pos": [
        4201,
        4280
      ]
    },
    {
      "content": "Microsoft was chosen as the strategic platform because it was the provider that offered the necessary services (for both collaboration and development), scale on demand, and affordable pricing.",
      "pos": [
        4281,
        4474
      ]
    },
    {
      "content": "High-level components",
      "pos": [
        4481,
        4502
      ]
    },
    {
      "content": "We built a business, not just a site.",
      "pos": [
        4504,
        4541
      ]
    },
    {
      "content": "Supporting the entire effort required a full range of tools and applications.",
      "pos": [
        4542,
        4619
      ]
    },
    {
      "content": "We adopted Visual Studio and Visual Studio Online for development, Team Foundation Service (TFS) Online for source control and scrum management, Office 365 for communication and collaboration, and of course Microsoft Azure for all site-related operations and storage.",
      "pos": [
        4620,
        4887
      ]
    },
    {
      "content": "With Visual Studio, the IDE provided direct provisioning to Azure, with integration to TFS Online providing an additional productivity boost.",
      "pos": [
        4888,
        5029
      ]
    },
    {
      "content": "The diagram below illustrates the high-level components used in the WhatToPedia infrastructure.",
      "pos": [
        5031,
        5126
      ]
    },
    {
      "content": "How we use Microsoft Azure",
      "pos": [
        5143,
        5169
      ]
    },
    {
      "content": "Looking at the green boxes in the previous diagram, you’ll see that the WhatToPedia solution is built on these services:",
      "pos": [
        5171,
        5291
      ]
    },
    {
      "content": "Azure Search",
      "pos": [
        5296,
        5308
      ]
    },
    {
      "content": "Azure Websites using MVC 4",
      "pos": [
        5358,
        5384
      ]
    },
    {
      "content": "Azure WebJobs for scheduled tasks",
      "pos": [
        5436,
        5469
      ]
    },
    {
      "content": "Azure SQL Database",
      "pos": [
        5508,
        5526
      ]
    },
    {
      "content": "Azure BLOB Storage",
      "pos": [
        5582,
        5600
      ]
    },
    {
      "content": "SendGrid Email Delivery",
      "pos": [
        5651,
        5674
      ]
    },
    {
      "content": "The very heart of the solution is data and search.",
      "pos": [
        5751,
        5801
      ]
    },
    {
      "content": "The flow of data from the Reseller provider to the end customer is illustrated below:",
      "pos": [
        5802,
        5887
      ]
    },
    {
      "content": "Primary data storage is the reseller and accounting data in Azure SQL Database.",
      "pos": [
        5899,
        5978
      ]
    },
    {
      "content": "This consists of the initial dataset, plus retailer-specific data added over time.",
      "pos": [
        5979,
        6061
      ]
    },
    {
      "content": "We’re using an Azure WebJob to post updates from SQL Database to the search corpus in Azure Search.",
      "pos": [
        6062,
        6161
      ]
    },
    {
      "content": "Presentation layer",
      "pos": [
        6167,
        6185
      ]
    },
    {
      "content": "The portal is an Azure Website, implemented in MVC 4 and <bpt id=\"p1\">[</bpt>Twitter Bootstrap<ept id=\"p1\">](http://en.wikipedia.org/wiki/Bootstrap_%28front-end_framework%29)</ept>.",
      "pos": [
        6187,
        6330
      ]
    },
    {
      "content": "We chose MVC because it offers a much cleaner approach to HTML than ASP.NET forms-based development.",
      "pos": [
        6331,
        6431
      ]
    },
    {
      "content": "To avoid having to create apps for multiple devices and maintain multiple mobile platforms, Twitter Bootstrap was chosen to support all devices and platforms.",
      "pos": [
        6432,
        6590
      ]
    },
    {
      "content": "Authentication, permissions and sensitive data",
      "pos": [
        6596,
        6642
      ]
    },
    {
      "content": "Shoppers browse the site anonymously.",
      "pos": [
        6644,
        6681
      ]
    },
    {
      "content": "As such, there are no login requirements for shoppers, nor do we store any consumer data.",
      "pos": [
        6682,
        6771
      ]
    },
    {
      "content": "Retailers are a different story.",
      "pos": [
        6774,
        6806
      ]
    },
    {
      "content": "Here, we store public-facing profile information, billing information, and media content that they want to expose on the site.",
      "pos": [
        6807,
        6933
      ]
    },
    {
      "content": "Every retailer who subscribes to the site get a user login, used to authenticate the user prior to making updates to the subscriber’s profile.",
      "pos": [
        6934,
        7076
      ]
    },
    {
      "content": "We securely store all subscriber data in Azure SQL Database and Azure BLOB storage.",
      "pos": [
        7078,
        7161
      ]
    },
    {
      "content": "We opted for an authentication model based on .NET forms-based authentication.",
      "pos": [
        7162,
        7240
      ]
    },
    {
      "content": "We chose this approach for its simplicity; we didn’t need the roles, UI support and other extraneous features that come with other approaches.",
      "pos": [
        7241,
        7383
      ]
    },
    {
      "content": "To ensure that retailers only see the data that belongs to them, we created a retailer ID for each retailer that is subsequently used on all read and write operations involving retailer-specific data.",
      "pos": [
        7386,
        7586
      ]
    },
    {
      "content": "With this approach, we found that we did not need to grant database permissions to individual retailers.",
      "pos": [
        7587,
        7691
      ]
    },
    {
      "content": "All retailers interact with the system under a single database role, with the retailer ID as our data isolation technique.",
      "pos": [
        7692,
        7814
      ]
    },
    {
      "content": "Because our business is all about the downstream effects (driving more business to retailers, creating incentive to advertise and subscribe), we can draw the line at handling purchases over the web.",
      "pos": [
        7816,
        8014
      ]
    },
    {
      "content": "As such, you won’t find a shopping cart on our site, which simplifies our security requirements.",
      "pos": [
        8015,
        8111
      ]
    },
    {
      "content": "Another simplification we employed was to outsource our billing and accounts payable operations.",
      "pos": [
        8114,
        8210
      ]
    },
    {
      "content": "By routing customer payment information directly to a third-party (<bpt id=\"p1\">[</bpt>SveaWebPay<ept id=\"p1\">](http://www.sveawebpay.se/)</ept>), we reduce the risks associating with storing and protecting sensitive data in our data stores.",
      "pos": [
        8211,
        8414
      ]
    },
    {
      "content": "Search Engine",
      "pos": [
        8421,
        8434
      ]
    },
    {
      "content": "The core of our solution is the search engine built on Azure Search service.",
      "pos": [
        8436,
        8512
      ]
    },
    {
      "content": "Initially, we built a custom search engine, but during this process, we realized the complexity and effort was very high indeed, and that prompted us to consider other alternatives.",
      "pos": [
        8513,
        8694
      ]
    },
    {
      "content": "Basic features that were most important to us included:",
      "pos": [
        8697,
        8752
      ]
    },
    {
      "content": "Filters",
      "pos": [
        8756,
        8763
      ]
    },
    {
      "content": "Faceted navigation",
      "pos": [
        8766,
        8784
      ]
    },
    {
      "content": "Boosting results",
      "pos": [
        8787,
        8803
      ]
    },
    {
      "content": "Paging through AJAX",
      "pos": [
        8806,
        8825
      ]
    },
    {
      "pos": [
        8827,
        9016
      ],
      "content": "An internet search brought us to the following video, which inspired us to give Azure Search a try: <bpt id=\"p1\">[</bpt>Deep Dive at TechEd Europe<ept id=\"p1\">](http://channel9.msdn.com/events/TechEd/Europe/2014/DBI-B410)</ept>"
    },
    {
      "content": "After watching the video, we were ready to build a prototype based on what we saw.",
      "pos": [
        9019,
        9101
      ]
    },
    {
      "content": "Because we already had a data model in MVC, creating the prototype was straightforward because the data contained searchable terms, and we had already worked out the requirements for how we wanted to sort, facet, and filter the data.",
      "pos": [
        9102,
        9335
      ]
    },
    {
      "content": "This is how we built the prototype.",
      "pos": [
        9338,
        9373
      ]
    },
    {
      "content": "Configure Azure Search Service",
      "pos": [
        9377,
        9407
      ]
    },
    {
      "content": "Login to Azure portal and added the Search service to our subscription.",
      "pos": [
        9414,
        9485
      ]
    },
    {
      "content": "We used the shared version (free with our subscription).",
      "pos": [
        9486,
        9542
      ]
    },
    {
      "content": "Create an index.",
      "pos": [
        9546,
        9562
      ]
    },
    {
      "content": "For the prototype, we used the portal UI to define the search fields and create the scoring profiles.",
      "pos": [
        9563,
        9664
      ]
    },
    {
      "content": "Our scoring profile is based on location data: country | city |address (see: Add scoring profiles).",
      "pos": [
        9665,
        9764
      ]
    },
    {
      "content": "Copy the service URL and admin api-key to our configuration files.",
      "pos": [
        9768,
        9834
      ]
    },
    {
      "content": "This key is on the Search service page in the portal, and it’s used to authenticate to the service.",
      "pos": [
        9835,
        9934
      ]
    },
    {
      "content": "Develop a Search Indexer Job – Windows Console",
      "pos": [
        9942,
        9988
      ]
    },
    {
      "content": "Read all resellers from database.",
      "pos": [
        9995,
        10028
      ]
    },
    {
      "content": "Call the Azure Search Service API to upload resellers one by one (see: http://msdn.microsoft.com/library/azure/dn798930.aspx).",
      "pos": [
        10032,
        10158
      ]
    },
    {
      "content": "Set a property in database that reseller is indexed for incremental indexing.",
      "pos": [
        10162,
        10239
      ]
    },
    {
      "content": "We did this by adding an ‘indexer’ field that stores the index status of each profile (indexed or not).",
      "pos": [
        10240,
        10343
      ]
    },
    {
      "content": "See the appendix for the code snippet that builds the indexer job.",
      "pos": [
        10346,
        10412
      ]
    },
    {
      "content": "Develop a Search Web Portal – MVC",
      "pos": [
        10416,
        10449
      ]
    },
    {
      "content": "Call Azure Search Service to get all documents from search (see: http://msdn.microsoft.com/library/azure/dn798927.aspx)",
      "pos": [
        10456,
        10575
      ]
    },
    {
      "content": "Extract following from the search service response (by using json.net http://james.newtonking.com/json)",
      "pos": [
        10579,
        10682
      ]
    },
    {
      "content": "Results",
      "pos": [
        10688,
        10695
      ]
    },
    {
      "content": "Facets",
      "pos": [
        10701,
        10707
      ]
    },
    {
      "content": "Result counts",
      "pos": [
        10713,
        10726
      ]
    },
    {
      "content": "Develop a user interface for displaying search results, facets and counts (we already had this).",
      "pos": [
        10732,
        10828
      ]
    },
    {
      "content": "This is the code we used to get the results from Azure Search:",
      "pos": [
        10830,
        10892
      ]
    },
    {
      "content": "Boosting by location",
      "pos": [
        11416,
        11436
      ]
    },
    {
      "content": "Probably the most important requirement to verify in the prototype included adding a location search keyword to the query.",
      "pos": [
        11440,
        11562
      ]
    },
    {
      "content": "It is vital to our portal that if a user enters a city name in the search query, that the resellers in the given city would rank higher than resellers having the city keyword in the description.",
      "pos": [
        11563,
        11757
      ]
    },
    {
      "content": "For this requirement, we used a scoring profile to rank the city field higher than other fields.",
      "pos": [
        11758,
        11854
      ]
    },
    {
      "content": "Supporting multiple languages",
      "pos": [
        11858,
        11887
      ]
    },
    {
      "content": "We needed to display correct search results in correct languages, and provide an option for finding the same results in different languages.",
      "pos": [
        11891,
        12031
      ]
    },
    {
      "content": "The two sides to this problem were:",
      "pos": [
        12032,
        12067
      ]
    },
    {
      "content": "Search for words in multiple languages",
      "pos": [
        12072,
        12110
      ]
    },
    {
      "content": "Display search results in correct language",
      "pos": [
        12113,
        12155
      ]
    },
    {
      "content": "We solved the presentation part by adding a document for each language with localized text and a property with the language.",
      "pos": [
        12157,
        12281
      ]
    },
    {
      "content": "When a user enters a search term, we user <ph id=\"ph1\">`$filter`</ph> expressions to filter on the language the user has chosen.",
      "pos": [
        12282,
        12392
      ]
    },
    {
      "content": "Each of the documents has a hidden property called \"cities\" built on the collection type.",
      "pos": [
        12394,
        12483
      ]
    },
    {
      "content": "This property stores city names in all languages, enabling the user to search in multiple languages.",
      "pos": [
        12484,
        12584
      ]
    },
    {
      "content": "Data storage",
      "pos": [
        12589,
        12601
      ]
    },
    {
      "content": "All data (profile, subscription, and accounting) is stored in SQL Database.",
      "pos": [
        12603,
        12678
      ]
    },
    {
      "content": "All media files are stored in Azure BLOB storage, including images and videos provided by the retailer.",
      "pos": [
        12679,
        12782
      ]
    },
    {
      "content": "Using separate BLOB storage isolates the effects of uploading files; files are never co-mingled with the website, so we don’t need to rebuild the site whenever we add files.",
      "pos": [
        12783,
        12956
      ]
    },
    {
      "content": "An important benefit of our storage design is that multiple developers can share a single development storage.",
      "pos": [
        12958,
        13068
      ]
    },
    {
      "content": "One of the requirements for the WhatToPedia project was to be able to create a development environment within 15 minutes, including reseller data, images, and videos.",
      "pos": [
        13069,
        13235
      ]
    },
    {
      "content": "By getting the latest data from TFS Online, running a SQL script, and running the import job, a complete environment can be stood up in no time at all.",
      "pos": [
        13236,
        13387
      ]
    },
    {
      "content": "This practice also improves the staging process.",
      "pos": [
        13388,
        13436
      ]
    },
    {
      "content": "WebJobs",
      "pos": [
        13441,
        13448
      ]
    },
    {
      "content": "We use Azure WebJobs to update data to the index.",
      "pos": [
        13450,
        13499
      ]
    },
    {
      "content": "By creating a search indexer job, the indexing part was very easy to integrate into our solution.",
      "pos": [
        13500,
        13597
      ]
    },
    {
      "content": "The only code change we made was to accommodate the indexer job was to add an <ph id=\"ph1\">`Indexed`</ph> field to our data model to indicate the index state.",
      "pos": [
        13598,
        13738
      ]
    },
    {
      "content": "Whenever a new profile is added or updated, the <ph id=\"ph1\">`Indexed`</ph> field is set to false.",
      "pos": [
        13739,
        13819
      ]
    },
    {
      "content": "The same applies if the retailer changes his or her profile data through the portal.",
      "pos": [
        13820,
        13904
      ]
    },
    {
      "content": "The job looks for all rows having <ph id=\"ph1\">`Indexed`</ph> set to false.",
      "pos": [
        13908,
        13965
      ]
    },
    {
      "content": "When it finds the row, the document is posted to Azure Search, and then the <ph id=\"ph1\">`Indexed`</ph> field is set to true.",
      "pos": [
        13966,
        14073
      ]
    },
    {
      "content": "We didn’t have to plan for adding versus updating data because the Azure Search service actually takes care of this.",
      "pos": [
        14074,
        14190
      ]
    },
    {
      "content": "If you add a document that is already present, the service will do an update automatically.",
      "pos": [
        14191,
        14282
      ]
    },
    {
      "content": "All web jobs have been developed as console applications that can be uploaded to Azure web sites as ZIP files, unzipped, and then scheduled.",
      "pos": [
        14284,
        14424
      ]
    },
    {
      "content": "The job is scheduled to run every 5 minutes as a scheduled web task.",
      "pos": [
        14426,
        14494
      ]
    },
    {
      "content": "We calculated that the service takes approximately three minutes to upload 3,000 documents, which was within our requirements.",
      "pos": [
        14495,
        14621
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> There is a prototype indexer feature that was recently introduced in Azure Search.",
      "pos": [
        14626,
        14721
      ]
    },
    {
      "content": "This feature came too late for us to use it in our first release, but it appears to solve the same problem we used our indexer job for, which is to automate data load operations.",
      "pos": [
        14722,
        14900
      ]
    },
    {
      "content": "Backup strategy",
      "pos": [
        14906,
        14921
      ]
    },
    {
      "content": "We designed a multi-tiered backup strategy to recover from a range of scenarios, from catastrophic failure, down to recovery of an individual transaction.",
      "pos": [
        14923,
        15077
      ]
    },
    {
      "content": "The assets to protect include three kinds of data (web site, subscriber data, and media files).",
      "pos": [
        15078,
        15173
      ]
    },
    {
      "content": "First, by keeping the web site source code in TFS Online, we know that if the site goes down, we can rebuild it by republishing from TFS.",
      "pos": [
        15176,
        15313
      ]
    },
    {
      "content": "Subscriber data in Azure SQL Database is the most sensitive asset.",
      "pos": [
        15316,
        15382
      ]
    },
    {
      "content": "We back this up using the built-in feature (see <bpt id=\"p1\">[</bpt>Azure SQL Database Backup and Restore<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/jj650016.aspx)</ept>).",
      "pos": [
        15383,
        15527
      ]
    },
    {
      "content": "The backup schedule is full database backup once a week, differential database backups once a day, and transaction log backups every 5 minutes.",
      "pos": [
        15528,
        15671
      ]
    },
    {
      "content": "Given the size of the data, this solution is more than adequate for our immediate and projected data volumes.",
      "pos": [
        15673,
        15782
      ]
    },
    {
      "content": "Third, we store image and video files in Azure BLOB storage.",
      "pos": [
        15784,
        15844
      ]
    },
    {
      "content": "We are still evaluating the ultimate backup plan for this data, considering Cloudberry Explorer for Azure as a potential solution.",
      "pos": [
        15845,
        15975
      ]
    },
    {
      "content": "For now, we use a WebJob to copy images and videos to another location.",
      "pos": [
        15976,
        16047
      ]
    },
    {
      "content": "What we learned",
      "pos": [
        16051,
        16066
      ]
    },
    {
      "content": "Because we already had data, it was easy to establish proof-of-concept.",
      "pos": [
        16068,
        16139
      ]
    },
    {
      "content": "Within hours, we had a prototype with facets and counters, paging, ranked profiles, and search results.",
      "pos": [
        16140,
        16243
      ]
    },
    {
      "content": "The search results were so precise, we decided to remove some of the filters presented to the end customer.",
      "pos": [
        16244,
        16351
      ]
    },
    {
      "content": "The biggest surprise for us was how fast we could learn Azure Search, and how much we got back.",
      "pos": [
        16354,
        16449
      ]
    },
    {
      "content": "Literally, we established proof-of-concept in a few hours (see the note below), replacing 500 lines of code with 3 lines of code in the front end application (plus a new WebJob), and getting better results.",
      "pos": [
        16450,
        16656
      ]
    },
    {
      "content": "Previously, our code implemented paging, counts, and other behaviors that are standard to search.",
      "pos": [
        16659,
        16756
      ]
    },
    {
      "content": "Using Azure Search, the results we get back include the search hits, facets, paging data, counts -- all the stuff we needed and were having to supply ourselves.",
      "pos": [
        16757,
        16917
      ]
    },
    {
      "content": "It also included boosting and built-in faceted navigation, which we didn’t have in our original solution.",
      "pos": [
        16918,
        17023
      ]
    },
    {
      "content": "The greatest challenge during implementation was that it was a Preview version and finding information and shared experiences was difficult.",
      "pos": [
        17025,
        17165
      ]
    },
    {
      "content": "Once we connected a few dots, we found that using Azure Search Service was pretty simple due to its REST API and JSON data format.",
      "pos": [
        17166,
        17296
      ]
    },
    {
      "content": "We could call the framework directly from most open source plugins like JQuery JSON.Net, and we could use tools like Fiddler for fast experimentation and debugging.",
      "pos": [
        17297,
        17461
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Besides having the data prepped, it helped that those of us building the prototype already understood how search technology works, making us more productive, and more appreciative of the built-in features.",
      "pos": [
        17466,
        17684
      ]
    },
    {
      "content": "If you need to ramp up on search query construction, faceted navigation, filters, etc. you should expect prototyping to take longer.",
      "pos": [
        17685,
        17817
      ]
    },
    {
      "content": "Controlling facets in the search presentation page",
      "pos": [
        17823,
        17873
      ]
    },
    {
      "content": "One of our learnings during the proof-of-concept was to plan facets carefully upfront.",
      "pos": [
        17875,
        17961
      ]
    },
    {
      "content": "After loading a lot of data into the solution, we saw that the sheer volume of facets was too high to present to the users.",
      "pos": [
        17962,
        18085
      ]
    },
    {
      "content": "We solved this by constraining the facet count parameter.",
      "pos": [
        18088,
        18145
      ]
    },
    {
      "content": "The count parameter imposes a hard limit on the number of facets returned to the user.",
      "pos": [
        18146,
        18232
      ]
    },
    {
      "content": "A link that includes a discussion of the count parameter can be found <bpt id=\"p1\">[</bpt>here<ept id=\"p1\">](search-faceted-navigation.md)</ept>.",
      "pos": [
        18233,
        18340
      ]
    },
    {
      "content": "WebJobs for scheduling tasks",
      "pos": [
        18345,
        18373
      ]
    },
    {
      "content": "Azure Search wasn’t the only pleasant surprise for us.",
      "pos": [
        18375,
        18429
      ]
    },
    {
      "content": "We discovered that using WebJobs to automate our data load operations to Azure Search was vastly superior to our previous approach, which entailed using a dedicated VM running Windows Scheduler, with scheduled tasks for updating the search index.",
      "pos": [
        18430,
        18676
      ]
    },
    {
      "content": "WebJobs was simpler to configure and easier to debug, and of course much cheaper than having to pay for a dedicated VM.",
      "pos": [
        18677,
        18796
      ]
    },
    {
      "content": "Azure BLOB Storage Explorer for updating images",
      "pos": [
        18801,
        18848
      ]
    },
    {
      "content": "We found that using <bpt id=\"p1\">[</bpt>Azure BLOB Storage Explorer<ept id=\"p1\">](https://azurestorageexplorer.codeplex.com/)</ept> (available on codeplex) to be very helpful in managing image and video updates to the site.",
      "pos": [
        18850,
        19035
      ]
    },
    {
      "content": "We use it as a developer tool to manually update images and videos that are part of our main site.",
      "pos": [
        19036,
        19134
      ]
    },
    {
      "content": "We found it to be more flexible than deploying changes to the portal, and eliminates a complete test iteration whenever we need to update an image.",
      "pos": [
        19135,
        19282
      ]
    },
    {
      "content": "A few final words",
      "pos": [
        19287,
        19304
      ]
    },
    {
      "content": "Thanks to the great folks at WhatToPedia for allowing us to share their story!",
      "pos": [
        19306,
        19384
      ]
    },
    {
      "content": "We hope you found this case study useful.",
      "pos": [
        19388,
        19429
      ]
    },
    {
      "content": "If you go on to use Azure Search, I recommend a few resources to speed you along:",
      "pos": [
        19430,
        19511
      ]
    },
    {
      "content": "MSDN forum dedicated to Azure Search",
      "pos": [
        19516,
        19552
      ]
    },
    {
      "content": "StackOverflow also has a tag",
      "pos": [
        19628,
        19656
      ]
    },
    {
      "content": "Documentation page on Azure.com",
      "pos": [
        19717,
        19748
      ]
    },
    {
      "content": "Azure Search documentation on MSDN",
      "pos": [
        19812,
        19846
      ]
    },
    {
      "content": "Appendix: Search Indexer WebJob",
      "pos": [
        19907,
        19938
      ]
    },
    {
      "content": "The following code builds the indexer mentioned in the section on building the prototype.",
      "pos": [
        19940,
        20029
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Azure Search Developer Case Study: How WhatToPedia built an infomedia portal on Microsoft Azure\" \n    description=\"Learn how to build an information portal and meta search engine using Search service on Microsoft Azure\" \n    services=\"search, sql-database,  storage, web-sites\" \n    documentationCenter=\"\" \n    authors=\"HeidiSteen\" \n    manager=\"mblythe\"/>\n\n<tags \n    ms.service=\"search\" \n    ms.devlang=\"NA\" \n    ms.topic=\"article\" \n    ms.tgt_pltfrm=\"na\" \n    ms.workload=\"search\" \n    ms.date=\"07/08/2015\" \n    ms.author=\"heidist\"/>\n\n# Azure Search Developer Case Study\n\n## How [WhatToPedia.com](http://whattopedia.com/) built an infomedia portal on Microsoft Azure\n\n ![][6]  &nbsp;&nbsp;&nbsp;  <font size=\"9\">The big idea</font> \n\n\nOur idea is to build an information portal that helps shoppers connect with retailers through a highly-relevant, scoped-search experience, similar to how travel portals match tourists up with the hotels, restaurants, and entertainment when in uncharted territory. \n\nThe portal we envision will deliver an exceptionally high-quality search experience over retailer data in a given market, helping shoppers find stores based on location and other amenities the retailer provides. We will seed the search engine with an initial dataset, but deeper value will be built over time, with the help of retailer subscribers who post information about their business. Promotions, new merchandise, popular brands, in-house specialty services -– all are examples of data that adds value to our site. This data is self-reported and integrated into the search corpus, once the retailer signs up as a subscriber. Advertising, plus the subscription model, provide the revenue stream for our new business.\n\nSearch will be the predominant user interaction model, on a pure cloud platform. For purposes of scale and low-costs, almost everything we do, from the portal experience to source control, will be through an online service. Using a search engine that provides the features we need out of the box, we can create a search application quickly, without having to build and manage a search engine ourselves.\n\n## What we built\n\nWhatToPedia is a start-up infomedia company. We built [WhatToPedia.com](http://whattopedia.com/) –- currently in test-market in northern Europe with a go-live date of February 2, 2015. Our customer base is primarily brick-and-mortar shops who need an affordable online presence that is easy to manage and maintain.\n\nOur task is to attract shoppers through a great online search experience, boosting results based on city or neighborhood, brands, store names, or store types. Attracting shoppers has a ripple effect, motivating retailers to subscribe to our portal site. Subscriptions are fee-based, at an affordable rate.\n\n ![][7] \n\nAfter signing up for a subscription, a retailer takes over their existing profile (created initially by us from purchased data), updating it with additional data about promotions, featured brands, or announcements. In-house capabilities, such as languages spoken, currencies accepted, tax-free shopping, can be self-reported to better attract shoppers who are looking for those amenities.\n\n## Who we are\n\nMy name is Thomas Segato (Microsoft Consulting) and I worked with Jesper Boelling, Lead Developer at WhatToPedia, to design the solution. \n\nWhatToPedia is a start-up, test marketing its new portal business in Sweden, where most of the 60,000 retailers are brick-and-mortar SMEs (small and medium sized enterprises). Because we know that a person shopping in Europe speaks multiple languages and carries multiple currencies, we build solutions that accommodate a multilingual shopper. We needed, and found, a search engine that supports our multilingual requirements in Azure Search.\n\nAzure Search was a game-changer for our project. Prior to the availability of Azure Search, we expended considerable energy working through the kinks of building our own search engine. Having Azure Search as an online service removed the biggest technical and administrative hurdle from our solution, which meant getting to market faster, and with a more robust search experience.  \n\n## How we did it\n\nOur vision was to build a complete infrastructure based on cloud services only. Microsoft was chosen as the strategic platform because it was the provider that offered the necessary services (for both collaboration and development), scale on demand, and affordable pricing.\n \n### High-level components\n\nWe built a business, not just a site. Supporting the entire effort required a full range of tools and applications. We adopted Visual Studio and Visual Studio Online for development, Team Foundation Service (TFS) Online for source control and scrum management, Office 365 for communication and collaboration, and of course Microsoft Azure for all site-related operations and storage. With Visual Studio, the IDE provided direct provisioning to Azure, with integration to TFS Online providing an additional productivity boost.\n\nThe diagram below illustrates the high-level components used in the WhatToPedia infrastructure.\n\n   ![][8]\n\n### How we use Microsoft Azure\n\nLooking at the green boxes in the previous diagram, you’ll see that the WhatToPedia solution is built on these services:\n\n- [Azure Search](http://azure.microsoft.com/services/search/)\n- [Azure Websites using MVC 4](http://azure.microsoft.com/services/websites/)\n- [Azure WebJobs for scheduled tasks](../websites-webjobs-resources.md)\n- [Azure SQL Database](http://azure.microsoft.com/services/sql-database/)\n- [Azure BLOB Storage](http://azure.microsoft.com/services/storage/)\n- [SendGrid Email Delivery](http://azure.microsoft.com/marketplace/partners/sendgrid/sendgrid-azure/)\n\nThe very heart of the solution is data and search. The flow of data from the Reseller provider to the end customer is illustrated below:\n\n  ![][9]\n\nPrimary data storage is the reseller and accounting data in Azure SQL Database. This consists of the initial dataset, plus retailer-specific data added over time. We’re using an Azure WebJob to post updates from SQL Database to the search corpus in Azure Search.\n\n### Presentation layer\n\nThe portal is an Azure Website, implemented in MVC 4 and [Twitter Bootstrap](http://en.wikipedia.org/wiki/Bootstrap_%28front-end_framework%29). We chose MVC because it offers a much cleaner approach to HTML than ASP.NET forms-based development. To avoid having to create apps for multiple devices and maintain multiple mobile platforms, Twitter Bootstrap was chosen to support all devices and platforms.\n\n### Authentication, permissions and sensitive data\n\nShoppers browse the site anonymously. As such, there are no login requirements for shoppers, nor do we store any consumer data. \n\nRetailers are a different story. Here, we store public-facing profile information, billing information, and media content that they want to expose on the site. Every retailer who subscribes to the site get a user login, used to authenticate the user prior to making updates to the subscriber’s profile.  We securely store all subscriber data in Azure SQL Database and Azure BLOB storage.\nWe opted for an authentication model based on .NET forms-based authentication. We chose this approach for its simplicity; we didn’t need the roles, UI support and other extraneous features that come with other approaches. \n\nTo ensure that retailers only see the data that belongs to them, we created a retailer ID for each retailer that is subsequently used on all read and write operations involving retailer-specific data. With this approach, we found that we did not need to grant database permissions to individual retailers. All retailers interact with the system under a single database role, with the retailer ID as our data isolation technique.\n\nBecause our business is all about the downstream effects (driving more business to retailers, creating incentive to advertise and subscribe), we can draw the line at handling purchases over the web. As such, you won’t find a shopping cart on our site, which simplifies our security requirements. \n\nAnother simplification we employed was to outsource our billing and accounts payable operations. By routing customer payment information directly to a third-party ([SveaWebPay](http://www.sveawebpay.se/)), we reduce the risks associating with storing and protecting sensitive data in our data stores. \n\n### Search Engine\n\nThe core of our solution is the search engine built on Azure Search service. Initially, we built a custom search engine, but during this process, we realized the complexity and effort was very high indeed, and that prompted us to consider other alternatives. \n\nBasic features that were most important to us included:\n\n- Filters\n- Faceted navigation\n- Boosting results\n- Paging through AJAX\n\nAn internet search brought us to the following video, which inspired us to give Azure Search a try: [Deep Dive at TechEd Europe](http://channel9.msdn.com/events/TechEd/Europe/2014/DBI-B410) \n\nAfter watching the video, we were ready to build a prototype based on what we saw. Because we already had a data model in MVC, creating the prototype was straightforward because the data contained searchable terms, and we had already worked out the requirements for how we wanted to sort, facet, and filter the data. \n\nThis is how we built the prototype.\n\n**Configure Azure Search Service**\n\n1. Login to Azure portal and added the Search service to our subscription. We used the shared version (free with our subscription).\n2. Create an index. For the prototype, we used the portal UI to define the search fields and create the scoring profiles. Our scoring profile is based on location data: country | city |address (see: Add scoring profiles).\n3. Copy the service URL and admin api-key to our configuration files. This key is on the Search service page in the portal, and it’s used to authenticate to the service.\n    \n**Develop a Search Indexer Job – Windows Console**\n\n1. Read all resellers from database.\n2. Call the Azure Search Service API to upload resellers one by one (see: http://msdn.microsoft.com/library/azure/dn798930.aspx).\n3. Set a property in database that reseller is indexed for incremental indexing. We did this by adding an ‘indexer’ field that stores the index status of each profile (indexed or not). \n\nSee the appendix for the code snippet that builds the indexer job.\n\n**Develop a Search Web Portal – MVC**\n\n1. Call Azure Search Service to get all documents from search (see: http://msdn.microsoft.com/library/azure/dn798927.aspx)\n2. Extract following from the search service response (by using json.net http://james.newtonking.com/json)\n   - Results\n   - Facets\n   - Result counts\n   - Develop a user interface for displaying search results, facets and counts (we already had this).\n\nThis is the code we used to get the results from Azure Search:\n\n    string requestUrl = \n    string.Format(\"https://{0}.search.windows.net/indexes/profiles/docs?searchMode=all&$count=true&search={1}&facet=city,count:20&facet=category&$top=10&$skip={2}&api-version=2014-07-31-Preview{3}\", Config.SearchServiceName, EscapeODataString(q), skip, filter);\n      var response = client.GetAsync(requestUrl).Result; //  + Uri.EscapeDataString(\"hennes\")\n      response.EnsureSuccessStatusCode();\n     dynamic json = JsonConvert.DeserializeObject(response.Content.ReadAsStringAsync().Result);\n\n**Boosting by location**\n\nProbably the most important requirement to verify in the prototype included adding a location search keyword to the query. It is vital to our portal that if a user enters a city name in the search query, that the resellers in the given city would rank higher than resellers having the city keyword in the description. For this requirement, we used a scoring profile to rank the city field higher than other fields.\n\n**Supporting multiple languages**\n\nWe needed to display correct search results in correct languages, and provide an option for finding the same results in different languages. The two sides to this problem were: \n\n- Search for words in multiple languages\n- Display search results in correct language\n\nWe solved the presentation part by adding a document for each language with localized text and a property with the language. When a user enters a search term, we user `$filter` expressions to filter on the language the user has chosen.\n\nEach of the documents has a hidden property called \"cities\" built on the collection type. This property stores city names in all languages, enabling the user to search in multiple languages.\n\n###Data storage\n\nAll data (profile, subscription, and accounting) is stored in SQL Database. All media files are stored in Azure BLOB storage, including images and videos provided by the retailer. Using separate BLOB storage isolates the effects of uploading files; files are never co-mingled with the website, so we don’t need to rebuild the site whenever we add files.\n\nAn important benefit of our storage design is that multiple developers can share a single development storage. One of the requirements for the WhatToPedia project was to be able to create a development environment within 15 minutes, including reseller data, images, and videos. By getting the latest data from TFS Online, running a SQL script, and running the import job, a complete environment can be stood up in no time at all. This practice also improves the staging process.\n\n###WebJobs\n\nWe use Azure WebJobs to update data to the index. By creating a search indexer job, the indexing part was very easy to integrate into our solution. The only code change we made was to accommodate the indexer job was to add an `Indexed` field to our data model to indicate the index state. Whenever a new profile is added or updated, the `Indexed` field is set to false. The same applies if the retailer changes his or her profile data through the portal.  \n\nThe job looks for all rows having `Indexed` set to false. When it finds the row, the document is posted to Azure Search, and then the `Indexed` field is set to true. We didn’t have to plan for adding versus updating data because the Azure Search service actually takes care of this. If you add a document that is already present, the service will do an update automatically.\n\nAll web jobs have been developed as console applications that can be uploaded to Azure web sites as ZIP files, unzipped, and then scheduled.\n\nThe job is scheduled to run every 5 minutes as a scheduled web task. We calculated that the service takes approximately three minutes to upload 3,000 documents, which was within our requirements. \n\n> [AZURE.NOTE] There is a prototype indexer feature that was recently introduced in Azure Search. This feature came too late for us to use it in our first release, but it appears to solve the same problem we used our indexer job for, which is to automate data load operations.\n\n\n###Backup strategy\n\nWe designed a multi-tiered backup strategy to recover from a range of scenarios, from catastrophic failure, down to recovery of an individual transaction. The assets to protect include three kinds of data (web site, subscriber data, and media files). \n\nFirst, by keeping the web site source code in TFS Online, we know that if the site goes down, we can rebuild it by republishing from TFS. \n\nSubscriber data in Azure SQL Database is the most sensitive asset. We back this up using the built-in feature (see [Azure SQL Database Backup and Restore](http://msdn.microsoft.com/library/azure/jj650016.aspx)). The backup schedule is full database backup once a week, differential database backups once a day, and transaction log backups every 5 minutes.  Given the size of the data, this solution is more than adequate for our immediate and projected data volumes.\n\nThird, we store image and video files in Azure BLOB storage. We are still evaluating the ultimate backup plan for this data, considering Cloudberry Explorer for Azure as a potential solution. For now, we use a WebJob to copy images and videos to another location.\n\n##What we learned\n\nBecause we already had data, it was easy to establish proof-of-concept. Within hours, we had a prototype with facets and counters, paging, ranked profiles, and search results. The search results were so precise, we decided to remove some of the filters presented to the end customer. \n\nThe biggest surprise for us was how fast we could learn Azure Search, and how much we got back. Literally, we established proof-of-concept in a few hours (see the note below), replacing 500 lines of code with 3 lines of code in the front end application (plus a new WebJob), and getting better results. \n\nPreviously, our code implemented paging, counts, and other behaviors that are standard to search. Using Azure Search, the results we get back include the search hits, facets, paging data, counts -- all the stuff we needed and were having to supply ourselves. It also included boosting and built-in faceted navigation, which we didn’t have in our original solution.\n\nThe greatest challenge during implementation was that it was a Preview version and finding information and shared experiences was difficult. Once we connected a few dots, we found that using Azure Search Service was pretty simple due to its REST API and JSON data format. We could call the framework directly from most open source plugins like JQuery JSON.Net, and we could use tools like Fiddler for fast experimentation and debugging. \n\n> [AZURE.NOTE] Besides having the data prepped, it helped that those of us building the prototype already understood how search technology works, making us more productive, and more appreciative of the built-in features. If you need to ramp up on search query construction, faceted navigation, filters, etc. you should expect prototyping to take longer. \n\n###Controlling facets in the search presentation page\n\nOne of our learnings during the proof-of-concept was to plan facets carefully upfront. After loading a lot of data into the solution, we saw that the sheer volume of facets was too high to present to the users. \n\nWe solved this by constraining the facet count parameter. The count parameter imposes a hard limit on the number of facets returned to the user. A link that includes a discussion of the count parameter can be found [here](search-faceted-navigation.md).\n\n###WebJobs for scheduling tasks\n\nAzure Search wasn’t the only pleasant surprise for us. We discovered that using WebJobs to automate our data load operations to Azure Search was vastly superior to our previous approach, which entailed using a dedicated VM running Windows Scheduler, with scheduled tasks for updating the search index. WebJobs was simpler to configure and easier to debug, and of course much cheaper than having to pay for a dedicated VM.\n\n###Azure BLOB Storage Explorer for updating images\n\nWe found that using [Azure BLOB Storage Explorer](https://azurestorageexplorer.codeplex.com/) (available on codeplex) to be very helpful in managing image and video updates to the site. We use it as a developer tool to manually update images and videos that are part of our main site. We found it to be more flexible than deploying changes to the portal, and eliminates a complete test iteration whenever we need to update an image. \n\n##A few final words\n\nThanks to the great folks at WhatToPedia for allowing us to share their story!  \n\nWe hope you found this case study useful. If you go on to use Azure Search, I recommend a few resources to speed you along:\n\n- [MSDN forum dedicated to Azure Search](https://social.msdn.microsoft.com/forums/azure/home?forum=azuresearch)\n- [StackOverflow also has a tag](http://stackoverflow.com/questions/tagged/azure-search)\n- [Documentation page on Azure.com](http://azure.microsoft.com/documentation/services/search/)\n- [Azure Search documentation on MSDN](http://msdn.microsoft.com/library/azure/dn798933.aspx)\n\n\n##Appendix: Search Indexer WebJob\n\nThe following code builds the indexer mentioned in the section on building the prototype.\n\n        static void Main(string[] args)\n        {\n            int success = 0;\n            int errors = 0;\n\n            Log.Write(\"Starting job\",\"\", System.Diagnostics.TraceLevel.Info);\n\n            var serviceName = ConfigurationManager.AppSettings[\"SearchServiceName\"];\n            var serviceKey = ConfigurationManager.AppSettings[\"SearchServiceKey\"];\n\n            HttpClient client = new HttpClient();\n            client.DefaultRequestHeaders.Add(\"api-key\", serviceKey);\n\n            var db = new DB(Config.ConectionString);\n\n            var recreateIndex = false;\n            Boolean.TryParse(ConfigurationManager.AppSettings[\"RecreateIndex\"], out recreateIndex);\n\n            if(recreateIndex)\n            {\n                Log.Write(\"Recreating index and set all to no index\", \"\", System.Diagnostics.TraceLevel.Info);\n                db.SetAllToNotIndexed();\n                RecreateIndex(serviceName, client);\n            }            \n            \n            var profiles = db.Profiles.Where(p=>!p.Indexed).ToList();\n\n            Log.Write(string.Format(\"Indexing {0} profiles\",profiles.Count),\"\", System.Diagnostics.TraceLevel.Info);\n\n            var cities = db.Cities.ToList();\n            var categories = db.Tags.Where(p=>p.ParentId==null).ToList();            \n\n            foreach (var profile in profiles)\n            {\n                Log.Write(string.Format(\"Indexing profile {0}\", profile.Name),\"\",profile.ProfileId,0,System.Diagnostics.TraceLevel.Verbose);\n\n                try\n                {\n                    var city = cities.Where(p => p.CityId == profile.CityId);\n                    var category = categories.Where(p => p.TagId == profile.CategoryId);\n\n                    var cityse = city.Where(p => p.Lang == \"se\").FirstOrDefault();\n                    var cityen = city.Where(p => p.Lang == \"en\").FirstOrDefault();\n                    var categoryse = category.Where(p => p.Lang == \"se\").FirstOrDefault();\n                    var categoryen = category.Where(p => p.Lang == \"en\").FirstOrDefault();\n\n                    var citysename = cityse == null ? \"\" : cityse.Name;\n                    var cityenname = cityen == null ? \"\" : cityen.Name;\n                    var categorysename = categoryse == null ? \"\" : categoryse.Name;\n                    var categoryenname = categoryen == null ? \"\" : categoryen.Name;\n\n                    var tags = db.GetTagsFromProfile(profile.ProfileId);\n\n                    var batch = new\n                    {\n                        value = new[] \n                    { \n                        new \n                        { \n                            id = profile.ProfileId.ToString()+\"_en\",\n                            profileid = profile.ProfileId.ToString(),\n                            city = cityenname,\n                            category = categoryenname,\n                            address = profile.Adress1,\n                            email = profile.Email,\n                            name = profile.Name,\n                            lang = \"en\",\n                            brands = profile.Brands,\n                            descen=profile.DescEn,\n                            descse=profile.DescSe,\n                            orgnumber=profile.OrgNumber,\n                            phone=profile.Phone,\n                            zip=profile.Zip,\n                            cities = city.Select(p=>p.Name).ToArray(),\n                            categories = category.Select(p=>p.Name).ToArray(),\n                            cityid = profile.CityId.ToString(),\n                            tags=tags.ToArray()\n                        },\n                        new \n                        { \n                            id = profile.ProfileId.ToString()+\"_se\",\n                            profileid = profile.ProfileId.ToString(),\n                            city = citysename,\n                            category = categorysename,\n                            address = profile.Adress1,\n                            email = profile.Email,\n                            name = profile.Name,\n                            lang = \"se\",\n                            brands = profile.Brands,\n                            descen=profile.DescEn,\n                            descse=profile.DescSe,\n                            orgnumber=profile.OrgNumber,\n                            phone=profile.Phone,\n                            zip=profile.Zip,\n                            cities = city.Select(p=>p.Name).ToArray(),\n                            categories = category.Select(p=>p.Name).ToArray(),\n                            cityid = profile.CityId.ToString(),\n                            tags=tags.ToArray()\n                        }\n                    },\n                    };\n\n                    var response = client.PostAsync(\"https://\" + serviceName + \".search.windows.net/indexes/profiles/docs/index?api-version=2014-10-20-Preview\", new StringContent(JsonConvert.SerializeObject(batch), Encoding.UTF8, \"application/json\")).Result;\n                    response.EnsureSuccessStatusCode();\n\n                    db.Entry(profile).State = System.Data.Entity.EntityState.Modified;\n                    profile.Indexed = true;\n                    db.SaveChanges();\n                    success++;\n                }\n                catch(Exception ex)\n                {\n                    Log.Write(\"Error indexing profile\", ex.Message, profile.ProfileId, 0, System.Diagnostics.TraceLevel.Verbose);\n                    errors++;\n                }\n            }\n            if(errors > 0)\n            {\n                Log.Write(string.Format(\"Job ended success ({0}), errors ({1})\", success, errors), \"\", System.Diagnostics.TraceLevel.Error);\n            }\n            else\n            {\n                Log.Write(string.Format(\"Job ended success ({0}), errors ({1})\", success, errors), \"\", System.Diagnostics.TraceLevel.Info);\n            }\n            \n        }\n\n        static void RecreateIndex( string ServiceName, HttpClient client)\n        {\n            var index = new\n            {\n                name = \"profiles\",\n                fields = new[] \n                { \n                    new { name = \"id\",              type = \"Edm.String\",         key = true,  searchable = false, filterable = false, sortable = false, facetable = false, retrievable = true,  suggestions = false },\n                    new { name = \"profileid\",       type = \"Edm.String\",         key = false,  searchable = false, filterable = false, sortable = false, facetable = false, retrievable = true,  suggestions = false },\n                    new { name = \"cityid\",          type = \"Edm.String\",         key = false,  searchable = false, filterable = false, sortable = false, facetable = false, retrievable = true,  suggestions = false },\n                    new { name = \"city\",            type = \"Edm.String\",         key = false, searchable = true,  filterable = true, sortable = true,  facetable = true, retrievable = true,  suggestions = true  },\n                    new { name = \"category\",        type = \"Edm.String\",         key = false, searchable = true,  filterable = true, sortable = false, facetable = true, retrievable = true,  suggestions = false  },\n                    new { name = \"address\",         type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"email\",           type = \"Edm.String\",         key = false, searchable = true,  filterable = false, sortable = true, facetable = false, retrievable = true,  suggestions = false },\n                    new { name = \"name\",            type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true, suggestions = true },\n                    new { name = \"lang\",            type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = false,  facetable = false,  retrievable = true, suggestions = false },\n                    new { name = \"brands\",          type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"descen\",          type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"descse\",          type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"orgnumber\",       type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"phone\",           type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"zip\",             type = \"Edm.String\",         key = false, searchable = true,  filterable = true,  sortable = true,  facetable = false,  retrievable = true,  suggestions = false },\n                    new { name = \"cities\",          type = \"Collection(Edm.String)\",         key = false, searchable = true,  filterable = false,  sortable = false,  facetable = false,  retrievable = false, suggestions = false },\n                   new { name = \"categories\",      type = \"Collection(Edm.String)\",         key = false, searchable = true,  filterable = false,  sortable = false,  facetable = false,  retrievable = false, suggestions = false },\n                    new { name = \"tags\",            type = \"Collection(Edm.String)\",         key = false, searchable = true,  filterable = false,  sortable = false,  facetable = false,  retrievable = false, suggestions = false }\n                    \n                }\n            };\n\n            var url = \"https://\" + ServiceName + \".search.windows.net/indexes/?api-version=2014-10-20-Preview\";\n\n            var deleteUrl = \"https://\" + ServiceName + \".search.windows.net/indexes/profiles?api-version=2014-10-20-Preview\";\n\n            try\n            {\n                var deleteResponseIndex = client.DeleteAsync(deleteUrl).Result;\n                deleteResponseIndex.EnsureSuccessStatusCode();\n            }\n            catch (Exception ex)\n            {\n\n            }\n\n            var responseIndex = client.PostAsync(url, new StringContent(JsonConvert.SerializeObject(index), Encoding.UTF8, \"application/json\")).Result;\n            responseIndex.EnsureSuccessStatusCode();            \n          \n\n\n<!--Anchors-->\n[Subheading 1]: #subheading-1\n[Subheading 2]: #subheading-2\n[Subheading 3]: #subheading-3\n[Subheading 4]: #subheading-4\n[Subheading 5]: #subheading-5\n[Next steps]: #next-steps\n\n<!--Image references-->\n[6]: ./media/search-dev-case-study-whattopedia/lightbulb.png\n[7]: ./media/search-dev-case-study-whattopedia/WhatToPedia-Search-Bageri.png\n[8]: ./media/search-dev-case-study-whattopedia/WhatToPedia-Stack.png\n[9]: ./media/search-dev-case-study-whattopedia/WhatToPedia-Archicture.png\n\n\n<!--Link references-->\n[Link 1 to another azure.microsoft.com documentation topic]: ../virtual-machines-windows-tutorial.md\n[Link 2 to another azure.microsoft.com documentation topic]: ../web-sites-custom-domain-name.md\n[Link 3 to another azure.microsoft.com documentation topic]: ../storage-whatis-account.md\n "
}