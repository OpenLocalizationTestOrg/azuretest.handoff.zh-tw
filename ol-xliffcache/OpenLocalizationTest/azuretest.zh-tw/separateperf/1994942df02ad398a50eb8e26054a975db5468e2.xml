{
  "nodes": [
    {
      "content": "Azure Data Factory Editor",
      "pos": [
        28,
        53
      ]
    },
    {
      "content": "Describes the Azure Data Factory Editor, which allows you to create, edit, and deploy JSON definitions of linked services, tables, and pipelines using a light-weight web-based UI.",
      "pos": [
        73,
        252
      ]
    },
    {
      "content": "Azure Data Factory Editor",
      "pos": [
        580,
        605
      ]
    },
    {
      "content": "The Azure Data Factory Editor is a light weight Web editor that is part of the Azure Preview Portal, which allows you to create, edit, and deploy JSON files of all Azure Data Factory entities.",
      "pos": [
        606,
        798
      ]
    },
    {
      "content": "This enables you to create linked services, data sets, and pipelines by using the JSON templates that ship with the Data Factory service.",
      "pos": [
        799,
        936
      ]
    },
    {
      "content": "If you are new PowerShell, this removes the need for installing and ramping up on Azure PowerShell to create Azure data factories.",
      "pos": [
        937,
        1067
      ]
    },
    {
      "content": "Launching Data Factory Editor",
      "pos": [
        1072,
        1101
      ]
    },
    {
      "pos": [
        1102,
        1222
      ],
      "content": "To launch Data Factory Editor, click <bpt id=\"p1\">**</bpt>Author &amp; Deploy<ept id=\"p1\">**</ept> tile on the <bpt id=\"p2\">**</bpt>Data Factory<ept id=\"p2\">**</ept> blade for your Azure data factory."
    },
    {
      "content": "Author and Deploy Tile",
      "pos": [
        1227,
        1249
      ]
    },
    {
      "content": "You will see the Data Factory Editor as shown in the following image:",
      "pos": [
        1276,
        1345
      ]
    },
    {
      "content": "Data Factory Editor",
      "pos": [
        1350,
        1369
      ]
    },
    {
      "content": "Creating new linked services, data sets, and pipelines",
      "pos": [
        1397,
        1451
      ]
    },
    {
      "content": "There are four buttons on the toolbar that you can use to create Azure Data Factory entities.",
      "pos": [
        1452,
        1545
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>New data store<ept id=\"p1\">**</ept> for creating a data store linked service.",
      "pos": [
        1550,
        1610
      ]
    },
    {
      "content": "When you click this button, you will see a menu with the following options: Azure storage, Azure SQL database, On-premises SQL server database.",
      "pos": [
        1611,
        1754
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>New compute<ept id=\"p1\">**</ept> for creating a compute linked service.",
      "pos": [
        1757,
        1811
      ]
    },
    {
      "content": "When you click this button, you will see a menu with the following options: On-demand HDInsight cluster, HDInsight cluster, AzureML linked service.",
      "pos": [
        1812,
        1959
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>New dataset<ept id=\"p1\">**</ept> for creating a dataset.",
      "pos": [
        1968,
        2007
      ]
    },
    {
      "content": "When you click this button, you will see the following options: Blob table, Azure SQL table, On-premises table.",
      "pos": [
        2008,
        2119
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>New pipeline<ept id=\"p1\">**</ept> for creating a pipeline.",
      "pos": [
        2124,
        2165
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>... (ellipsis)<ept id=\"p1\">**</ept> on the toolbar if you do not see this button on the toolbar.",
      "pos": [
        2166,
        2251
      ]
    },
    {
      "content": "To create a storage linked service",
      "pos": [
        2258,
        2292
      ]
    },
    {
      "pos": [
        2296,
        2363
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>New data store<ept id=\"p1\">**</ept>, and click one of the options in the menu."
    },
    {
      "content": "New data store menu",
      "pos": [
        2372,
        2391
      ]
    },
    {
      "content": "You will see the JSON template for creating a storage linked service in the <bpt id=\"p1\">**</bpt>Editor canvas<ept id=\"p1\">**</ept> to the right.",
      "pos": [
        2418,
        2525
      ]
    },
    {
      "content": "You will also see that a draft node appears under <bpt id=\"p1\">**</bpt>Drafts<ept id=\"p1\">**</ept>.",
      "pos": [
        2526,
        2587
      ]
    },
    {
      "content": "Do the following:",
      "pos": [
        2588,
        2605
      ]
    },
    {
      "pos": [
        2613,
        2733
      ],
      "content": "For <bpt id=\"p1\">**</bpt>Azure storage<ept id=\"p1\">**</ept>: replace <bpt id=\"p2\">**</bpt>&lt;accountname\\&gt;<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>&lt;accountkey\\&gt;<ept id=\"p3\">**</ept> with name and key of your Azure storage account."
    },
    {
      "pos": [
        2741,
        2998
      ],
      "content": "For <bpt id=\"p1\">**</bpt>Azure SQL database<ept id=\"p1\">**</ept>: replace <bpt id=\"p2\">**</bpt>&lt;servername\\&gt;<ept id=\"p2\">**</ept> with name of your Azure SQL server, <bpt id=\"p3\">**</bpt>&lt;databasename\\&gt;<ept id=\"p3\">**</ept> with the name of the database, <bpt id=\"p4\">**</bpt>&lt;username\\&gt;@&lt;servername\\&gt;<ept id=\"p4\">**</ept> with the name of the user, and <bpt id=\"p5\">**</bpt>&lt;password\\&gt;<ept id=\"p5\">**</ept> with the password for the user account."
    },
    {
      "pos": [
        3007,
        3269
      ],
      "content": "For <bpt id=\"p1\">**</bpt>On-premises SQL server database<ept id=\"p1\">**</ept>: replace <bpt id=\"p2\">**</bpt>&lt;servername\\&gt;<ept id=\"p2\">**</ept> with name of your on-premises SQL server, <bpt id=\"p3\">**</bpt>&lt;databasename\\&gt;<ept id=\"p3\">**</ept> with the name of the database, <bpt id=\"p4\">**</bpt>&lt;username\\&gt;<ept id=\"p4\">**</ept> with the name of the user, and <bpt id=\"p5\">**</bpt>&lt;password\\&gt;<ept id=\"p5\">**</ept> with the password for the user account."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to deploy the linked service.",
      "pos": [
        3273,
        3334
      ]
    },
    {
      "content": "You can click <bpt id=\"p1\">**</bpt>Discard<ept id=\"p1\">**</ept> to discoard the JSON draft you created.",
      "pos": [
        3335,
        3400
      ]
    },
    {
      "content": "Deploy button",
      "pos": [
        3409,
        3422
      ]
    },
    {
      "content": "You should see the status of the Deploy operation on the title bar.",
      "pos": [
        3443,
        3510
      ]
    },
    {
      "content": "Deploy success message",
      "pos": [
        3518,
        3540
      ]
    },
    {
      "content": "If the deploy operation is successful, you should see the created linked service in the tree view to the left.",
      "pos": [
        3569,
        3679
      ]
    },
    {
      "content": "StorageLinkedService in tree view",
      "pos": [
        3689,
        3722
      ]
    },
    {
      "content": "To create a compute linked service",
      "pos": [
        3762,
        3796
      ]
    },
    {
      "pos": [
        3800,
        3863
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>New compute<ept id=\"p1\">**</ept> and click one of the options in the menu."
    },
    {
      "content": "New compute menu",
      "pos": [
        3872,
        3888
      ]
    },
    {
      "content": "You will see the JSON template for creating a compute linked service in the Editor canvas to the right.",
      "pos": [
        3912,
        4015
      ]
    },
    {
      "content": "Do the following:",
      "pos": [
        4016,
        4033
      ]
    },
    {
      "pos": [
        4041,
        4122
      ],
      "content": "For <bpt id=\"p1\">**</bpt>On-demand HDInsight cluster<ept id=\"p1\">**</ept>, specify values for the following properties:"
    },
    {
      "pos": [
        4135,
        4266
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>clusterSize<ept id=\"p1\">**</ept> property, specify the size of the HDInsight cluster you want the Data Factory service to create at runtime."
    },
    {
      "pos": [
        4279,
        4409
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>jobsContainer<ept id=\"p1\">**</ept> property, specify the name of the default blob container where you want the cluster logs will be stored."
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>timeToLive<ept id=\"p1\">**</ept> property, specify the allowed idle time before the HDInsight cluster is deleted.",
      "pos": [
        4421,
        4524
      ]
    },
    {
      "content": "For example: 00:05:00 indicates that the cluster should be be deleted after 5 minutes of idle time.",
      "pos": [
        4525,
        4624
      ]
    },
    {
      "pos": [
        4636,
        4731
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>version<ept id=\"p1\">**</ept> property, specify HDInsight version for the cluster (default: version 3.1)."
    },
    {
      "pos": [
        4743,
        4868
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> property, specify the Azure storage linked service to be associated with the HDInsight cluster."
    },
    {
      "pos": [
        4877,
        4965
      ],
      "content": "For <bpt id=\"p1\">**</bpt>HDInsight cluster<ept id=\"p1\">**</ept> (Bring-your-own), specify values for the following properties:"
    },
    {
      "pos": [
        4977,
        5057
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>clusterUri<ept id=\"p1\">**</ept> property, specify the URL for your own HDInsight cluster."
    },
    {
      "pos": [
        5070,
        5204
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>userName<ept id=\"p1\">**</ept> property, specify the user account that the Data Factory service should use to connect to your HDInsight cluster."
    },
    {
      "pos": [
        5217,
        5290
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>password<ept id=\"p1\">**</ept> property, specify the password for the user account."
    },
    {
      "pos": [
        5303,
        5432
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> property, specify the Azure storage linked service that is associated with yhour HDInsight cluster."
    },
    {
      "pos": [
        5440,
        5505
      ],
      "content": "For <bpt id=\"p1\">**</bpt>AzureML linked service<ept id=\"p1\">**</ept>, specify the following properties:"
    },
    {
      "pos": [
        5517,
        5603
      ],
      "content": "for the <bpt id=\"p1\">**</bpt>mlEndPoint<ept id=\"p1\">**</ept> property, specify the Azure Machine Learning batch scoring URL."
    },
    {
      "pos": [
        5615,
        5698
      ],
      "content": "for the <bpt id=\"p1\">**</bpt>apiKey<ept id=\"p1\">**</ept> property, specify the API key for the published workspace model."
    },
    {
      "pos": [
        5702,
        5763
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the toolbar to deploy the linked service."
    },
    {
      "pos": [
        5767,
        5955
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> See the <bpt id=\"p1\">[</bpt>Linked Services<ept id=\"p1\">][msdn-linkedservices-reference]</ept> topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory linked service.."
    },
    {
      "content": "To create a new dataset",
      "pos": [
        5963,
        5986
      ]
    },
    {
      "pos": [
        5990,
        6053
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>New dataset<ept id=\"p1\">**</ept> and click one of the options in the menu."
    },
    {
      "content": "You will see the JSON template for creating a dataset in the Editor canvas to the right.",
      "pos": [
        6057,
        6145
      ]
    },
    {
      "content": "Do the following:",
      "pos": [
        6146,
        6163
      ]
    },
    {
      "pos": [
        6172,
        6236
      ],
      "content": "For <bpt id=\"p1\">**</bpt>Blob table<ept id=\"p1\">**</ept>, specify values for the following properties:"
    },
    {
      "pos": [
        6244,
        6338
      ],
      "content": "For <bpt id=\"p1\">**</bpt>Azure SQL table<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>On-premises table<ept id=\"p2\">**</ept>, specify values for the following properties:"
    },
    {
      "pos": [
        6351,
        6379
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>location<ept id=\"p1\">**</ept> section:"
    },
    {
      "pos": [
        6396,
        6538
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> property, specify the name of the linked service that referes to your Azure SQL/On-premises SQL Server database."
    },
    {
      "pos": [
        6554,
        6709
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>tableName<ept id=\"p1\">**</ept> property, specify the name of the table in the Azure SQL Database instance/On-premises SQL server that the linked service referes to."
    },
    {
      "pos": [
        6721,
        6753
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> section:"
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>frequency<ept id=\"p1\">**</ept> property, specify the time unit for data slice production.",
      "pos": [
        6769,
        6849
      ]
    },
    {
      "content": "The supported frequency values:Minute, Hour, Day, Week, Month.",
      "pos": [
        6850,
        6912
      ]
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>interval<ept id=\"p1\">**</ept> property, specify the interval within the defined frequency.",
      "pos": [
        6928,
        7009
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>frequency<ept id=\"p1\">**</ept> set to <bpt id=\"p2\">**</bpt>Hour<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>interval<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>1<ept id=\"p4\">**</ept> indicates that new data slices should be produced hourly.",
      "pos": [
        7010,
        7127
      ]
    },
    {
      "pos": [
        7140,
        7169
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>structure<ept id=\"p1\">**</ept> section:"
    },
    {
      "content": "specify names and types of columns as shown in the following example:",
      "pos": [
        7186,
        7255
      ]
    },
    {
      "pos": [
        7493,
        7654
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> See the <bpt id=\"p1\">[</bpt>Tables<ept id=\"p1\">][msdn-tables-reference]</ept> topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory table."
    },
    {
      "content": "To create and activate a pipeline",
      "pos": [
        7681,
        7714
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>New pipeline<ept id=\"p1\">**</ept> on the toolbar.",
      "pos": [
        7719,
        7757
      ]
    },
    {
      "content": "If you do not see the <bpt id=\"p1\">**</bpt>New pipeline<ept id=\"p1\">**</ept> button, click <bpt id=\"p2\">**</bpt>...(ellipsis)<ept id=\"p2\">**</ept> to see it.",
      "pos": [
        7758,
        7839
      ]
    },
    {
      "content": "You will see the JSON template for creating a pipeline in the Editor canvas to the right.",
      "pos": [
        7846,
        7935
      ]
    },
    {
      "content": "Do the following:",
      "pos": [
        7936,
        7953
      ]
    },
    {
      "pos": [
        7962,
        8033
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>description<ept id=\"p1\">**</ept> property, specify description for the pipeline."
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>activities<ept id=\"p1\">**</ept> section, add activities to the pipeline.",
      "pos": [
        8041,
        8104
      ]
    },
    {
      "content": "Example:",
      "pos": [
        8105,
        8113
      ]
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> property, specify when data processing starts or the data slices will be processed.",
      "pos": [
        9240,
        9341
      ]
    },
    {
      "content": "Example : 2014-05-01T00:00:00Z.",
      "pos": [
        9342,
        9373
      ]
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> property, specify when data processing ends.",
      "pos": [
        9381,
        9441
      ]
    },
    {
      "content": "Example : 2014-05-01T00:00:00Z.",
      "pos": [
        9442,
        9473
      ]
    },
    {
      "pos": [
        9484,
        9669
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> See the <bpt id=\"p1\">[</bpt>Pipelines and Activities<ept id=\"p1\">][msdn-pipelines-reference]</ept> topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory pipeline."
    },
    {
      "content": "To add an activity definition to a pipeline JSON",
      "pos": [
        9674,
        9722
      ]
    },
    {
      "content": "You can add an activity definition to a pipeline JSON by clicking <bpt id=\"p1\">**</bpt>Add Activity<ept id=\"p1\">**</ept> on the toolbar.",
      "pos": [
        9723,
        9821
      ]
    },
    {
      "content": "When you click this button, you choose the type of activity that you want to be added to the pipeline.",
      "pos": [
        9822,
        9924
      ]
    },
    {
      "content": "Add Activity options",
      "pos": [
        9930,
        9950
      ]
    },
    {
      "content": "If you want to copy data from an Azure SQL database to Azure blob storage and process the data in the blob storage by using Pig script on a HDInsight cluster, you first add a <bpt id=\"p1\">**</bpt>Copy activity<ept id=\"p1\">**</ept> and then add a <bpt id=\"p2\">**</bpt>Pig activity<ept id=\"p2\">**</ept> to the pipeline.",
      "pos": [
        9975,
        10216
      ]
    },
    {
      "content": "This creates two sections with in the activities[] section of the pipeline JSON.",
      "pos": [
        10217,
        10297
      ]
    },
    {
      "content": "Pig activity is nothing but the HDInsight Activity with Pig transformation.",
      "pos": [
        10298,
        10373
      ]
    },
    {
      "content": "Starting a pipeline",
      "pos": [
        10747,
        10766
      ]
    },
    {
      "content": "You can specify the start date-time and end date-time for a pipeline by specifying values for the start and end properties in the JSON.",
      "pos": [
        10767,
        10902
      ]
    },
    {
      "content": "Drafts in the editor",
      "pos": [
        11281,
        11301
      ]
    },
    {
      "content": "Drafts allows you to temporarily save your work when you are context switching or navigating to a different entity in the Data Factory.",
      "pos": [
        11302,
        11437
      ]
    },
    {
      "content": "The lifetime of the Drafts is associated with the browser session.",
      "pos": [
        11438,
        11504
      ]
    },
    {
      "content": "If you close the browser or use another machine, the drafts are not going to be available.",
      "pos": [
        11505,
        11595
      ]
    },
    {
      "content": "To discard a JSON draft of a Data Factory entity",
      "pos": [
        11600,
        11648
      ]
    },
    {
      "pos": [
        11649,
        11763
      ],
      "content": "You can discard the JSON definition of an Azure Data Factory entity by clicking <bpt id=\"p1\">**</bpt>Discard<ept id=\"p1\">**</ept> button on the toolbar."
    },
    {
      "content": "To clone a Data Factory entity",
      "pos": [
        11771,
        11801
      ]
    },
    {
      "pos": [
        11802,
        11981
      ],
      "content": "You can clone an existing Azure Data Factory entity (linked service, table, or pipeline) by selecting the entity in the tree view and clicking the <bpt id=\"p1\">**</bpt>Clone<ept id=\"p1\">**</ept> button on the toolbar."
    },
    {
      "content": "Clone data factory entity",
      "pos": [
        11985,
        12010
      ]
    },
    {
      "pos": [
        12039,
        12111
      ],
      "content": "You will see a new draft created under <bpt id=\"p1\">**</bpt>Drafts<ept id=\"p1\">**</ept> node in the tree view."
    },
    {
      "content": "To delete a Data Factory entity",
      "pos": [
        12117,
        12148
      ]
    },
    {
      "pos": [
        12149,
        12351
      ],
      "content": "You can delete an Azure Data Factory entity (linked service, table, or pipeline), select the entity in the tree view and click <bpt id=\"p1\">**</bpt>Delete<ept id=\"p1\">**</ept> on the toolbar (or) right-click the entity and click <bpt id=\"p2\">**</bpt>Delete<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Delete data factory entity",
      "pos": [
        12355,
        12381
      ]
    },
    {
      "content": "See Also",
      "pos": [
        12415,
        12423
      ]
    },
    {
      "pos": [
        12424,
        12597
      ],
      "content": "See the <bpt id=\"p1\">[</bpt>Get started with Azure Data Factory<ept id=\"p1\">][data-factory-get-started]</ept> topic for step-by-step instructions to create an Azure data factory by using the Data Factory Editor."
    },
    {
      "content": "test",
      "pos": [
        12900,
        12904
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Azure Data Factory Editor\" \n    description=\"Describes the Azure Data Factory Editor, which allows you to create, edit, and deploy JSON definitions of linked services, tables, and pipelines using a light-weight web-based UI.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/07/2015\" \n    ms.author=\"spelluru\"/>\n\n# Azure Data Factory Editor\nThe Azure Data Factory Editor is a light weight Web editor that is part of the Azure Preview Portal, which allows you to create, edit, and deploy JSON files of all Azure Data Factory entities. This enables you to create linked services, data sets, and pipelines by using the JSON templates that ship with the Data Factory service. If you are new PowerShell, this removes the need for installing and ramping up on Azure PowerShell to create Azure data factories.\n\n## Launching Data Factory Editor\nTo launch Data Factory Editor, click **Author & Deploy** tile on the **Data Factory** blade for your Azure data factory. \n\n![Author and Deploy Tile][author-and-deploy-tile]\n\nYou will see the Data Factory Editor as shown in the following image:\n \n![Data Factory Editor][data=factory-editor]\n \n## Creating new linked services, data sets, and pipelines\nThere are four buttons on the toolbar that you can use to create Azure Data Factory entities.\n \n- **New data store** for creating a data store linked service. When you click this button, you will see a menu with the following options: Azure storage, Azure SQL database, On-premises SQL server database.\n- **New compute** for creating a compute linked service. When you click this button, you will see a menu with the following options: On-demand HDInsight cluster, HDInsight cluster, AzureML linked service.      \n- **New dataset** for creating a dataset. When you click this button, you will see the following options: Blob table, Azure SQL table, On-premises table.  \n- **New pipeline** for creating a pipeline. Click **... (ellipsis)** on the toolbar if you do not see this button on the toolbar.\n \n### To create a storage linked service\n1. Click **New data store**, and click one of the options in the menu.\n \n    ![New data store menu][new-data-store-menu] \n2. You will see the JSON template for creating a storage linked service in the **Editor canvas** to the right. You will also see that a draft node appears under **Drafts**. Do the following:\n    1. For **Azure storage**: replace **<accountname\\>** and **<accountkey\\>** with name and key of your Azure storage account.\n    2. For **Azure SQL database**: replace **<servername\\>** with name of your Azure SQL server, **<databasename\\>** with the name of the database, **<username\\>@<servername\\>** with the name of the user, and **<password\\>** with the password for the user account. \n    3. For **On-premises SQL server database**: replace **<servername\\>** with name of your on-premises SQL server, **<databasename\\>** with the name of the database, **<username\\>** with the name of the user, and **<password\\>** with the password for the user account.\n4. Click **Deploy** on the toolbar to deploy the linked service. You can click **Discard** to discoard the JSON draft you created.\n \n    ![Deploy button][deploy-button]\n\n1. You should see the status of the Deploy operation on the title bar.\n\n    ![Deploy success message][deploy-success-message]\n2. If the deploy operation is successful, you should see the created linked service in the tree view to the left.\n  \n    ![StorageLinkedService in tree view][storagelinkedservice-in-treview]\n\n### To create a compute linked service\n1. Click **New compute** and click one of the options in the menu.\n \n    ![New compute menu][new-compute-menu] \n2. You will see the JSON template for creating a compute linked service in the Editor canvas to the right. Do the following:\n    1. For **On-demand HDInsight cluster**, specify values for the following properties: \n        1. For the **clusterSize** property, specify the size of the HDInsight cluster you want the Data Factory service to create at runtime. \n        2. For the **jobsContainer** property, specify the name of the default blob container where you want the cluster logs will be stored.\n        3. For the **timeToLive** property, specify the allowed idle time before the HDInsight cluster is deleted. For example: 00:05:00 indicates that the cluster should be be deleted after 5 minutes of idle time.\n        4. For the **version** property, specify HDInsight version for the cluster (default: version 3.1).\n        5. For the **linkedServiceName** property, specify the Azure storage linked service to be associated with the HDInsight cluster. \n    6. For **HDInsight cluster** (Bring-your-own), specify values for the following properties:\n        1. For the **clusterUri** property, specify the URL for your own HDInsight cluster. \n        2. For the **userName** property, specify the user account that the Data Factory service should use to connect to your HDInsight cluster. \n        3. For the **password** property, specify the password for the user account. \n        4. For the **linkedServiceName** property, specify the Azure storage linked service that is associated with yhour HDInsight cluster.\n    5. For **AzureML linked service**, specify the following properties:\n        1. for the **mlEndPoint** property, specify the Azure Machine Learning batch scoring URL.\n        2. for the **apiKey** property, specify the API key for the published workspace model.\n3. Click **Deploy** on the toolbar to deploy the linked service.\n\n> [AZURE.NOTE] See the [Linked Services][msdn-linkedservices-reference] topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory linked service..  \n\n### To create a new dataset\n1. Click **New dataset** and click one of the options in the menu.\n2. You will see the JSON template for creating a dataset in the Editor canvas to the right. Do the following: \n    1. For **Blob table**, specify values for the following properties:\n    2. For **Azure SQL table** or **On-premises table**, specify values for the following properties: \n        1. In the **location** section: \n            2. For the **linkedServiceName** property, specify the name of the linked service that referes to your Azure SQL/On-premises SQL Server database.\n            2. For the **tableName** property, specify the name of the table in the Azure SQL Database instance/On-premises SQL server that the linked service referes to.\n        3. In the **availability** section:\n            1. For the **frequency** property, specify the time unit for data slice production. The supported frequency values:Minute, Hour, Day, Week, Month.\n            2. For the **interval** property, specify the interval within the defined frequency. **frequency** set to **Hour** and **interval** set to **1** indicates that new data slices should be produced hourly. \n        3. In the **structure** section: \n            1. specify names and types of columns as shown in the following example:\n                \n                    \"structure\":\n                    [\n                        { \"name\": \"FirstName\", \"type\": \"String\"},\n                        { \"name\": \"LastName\", \"type\": \"String\"}\n                    ],\n         \n> [AZURE.NOTE] See the [Tables][msdn-tables-reference] topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory table.  \n                   \n### To create and activate a pipeline \n1. Click **New pipeline** on the toolbar. If you do not see the **New pipeline** button, click **...(ellipsis)** to see it.   \n2. You will see the JSON template for creating a pipeline in the Editor canvas to the right. Do the following: \n    1. For the **description** property, specify description for the pipeline.\n    2. For the **activities** section, add activities to the pipeline. Example:\n     \n            \"activities\":   \n            [\n                {\n                    \"name\": \"CopyFromBlobToSQL\",\n                    \"description\": \"Push Regional Effectiveness Campaign data to Azure SQL\",\n                    \"type\": \"CopyActivity\",\n                    \"inputs\": [ {\"name\": \"EmpTableFromBlob\"} ],\n                    \"outputs\": [ {\"name\": \"EmpSQLTable\"} ],     \n                    \"transformation\":\n                    {\n                        \"source\":\n                        {                               \n                            \"type\": \"BlobSource\"\n                        },\n                        \"sink\":\n                        {\n                            \"type\": \"SqlSink\"\n                        }   \n                    },\n                    \"Policy\":\n                    {\n                        \"concurrency\": 1,\n                        \"executionPriorityOrder\": \"NewestFirst\",\n                        \"style\": \"StartOfInterval\",\n                        \"retry\": 0,\n                        \"timeout\": \"01:00:00\"\n                    }       \n                }\n            ]\n    3. For the **start** property, specify when data processing starts or the data slices will be processed. Example : 2014-05-01T00:00:00Z.\n    4. For the **end** property, specify when data processing ends. Example : 2014-05-01T00:00:00Z.       \n\n> [AZURE.NOTE] See the [Pipelines and Activities][msdn-pipelines-reference] topic in MSDN Library for descriptions of JSON elements that are used to define an Azure Data Factory pipeline.\n\n## To add an activity definition to a pipeline JSON\nYou can add an activity definition to a pipeline JSON by clicking **Add Activity** on the toolbar. When you click this button, you choose the type of activity that you want to be added to the pipeline.  \n\n![Add Activity options][add-activity-options]\n\nIf you want to copy data from an Azure SQL database to Azure blob storage and process the data in the blob storage by using Pig script on a HDInsight cluster, you first add a **Copy activity** and then add a **Pig activity** to the pipeline. This creates two sections with in the activities[] section of the pipeline JSON. Pig activity is nothing but the HDInsight Activity with Pig transformation. \n\n    \"activities\": [\n        {\n            \"name\": \"CopyFromTabletoBlob\",\n            \"type\": \"CopyActivity\",\n            ...\n        }\n        {\n            \"name\": \"ProcessBlobDataWithPigScript\",\n            \"type\": \"HDInsightActivity\",\n            ...\n            \"transformation\": {\n                \"type\": \"Pig\",\n                ...\n            }\n        }\n    ]\n\n## Starting a pipeline\nYou can specify the start date-time and end date-time for a pipeline by specifying values for the start and end properties in the JSON. \n\n    {\n        \"name\": \"ADFTutorialPipeline\",\n        \"properties\":\n        {   \n            \"description\" : \"Copy data from a blob to Azure SQL table\",\n            \"activities\":   \n            [\n                ...\n            ],\n    \n            \"start\": \"2015-02-13T00:00:00Z\",\n            \"end\": \"2015-02-14T00:00:00Z\",\n            \"isPaused\": false\n        }\n    } \n  \n## Drafts in the editor\nDrafts allows you to temporarily save your work when you are context switching or navigating to a different entity in the Data Factory. The lifetime of the Drafts is associated with the browser session. If you close the browser or use another machine, the drafts are not going to be available.\n\n## To discard a JSON draft of a Data Factory entity\nYou can discard the JSON definition of an Azure Data Factory entity by clicking **Discard** button on the toolbar.   \n\n## To clone a Data Factory entity\nYou can clone an existing Azure Data Factory entity (linked service, table, or pipeline) by selecting the entity in the tree view and clicking the **Clone** button on the toolbar.\n\n![Clone data factory entity][clone-datafactory-entity]\n\nYou will see a new draft created under **Drafts** node in the tree view. \n\n## To delete a Data Factory entity\nYou can delete an Azure Data Factory entity (linked service, table, or pipeline), select the entity in the tree view and click **Delete** on the toolbar (or) right-click the entity and click **Delete**.\n\n![Delete data factory entity][delete-datafactory-entity] \n\n## See Also\nSee the [Get started with Azure Data Factory][data-factory-get-started] topic for step-by-step instructions to create an Azure data factory by using the Data Factory Editor. \n\n[msdn-tables-reference]: https://msdn.microsoft.com/library/dn835002.aspx\n[msdn-linkedservices-reference]: https://msdn.microsoft.com/library/dn834986.aspx       \n[msdn-pipelines-reference]: https://msdn.microsoft.com/library/dn834988.aspx  \n\n[data-factory-get-started]: data-factory-get-started.md\n\n[author-and-deploy-tile]: ./media/data-factory-editor/author-and-deploy-tile.png\n[data=factory-editor]: ./media/data-factory-editor/data-factory-editor.png\n[new-data-store-menu]: ./media/data-factory-editor/new-data-store-menu.png\n[new-compute-menu]: ./media/data-factory-editor/new-compute-menu.png\n[deploy-button]: ./media/data-factory-editor/deploy-button.png\n[deploy-success-message]: ./media/data-factory-editor/deploy-success-message.png\n[storagelinkedservice-in-treview]: ./media/data-factory-editor/storagelinkedservice-in-treeview.png\n[delete-datafactory-entity]: ./media/data-factory-editor/delete-datafactory-entity.png\n[clone-datafactory-entity]: ./media/data-factory-editor/clone-datafactory-entity.png\n[add-activity-options]: ./media/data-factory-editor/add-activity-options.png \ntest\n"
}