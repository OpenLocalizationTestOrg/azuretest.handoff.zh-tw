{
  "nodes": [
    {
      "content": "Develop Java MapReduce programs for Hadoop | Microsoft Azure",
      "pos": [
        27,
        87
      ]
    },
    {
      "content": "Learn how to develop Java MapReduce programs on HDInsight emulator, how to deploy them to HDInsight.",
      "pos": [
        106,
        206
      ]
    },
    {
      "content": "Develop Java MapReduce programs for Hadoop in HDInsight",
      "pos": [
        538,
        593
      ]
    },
    {
      "content": "This documents walks you through using Apache Maven to create a MapReduce application, then deploy and run it on a Linux-based Hadoop on HDInsight cluster.",
      "pos": [
        681,
        836
      ]
    },
    {
      "pos": [
        840,
        881
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"prerequisites\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "Before you begin this tutorial, you must have the following:",
      "pos": [
        883,
        943
      ]
    },
    {
      "pos": [
        947,
        1062
      ],
      "content": "<bpt id=\"p1\">[</bpt>Java JDK<ept id=\"p1\">](http://www.oracle.com/technetwork/java/javase/downloads/)</ept> 7 or later (or an equivalent, such as OpenJDK)"
    },
    {
      "content": "Apache Maven",
      "pos": [
        1067,
        1079
      ]
    },
    {
      "pos": [
        1110,
        1221
      ],
      "content": "<bpt id=\"p1\">**</bpt>An Azure subscription<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Get Azure free trial<ept id=\"p2\">](get-azure-free-trial-for-testing-hadoop-in-hdinsight.md)</ept>."
    },
    {
      "pos": [
        1225,
        1316
      ],
      "content": "<bpt id=\"p1\">**</bpt>Azure CLI<ept id=\"p1\">**</ept>: For more information, see <bpt id=\"p2\">[</bpt>Install and configure Azure CLI<ept id=\"p2\">](../xplat-cli.md)</ept>"
    },
    {
      "content": "Configure environment variables",
      "pos": [
        1320,
        1351
      ]
    },
    {
      "content": "The following environment variables may be set when you install Java and the JDK.",
      "pos": [
        1353,
        1434
      ]
    },
    {
      "content": "However, you should check that they exist and that they contain the correct values for your system.",
      "pos": [
        1435,
        1534
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>JAVA_HOME<ept id=\"p1\">**</ept> - should point to the directory where the Java runtime environment (JRE) is installed.",
      "pos": [
        1538,
        1638
      ]
    },
    {
      "content": "For example, in a OS X, Unix or Linux system, it should have a value similar to <ph id=\"ph1\">`/usr/lib/jvm/java-7-oracle`</ph>.",
      "pos": [
        1639,
        1748
      ]
    },
    {
      "content": "In Windows, it would have a value similar to <ph id=\"ph1\">`c:\\Program Files (x86)\\Java\\jre1.7`</ph>",
      "pos": [
        1749,
        1830
      ]
    },
    {
      "pos": [
        1834,
        1880
      ],
      "content": "<bpt id=\"p1\">**</bpt>PATH<ept id=\"p1\">**</ept> - should contain the following paths:"
    },
    {
      "pos": [
        1888,
        1926
      ],
      "content": "<bpt id=\"p1\">**</bpt>JAVA_HOME<ept id=\"p1\">**</ept> (or the equivalent path)"
    },
    {
      "pos": [
        1934,
        1976
      ],
      "content": "<bpt id=\"p1\">**</bpt>JAVA_HOME\\bin<ept id=\"p1\">**</ept> (or the equivalent path)"
    },
    {
      "content": "The directory where Maven is installed",
      "pos": [
        1984,
        2022
      ]
    },
    {
      "content": "Create a new Maven project",
      "pos": [
        2026,
        2052
      ]
    },
    {
      "content": "From a terminal session, or command line in your development environment, change directories to the location you want to store this project.",
      "pos": [
        2057,
        2197
      ]
    },
    {
      "pos": [
        2202,
        2302
      ],
      "content": "Use the <bpt id=\"p1\">__</bpt>mvn<ept id=\"p1\">__</ept> command, which is installed with Maven, to generate the scaffolding for the project."
    },
    {
      "pos": [
        2477,
        2677
      ],
      "content": "This will create a new directory in the current directory, with the name specified by the <bpt id=\"p1\">__</bpt>artifactID<ept id=\"p1\">__</ept> parameter (<bpt id=\"p2\">**</bpt>wordcountjava<ept id=\"p2\">**</ept> in this example.) This directory will contain the following items:"
    },
    {
      "pos": [
        2685,
        2884
      ],
      "content": "<bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> - The <bpt id=\"p2\">[</bpt>Project Object Model (POM)<ept id=\"p2\">](http://maven.apache.org/guides/introduction/introduction-to-the-pom.html)</ept> that contains information and configuration details used to build the project."
    },
    {
      "pos": [
        2892,
        3024
      ],
      "content": "<bpt id=\"p1\">__</bpt>src<ept id=\"p1\">__</ept> - The directory that contains the <bpt id=\"p2\">__</bpt>main/java/org/apache/hadoop/examples<ept id=\"p2\">__</ept> directory, where you will author the application."
    },
    {
      "pos": [
        3029,
        3143
      ],
      "content": "Delete the <bpt id=\"p1\">__</bpt>src/test/java/org/apache/hadoop/examples/apptest.java<ept id=\"p1\">__</ept> file, as it will not be used in this example."
    },
    {
      "content": "Add dependencies",
      "pos": [
        3147,
        3163
      ]
    },
    {
      "pos": [
        3168,
        3252
      ],
      "content": "Edit the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file and add the following inside the <ph id=\"ph1\">`&lt;dependencies&gt;`</ph> section:"
    },
    {
      "content": "This tells Maven that the project requires the libraries (listed within &lt;artifactId\\&gt;) with a specific version (listed within &lt;version\\&gt;).",
      "pos": [
        3912,
        4050
      ]
    },
    {
      "content": "At compile time, this will be downloaded from the default Maven repository.",
      "pos": [
        4051,
        4126
      ]
    },
    {
      "content": "You can use the <bpt id=\"p1\">[</bpt>Maven repository search<ept id=\"p1\">](http://search.maven.org/#artifactdetails%7Corg.apache.hadoop%7Chadoop-mapreduce-examples%7C2.5.1%7Cjar)</ept> to view more.",
      "pos": [
        4127,
        4286
      ]
    },
    {
      "pos": [
        4292,
        4465
      ],
      "content": "The <ph id=\"ph1\">`&lt;scope&gt;provided&lt;/scope&gt;`</ph> tells Maven that these dependencies should not be packaged with the application, as they will be provided by the HDInsight cluster at run-time."
    },
    {
      "content": "Add the following to the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file.",
      "pos": [
        4470,
        4512
      ]
    },
    {
      "content": "This must be inside the <ph id=\"ph1\">`&lt;project&gt;...&lt;/project&gt;`</ph> tags in the file; for example, between <ph id=\"ph2\">`&lt;/dependencies&gt;`</ph> and <ph id=\"ph3\">`&lt;/project&gt;`</ph>.",
      "pos": [
        4513,
        4636
      ]
    },
    {
      "content": "The first plugin configures the <bpt id=\"p1\">[</bpt>Maven Shade Plugin<ept id=\"p1\">](http://maven.apache.org/plugins/maven-shade-plugin/)</ept>, which is used to build an uberjar (sometimes called a fatjar), which contains dependencies required by the application.",
      "pos": [
        5738,
        5964
      ]
    },
    {
      "content": "It also prevents duplication of licenses within the jar package, which can cause problems on some systems.",
      "pos": [
        5965,
        6071
      ]
    },
    {
      "content": "The second plugin configures the Maven compiler, which is used to set the version of Java required by this application to the version used on the HDInsight cluster.",
      "pos": [
        6077,
        6241
      ]
    },
    {
      "pos": [
        6246,
        6272
      ],
      "content": "Save the <bpt id=\"p1\">__</bpt>pom.xml<ept id=\"p1\">__</ept> file."
    },
    {
      "content": "Create the MapReduce application",
      "pos": [
        6276,
        6308
      ]
    },
    {
      "pos": [
        6313,
        6448
      ],
      "content": "Go to the <bpt id=\"p1\">__</bpt>wordcountjava/src/main/java/org/apache/hadoop/examples<ept id=\"p1\">__</ept> directory and rename the <bpt id=\"p2\">__</bpt>App.java<ept id=\"p2\">__</ept>  file to <bpt id=\"p3\">__</bpt>WordCount.java<ept id=\"p3\">__</ept>."
    },
    {
      "pos": [
        6453,
        6547
      ],
      "content": "Open the <bpt id=\"p1\">__</bpt>WordCount.java<ept id=\"p1\">__</ept> file in a text editor and replace the contents with the following:"
    },
    {
      "content": "Notice the package name is <bpt id=\"p1\">**</bpt>org.apache.hadoop.examples<ept id=\"p1\">**</ept> and the class name is <bpt id=\"p2\">**</bpt>WordCount<ept id=\"p2\">**</ept>.",
      "pos": [
        9413,
        9507
      ]
    },
    {
      "content": "You will use these names when you submit the MapReduce job.",
      "pos": [
        9508,
        9567
      ]
    },
    {
      "content": "Save the file.",
      "pos": [
        9572,
        9586
      ]
    },
    {
      "content": "Build the application",
      "pos": [
        9590,
        9611
      ]
    },
    {
      "pos": [
        9616,
        9688
      ],
      "content": "Change to the <bpt id=\"p1\">__</bpt>wordcountjava<ept id=\"p1\">__</ept> directory, if you are not already there."
    },
    {
      "content": "Use the following command to build a JAR file containing the application:",
      "pos": [
        9693,
        9766
      ]
    },
    {
      "content": "This will clean any previous build artifacts, download any dependencies that have not already been installed, and then build and package the application.",
      "pos": [
        9799,
        9952
      ]
    },
    {
      "pos": [
        9957,
        10084
      ],
      "content": "Once the command finishes, the <bpt id=\"p1\">__</bpt>wordcountjava/target<ept id=\"p1\">__</ept> directory will contain a file named <bpt id=\"p2\">__</bpt>wordcountjava-1.0-SNAPSHOT.jar<ept id=\"p2\">__</ept>."
    },
    {
      "pos": [
        10092,
        10262
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The <bpt id=\"p1\">__</bpt>wordcountjava-1.0-SNAPSHOT.jar<ept id=\"p1\">__</ept> file is an uberjar, which contains not only the WordCount job, but also dependencies that the job requires at runtime."
    },
    {
      "pos": [
        10267,
        10300
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"upload\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Upload the jar"
    },
    {
      "content": "Use the following command to upload the jar file to the HDInsight headnode:",
      "pos": [
        10302,
        10377
      ]
    },
    {
      "content": "This copies the files from the local system to the head node.",
      "pos": [
        10588,
        10649
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you used a password to secure your SSH account, you will be prompted for the password.",
      "pos": [
        10653,
        10755
      ]
    },
    {
      "content": "If you used an SSH key, you may have to use the <ph id=\"ph1\">`-i`</ph> parameter and the path to the private key.",
      "pos": [
        10756,
        10851
      ]
    },
    {
      "content": "For example, <ph id=\"ph1\">`scp -i /path/to/private/key wordcountjava-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:`</ph>.",
      "pos": [
        10852,
        10971
      ]
    },
    {
      "pos": [
        10975,
        11014
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run the MapReduce job"
    },
    {
      "content": "Connect to HDInsight using SSH as described in the following articles:",
      "pos": [
        11019,
        11089
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X",
      "pos": [
        11098,
        11168
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Windows",
      "pos": [
        11218,
        11275
      ]
    },
    {
      "content": "From the SSH session, use the following command to run the MapReduce application:",
      "pos": [
        11324,
        11405
      ]
    },
    {
      "content": "This will use the WordCount MapReduce application to count the words in the davinci.txt file, and store the results to <bpt id=\"p1\">__</bpt>wasb:///example/data/wordcountout<ept id=\"p1\">__</ept>.",
      "pos": [
        11563,
        11720
      ]
    },
    {
      "content": "Both the input file and output are stored to the default storage for the cluster.",
      "pos": [
        11721,
        11802
      ]
    },
    {
      "content": "Once the job completes, use the following to view the results:",
      "pos": [
        11807,
        11869
      ]
    },
    {
      "content": "You should receive a list of words and counts, with values similar to the following:",
      "pos": [
        11935,
        12019
      ]
    },
    {
      "pos": [
        12078,
        12110
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "In this document, you have learned how to develop a Java MapReduce job.",
      "pos": [
        12112,
        12183
      ]
    },
    {
      "content": "See the following documents for other ways to work with HDInsight.",
      "pos": [
        12184,
        12250
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        12255,
        12278
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        12303,
        12325
      ]
    },
    {
      "content": "Use MapReduce with HDInsight",
      "pos": [
        12349,
        12377
      ]
    },
    {
      "content": "test",
      "pos": [
        13878,
        13882
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Develop Java MapReduce programs for Hadoop | Microsoft Azure\"\n    description=\"Learn how to develop Java MapReduce programs on HDInsight emulator, how to deploy them to HDInsight.\"\n    services=\"hdinsight\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    authors=\"Blackmist\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"Java\"\n    ms.topic=\"article\"\n    ms.date=\"09/02/2015\"\n    ms.author=\"larryfr\"/>\n\n# Develop Java MapReduce programs for Hadoop in HDInsight\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-maven-mapreduce-selector.md)]\n\nThis documents walks you through using Apache Maven to create a MapReduce application, then deploy and run it on a Linux-based Hadoop on HDInsight cluster.\n\n##<a name=\"prerequisites\"></a>Prerequisites\n\nBefore you begin this tutorial, you must have the following:\n\n- [Java JDK](http://www.oracle.com/technetwork/java/javase/downloads/) 7 or later (or an equivalent, such as OpenJDK)\n\n- [Apache Maven](http://maven.apache.org/)\n\n- **An Azure subscription**: See [Get Azure free trial](get-azure-free-trial-for-testing-hadoop-in-hdinsight.md).\n\n- **Azure CLI**: For more information, see [Install and configure Azure CLI](../xplat-cli.md)\n\n##Configure environment variables\n\nThe following environment variables may be set when you install Java and the JDK. However, you should check that they exist and that they contain the correct values for your system.\n\n* **JAVA_HOME** - should point to the directory where the Java runtime environment (JRE) is installed. For example, in a OS X, Unix or Linux system, it should have a value similar to `/usr/lib/jvm/java-7-oracle`. In Windows, it would have a value similar to `c:\\Program Files (x86)\\Java\\jre1.7`\n\n* **PATH** - should contain the following paths:\n\n    * **JAVA_HOME** (or the equivalent path)\n\n    * **JAVA_HOME\\bin** (or the equivalent path)\n\n    * The directory where Maven is installed\n\n##Create a new Maven project\n\n1. From a terminal session, or command line in your development environment, change directories to the location you want to store this project.\n\n3. Use the __mvn__ command, which is installed with Maven, to generate the scaffolding for the project.\n\n        mvn archetype:generate -DgroupId=org.apache.hadoop.examples -DartifactId=wordcountjava -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n    This will create a new directory in the current directory, with the name specified by the __artifactID__ parameter (**wordcountjava** in this example.) This directory will contain the following items:\n\n    * __pom.xml__ - The [Project Object Model (POM)](http://maven.apache.org/guides/introduction/introduction-to-the-pom.html) that contains information and configuration details used to build the project.\n\n    * __src__ - The directory that contains the __main/java/org/apache/hadoop/examples__ directory, where you will author the application.\n\n3. Delete the __src/test/java/org/apache/hadoop/examples/apptest.java__ file, as it will not be used in this example.\n\n##Add dependencies\n\n1. Edit the __pom.xml__ file and add the following inside the `<dependencies>` section:\n\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-mapreduce-examples</artifactId>\n          <version>2.5.1</version>\n          <scope>provided</scope>\n        </dependency>\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-mapreduce-client-common</artifactId>\n          <version>2.5.1</version>\n          <scope>provided</scope>\n        </dependency>\n        <dependency>\n          <groupId>org.apache.hadoop</groupId>\n          <artifactId>hadoop-common</artifactId>\n          <version>2.5.1</version>\n          <scope>provided</scope>\n        </dependency>\n\n    This tells Maven that the project requires the libraries (listed within <artifactId\\>) with a specific version (listed within <version\\>). At compile time, this will be downloaded from the default Maven repository. You can use the [Maven repository search](http://search.maven.org/#artifactdetails%7Corg.apache.hadoop%7Chadoop-mapreduce-examples%7C2.5.1%7Cjar) to view more.\n\n    The `<scope>provided</scope>` tells Maven that these dependencies should not be packaged with the application, as they will be provided by the HDInsight cluster at run-time.\n\n2. Add the following to the __pom.xml__ file. This must be inside the `<project>...</project>` tags in the file; for example, between `</dependencies>` and `</project>`.\n\n        <build>\n          <plugins>\n            <plugin>\n              <groupId>org.apache.maven.plugins</groupId>\n              <artifactId>maven-shade-plugin</artifactId>\n              <version>2.3</version>\n              <configuration>\n                <transformers>\n                  <transformer implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\">\n                  </transformer>\n                </transformers>\n              </configuration>\n              <executions>\n                <execution>\n                  <phase>package</phase>\n                    <goals>\n                      <goal>shade</goal>\n                    </goals>\n                </execution>\n              </executions>\n            </plugin>\n            <plugin>\n              <groupId>org.apache.maven.plugins</groupId>\n              <artifactId>maven-compiler-plugin</artifactId>\n              <configuration>\n               <source>1.7</source>\n               <target>1.7</target>\n              </configuration>\n            </plugin>\n          </plugins>\n        </build>\n\n    The first plugin configures the [Maven Shade Plugin](http://maven.apache.org/plugins/maven-shade-plugin/), which is used to build an uberjar (sometimes called a fatjar), which contains dependencies required by the application. It also prevents duplication of licenses within the jar package, which can cause problems on some systems.\n\n    The second plugin configures the Maven compiler, which is used to set the version of Java required by this application to the version used on the HDInsight cluster.\n\n3. Save the __pom.xml__ file.\n\n##Create the MapReduce application\n\n1. Go to the __wordcountjava/src/main/java/org/apache/hadoop/examples__ directory and rename the __App.java__  file to __WordCount.java__.\n\n2. Open the __WordCount.java__ file in a text editor and replace the contents with the following:\n\n        package org.apache.hadoop.examples;\n\n        import java.io.IOException;\n        import java.util.StringTokenizer;\n        import org.apache.hadoop.conf.Configuration;\n        import org.apache.hadoop.fs.Path;\n        import org.apache.hadoop.io.IntWritable;\n        import org.apache.hadoop.io.Text;\n        import org.apache.hadoop.mapreduce.Job;\n        import org.apache.hadoop.mapreduce.Mapper;\n        import org.apache.hadoop.mapreduce.Reducer;\n        import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n        import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n        import org.apache.hadoop.util.GenericOptionsParser;\n\n        public class WordCount {\n\n          public static class TokenizerMapper\n               extends Mapper<Object, Text, Text, IntWritable>{\n\n            private final static IntWritable one = new IntWritable(1);\n            private Text word = new Text();\n\n            public void map(Object key, Text value, Context context\n                            ) throws IOException, InterruptedException {\n              StringTokenizer itr = new StringTokenizer(value.toString());\n              while (itr.hasMoreTokens()) {\n                word.set(itr.nextToken());\n                context.write(word, one);\n              }\n            }\n          }\n\n          public static class IntSumReducer\n               extends Reducer<Text,IntWritable,Text,IntWritable> {\n            private IntWritable result = new IntWritable();\n\n            public void reduce(Text key, Iterable<IntWritable> values,\n                               Context context\n                               ) throws IOException, InterruptedException {\n              int sum = 0;\n              for (IntWritable val : values) {\n                sum += val.get();\n              }\n              result.set(sum);\n              context.write(key, result);\n            }\n          }\n\n          public static void main(String[] args) throws Exception {\n            Configuration conf = new Configuration();\n            String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n            if (otherArgs.length != 2) {\n              System.err.println(\"Usage: wordcount <in> <out>\");\n              System.exit(2);\n            }\n            Job job = new Job(conf, \"word count\");\n            job.setJarByClass(WordCount.class);\n            job.setMapperClass(TokenizerMapper.class);\n            job.setCombinerClass(IntSumReducer.class);\n            job.setReducerClass(IntSumReducer.class);\n            job.setOutputKeyClass(Text.class);\n            job.setOutputValueClass(IntWritable.class);\n            FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n            FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n            System.exit(job.waitForCompletion(true) ? 0 : 1);\n          }\n        }\n\n    Notice the package name is **org.apache.hadoop.examples** and the class name is **WordCount**. You will use these names when you submit the MapReduce job.\n\n3. Save the file.\n\n##Build the application\n\n1. Change to the __wordcountjava__ directory, if you are not already there.\n\n2. Use the following command to build a JAR file containing the application:\n\n        mvn clean package\n\n    This will clean any previous build artifacts, download any dependencies that have not already been installed, and then build and package the application.\n\n3. Once the command finishes, the __wordcountjava/target__ directory will contain a file named __wordcountjava-1.0-SNAPSHOT.jar__.\n\n    > [AZURE.NOTE] The __wordcountjava-1.0-SNAPSHOT.jar__ file is an uberjar, which contains not only the WordCount job, but also dependencies that the job requires at runtime.\n\n\n##<a id=\"upload\"></a>Upload the jar\n\nUse the following command to upload the jar file to the HDInsight headnode:\n\n    scp wordcountjava-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:\n\n    Replace __USERNAME__ with your SSH user name for the cluster. Replace __CLUSTERNAME__ with the HDInsight cluster name.\n\nThis copies the files from the local system to the head node.\n\n> [AZURE.NOTE] If you used a password to secure your SSH account, you will be prompted for the password. If you used an SSH key, you may have to use the `-i` parameter and the path to the private key. For example, `scp -i /path/to/private/key wordcountjava-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:`.\n\n##<a name=\"run\"></a>Run the MapReduce job\n\n1. Connect to HDInsight using SSH as described in the following articles:\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n2. From the SSH session, use the following command to run the MapReduce application:\n\n        hadoop jar wordcountjava.jar org.apache.hadoop.examples.WordCount wasb:///example/data/gutenberg/davinci.txt wasb:///example/data/wordcountout\n\n    This will use the WordCount MapReduce application to count the words in the davinci.txt file, and store the results to __wasb:///example/data/wordcountout__. Both the input file and output are stored to the default storage for the cluster.\n\n3. Once the job completes, use the following to view the results:\n\n        hadoop fs -cat wasb:///example/data/wordcountout/*\n\n    You should receive a list of words and counts, with values similar to the following:\n\n        zeal    1\n        zelus   1\n        zenith  2\n\n##<a id=\"nextsteps\"></a>Next steps\n\nIn this document, you have learned how to develop a Java MapReduce job. See the following documents for other ways to work with HDInsight.\n\n- [Use Hive with HDInsight][hdinsight-use-hive]\n- [Use Pig with HDInsight][hdinsight-use-pig]\n- [Use MapReduce with HDInsight](hdinsight-use-mapreduce.md)\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[hdinsight-use-sqoop]: hdinsight-use-sqoop.md\n[hdinsight-ODBC]: hdinsight-connect-excel-hive-ODBC-driver.md\n[hdinsight-power-query]: hdinsight-connect-excel-power-query.md\n\n[hdinsight-develop-streaming]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n\n[hdinsight-get-started]: ../hdinsight-get-started.md\n[hdinsight-emulator]: ../hdinsight-get-started-emulator.md\n[hdinsight-emulator-wasb]: ../hdinsight-get-started-emulator.md#blobstorage\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-storage]: ../hdinsight-use-blob-storage.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-power-query]: hdinsight-connect-excel-power-query.md\n\n[powershell-PSCredential]: http://social.technet.microsoft.com/wiki/contents/articles/4546.working-with-passwords-secure-strings-and-credentials-in-windows-powershell.aspx\n[powershell-install-configure]: ../install-configure-powershell.md\n\n\n\n[image-emulator-wordcount-compile]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Compile-Java-MapReduce.png\n[image-emulator-wordcount-run]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Run-Java-MapReduce.png\n\ntest\n"
}