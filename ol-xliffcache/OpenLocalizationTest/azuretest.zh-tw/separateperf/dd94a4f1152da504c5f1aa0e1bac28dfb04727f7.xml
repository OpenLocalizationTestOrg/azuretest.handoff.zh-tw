{
  "nodes": [
    {
      "content": "Azure Data Factory - Frequently Asked Questions",
      "pos": [
        28,
        75
      ]
    },
    {
      "content": "Frequently asked questions about Azure Data Factory.",
      "pos": [
        95,
        147
      ]
    },
    {
      "content": "Azure Data Factory - Frequently Asked Questions",
      "pos": [
        475,
        522
      ]
    },
    {
      "content": "General questions",
      "pos": [
        527,
        544
      ]
    },
    {
      "content": "What is Azure Data Factory?",
      "pos": [
        550,
        577
      ]
    },
    {
      "content": "Data Factory is a cloud-based data integration service that orchestrates and automates the movement and transformation of data.",
      "pos": [
        579,
        706
      ]
    },
    {
      "content": "Just like a manufacturing factory that runs equipment to take raw materials and transform them into finished goods, Data Factory orchestrates existing services that collect raw data and transform it into ready-to-use information.",
      "pos": [
        707,
        936
      ]
    },
    {
      "content": "Data Factory works across on-premises and cloud data sources and SaaS to ingest, prepare, transform, analyze, and publish your data.",
      "pos": [
        939,
        1071
      ]
    },
    {
      "content": "Use Data Factory to compose services into managed data flow pipelines to transform your data using services like <bpt id=\"p1\">[</bpt>Azure HDInsight (Hadoop)<ept id=\"p1\">](http://azure.microsoft.com/documentation/services/hdinsight/)</ept> and <bpt id=\"p2\">[</bpt>Azure Batch<ept id=\"p2\">](http://azure.microsoft.com/documentation/services/batch/)</ept> for your big data computing needs, and with <bpt id=\"p3\">[</bpt>Azure Machine Learning<ept id=\"p3\">](http://azure.microsoft.com/documentation/services/machine-learning/)</ept> to operationalize your analytics solutions.",
      "pos": [
        1073,
        1532
      ]
    },
    {
      "content": "Go beyond just a tabular monitoring view, and use the rich visualizations of Data Factory to quickly display the lineage and dependencies between your data pipelines.",
      "pos": [
        1534,
        1700
      ]
    },
    {
      "content": "Monitor all of your data flow pipelines from a single unified view to easily pinpoint issues and setup monitoring alerts.",
      "pos": [
        1701,
        1822
      ]
    },
    {
      "pos": [
        1824,
        1901
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Overview &amp; Key Concepts<ept id=\"p1\">](data-factory-introduction.md)</ept> for more details."
    },
    {
      "content": "What customer challenge does Data Factory solve?",
      "pos": [
        1909,
        1957
      ]
    },
    {
      "content": "Azure Data Factory balances the agility of leveraging diverse data storage, processing and movement services across traditional relational storage alongside unstructured data, with the control and monitoring capabilities of a fully managed service.",
      "pos": [
        1959,
        2207
      ]
    },
    {
      "content": "Who are the target audiences for Data Factory?",
      "pos": [
        2213,
        2259
      ]
    },
    {
      "content": "Data Developers: who are responsible for building integration services between Hadoop and other systems:",
      "pos": [
        2264,
        2368
      ]
    },
    {
      "content": "Must keep up and integrate with a continually changing and growing data landscape",
      "pos": [
        2375,
        2456
      ]
    },
    {
      "content": "Must write custom code for information production, and it  is expensive, hard to maintain, and not highly available or fault tolerant",
      "pos": [
        2463,
        2596
      ]
    },
    {
      "content": "IT Professionals: who are looking to incorporate more diverse data within their IT infrastructure:",
      "pos": [
        2600,
        2698
      ]
    },
    {
      "content": "Required to look across all of an organization’s data to derive rich business insights",
      "pos": [
        2705,
        2791
      ]
    },
    {
      "content": "Must manage compute and storage resources to balance cost and scale across on-premises and cloud",
      "pos": [
        2798,
        2894
      ]
    },
    {
      "content": "Must quickly add diverse sources and processing to address new business needs, while maintaining visibility across all compute and storage assets",
      "pos": [
        2901,
        3046
      ]
    },
    {
      "content": "Where can I find pricing details for Azure Data Factory?",
      "pos": [
        3052,
        3108
      ]
    },
    {
      "pos": [
        3110,
        3222
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Pricing Details page<ept id=\"p1\">][adf-pricing-details]</ept> for the pricing details for the Azure Data Factory."
    },
    {
      "content": "How do I get started with Azure Data Factory?",
      "pos": [
        3230,
        3275
      ]
    },
    {
      "pos": [
        3279,
        3377
      ],
      "content": "For an overview of Azure Data Factory, see <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">][adf-introduction]</ept>."
    },
    {
      "pos": [
        3380,
        3459
      ],
      "content": "For a quick tutorial, see <bpt id=\"p1\">[</bpt>Get started with Azure Data Factory<ept id=\"p1\">][adfgetstarted]</ept>."
    },
    {
      "pos": [
        3462,
        3565
      ],
      "content": "For comprehensive documentation, see <bpt id=\"p1\">[</bpt>Azure Data Factory documentation<ept id=\"p1\">][adf-documentation-landingpage]</ept>."
    },
    {
      "content": "How do customers access Data Factory?",
      "pos": [
        3574,
        3611
      ]
    },
    {
      "pos": [
        3613,
        3711
      ],
      "content": "Customers can get access to Data Factory through the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">][azure-preview-portal]</ept>."
    },
    {
      "content": "What is the Data Factory’s region availability?",
      "pos": [
        3717,
        3764
      ]
    },
    {
      "content": "Data Factory is available in US West and North Europe.",
      "pos": [
        3766,
        3820
      ]
    },
    {
      "content": "The compute and storage services used by data factories can be in other regions.",
      "pos": [
        3821,
        3901
      ]
    },
    {
      "content": "What are the limits on number of data factories/pipelines/activities/datasets?",
      "pos": [
        3908,
        3986
      ]
    },
    {
      "pos": [
        3989,
        4172
      ],
      "content": "See <bpt id=\"p1\">**</bpt>Azure Data Factory Limits<ept id=\"p1\">**</ept> section of the <bpt id=\"p2\">[</bpt>Azure Subscription and Service Limits, Quotas, and Constraints<ept id=\"p2\">](../azure-subscription-service-limits.md#data-factory-limits)</ept> article."
    },
    {
      "content": "What is the authoring/developer experience with Azure Data Factory service?",
      "pos": [
        4179,
        4254
      ]
    },
    {
      "content": "You can author/create data factories using one of the following:",
      "pos": [
        4256,
        4320
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Preview Portal<ept id=\"p1\">**</ept>.",
      "pos": [
        4324,
        4349
      ]
    },
    {
      "content": "The Data Factory blades in the Azure Preview Portal provide rich user interface for you to create data factories ad linked services.",
      "pos": [
        4350,
        4482
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, which is also part of the portal, allows you to easily create linked services, tables, data sets, and pipelines by specifying JSON definitions for these artifacts.",
      "pos": [
        4483,
        4675
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Editor<ept id=\"p1\">][data-factory-editor]</ept> for an overview of the editor and <bpt id=\"p2\">[</bpt>Get started with Data Factory<ept id=\"p2\">][datafactory-getstarted]</ept> for an example of using the portal/editor to create and deploy a data factory.",
      "pos": [
        4676,
        4891
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>.",
      "pos": [
        4897,
        4918
      ]
    },
    {
      "content": "If you are a PowerShell user and prefer to use PowerShell instead of Portal UI, you can use Azure Data Factory cmdlets that are shipped as part of Azure PowerShell to create and deploy data factories.",
      "pos": [
        4919,
        5119
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Create and monitor Azure Data Factory using Azure PowerShell<ept id=\"p1\">][create-data-factory-using-powershell]</ept> for a simple example and <bpt id=\"p2\">[</bpt>Tutorial: Move and process log files using Data Factory<ept id=\"p2\">][adf-tutorial]</ept> for an advanced example of using PowerShell cmdles to create ad deploy a data factory.",
      "pos": [
        5120,
        5408
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Cmdlet Reference<ept id=\"p1\">][adf-powershell-reference]</ept> content on MSDN Library for a comprehensive documentation of Data Factory cmdlets.",
      "pos": [
        5409,
        5553
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Visual Studio<ept id=\"p1\">**</ept>.",
      "pos": [
        5558,
        5576
      ]
    },
    {
      "content": "You can also use Visual Studio to programmatically create, monitor, and manage data factories.",
      "pos": [
        5577,
        5671
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Create, monitor, and manage Azure data factories using Data Factory .NET SDK<ept id=\"p1\">](data-factory-create-data-factories-programmatically)</ept> article for details.",
      "pos": [
        5672,
        5828
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>.NET Class Library<ept id=\"p1\">**</ept>.",
      "pos": [
        5833,
        5856
      ]
    },
    {
      "content": "You can programmatically create data factories by using Data Factory .NET SDK.",
      "pos": [
        5857,
        5935
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Create, monitor, and manage data factories using .NET SDK<ept id=\"p1\">][create-factory-using-dotnet-sdk]</ept> for a walkthrough of creating a data factory using .NET SDK.",
      "pos": [
        5936,
        6093
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory Class Library Reference<ept id=\"p1\">][msdn-class-library-reference]</ept> for a comprehensive documentation of Data Factory .NET SDK.",
      "pos": [
        6094,
        6226
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>REST API<ept id=\"p1\">**</ept>.",
      "pos": [
        6231,
        6244
      ]
    },
    {
      "content": "You can also use the REST API exposed by the Azure Data Factory service to create and deploy data factories.",
      "pos": [
        6245,
        6353
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Data Factory REST API Reference<ept id=\"p1\">][msdn-rest-api-reference]</ept> for  a comprehensive documentation of Data Factory REST API.",
      "pos": [
        6354,
        6477
      ]
    },
    {
      "content": "Can I rename a data factory?",
      "pos": [
        6484,
        6512
      ]
    },
    {
      "content": "No.",
      "pos": [
        6513,
        6516
      ]
    },
    {
      "content": "Like other Azure resources, the name of an Azure data factory cannot be changed.",
      "pos": [
        6517,
        6597
      ]
    },
    {
      "content": "Activities - FAQ",
      "pos": [
        6603,
        6619
      ]
    },
    {
      "content": "What are the supported data sources and activities?",
      "pos": [
        6624,
        6675
      ]
    },
    {
      "pos": [
        6677,
        6890
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Data Movement Activities<ept id=\"p1\">](data-factory-data-movement-activities.md)</ept> and <bpt id=\"p2\">[</bpt>Data Transformation Activities<ept id=\"p2\">](data-factory-data-transformation-activities.md)</ept> articles for the supported data sources and activities."
    },
    {
      "content": "When does an activity run?",
      "pos": [
        6898,
        6924
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> configuration setting in the output data table determines when the activity is run.",
      "pos": [
        6925,
        7029
      ]
    },
    {
      "content": "The activity checks whether all the input data dependencies are satisfied (i.e., <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state) before it starts running.",
      "pos": [
        7030,
        7153
      ]
    },
    {
      "content": "Copy Activity - FAQ",
      "pos": [
        7158,
        7177
      ]
    },
    {
      "content": "Is it better to have a pipeline with multiple activities or a separate pipeline for each activity?",
      "pos": [
        7182,
        7280
      ]
    },
    {
      "content": "Pipelines are supposed to bundle related activities.",
      "pos": [
        7282,
        7334
      ]
    },
    {
      "content": "Logically, you can keep the activities in one pipeline if the tables that connect them are not consumed by any other activity outside the pipeline.",
      "pos": [
        7336,
        7483
      ]
    },
    {
      "content": "This way, you would not need to chain pipeline active periods so that they align with each other.",
      "pos": [
        7484,
        7581
      ]
    },
    {
      "content": "Also, the data integrity in the tables internal to the pipeline will be better preserved when updating the pipeline.",
      "pos": [
        7582,
        7698
      ]
    },
    {
      "content": "Pipeline update essentially stops all the activities within the pipeline, removes them, and creates them again.",
      "pos": [
        7699,
        7810
      ]
    },
    {
      "content": "From authoring perspective, it might also be easier to see the flow of data within the related activities in one JSON file for the pipeline.",
      "pos": [
        7811,
        7951
      ]
    },
    {
      "content": "HDInsight Activity - FAQ",
      "pos": [
        7957,
        7981
      ]
    },
    {
      "content": "What regions are supported by HDInsight?",
      "pos": [
        7987,
        8027
      ]
    },
    {
      "pos": [
        8029,
        8155
      ],
      "content": "See the Geographic Availability section in the following article: or <bpt id=\"p1\">[</bpt>HDInsight Pricing Details<ept id=\"p1\">][hdinsight-supported-regions]</ept>."
    },
    {
      "content": "What region is used by an on-demand HDInsight cluster?",
      "pos": [
        8161,
        8215
      ]
    },
    {
      "content": "The on-demand HDInsight cluster is created in the same region where the storage you specified to be used with the cluster exists.",
      "pos": [
        8217,
        8346
      ]
    },
    {
      "content": "How to associate additional storage accounts to your HDInsight cluster?",
      "pos": [
        8356,
        8427
      ]
    },
    {
      "content": "If you are using your own HDInsight Cluster (BYOC - Bring Your Own Cluster), see the following topics:",
      "pos": [
        8429,
        8531
      ]
    },
    {
      "content": "Using an HDInsight Cluster with Alternate Storage Accounts and Metastores",
      "pos": [
        8537,
        8610
      ]
    },
    {
      "content": "Use Additional Storage Accounts with HDInsight Hive",
      "pos": [
        8644,
        8695
      ]
    },
    {
      "content": "If you are using an on-demand cluster that is created by the Data Factory service, you need to specify additional storage accounts for the HDInsight linked service so that the Data Factory service can register them on your behalf.",
      "pos": [
        8729,
        8959
      ]
    },
    {
      "content": "In the JSON definition for the on-demand linked service, use <bpt id=\"p1\">**</bpt>additionalLinkedServiceNames<ept id=\"p1\">**</ept> property to specify alternate storage accounts as shown in the following JSON snippet:",
      "pos": [
        8960,
        9140
      ]
    },
    {
      "content": "In the example above, otherLinkedServiceName1 and otherLinkedServiceName2 represent linked services whose definitions contain credentials that the HDInsight cluster needs to access alternate storage accounts.",
      "pos": [
        9536,
        9744
      ]
    },
    {
      "content": "Stored Procedure Activity - FAQ",
      "pos": [
        9749,
        9780
      ]
    },
    {
      "content": "What data sources does the Stored Procedure Activity support?",
      "pos": [
        9785,
        9846
      ]
    },
    {
      "content": "The Stored Procedure Activity supports only Azure SQL Database at this time.",
      "pos": [
        9847,
        9923
      ]
    },
    {
      "content": "Slices - FAQ",
      "pos": [
        9929,
        9941
      ]
    },
    {
      "content": "How can I rerun a slice?",
      "pos": [
        9947,
        9971
      ]
    },
    {
      "content": "You can rerun a slice in one of the following ways:",
      "pos": [
        9972,
        10023
      ]
    },
    {
      "pos": [
        10028,
        10117
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> in the command bar on the <bpt id=\"p2\">**</bpt>DATA SLICE<ept id=\"p2\">**</ept> blade for the slice in the portal."
    },
    {
      "pos": [
        10121,
        10222
      ],
      "content": "Run <bpt id=\"p1\">**</bpt>Set-AzureDataFactorySliceStatus<ept id=\"p1\">**</ept> cmdlet with Status set to <bpt id=\"p2\">**</bpt>PendingExecution<ept id=\"p2\">**</ept> for the slice."
    },
    {
      "pos": [
        10443,
        10546
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Set-AzureDataFactorySliceStatus<ept id=\"p1\">][set-azure-datafactory-slice-status]</ept> for details about the cmdlet."
    },
    {
      "content": "How long did it take to process a slice?",
      "pos": [
        10553,
        10593
      ]
    },
    {
      "pos": [
        10597,
        10673
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> tile on the <bpt id=\"p2\">**</bpt>DATA FACTORY<ept id=\"p2\">**</ept> blade for your data factory."
    },
    {
      "pos": [
        10677,
        10730
      ],
      "content": "Click the specific dataset on the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> blade."
    },
    {
      "pos": [
        10734,
        10833
      ],
      "content": "Select the slice that you are interested in from the <bpt id=\"p1\">**</bpt>Recent slices<ept id=\"p1\">**</ept> list on the <bpt id=\"p2\">**</bpt>TABLE<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        10837,
        10920
      ],
      "content": "Click the activity run from the <bpt id=\"p1\">**</bpt>Activity Runs<ept id=\"p1\">**</ept> list on the <bpt id=\"p2\">**</bpt>DATA SLICE<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        10925,
        10989
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Properties<ept id=\"p1\">**</ept> tile on the <bpt id=\"p2\">**</bpt>ACTIVITY RUN DETAILS<ept id=\"p2\">**</ept> blade."
    },
    {
      "content": "You should see the <bpt id=\"p1\">**</bpt>DURATION<ept id=\"p1\">**</ept> field with a value.",
      "pos": [
        10994,
        11045
      ]
    },
    {
      "content": "This is the time taken to process the slice.",
      "pos": [
        11046,
        11090
      ]
    },
    {
      "content": "How to stop a running slice?",
      "pos": [
        11099,
        11127
      ]
    },
    {
      "content": "If you need to stop the pipeline from executing, you can use <bpt id=\"p1\">[</bpt>Suspend-AzureDataFactoryPipeline<ept id=\"p1\">](https://msdn.microsoft.com/library/dn834939.aspx)</ept> cmdlet.",
      "pos": [
        11128,
        11281
      ]
    },
    {
      "content": "Currently, suspending the pipeline does not stop the slice executions that are in progress.",
      "pos": [
        11282,
        11373
      ]
    },
    {
      "content": "Once the in-progress executions finish, no extra slice is picked up.",
      "pos": [
        11374,
        11442
      ]
    },
    {
      "content": "If you really want to stop all the executions immediately, the only way would be to delete the pipeline and create it again.",
      "pos": [
        11444,
        11568
      ]
    },
    {
      "content": "If you choose to delete the pipeline, you do NOT need to delete tables and linked services used by the pipeline.",
      "pos": [
        11569,
        11681
      ]
    },
    {
      "content": "test",
      "pos": [
        13144,
        13148
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Azure Data Factory - Frequently Asked Questions\" \n    description=\"Frequently asked questions about Azure Data Factory.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"06/16/2015\" \n    ms.author=\"spelluru\"/>\n\n# Azure Data Factory - Frequently Asked Questions\n\n## General questions\n\n### What is Azure Data Factory?\n\nData Factory is a cloud-based data integration service that orchestrates and automates the movement and transformation of data. Just like a manufacturing factory that runs equipment to take raw materials and transform them into finished goods, Data Factory orchestrates existing services that collect raw data and transform it into ready-to-use information. \n\nData Factory works across on-premises and cloud data sources and SaaS to ingest, prepare, transform, analyze, and publish your data.  Use Data Factory to compose services into managed data flow pipelines to transform your data using services like [Azure HDInsight (Hadoop)](http://azure.microsoft.com/documentation/services/hdinsight/) and [Azure Batch](http://azure.microsoft.com/documentation/services/batch/) for your big data computing needs, and with [Azure Machine Learning](http://azure.microsoft.com/documentation/services/machine-learning/) to operationalize your analytics solutions.  Go beyond just a tabular monitoring view, and use the rich visualizations of Data Factory to quickly display the lineage and dependencies between your data pipelines. Monitor all of your data flow pipelines from a single unified view to easily pinpoint issues and setup monitoring alerts.\n\nSee [Overview & Key Concepts](data-factory-introduction.md) for more details. \n \n### What customer challenge does Data Factory solve?\n\nAzure Data Factory balances the agility of leveraging diverse data storage, processing and movement services across traditional relational storage alongside unstructured data, with the control and monitoring capabilities of a fully managed service.\n\n### Who are the target audiences for Data Factory?\n\n\n- Data Developers: who are responsible for building integration services between Hadoop and other systems:\n    - Must keep up and integrate with a continually changing and growing data landscape\n    - Must write custom code for information production, and it  is expensive, hard to maintain, and not highly available or fault tolerant\n\n- IT Professionals: who are looking to incorporate more diverse data within their IT infrastructure:\n    - Required to look across all of an organization’s data to derive rich business insights\n    - Must manage compute and storage resources to balance cost and scale across on-premises and cloud\n    - Must quickly add diverse sources and processing to address new business needs, while maintaining visibility across all compute and storage assets\n\n### Where can I find pricing details for Azure Data Factory?\n\nSee [Data Factory Pricing Details page][adf-pricing-details] for the pricing details for the Azure Data Factory.  \n\n### How do I get started with Azure Data Factory?\n\n- For an overview of Azure Data Factory, see [Introduction to Azure Data Factory][adf-introduction].\n- For a quick tutorial, see [Get started with Azure Data Factory][adfgetstarted].\n- For comprehensive documentation, see [Azure Data Factory documentation][adf-documentation-landingpage].\n\n  \n### How do customers access Data Factory?\n\nCustomers can get access to Data Factory through the [Azure Preview Portal][azure-preview-portal].\n\n### What is the Data Factory’s region availability?\n\nData Factory is available in US West and North Europe. The compute and storage services used by data factories can be in other regions.\n \n### What are the limits on number of data factories/pipelines/activities/datasets?\n \nSee **Azure Data Factory Limits** section of the [Azure Subscription and Service Limits, Quotas, and Constraints](../azure-subscription-service-limits.md#data-factory-limits) article.\n\n\n### What is the authoring/developer experience with Azure Data Factory service?\n\nYou can author/create data factories using one of the following:\n\n- **Azure Preview Portal**. The Data Factory blades in the Azure Preview Portal provide rich user interface for you to create data factories ad linked services. The **Data Factory Editor**, which is also part of the portal, allows you to easily create linked services, tables, data sets, and pipelines by specifying JSON definitions for these artifacts. See [Data Factory Editor][data-factory-editor] for an overview of the editor and [Get started with Data Factory][datafactory-getstarted] for an example of using the portal/editor to create and deploy a data factory.   \n- **Azure PowerShell**. If you are a PowerShell user and prefer to use PowerShell instead of Portal UI, you can use Azure Data Factory cmdlets that are shipped as part of Azure PowerShell to create and deploy data factories. See [Create and monitor Azure Data Factory using Azure PowerShell][create-data-factory-using-powershell] for a simple example and [Tutorial: Move and process log files using Data Factory][adf-tutorial] for an advanced example of using PowerShell cmdles to create ad deploy a data factory. See [Data Factory Cmdlet Reference][adf-powershell-reference] content on MSDN Library for a comprehensive documentation of Data Factory cmdlets.  \n- **Visual Studio**. You can also use Visual Studio to programmatically create, monitor, and manage data factories. See [Create, monitor, and manage Azure data factories using Data Factory .NET SDK](data-factory-create-data-factories-programmatically) article for details.  \n- **.NET Class Library**. You can programmatically create data factories by using Data Factory .NET SDK. See [Create, monitor, and manage data factories using .NET SDK][create-factory-using-dotnet-sdk] for a walkthrough of creating a data factory using .NET SDK. See [Data Factory Class Library Reference][msdn-class-library-reference] for a comprehensive documentation of Data Factory .NET SDK.  \n- **REST API**. You can also use the REST API exposed by the Azure Data Factory service to create and deploy data factories. See [Data Factory REST API Reference][msdn-rest-api-reference] for  a comprehensive documentation of Data Factory REST API. \n\n### Can I rename a data factory?\nNo. Like other Azure resources, the name of an Azure data factory cannot be changed. \n\n## Activities - FAQ\n### What are the supported data sources and activities?\n\nSee [Data Movement Activities](data-factory-data-movement-activities.md) and [Data Transformation Activities](data-factory-data-transformation-activities.md) articles for the supported data sources and activities.  \n\n### When does an activity run?\nThe **availability** configuration setting in the output data table determines when the activity is run. The activity checks whether all the input data dependencies are satisfied (i.e., **Ready** state) before it starts running.\n\n## Copy Activity - FAQ\n### Is it better to have a pipeline with multiple activities or a separate pipeline for each activity? \nPipelines are supposed to bundle related activities.  Logically, you can keep the activities in one pipeline if the tables that connect them are not consumed by any other activity outside the pipeline. This way, you would not need to chain pipeline active periods so that they align with each other. Also, the data integrity in the tables internal to the pipeline will be better preserved when updating the pipeline. Pipeline update essentially stops all the activities within the pipeline, removes them, and creates them again. From authoring perspective, it might also be easier to see the flow of data within the related activities in one JSON file for the pipeline. \n\n## HDInsight Activity - FAQ\n\n### What regions are supported by HDInsight?\n\nSee the Geographic Availability section in the following article: or [HDInsight Pricing Details][hdinsight-supported-regions].\n\n### What region is used by an on-demand HDInsight cluster?\n\nThe on-demand HDInsight cluster is created in the same region where the storage you specified to be used with the cluster exists.    \n\n### How to associate additional storage accounts to your HDInsight cluster?\n\nIf you are using your own HDInsight Cluster (BYOC - Bring Your Own Cluster), see the following topics: \n\n- [Using an HDInsight Cluster with Alternate Storage Accounts and Metastores][hdinsight-alternate-storage]\n- [Use Additional Storage Accounts with HDInsight Hive][hdinsight-alternate-storage-2]\n\nIf you are using an on-demand cluster that is created by the Data Factory service, you need to specify additional storage accounts for the HDInsight linked service so that the Data Factory service can register them on your behalf. In the JSON definition for the on-demand linked service, use **additionalLinkedServiceNames** property to specify alternate storage accounts as shown in the following JSON snippet:\n \n    {\n        \"name\": \"MyHDInsightOnDemandLinkedService\",\n        \"properties\":\n        {\n            \"type\": \"HDInsightOnDemandLinkedService\",\n            \"clusterSize\": 1,\n            \"timeToLive\": \"00:01:00\",\n            \"linkedServiceName\": \"LinkedService-SampleData\",\n            \"additionalLinkedServiceNames\": [ \"otherLinkedServiceName1\", \"otherLinkedServiceName2\" ] \n        }\n    } \n\nIn the example above, otherLinkedServiceName1 and otherLinkedServiceName2 represent linked services whose definitions contain credentials that the HDInsight cluster needs to access alternate storage accounts.\n\n## Stored Procedure Activity - FAQ\n### What data sources does the Stored Procedure Activity support?\nThe Stored Procedure Activity supports only Azure SQL Database at this time. \n\n## Slices - FAQ\n\n### How can I rerun a slice?\nYou can rerun a slice in one of the following ways: \n\n- Click **Run** in the command bar on the **DATA SLICE** blade for the slice in the portal. \n- Run **Set-AzureDataFactorySliceStatus** cmdlet with Status set to **PendingExecution** for the slice.   \n    \n        Set-AzureDataFactorySliceStatus -Status PendingExecution -ResourceGroupName $ResourceGroup -DataFactoryName $df -TableName $table -StartDateTime \"02/26/2015 19:00:00\" -EndDateTime \"02/26/2015 20:00:00\" \n\nSee [Set-AzureDataFactorySliceStatus][set-azure-datafactory-slice-status] for details about the cmdlet. \n\n### How long did it take to process a slice?\n1. Click **Datasets** tile on the **DATA FACTORY** blade for your data factory.\n2. Click the specific dataset on the **Datasets** blade.\n3. Select the slice that you are interested in from the **Recent slices** list on the **TABLE** blade.\n4. Click the activity run from the **Activity Runs** list on the **DATA SLICE** blade. \n5. Click **Properties** tile on the **ACTIVITY RUN DETAILS** blade. \n6. You should see the **DURATION** field with a value. This is the time taken to process the slice.   \n\n### How to stop a running slice?\nIf you need to stop the pipeline from executing, you can use [Suspend-AzureDataFactoryPipeline](https://msdn.microsoft.com/library/dn834939.aspx) cmdlet. Currently, suspending the pipeline does not stop the slice executions that are in progress. Once the in-progress executions finish, no extra slice is picked up.\n\nIf you really want to stop all the executions immediately, the only way would be to delete the pipeline and create it again. If you choose to delete the pipeline, you do NOT need to delete tables and linked services used by the pipeline. \n\n\n\n[image-rerun-slice]: ./media/data-factory-faq/rerun-slice.png\n\n[adfgetstarted]: data-factory-get-started.md\n[adf-introduction]: data-factory-introduction.md\n[adf-troubleshoot]: data-factory-troubleshoot.md\n[data-factory-editor]: data-factory-editor.md\n[datafactory-getstarted]: data-factory-get-started.md\n[create-data-factory-using-powershell]: data-factory-monitor-manage-using-powershell.md\n[adf-tutorial]: data-factory-tutorial.md\n[create-factory-using-dotnet-sdk]: data-factory-create-data-factories-programmatically.md\n[msdn-class-library-reference]: https://msdn.microsoft.com/library/dn883654.aspx\n[msdn-rest-api-reference]: https://msdn.microsoft.com/library/dn906738.aspx\n\n[adf-powershell-reference]: https://msdn.microsoft.com/library/dn820234.aspx \n[adf-documentation-landingpage]: http://go.microsoft.com/fwlink/?LinkId=516909\n[azure-preview-portal]: http://portal.azure.com\n[set-azure-datafactory-slice-status]: https://msdn.microsoft.com/library/azure/dn835095.aspx\n\n[adf-pricing-details]: http://go.microsoft.com/fwlink/?LinkId=517777\n[hdinsight-supported-regions]: http://azure.microsoft.com/pricing/details/hdinsight/\n[hdinsight-alternate-storage]: http://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx\n[hdinsight-alternate-storage-2]: http://blogs.msdn.com/b/cindygross/archive/2014/05/05/use-additional-storage-accounts-with-hdinsight-hive.aspx\n \n\ntest\n"
}