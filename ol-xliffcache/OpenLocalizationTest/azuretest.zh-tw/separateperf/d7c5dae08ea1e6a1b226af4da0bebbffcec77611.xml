{
  "nodes": [
    {
      "content": "Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure",
      "pos": [
        26,
        91
      ]
    },
    {
      "content": "Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job.",
      "pos": [
        109,
        259
      ]
    },
    {
      "content": "Run Pig jobs on a Linux-based cluster with the Pig command (SSH)",
      "pos": [
        576,
        640
      ]
    },
    {
      "content": "In this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job.",
      "pos": [
        720,
        943
      ]
    },
    {
      "content": "The Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output.",
      "pos": [
        945,
        1084
      ]
    },
    {
      "pos": [
        1088,
        1266
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id=\"p1\">[</bpt>Linux-based HDInsight Tips<ept id=\"p1\">](hdinsight-hadoop-linux-information.md)</ept>."
    },
    {
      "pos": [
        1270,
        1302
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"prereq\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "To complete the steps in this article, you will need the following.",
      "pos": [
        1304,
        1371
      ]
    },
    {
      "content": "A Linux-based HDInsight (Hadoop on HDInsight) cluster.",
      "pos": [
        1375,
        1429
      ]
    },
    {
      "content": "An SSH client.",
      "pos": [
        1433,
        1447
      ]
    },
    {
      "content": "Linux, Unix, and Mac OS should come with an SSH client.",
      "pos": [
        1448,
        1503
      ]
    },
    {
      "content": "Windows users must download a client, such as <bpt id=\"p1\">[</bpt>PuTTY<ept id=\"p1\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "pos": [
        1504,
        1623
      ]
    },
    {
      "pos": [
        1627,
        1659
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"ssh\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Connect with SSH"
    },
    {
      "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.",
      "pos": [
        1661,
        1762
      ]
    },
    {
      "content": "The FQDN will be the name you gave the cluster, then <bpt id=\"p1\">**</bpt>.azurehdinsight.net<ept id=\"p1\">**</ept>.",
      "pos": [
        1763,
        1840
      ]
    },
    {
      "content": "For example, the following would connect to a cluster named <bpt id=\"p1\">**</bpt>myhdinsight<ept id=\"p1\">**</ept>.",
      "pos": [
        1841,
        1917
      ]
    },
    {
      "pos": [
        1969,
        2148
      ],
      "content": "<bpt id=\"p1\">**</bpt>If you provided a certificate key for SSH authentication<ept id=\"p1\">**</ept> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system."
    },
    {
      "pos": [
        2215,
        2361
      ],
      "content": "<bpt id=\"p1\">**</bpt>If you provided a password for SSH authentication<ept id=\"p1\">**</ept> when you created the HDInsight cluster, you will need to provide the password when prompted."
    },
    {
      "pos": [
        2363,
        2531
      ],
      "content": "For more information on using SSH with HDInsight, see <bpt id=\"p1\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>."
    },
    {
      "content": "PuTTY (Windows-based clients)",
      "pos": [
        2536,
        2565
      ]
    },
    {
      "content": "Windows does not provide a built-in SSH client.",
      "pos": [
        2567,
        2614
      ]
    },
    {
      "content": "We recommend using <bpt id=\"p1\">**</bpt>PuTTY<ept id=\"p1\">**</ept>, which can be downloaded from <bpt id=\"p2\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p2\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "pos": [
        2615,
        2805
      ]
    },
    {
      "pos": [
        2807,
        2952
      ],
      "content": "For more information on using PuTTY, see <bpt id=\"p1\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id=\"p1\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>."
    },
    {
      "pos": [
        2956,
        2991
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"pig\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Use the Pig command"
    },
    {
      "content": "Once connected, start the Pig command-line interface (CLI) by using the following command.",
      "pos": [
        2996,
        3086
      ]
    },
    {
      "pos": [
        3105,
        3154
      ],
      "content": "After a moment, you should see a <ph id=\"ph1\">`grunt&gt;`</ph> prompt."
    },
    {
      "content": "Enter the following statement.",
      "pos": [
        3159,
        3189
      ]
    },
    {
      "content": "This command loads the contents of the sample.log file into LOGS.",
      "pos": [
        3251,
        3316
      ]
    },
    {
      "content": "You can view the contents of the file by using the following.",
      "pos": [
        3317,
        3378
      ]
    },
    {
      "content": "Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following.",
      "pos": [
        3403,
        3535
      ]
    },
    {
      "content": "You can use <bpt id=\"p1\">**</bpt>DUMP<ept id=\"p1\">**</ept> to view the data after the transformation.",
      "pos": [
        3655,
        3718
      ]
    },
    {
      "content": "In this case, use <ph id=\"ph1\">`DUMP LEVELS;`</ph>.",
      "pos": [
        3719,
        3752
      ]
    },
    {
      "content": "Continue applying transformations by using the following statements.",
      "pos": [
        3757,
        3825
      ]
    },
    {
      "content": "Use <ph id=\"ph1\">`DUMP`</ph> to view the result of the transformation after each step.",
      "pos": [
        3826,
        3894
      ]
    },
    {
      "pos": [
        3900,
        4768
      ],
      "content": "<table>\n <tr>\n <th>Statement</th><th>What it does</th>\n </tr>\n <tr>\n <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n </tr>\n <tr>\n <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n </tr>\n <tr>\n <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES.</td>\n </tr>\n <tr>\n <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending) and stores into RESULT.</td>\n </tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Statement",
          "pos": [
            19,
            28
          ]
        },
        {
          "content": "What it does",
          "pos": [
            37,
            49
          ]
        },
        {
          "content": "FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;",
          "pos": [
            73,
            128
          ]
        },
        {
          "content": "Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.",
          "pos": [
            137,
            237
          ]
        },
        {
          "content": "GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;",
          "pos": [
            261,
            310
          ]
        },
        {
          "content": "Groups the rows by log level and stores the results into GROUPEDLEVELS.",
          "pos": [
            319,
            390
          ]
        },
        {
          "content": "FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;",
          "pos": [
            414,
            518
          ]
        },
        {
          "content": "Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES.",
          "pos": [
            527,
            657
          ],
          "nodes": [
            {
              "content": "Creates a new set of data that contains each unique log level value and how many times it occurs.",
              "pos": [
                0,
                97
              ]
            },
            {
              "content": "This is stored into FREQUENCIES.",
              "pos": [
                98,
                130
              ]
            }
          ]
        },
        {
          "content": "RESULT = order FREQUENCIES by COUNT desc;",
          "pos": [
            681,
            722
          ]
        },
        {
          "content": "Orders the log levels by count (descending) and stores into RESULT.",
          "pos": [
            731,
            798
          ]
        }
      ]
    },
    {
      "content": "You can also save the results of a transformation by using the <ph id=\"ph1\">`STORE`</ph> statement.",
      "pos": [
        4773,
        4854
      ]
    },
    {
      "content": "For example, the following saves the <ph id=\"ph1\">`RESULT`</ph> to the <bpt id=\"p1\">**</bpt>/example/data/pigout<ept id=\"p1\">**</ept> directory on the default storage container for your cluster.",
      "pos": [
        4855,
        4993
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The data is stored in the specified directory in files named <bpt id=\"p1\">**</bpt>part-nnnnn<ept id=\"p1\">**</ept>.",
      "pos": [
        5058,
        5147
      ]
    },
    {
      "content": "If the directory already exists, you will receive an error.",
      "pos": [
        5148,
        5207
      ]
    },
    {
      "content": "To exit the grunt prompt, enter the following statement.",
      "pos": [
        5212,
        5268
      ]
    },
    {
      "content": "Pig Latin batch files",
      "pos": [
        5288,
        5309
      ]
    },
    {
      "content": "You can also use the Pig command to run Pig Latin contained in a file.",
      "pos": [
        5311,
        5381
      ]
    },
    {
      "content": "After exiting the grunt prompt, use the following command to pipe STDIN into a file named <bpt id=\"p1\">**</bpt>pigbatch.pig<ept id=\"p1\">**</ept>.",
      "pos": [
        5386,
        5493
      ]
    },
    {
      "content": "This file will be created in the home directory for the account you are logged in to for the SSH session.",
      "pos": [
        5494,
        5599
      ]
    },
    {
      "content": "Type or paste the following lines, and then use Ctrl+D when finished.",
      "pos": [
        5634,
        5703
      ]
    },
    {
      "pos": [
        6183,
        6259
      ],
      "content": "Use the following to run the <bpt id=\"p1\">**</bpt>pigbatch.pig<ept id=\"p1\">**</ept> file by using the Pig command."
    },
    {
      "pos": [
        6293,
        6438
      ],
      "content": "Once the batch job finishes, you should see the following output, which should be the same as when you used <ph id=\"ph1\">`DUMP RESULT;`</ph> in the previous steps."
    },
    {
      "pos": [
        6555,
        6582
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"summary\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Summary"
    },
    {
      "content": "As you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file.",
      "pos": [
        6584,
        6738
      ]
    },
    {
      "pos": [
        6742,
        6774
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps"
    },
    {
      "content": "For general information on Pig in HDInsight.",
      "pos": [
        6776,
        6820
      ]
    },
    {
      "content": "Use Pig with Hadoop on HDInsight",
      "pos": [
        6825,
        6857
      ]
    },
    {
      "content": "For information on other ways you can work with Hadoop on HDInsight.",
      "pos": [
        6882,
        6950
      ]
    },
    {
      "content": "Use Hive with Hadoop on HDInsight",
      "pos": [
        6955,
        6988
      ]
    },
    {
      "content": "Use MapReduce with Hadoop on HDInsight",
      "pos": [
        7017,
        7055
      ]
    },
    {
      "content": "test",
      "pos": [
        7086,
        7090
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure\"\n   description=\"Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/06/2015\"\n   ms.author=\"larryfr\"/>\n\n#Run Pig jobs on a Linux-based cluster with the Pig command (SSH)\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-selector-use-pig.md)]\n\nIn this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job.\n\nThe Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output.\n\n> [AZURE.NOTE] If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see [Linux-based HDInsight Tips](hdinsight-hadoop-linux-information.md).\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following.\n\n* A Linux-based HDInsight (Hadoop on HDInsight) cluster.\n\n* An SSH client. Linux, Unix, and Mac OS should come with an SSH client. Windows users must download a client, such as [PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n##<a id=\"ssh\"></a>Connect with SSH\n\nConnect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, then **.azurehdinsight.net**. For example, the following would connect to a cluster named **myhdinsight**.\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net\n\n**If you provided a certificate key for SSH authentication** when you created the HDInsight cluster, you may need to specify the location of the private key on your client system.\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net -i ~/mykey.key\n\n**If you provided a password for SSH authentication** when you created the HDInsight cluster, you will need to provide the password when prompted.\n\nFor more information on using SSH with HDInsight, see [Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix](hdinsight-hadoop-linux-use-ssh-unix.md).\n\n###PuTTY (Windows-based clients)\n\nWindows does not provide a built-in SSH client. We recommend using **PuTTY**, which can be downloaded from [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\nFor more information on using PuTTY, see [Use SSH with Linux-based Hadoop on HDInsight from Windows ](hdinsight-hadoop-linux-use-ssh-windows.md).\n\n##<a id=\"pig\"></a>Use the Pig command\n\n2. Once connected, start the Pig command-line interface (CLI) by using the following command.\n\n        pig\n\n    After a moment, you should see a `grunt>` prompt.\n\n3. Enter the following statement.\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n\n    This command loads the contents of the sample.log file into LOGS. You can view the contents of the file by using the following.\n\n        DUMP LOGS;\n\n4. Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following.\n\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n\n    You can use **DUMP** to view the data after the transformation. In this case, use `DUMP LEVELS;`.\n\n5. Continue applying transformations by using the following statements. Use `DUMP` to view the result of the transformation after each step.\n\n    <table>\n    <tr>\n    <th>Statement</th><th>What it does</th>\n    </tr>\n    <tr>\n    <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES.</td>\n    </tr>\n    <tr>\n    <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending) and stores into RESULT.</td>\n    </tr>\n    </table>\n\n6. You can also save the results of a transformation by using the `STORE` statement. For example, the following saves the `RESULT` to the **/example/data/pigout** directory on the default storage container for your cluster.\n\n        STORE RESULT into 'wasb:///example/data/pigout'\n\n    > [AZURE.NOTE] The data is stored in the specified directory in files named **part-nnnnn**. If the directory already exists, you will receive an error.\n\n7. To exit the grunt prompt, enter the following statement.\n\n        QUIT;\n\n###Pig Latin batch files\n\nYou can also use the Pig command to run Pig Latin contained in a file.\n\n3. After exiting the grunt prompt, use the following command to pipe STDIN into a file named **pigbatch.pig**. This file will be created in the home directory for the account you are logged in to for the SSH session.\n\n        cat > ~/pigbatch.pig\n\n4. Type or paste the following lines, and then use Ctrl+D when finished.\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n        FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n        GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n        FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n        RESULT = order FREQUENCIES by COUNT desc;\n        DUMP RESULT;\n\n5. Use the following to run the **pigbatch.pig** file by using the Pig command.\n\n        pig ~/pigbatch.pig\n\n    Once the batch job finishes, you should see the following output, which should be the same as when you used `DUMP RESULT;` in the previous steps.\n\n        (TRACE,816)\n        (DEBUG,434)\n        (INFO,96)\n        (WARN,11)\n        (ERROR,6)\n        (FATAL,2)\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information on Pig in HDInsight.\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\nFor information on other ways you can work with Hadoop on HDInsight.\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n\ntest\n"
}