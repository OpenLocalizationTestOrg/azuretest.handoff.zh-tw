<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Using load-balanced sets to clusterize MySQL on Linux</source>
          <target state="new">Using load-balanced sets to clusterize MySQL on Linux</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>An article that illustrates patterns to setup a load-balanced, high availability Linux cluster on Azure using MySQL as an example</source>
          <target state="new">An article that illustrates patterns to setup a load-balanced, high availability Linux cluster on Azure using MySQL as an example</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Using load-balanced sets to clusterize MySQL on Linux</source>
          <target state="new">Using load-balanced sets to clusterize MySQL on Linux</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Getting ready</source>
          <target state="new">Getting ready</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Setting up the cluster</source>
          <target state="new">Setting up the cluster</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Setting up MySQL</source>
          <target state="new">Setting up MySQL</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Setting up Corosync</source>
          <target state="new">Setting up Corosync</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Setting up Pacemaker</source>
          <target state="new">Setting up Pacemaker</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Testing</source>
          <target state="new">Testing</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>STONITH</source>
          <target state="new">STONITH</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="new">Limitations</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Introduction</source>
          <target state="new">Introduction</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>The purpose of this article is to explore and illustrate the different approaches available to deploy highly available Linux-based services on Microsoft Azure, exploring MySQL Server high availability as a primer.</source>
          <target state="new">The purpose of this article is to explore and illustrate the different approaches available to deploy highly available Linux-based services on Microsoft Azure, exploring MySQL Server high availability as a primer.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>A video illustrating this approach is available on <bpt id="p1">[</bpt>Channel 9<ept id="p1">](http://channel9.msdn.com/Blogs/Open/Load-balancing-highly-available-Linux-services-on-Windows-Azure-OpenLDAP-and-MySQL)</ept>.</source>
          <target state="new">A video illustrating this approach is available on <bpt id="p1">[</bpt>Channel 9<ept id="p1">](http://channel9.msdn.com/Blogs/Open/Load-balancing-highly-available-Linux-services-on-Windows-Azure-OpenLDAP-and-MySQL)</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>We outline a shared-nothing two-node single-master MySQL high availability solution based on DRBD, Corosync and Pacemaker.</source>
          <target state="new">We outline a shared-nothing two-node single-master MySQL high availability solution based on DRBD, Corosync and Pacemaker.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Only one node is running MySQL at a time.</source>
          <target state="new">Only one node is running MySQL at a time.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Reading and writing from the DRBD resource is also limited to only one node at a time.</source>
          <target state="new">Reading and writing from the DRBD resource is also limited to only one node at a time.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>There is no need for a VIP solution like LVS since we use Microsoft Azure's Load Balanced Sets to provide both the round-robin functionality and the endpoint detection, removal and graceful recovery of the VIP.</source>
          <target state="new">There is no need for a VIP solution like LVS since we use Microsoft Azure's Load Balanced Sets to provide both the round-robin functionality and the endpoint detection, removal and graceful recovery of the VIP.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The VIP is a globally routable IPv4 address assigned by Microsoft Azure when we first create the cloud service.</source>
          <target state="new">The VIP is a globally routable IPv4 address assigned by Microsoft Azure when we first create the cloud service.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>There are other possible architectures for MySQL including NBD Cluster, Percona and Galera as well as several middleware solutions, including at least one available as a VM on <bpt id="p1">[</bpt>VM Depot<ept id="p1">](http://vmdepot.msopentech.com)</ept>.</source>
          <target state="new">There are other possible architectures for MySQL including NBD Cluster, Percona and Galera as well as several middleware solutions, including at least one available as a VM on <bpt id="p1">[</bpt>VM Depot<ept id="p1">](http://vmdepot.msopentech.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>As long as these solutions can replicate on unicast vs. multicast or broadcast and don't rely on shared storage or multiple network interfaces, the scenarios should be easy to deploy on Microsoft Azure.</source>
          <target state="new">As long as these solutions can replicate on unicast vs. multicast or broadcast and don't rely on shared storage or multiple network interfaces, the scenarios should be easy to deploy on Microsoft Azure.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Of course these clustering architectures can be extended to other products like PostgreSQL and OpenLDAP on a similar fashion.</source>
          <target state="new">Of course these clustering architectures can be extended to other products like PostgreSQL and OpenLDAP on a similar fashion.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>For example, this load balancing procedure with shared nothing was successfully tested with multi-master OpenLDAP, and you can watch it on our Channel 9 blog.</source>
          <target state="new">For example, this load balancing procedure with shared nothing was successfully tested with multi-master OpenLDAP, and you can watch it on our Channel 9 blog.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Getting ready</source>
          <target state="new">Getting ready</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>You will need a Microsoft Azure account with a valid subscription able to create at least two (2) VMs (XS was used in this example), a network and a subnet, an affinity group and an availability set, as well as the ability to create new VHDs in the same region as the cloud service, and to attach them to the Linux VMs.</source>
          <target state="new">You will need a Microsoft Azure account with a valid subscription able to create at least two (2) VMs (XS was used in this example), a network and a subnet, an affinity group and an availability set, as well as the ability to create new VHDs in the same region as the cloud service, and to attach them to the Linux VMs.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Tested environment</source>
          <target state="new">Tested environment</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Ubuntu 13.10</source>
          <target state="new">Ubuntu 13.10</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>DRBD</source>
          <target state="new">DRBD</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>MySQL Server</source>
          <target state="new">MySQL Server</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Corosync and Pacemaker</source>
          <target state="new">Corosync and Pacemaker</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Affinity group</source>
          <target state="new">Affinity group</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>An affinity group for the solution is created by logging into the Azure Portal scrolling down to Settings and creating a new affinity group.</source>
          <target state="new">An affinity group for the solution is created by logging into the Azure Portal scrolling down to Settings and creating a new affinity group.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Allocated resources created later will be assigned to this affinity group.</source>
          <target state="new">Allocated resources created later will be assigned to this affinity group.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Networks</source>
          <target state="new">Networks</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>A new network is created, and a subnet is created inside the network.</source>
          <target state="new">A new network is created, and a subnet is created inside the network.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>We chose a 10.10.10.0/24 network with only one /24 subnet inside.</source>
          <target state="new">We chose a 10.10.10.0/24 network with only one /24 subnet inside.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Virtual machines</source>
          <target state="new">Virtual machines</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>The first Ubuntu 13.10 VM is created using an Endorsed Ubuntu Gallery image, and called <ph id="ph1">`hadb01`</ph>.</source>
          <target state="new">The first Ubuntu 13.10 VM is created using an Endorsed Ubuntu Gallery image, and called <ph id="ph1">`hadb01`</ph>.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>A new cloud service is created in the process, called hadb.</source>
          <target state="new">A new cloud service is created in the process, called hadb.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>We call it this way to illustrate the shared, load-balanced nature that the service will have when we add more resources.</source>
          <target state="new">We call it this way to illustrate the shared, load-balanced nature that the service will have when we add more resources.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The creation of <ph id="ph1">`hadb01`</ph> is uneventful and completed using the portal.</source>
          <target state="new">The creation of <ph id="ph1">`hadb01`</ph> is uneventful and completed using the portal.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>An endpoint for SSH is automatically created, and our created network is selected.</source>
          <target state="new">An endpoint for SSH is automatically created, and our created network is selected.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>We also choose to create a new availability set for the VMs.</source>
          <target state="new">We also choose to create a new availability set for the VMs.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Once the first VM is created (technically, when the cloud service is created) we proceed to create the second VM, <ph id="ph1">`hadb02`</ph>.</source>
          <target state="new">Once the first VM is created (technically, when the cloud service is created) we proceed to create the second VM, <ph id="ph1">`hadb02`</ph>.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>For the second VM we will also use Ubuntu 13.10 VM from the Gallery using the Portal but we will choose to use an existing cloud service, <ph id="ph1">`hadb.cloudapp.net`</ph>, instead of creating a new one.</source>
          <target state="new">For the second VM we will also use Ubuntu 13.10 VM from the Gallery using the Portal but we will choose to use an existing cloud service, <ph id="ph1">`hadb.cloudapp.net`</ph>, instead of creating a new one.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>The network and availability set should be automatically selected for us.</source>
          <target state="new">The network and availability set should be automatically selected for us.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>An SSH endpoint will be created, too.</source>
          <target state="new">An SSH endpoint will be created, too.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>After both VMs have been created, we will take note of the SSH port for <ph id="ph1">`hadb01`</ph> (TCP 22) and <ph id="ph2">`hadb02`</ph> (automatically assigned by Azure)</source>
          <target state="new">After both VMs have been created, we will take note of the SSH port for <ph id="ph1">`hadb01`</ph> (TCP 22) and <ph id="ph2">`hadb02`</ph> (automatically assigned by Azure)</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Attached storage</source>
          <target state="new">Attached storage</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>We attach a new disk to both VMs, and create new 5 GB disks in the process.</source>
          <target state="new">We attach a new disk to both VMs, and create new 5 GB disks in the process.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>The disks will be hosted in the VHD container in use for our main operating system disks.</source>
          <target state="new">The disks will be hosted in the VHD container in use for our main operating system disks.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Once disks are created and attached there is no need for us to restart Linux as the kernel will see the new device (usually <ph id="ph1">`/dev/sdc`</ph>, you can check <ph id="ph2">`dmesg`</ph> for the output)</source>
          <target state="new">Once disks are created and attached there is no need for us to restart Linux as the kernel will see the new device (usually <ph id="ph1">`/dev/sdc`</ph>, you can check <ph id="ph2">`dmesg`</ph> for the output)</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>On each VM we proceed to create a new partition using <ph id="ph1">`cfdisk`</ph> (primary, Linux partition) and write the new partition table.</source>
          <target state="new">On each VM we proceed to create a new partition using <ph id="ph1">`cfdisk`</ph> (primary, Linux partition) and write the new partition table.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Do not create a filesystem on this partition<ept id="p1">**</ept> .</source>
          <target state="new"><bpt id="p1">**</bpt>Do not create a filesystem on this partition<ept id="p1">**</ept> .</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Setting up the cluster</source>
          <target state="new">Setting up the cluster</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>In both Ubuntu VMs, we need to use APT to install Corosync, Pacemaker and DRBD.</source>
          <target state="new">In both Ubuntu VMs, we need to use APT to install Corosync, Pacemaker and DRBD.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Using <ph id="ph1">`apt-get`</ph>:</source>
          <target state="new">Using <ph id="ph1">`apt-get`</ph>:</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Do not install MySQL at this time<ept id="p1">**</ept> .</source>
          <target state="new"><bpt id="p1">**</bpt>Do not install MySQL at this time<ept id="p1">**</ept> .</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Debian and Ubuntu installation scripts will initialize a MySQL data directory on <ph id="ph1">`/var/lib/mysql`</ph>, but since the directory will be superseded by a DRBD filesystem, we need to do this later.</source>
          <target state="new">Debian and Ubuntu installation scripts will initialize a MySQL data directory on <ph id="ph1">`/var/lib/mysql`</ph>, but since the directory will be superseded by a DRBD filesystem, we need to do this later.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>At this point we should also verify (using <ph id="ph1">`/sbin/ifconfig`</ph>) that both VMs are using addresses in the 10.10.10.0/24 subnet and that they can ping each other by name.</source>
          <target state="new">At this point we should also verify (using <ph id="ph1">`/sbin/ifconfig`</ph>) that both VMs are using addresses in the 10.10.10.0/24 subnet and that they can ping each other by name.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>If desired you can also use <ph id="ph1">`ssh-keygen`</ph> and <ph id="ph2">`ssh-copy-id`</ph> to make sure both VMs can communicate via SSH without requiring a password.</source>
          <target state="new">If desired you can also use <ph id="ph1">`ssh-keygen`</ph> and <ph id="ph2">`ssh-copy-id`</ph> to make sure both VMs can communicate via SSH without requiring a password.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Setting up DRBD</source>
          <target state="new">Setting up DRBD</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>We will create a DRBD resource that uses the underlying <ph id="ph1">`/dev/sdc1`</ph> partition to produce a <ph id="ph2">`/dev/drbd1`</ph> resource able to be formatted using ext3 and used in both primary and secondary nodes.</source>
          <target state="new">We will create a DRBD resource that uses the underlying <ph id="ph1">`/dev/sdc1`</ph> partition to produce a <ph id="ph2">`/dev/drbd1`</ph> resource able to be formatted using ext3 and used in both primary and secondary nodes.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>To do this, open <ph id="ph1">`/etc/drbd.d/r0.res`</ph> and copy the following resource definition.</source>
          <target state="new">To do this, open <ph id="ph1">`/etc/drbd.d/r0.res`</ph> and copy the following resource definition.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Do this in both VMs:</source>
          <target state="new">Do this in both VMs:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>After doing this, initialize the resource using <ph id="ph1">`drbdadm`</ph> in both VMs:</source>
          <target state="new">After doing this, initialize the resource using <ph id="ph1">`drbdadm`</ph> in both VMs:</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>And finally, on the primary (<ph id="ph1">`hadb01`</ph>) force ownership (primary) of the DRBD resource:</source>
          <target state="new">And finally, on the primary (<ph id="ph1">`hadb01`</ph>) force ownership (primary) of the DRBD resource:</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>If you examine the contents of /proc/drbd (<ph id="ph1">`sudo cat /proc/drbd`</ph>) on both VMs, you should see <ph id="ph2">`Primary/Secondary`</ph> on <ph id="ph3">`hadb01`</ph> and <ph id="ph4">`Secondary/Primary`</ph> on <ph id="ph5">`hadb02`</ph>, consistent with the solution at this point.</source>
          <target state="new">If you examine the contents of /proc/drbd (<ph id="ph1">`sudo cat /proc/drbd`</ph>) on both VMs, you should see <ph id="ph2">`Primary/Secondary`</ph> on <ph id="ph3">`hadb01`</ph> and <ph id="ph4">`Secondary/Primary`</ph> on <ph id="ph5">`hadb02`</ph>, consistent with the solution at this point.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The 5 GB disk will be synchronized over the 10.10.10.0/24 network at no charge to customers.</source>
          <target state="new">The 5 GB disk will be synchronized over the 10.10.10.0/24 network at no charge to customers.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Once the disk is synchronized you can create the filesystem on <ph id="ph1">`hadb01`</ph>.</source>
          <target state="new">Once the disk is synchronized you can create the filesystem on <ph id="ph1">`hadb01`</ph>.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>For testing purposes we used ext2 but the following instruction will create an ext3 filesystem:</source>
          <target state="new">For testing purposes we used ext2 but the following instruction will create an ext3 filesystem:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Mounting the DRBD resource</source>
          <target state="new">Mounting the DRBD resource</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb01`</ph> we're now ready to mount the DRBD resources.</source>
          <target state="new">On <ph id="ph1">`hadb01`</ph> we're now ready to mount the DRBD resources.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Debian and derivatives use <ph id="ph1">`/var/lib/mysql`</ph> as MySQL's data directory.</source>
          <target state="new">Debian and derivatives use <ph id="ph1">`/var/lib/mysql`</ph> as MySQL's data directory.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Since we haven't installed MySQL, we'll create the directory and mount the DRBD resource.</source>
          <target state="new">Since we haven't installed MySQL, we'll create the directory and mount the DRBD resource.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb01`</ph>:</source>
          <target state="new">On <ph id="ph1">`hadb01`</ph>:</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Setting up MySQL</source>
          <target state="new">Setting up MySQL</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Now you're ready to install MySQL on <ph id="ph1">`hadb01`</ph>:</source>
          <target state="new">Now you're ready to install MySQL on <ph id="ph1">`hadb01`</ph>:</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>For <ph id="ph1">`hadb02`</ph>, you have two options.</source>
          <target state="new">For <ph id="ph1">`hadb02`</ph>, you have two options.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>You can install mysql-server now, which will create /var/lib/mysql and fill it with a new data directory, and then proceed to remove the contents.</source>
          <target state="new">You can install mysql-server now, which will create /var/lib/mysql and fill it with a new data directory, and then proceed to remove the contents.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb02`</ph>:</source>
          <target state="new">On <ph id="ph1">`hadb02`</ph>:</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>The second option is to failover to <ph id="ph1">`hadb02`</ph> and then install mysql-server there (installation scripts will notice the existing installation and won't touch it)</source>
          <target state="new">The second option is to failover to <ph id="ph1">`hadb02`</ph> and then install mysql-server there (installation scripts will notice the existing installation and won't touch it)</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb01`</ph>:</source>
          <target state="new">On <ph id="ph1">`hadb01`</ph>:</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb02`</ph>:</source>
          <target state="new">On <ph id="ph1">`hadb02`</ph>:</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>If you don't plan to failover DRBD now, the first option is easier although arguably less elegant.</source>
          <target state="new">If you don't plan to failover DRBD now, the first option is easier although arguably less elegant.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>After you set this up, you can start working on your MySQL database.</source>
          <target state="new">After you set this up, you can start working on your MySQL database.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`hadb02`</ph> (or whichever one of the servers is active, according to DRBD):</source>
          <target state="new">On <ph id="ph1">`hadb02`</ph> (or whichever one of the servers is active, according to DRBD):</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Warning<ept id="p1">**</ept>: this last statement effectively disables authentication for the root user in this table.</source>
          <target state="new"><bpt id="p1">**</bpt>Warning<ept id="p1">**</ept>: this last statement effectively disables authentication for the root user in this table.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>This should be replaced by your production-grade GRANT statements and is included only for illustrative purposes.</source>
          <target state="new">This should be replaced by your production-grade GRANT statements and is included only for illustrative purposes.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>You also need to enable networking for MySQL if you want to make queries from outside the VMs, which is the purpose of this guide.</source>
          <target state="new">You also need to enable networking for MySQL if you want to make queries from outside the VMs, which is the purpose of this guide.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>On both VMs, open <ph id="ph1">`/etc/mysql/my.cnf`</ph> and browse to <ph id="ph2">`bind-address`</ph>, changing it from 127.0.0.1 to 0.0.0.0.</source>
          <target state="new">On both VMs, open <ph id="ph1">`/etc/mysql/my.cnf`</ph> and browse to <ph id="ph2">`bind-address`</ph>, changing it from 127.0.0.1 to 0.0.0.0.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>After saving the file, issue a <ph id="ph1">`sudo service mysql restart`</ph> on your current primary.</source>
          <target state="new">After saving the file, issue a <ph id="ph1">`sudo service mysql restart`</ph> on your current primary.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Creating the MySQL Load Balanced Set</source>
          <target state="new">Creating the MySQL Load Balanced Set</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>We will go back to the Azure Portal and browse to the <ph id="ph1">`hadb01`</ph> VM, then Endpoints.</source>
          <target state="new">We will go back to the Azure Portal and browse to the <ph id="ph1">`hadb01`</ph> VM, then Endpoints.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>We will create a new Endpoint, choose MySQL (TCP 3306) from the dropdown and tick on the <bpt id="p1">*</bpt>Create new load balanced set<ept id="p1">*</ept> box.</source>
          <target state="new">We will create a new Endpoint, choose MySQL (TCP 3306) from the dropdown and tick on the <bpt id="p1">*</bpt>Create new load balanced set<ept id="p1">*</ept> box.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>We will call our load balanced endpoint <ph id="ph1">`lb-mysql`</ph>.</source>
          <target state="new">We will call our load balanced endpoint <ph id="ph1">`lb-mysql`</ph>.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>We will leave most of the options alone except for time which we'll reduce to 5 (seconds, minimum)</source>
          <target state="new">We will leave most of the options alone except for time which we'll reduce to 5 (seconds, minimum)</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>After the endpoint is created we go to <ph id="ph1">`hadb02`</ph>, Endpoints, and create a new endpoint but we will choose <ph id="ph2">`lb-mysql`</ph>, then select MySQL from the dropdown menu.</source>
          <target state="new">After the endpoint is created we go to <ph id="ph1">`hadb02`</ph>, Endpoints, and create a new endpoint but we will choose <ph id="ph2">`lb-mysql`</ph>, then select MySQL from the dropdown menu.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>You can also use the Azure CLI for this step.</source>
          <target state="new">You can also use the Azure CLI for this step.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>At this moment we have everything we need for a manual operation of the cluster.</source>
          <target state="new">At this moment we have everything we need for a manual operation of the cluster.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Testing the load balanced set</source>
          <target state="new">Testing the load balanced set</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Tests can be performed from an outside machine, by using any MySQL client, as well as applications (for example, phpMyAdmin running as an Azure Website) In this case we used MySQL's command line tool on another Linux box:</source>
          <target state="new">Tests can be performed from an outside machine, by using any MySQL client, as well as applications (for example, phpMyAdmin running as an Azure Website) In this case we used MySQL's command line tool on another Linux box:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Manually failing over</source>
          <target state="new">Manually failing over</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>You can simulate failovers now by shutting MySQL down, switching DRBD's primary, and starting MySQL again.</source>
          <target state="new">You can simulate failovers now by shutting MySQL down, switching DRBD's primary, and starting MySQL again.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>On hadb01:</source>
          <target state="new">On hadb01:</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Then, on hadb02:</source>
          <target state="new">Then, on hadb02:</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Once you failover manually you can repeat your remote query and it should be working perfectly.</source>
          <target state="new">Once you failover manually you can repeat your remote query and it should be working perfectly.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Setting up Corosync</source>
          <target state="new">Setting up Corosync</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Corosync is the underlying cluster infrastructure required for Pacemaker to work.</source>
          <target state="new">Corosync is the underlying cluster infrastructure required for Pacemaker to work.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>For Heartbeat v1 and v2 users (and other methodologies like Ultramonkey) Corosync is a split of the CRM functionalities, while Pacemaker remains more similar to Hearbeat in functionality.</source>
          <target state="new">For Heartbeat v1 and v2 users (and other methodologies like Ultramonkey) Corosync is a split of the CRM functionalities, while Pacemaker remains more similar to Hearbeat in functionality.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>The main constraint for Corosync on Azure is that Corosync prefers multicast over broadcast over unicast communications, but Microsoft Azure networking only supports unicast.</source>
          <target state="new">The main constraint for Corosync on Azure is that Corosync prefers multicast over broadcast over unicast communications, but Microsoft Azure networking only supports unicast.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Fortunately, Corosync has a working unicast mode and the only real constraint is that, since all nodes are not communicating among themselves <bpt id="p1">*</bpt>automagically<ept id="p1">*</ept>, you need to define the nodes in your configuration files, including their IP addresses.</source>
          <target state="new">Fortunately, Corosync has a working unicast mode and the only real constraint is that, since all nodes are not communicating among themselves <bpt id="p1">*</bpt>automagically<ept id="p1">*</ept>, you need to define the nodes in your configuration files, including their IP addresses.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>We can use the Corosync example files for Unicast and just change bind address, node lists and logging directory (Ubuntu uses <ph id="ph1">`/var/log/corosync`</ph> while the example files use <ph id="ph2">`/var/log/cluster`</ph>) and enabling quorum tools.</source>
          <target state="new">We can use the Corosync example files for Unicast and just change bind address, node lists and logging directory (Ubuntu uses <ph id="ph1">`/var/log/corosync`</ph> while the example files use <ph id="ph2">`/var/log/cluster`</ph>) and enabling quorum tools.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note the <ph id="ph1">`transport: udpu`</ph> directive below and the manually defined IP addresses for the nodes<ept id="p1">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>Note the <ph id="ph1">`transport: udpu`</ph> directive below and the manually defined IP addresses for the nodes<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>On <ph id="ph1">`/etc/corosync/corosync.conf`</ph> for both nodes:</source>
          <target state="new">On <ph id="ph1">`/etc/corosync/corosync.conf`</ph> for both nodes:</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>We copy this configuration file in both VMs and start Corosync in both nodes:</source>
          <target state="new">We copy this configuration file in both VMs and start Corosync in both nodes:</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Shortly after starting the service the cluster should be established in the current ring and quorum should be constituted.</source>
          <target state="new">Shortly after starting the service the cluster should be established in the current ring and quorum should be constituted.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>We can check this functionality by reviewing logs or:</source>
          <target state="new">We can check this functionality by reviewing logs or:</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>An output similar to the image below should follow:</source>
          <target state="new">An output similar to the image below should follow:</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>corosync-quorumtool -l sample output</source>
          <target state="new">corosync-quorumtool -l sample output</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>Setting up Pacemaker</source>
          <target state="new">Setting up Pacemaker</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>Pacemaker uses the cluster to monitor for resources, define when primaries go down and switch those resources to secondaries.</source>
          <target state="new">Pacemaker uses the cluster to monitor for resources, define when primaries go down and switch those resources to secondaries.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Resources can be defined from a set of available scripts or from LSB (init-like) scripts, among other choices.</source>
          <target state="new">Resources can be defined from a set of available scripts or from LSB (init-like) scripts, among other choices.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>We want Pacemaker to "own" the DRBD resource, the mountpoint and the MySQL service.</source>
          <target state="new">We want Pacemaker to "own" the DRBD resource, the mountpoint and the MySQL service.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>If Pacemaker can turn on and off DRBD, mount it/umount it and start/stop MySQL in the right order when something bad happens with the primary, our setup is complete.</source>
          <target state="new">If Pacemaker can turn on and off DRBD, mount it/umount it and start/stop MySQL in the right order when something bad happens with the primary, our setup is complete.</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>When you first install Pacemaker, your configuration should be simple enough, something like:</source>
          <target state="new">When you first install Pacemaker, your configuration should be simple enough, something like:</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>Check it by running <ph id="ph1">`sudo crm configure show`</ph>.</source>
          <target state="new">Check it by running <ph id="ph1">`sudo crm configure show`</ph>.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Now, create a file (say, <ph id="ph1">`/tmp/cluster.conf`</ph>) with the following resources:</source>
          <target state="new">Now, create a file (say, <ph id="ph1">`/tmp/cluster.conf`</ph>) with the following resources:</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>And now load it into the configuration (you only need to do this in one node):</source>
          <target state="new">And now load it into the configuration (you only need to do this in one node):</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>Also, make sure that Pacemaker starts at boot in both nodes:</source>
          <target state="new">Also, make sure that Pacemaker starts at boot in both nodes:</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>After a few seconds, and using <ph id="ph1">`sudo crm_mon –L`</ph>, verify that one of your nodes has become the master for the cluster, and is running all the resources.</source>
          <target state="new">After a few seconds, and using <ph id="ph1">`sudo crm_mon –L`</ph>, verify that one of your nodes has become the master for the cluster, and is running all the resources.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>You can use mount and ps to check that the resources are running.</source>
          <target state="new">You can use mount and ps to check that the resources are running.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>The following screenshot shows <ph id="ph1">`crm_mon`</ph> with one node stopped (exit using Control-C)</source>
          <target state="new">The following screenshot shows <ph id="ph1">`crm_mon`</ph> with one node stopped (exit using Control-C)</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>crm_mon node stopped</source>
          <target state="new">crm_mon node stopped</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>And this screenshot shows both nodes, with one master and one slave:</source>
          <target state="new">And this screenshot shows both nodes, with one master and one slave:</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>crm_mon operational master/slave</source>
          <target state="new">crm_mon operational master/slave</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>Testing</source>
          <target state="new">Testing</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>We're ready for an automatic failover simulation.</source>
          <target state="new">We're ready for an automatic failover simulation.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>There are two ways to doing this: soft and hard.</source>
          <target state="new">There are two ways to doing this: soft and hard.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>The soft way is using the cluster's shutdown function: <ph id="ph1">``crm_standby -U `uname -n` -v on``</ph>.</source>
          <target state="new">The soft way is using the cluster's shutdown function: <ph id="ph1">``crm_standby -U `uname -n` -v on``</ph>.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>Using this on the master, the slave will take over.</source>
          <target state="new">Using this on the master, the slave will take over.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>Remember to set this back to off (crm_mon will tell you one node is on standby otherwise)</source>
          <target state="new">Remember to set this back to off (crm_mon will tell you one node is on standby otherwise)</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>The hard way is shutting down the primary VM (hadb01) via the Portal or changing the runlevel on the VM (i.e., halt, shutdown) then we're helping Corosync and Pacemaker by signaling master's going down.</source>
          <target state="new">The hard way is shutting down the primary VM (hadb01) via the Portal or changing the runlevel on the VM (i.e., halt, shutdown) then we're helping Corosync and Pacemaker by signaling master's going down.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>We can test this (useful for maintenance windows) but we can also force the scenario by just freezing the VM.</source>
          <target state="new">We can test this (useful for maintenance windows) but we can also force the scenario by just freezing the VM.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>STONITH</source>
          <target state="new">STONITH</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>It should be possible to issue a VM shutdown via the Azure CLI in lieu of a STONITH script that controls a physical device.</source>
          <target state="new">It should be possible to issue a VM shutdown via the Azure CLI in lieu of a STONITH script that controls a physical device.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>You can use <ph id="ph1">`/usr/lib/stonith/plugins/external/ssh`</ph> as a base and enable STONITH in the cluster's configuration.</source>
          <target state="new">You can use <ph id="ph1">`/usr/lib/stonith/plugins/external/ssh`</ph> as a base and enable STONITH in the cluster's configuration.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>Azure CLI should be globally installed and the publish settings/profile should be loaded for the cluster's user.</source>
          <target state="new">Azure CLI should be globally installed and the publish settings/profile should be loaded for the cluster's user.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>Sample code for the resource available on <bpt id="p1">[</bpt>GitHub<ept id="p1">](https://github.com/bureado/aztonith)</ept>.</source>
          <target state="new">Sample code for the resource available on <bpt id="p1">[</bpt>GitHub<ept id="p1">](https://github.com/bureado/aztonith)</ept>.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>You need to change the cluster's configuration by adding the following to <ph id="ph1">`sudo crm configure`</ph>:</source>
          <target state="new">You need to change the cluster's configuration by adding the following to <ph id="ph1">`sudo crm configure`</ph>:</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> the script doesn't perform up/down checks.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> the script doesn't perform up/down checks.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>The original SSH resource had 15 ping checks but recovery time for an Azure VM might be more variable.</source>
          <target state="new">The original SSH resource had 15 ping checks but recovery time for an Azure VM might be more variable.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="new">Limitations</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>The following limitations apply:</source>
          <target state="new">The following limitations apply:</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>The linbit DRBD resource script that manages DRBD as a resource in Pacemaker uses <ph id="ph1">`drbdadm down`</ph> when shutting down a node, even if the node is just going standby.</source>
          <target state="new">The linbit DRBD resource script that manages DRBD as a resource in Pacemaker uses <ph id="ph1">`drbdadm down`</ph> when shutting down a node, even if the node is just going standby.</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>This is not ideal since the slave will not be synchronizing the DRBD resource while the master gets writes.</source>
          <target state="new">This is not ideal since the slave will not be synchronizing the DRBD resource while the master gets writes.</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>If the master does not fail graciously, the slave can take over an older filesystem state.</source>
          <target state="new">If the master does not fail graciously, the slave can take over an older filesystem state.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>There are two potential ways of solving this:</source>
          <target state="new">There are two potential ways of solving this:</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>Enforcing a <ph id="ph1">`drbdadm up r0`</ph> in all cluster nodes via a local (not clusterized) watchdog, or,</source>
          <target state="new">Enforcing a <ph id="ph1">`drbdadm up r0`</ph> in all cluster nodes via a local (not clusterized) watchdog, or,</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>Editing the linbit DRBD script making sure that <ph id="ph1">`down`</ph> is not called, in <ph id="ph2">`/usr/lib/ocf/resource.d/linbit/drbd`</ph>.</source>
          <target state="new">Editing the linbit DRBD script making sure that <ph id="ph1">`down`</ph> is not called, in <ph id="ph2">`/usr/lib/ocf/resource.d/linbit/drbd`</ph>.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Load balancer needs at least 5 seconds to respond, so applications should be cluster aware and be more tolerant of timeout; other architectures can also help, for example in-app queues, query middlewares, etc.</source>
          <target state="new">Load balancer needs at least 5 seconds to respond, so applications should be cluster aware and be more tolerant of timeout; other architectures can also help, for example in-app queues, query middlewares, etc.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>MySQL tuning is necessary to ensure writing is done at a sane pace and caches are flushed to disk as frequently as possible to minimize memory loss</source>
          <target state="new">MySQL tuning is necessary to ensure writing is done at a sane pace and caches are flushed to disk as frequently as possible to minimize memory loss</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Write performance will be dependent in VM interconnect in the virtual switch as this is the mechanism used by DRBD to replicate the device</source>
          <target state="new">Write performance will be dependent in VM interconnect in the virtual switch as this is the mechanism used by DRBD to replicate the device</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f2209976abc33e577a4c2222e74442b8aaf25d83</xliffext:olfilehash>
  </header>
</xliff>