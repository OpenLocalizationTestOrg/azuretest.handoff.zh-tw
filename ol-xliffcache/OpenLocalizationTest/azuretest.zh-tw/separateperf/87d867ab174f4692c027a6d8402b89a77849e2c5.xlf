<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Load data into SQL Data Warehouse | Microsoft Azure</source>
          <target state="new">Load data into SQL Data Warehouse | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn the common scenarios for data loading in SQL Data Warehouse</source>
          <target state="new">Learn the common scenarios for data loading in SQL Data Warehouse</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Load data into SQL Data Warehouse</source>
          <target state="new">Load data into SQL Data Warehouse</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>SQL Data Warehouse presents numerous options for loading data including:</source>
          <target state="new">SQL Data Warehouse presents numerous options for loading data including:</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Azure Data Factory</source>
          <target state="new">Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>BCP command-line utility</source>
          <target state="new">BCP command-line utility</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>PolyBase</source>
          <target state="new">PolyBase</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>SQL Server Integration Services (SSIS)</source>
          <target state="new">SQL Server Integration Services (SSIS)</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>3rd party data loading tools</source>
          <target state="new">3rd party data loading tools</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>While all of the above methods can be used with SQL Data Warehouse.</source>
          <target state="new">While all of the above methods can be used with SQL Data Warehouse.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Many of our users are looking at initial loads in the 100s of Gigabytes to the 10s of Terabytes.</source>
          <target state="new">Many of our users are looking at initial loads in the 100s of Gigabytes to the 10s of Terabytes.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>In the below sections, we provide some guidance on initial data loading.</source>
          <target state="new">In the below sections, we provide some guidance on initial data loading.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Initial Loading into SQL Data Warehouse from SQL Server</source>
          <target state="new">Initial Loading into SQL Data Warehouse from SQL Server</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>When loading into SQL Data Warehouse from an on-premise SQL Server instance, we recommend the following steps:</source>
          <target state="new">When loading into SQL Data Warehouse from an on-premise SQL Server instance, we recommend the following steps:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>BCP<ept id="p1">**</ept> SQL Server data into flat files</source>
          <target state="new"><bpt id="p1">**</bpt>BCP<ept id="p1">**</ept> SQL Server data into flat files</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Use <bpt id="p1">**</bpt>AZCopy<ept id="p1">**</ept> or <bpt id="p2">**</bpt>Import/Export<ept id="p2">**</ept> (for larger datasets) to move your files into Azure</source>
          <target state="new">Use <bpt id="p1">**</bpt>AZCopy<ept id="p1">**</ept> or <bpt id="p2">**</bpt>Import/Export<ept id="p2">**</ept> (for larger datasets) to move your files into Azure</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Configure PolyBase to read your files from your storage account</source>
          <target state="new">Configure PolyBase to read your files from your storage account</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Create new tables and load data with <bpt id="p1">**</bpt>PolyBase<ept id="p1">**</ept></source>
          <target state="new">Create new tables and load data with <bpt id="p1">**</bpt>PolyBase<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>In the following sections we will take a look at each step in great depth and provide examples of the process.</source>
          <target state="new">In the following sections we will take a look at each step in great depth and provide examples of the process.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Before moving data from a system such as SQL Server, we suggest reviewing the <bpt id="p1">[</bpt>Migrate schema<ept id="p1">][]</ept> and <bpt id="p2">[</bpt>Migrate code<ept id="p2">][]</ept> articles of our documentation.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Before moving data from a system such as SQL Server, we suggest reviewing the <bpt id="p1">[</bpt>Migrate schema<ept id="p1">][]</ept> and <bpt id="p2">[</bpt>Migrate code<ept id="p2">][]</ept> articles of our documentation.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Exporting files with BCP</source>
          <target state="new">Exporting files with BCP</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>In order to prep your files for movement to Azure, you will need to export them to flat files.</source>
          <target state="new">In order to prep your files for movement to Azure, you will need to export them to flat files.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>This is best done using the BCP Command-line Utility.</source>
          <target state="new">This is best done using the BCP Command-line Utility.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>If you do not have the utility yet, it can be downloaded with the <bpt id="p1">[</bpt>Microsoft Command Line Utilities for SQL Server<ept id="p1">][]</ept>.</source>
          <target state="new">If you do not have the utility yet, it can be downloaded with the <bpt id="p1">[</bpt>Microsoft Command Line Utilities for SQL Server<ept id="p1">][]</ept>.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>A sample BCP command might look like the following:</source>
          <target state="new">A sample BCP command might look like the following:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>This command will take the results of a query and export them to a file in the directory of your choice.</source>
          <target state="new">This command will take the results of a query and export them to a file in the directory of your choice.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>You can parallelize the process by running multiple BCP commands for separate tables at once  This will enable you to run up to one BCP process per core of your server, our advice is try a few smaller operations at different configurations to see what works best for your environment.</source>
          <target state="new">You can parallelize the process by running multiple BCP commands for separate tables at once  This will enable you to run up to one BCP process per core of your server, our advice is try a few smaller operations at different configurations to see what works best for your environment.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>In addition, as we will be loading using PolyBase, please note that PolyBase does not yet support UTF-16, and all files must be in UTF-8.</source>
          <target state="new">In addition, as we will be loading using PolyBase, please note that PolyBase does not yet support UTF-16, and all files must be in UTF-8.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>This can easily be accomplished by including the '-c' flag in your BCP command or you can also convert flat files from UTF-16 to UTF-8 with the below code:</source>
          <target state="new">This can easily be accomplished by including the '-c' flag in your BCP command or you can also convert flat files from UTF-16 to UTF-8 with the below code:</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Once you have successfully exported your data to files, it is time to move them to Azure.</source>
          <target state="new">Once you have successfully exported your data to files, it is time to move them to Azure.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>This can be accomplished with AZCopy or with the Import/Export service as described in the next section.</source>
          <target state="new">This can be accomplished with AZCopy or with the Import/Export service as described in the next section.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Loading into Azure with AZCopy or Import/Export</source>
          <target state="new">Loading into Azure with AZCopy or Import/Export</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>If you are moving data in the 5-10 terabyte range or above, we recommend that you use our disk shipping service <bpt id="p1">[</bpt>Import/Export<ept id="p1">][]</ept> in order to accomplish the move.</source>
          <target state="new">If you are moving data in the 5-10 terabyte range or above, we recommend that you use our disk shipping service <bpt id="p1">[</bpt>Import/Export<ept id="p1">][]</ept> in order to accomplish the move.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>However, in our studies, we have been able to move data in the single digit TB range comfortably using public internet with AZCopy.</source>
          <target state="new">However, in our studies, we have been able to move data in the single digit TB range comfortably using public internet with AZCopy.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>This process can also be sped up or extended with ExpressRoute.</source>
          <target state="new">This process can also be sped up or extended with ExpressRoute.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>The following steps will detail out how to move data from on-premise into an Azure Storage account using AZCopy.</source>
          <target state="new">The following steps will detail out how to move data from on-premise into an Azure Storage account using AZCopy.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>If you don't have an Azure Storage account in the same region you can create one by following the <bpt id="p1">[</bpt>Azure Storage Documentation<ept id="p1">][]</ept>.</source>
          <target state="new">If you don't have an Azure Storage account in the same region you can create one by following the <bpt id="p1">[</bpt>Azure Storage Documentation<ept id="p1">][]</ept>.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>You can also load data from a storage account in a different region, but the performance in this case will not be optimal.</source>
          <target state="new">You can also load data from a storage account in a different region, but the performance in this case will not be optimal.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This documentation assumes that you have installed the AZCopy command line utility and are able to run it with Powershell.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This documentation assumes that you have installed the AZCopy command line utility and are able to run it with Powershell.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>If this is not the case, then please follow the <bpt id="p1">[</bpt>AZCopy Installation Instructions<ept id="p1">][]</ept>.</source>
          <target state="new">If this is not the case, then please follow the <bpt id="p1">[</bpt>AZCopy Installation Instructions<ept id="p1">][]</ept>.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Now, given a set of files that have created using BCP, AzCopy can simply be run from the Azure powershell or by running a powershell script.</source>
          <target state="new">Now, given a set of files that have created using BCP, AzCopy can simply be run from the Azure powershell or by running a powershell script.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>At a high level, the prompt needed to run AZCopy will take the form:</source>
          <target state="new">At a high level, the prompt needed to run AZCopy will take the form:</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>In addition to the basic, we recommend the following best practices for loading with AZCopy:</source>
          <target state="new">In addition to the basic, we recommend the following best practices for loading with AZCopy:</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Concurrent Connections<ept id="p1">**</ept>: In addition to increasing the number of AZCopy operations that run at once, the AZCopy operation itself can be further parallelized by setting the /NC parameter, which opens a number of concurrent connections to the destination.</source>
          <target state="new"><bpt id="p1">**</bpt>Concurrent Connections<ept id="p1">**</ept>: In addition to increasing the number of AZCopy operations that run at once, the AZCopy operation itself can be further parallelized by setting the /NC parameter, which opens a number of concurrent connections to the destination.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>While the parameter can be set as high as 512, we found optimal data transfer took place at 256, and recommend that a number of values are tested to find what is optimal for your configuration.</source>
          <target state="new">While the parameter can be set as high as 512, we found optimal data transfer took place at 256, and recommend that a number of values are tested to find what is optimal for your configuration.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Express Route<ept id="p1">**</ept>: As stated above, this process can be sped up if express route is enabled.</source>
          <target state="new"><bpt id="p1">**</bpt>Express Route<ept id="p1">**</ept>: As stated above, this process can be sped up if express route is enabled.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>An overview of Express Route and steps to configure can be found in the <bpt id="p1">[</bpt>ExpressRoute documentation<ept id="p1">][]</ept>.</source>
          <target state="new">An overview of Express Route and steps to configure can be found in the <bpt id="p1">[</bpt>ExpressRoute documentation<ept id="p1">][]</ept>.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Folder Structure<ept id="p1">**</ept>: To make transfer with PolyBase easier, ensure that each table is mapped to its own folder.</source>
          <target state="new"><bpt id="p1">**</bpt>Folder Structure<ept id="p1">**</ept>: To make transfer with PolyBase easier, ensure that each table is mapped to its own folder.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>This will minimize and simplify your steps when loading with PolyBase later.</source>
          <target state="new">This will minimize and simplify your steps when loading with PolyBase later.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>That being said, there is no impact if a table is split into multiple files or even sub directories within the folder.</source>
          <target state="new">That being said, there is no impact if a table is split into multiple files or even sub directories within the folder.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Configuring PolyBase</source>
          <target state="new">Configuring PolyBase</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Now that your data resides in Azure storage blobs, we will import it into your SQL Data Warehouse instance using PolyBase.</source>
          <target state="new">Now that your data resides in Azure storage blobs, we will import it into your SQL Data Warehouse instance using PolyBase.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>The below steps are for configuration only, and many of them will only need to be completed once per SQL Data Warehouse instance, user, or storage account.</source>
          <target state="new">The below steps are for configuration only, and many of them will only need to be completed once per SQL Data Warehouse instance, user, or storage account.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>These steps have also been outlined in in greater detail in our <bpt id="p1">[</bpt>Load with PolyBase<ept id="p1">][]</ept> documentation.</source>
          <target state="new">These steps have also been outlined in in greater detail in our <bpt id="p1">[</bpt>Load with PolyBase<ept id="p1">][]</ept> documentation.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Create a database master key.</source>
          <target state="new">Create a database master key.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>This operation will only need to be completed once per database.</source>
          <target state="new">This operation will only need to be completed once per database.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Create a database scoped credential.</source>
          <target state="new">Create a database scoped credential.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>This operation only needs to be created if you are looking at creating a new credential/user, otherwise a previously created credential can be used.</source>
          <target state="new">This operation only needs to be created if you are looking at creating a new credential/user, otherwise a previously created credential can be used.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Create an external file format.</source>
          <target state="new">Create an external file format.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>External file formats are reusable as well, you will only need to create one if you are uploading a new type of file.</source>
          <target state="new">External file formats are reusable as well, you will only need to create one if you are uploading a new type of file.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Create an external data source.</source>
          <target state="new">Create an external data source.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>When pointing at a storage account, an external data source can be used when loading from the same container.</source>
          <target state="new">When pointing at a storage account, an external data source can be used when loading from the same container.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>For your 'LOCATION' parameter, use a location of the format: 'wasbs://mycontainer@ test.blob.core.windows.net/path'.</source>
          <target state="new">For your 'LOCATION' parameter, use a location of the format: 'wasbs://mycontainer@ test.blob.core.windows.net/path'.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Now that your storage account is properly configured to you can proceed to loading your data into SQL Data Warehouse.</source>
          <target state="new">Now that your storage account is properly configured to you can proceed to loading your data into SQL Data Warehouse.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Loading data with PolyBase</source>
          <target state="new">Loading data with PolyBase</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>After configuring PolyBase, you can load data directly into your SQL Data Warehouse by simply creating an external table that points to your data in storage and then mapping that data to a new table within SQL Data Warehouse.</source>
          <target state="new">After configuring PolyBase, you can load data directly into your SQL Data Warehouse by simply creating an external table that points to your data in storage and then mapping that data to a new table within SQL Data Warehouse.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>This can be accomplished with the two simple commands below.</source>
          <target state="new">This can be accomplished with the two simple commands below.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Use the 'CREATE EXTERNAL TABLE' command to define the structure of your data.</source>
          <target state="new">Use the 'CREATE EXTERNAL TABLE' command to define the structure of your data.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>To make sure you capture the state of your data quickly and efficiently, we recommend scripting out the SQL Server table in SSMS, and then adjusting by hand to account for the external table differences.</source>
          <target state="new">To make sure you capture the state of your data quickly and efficiently, we recommend scripting out the SQL Server table in SSMS, and then adjusting by hand to account for the external table differences.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>After creating an external table in Azure it will continue to point to the same location, even if data is updated or additional data is added.</source>
          <target state="new">After creating an external table in Azure it will continue to point to the same location, even if data is updated or additional data is added.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Load data with a 'CREATE TABLE...AS SELECT' statement.</source>
          <target state="new">Load data with a 'CREATE TABLE...AS SELECT' statement.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Note that you can also load a subsection of the rows from a table using a more detailed SELECT statement.</source>
          <target state="new">Note that you can also load a subsection of the rows from a table using a more detailed SELECT statement.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>However, as PolyBase does not push additional compute to storage accounts at this time, if you load a subsection with a SELECT statement this will not be faster than loading the entire dataset.</source>
          <target state="new">However, as PolyBase does not push additional compute to storage accounts at this time, if you load a subsection with a SELECT statement this will not be faster than loading the entire dataset.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>In addition to the <ph id="ph1">`CREATE TABLE...AS SELECT`</ph> statement, you can also load data from external tables into pre-existing tables with a 'INSERT...INTO' statement.</source>
          <target state="new">In addition to the <ph id="ph1">`CREATE TABLE...AS SELECT`</ph> statement, you can also load data from external tables into pre-existing tables with a 'INSERT...INTO' statement.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>For more development tips, see the <bpt id="p1">[</bpt>development overview<ept id="p1">][]</ept>.</source>
          <target state="new">For more development tips, see the <bpt id="p1">[</bpt>development overview<ept id="p1">][]</ept>.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">87d867ab174f4692c027a6d8402b89a77849e2c5</xliffext:olfilehash>
  </header>
</xliff>