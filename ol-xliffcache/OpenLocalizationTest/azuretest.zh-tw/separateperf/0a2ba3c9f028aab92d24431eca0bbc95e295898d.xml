{
  "nodes": [
    {
      "content": "Use Hadoop Oozie in HDInsight | Microsoft Azure",
      "pos": [
        27,
        74
      ]
    },
    {
      "content": "Use Hadoop Oozie in HDInsight, a big data service.",
      "pos": [
        93,
        143
      ]
    },
    {
      "content": "Learn how to define an Oozie workflow, and submit an Oozie job.",
      "pos": [
        144,
        207
      ]
    },
    {
      "content": "Use Oozie with Hadoop to define and run a workflow on Linux-based HDInsight (Preview)",
      "pos": [
        538,
        623
      ]
    },
    {
      "content": "Learn how to use Apache Oozie to define a workflow that uses Hive and Sqoop, and then run the workflow on a Linux-based HDInsight cluster.",
      "pos": [
        703,
        841
      ]
    },
    {
      "content": "Apache Oozie is a workflow/coordination system that manages Hadoop jobs.",
      "pos": [
        843,
        915
      ]
    },
    {
      "content": "It is integrated with the Hadoop stack, and it supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop.",
      "pos": [
        916,
        1048
      ]
    },
    {
      "content": "It can also be used to schedule jobs that are specific to a system, like Java programs or shell scripts",
      "pos": [
        1049,
        1152
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Another option for defining workflows with HDInsight is Azure Data Factory.",
      "pos": [
        1156,
        1244
      ]
    },
    {
      "content": "To learn more about Azure Data Factory, see <bpt id=\"p1\">[</bpt>Use Pig and Hive with Data Factory<ept id=\"p1\">][azure-data-factory-pig-hive]</ept>.",
      "pos": [
        1245,
        1355
      ]
    },
    {
      "content": "Prerequisites",
      "pos": [
        1359,
        1372
      ]
    },
    {
      "content": "Before you begin this tutorial, you must have the following:",
      "pos": [
        1374,
        1434
      ]
    },
    {
      "pos": [
        1438,
        1549
      ],
      "content": "<bpt id=\"p1\">**</bpt>An Azure subscription<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Get Azure free trial<ept id=\"p2\">](get-azure-free-trial-for-testing-hadoop-in-hdinsight.md)</ept>."
    },
    {
      "pos": [
        1553,
        1623
      ],
      "content": "<bpt id=\"p1\">**</bpt>Azure CLI<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Install and Configure the Azure CLI<ept id=\"p2\">](xplat-cli.md)</ept>"
    },
    {
      "pos": [
        1627,
        1742
      ],
      "content": "<bpt id=\"p1\">**</bpt>An HDInsight cluster<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Get Started with HDInsight on Linux<ept id=\"p2\">](hdinsight-hadoop-linux-tutorial-get-started.md)</ept>"
    },
    {
      "pos": [
        1746,
        1826
      ],
      "content": "<bpt id=\"p1\">**</bpt>An Azure SQL database<ept id=\"p1\">**</ept>: This will be created using the steps in this document"
    },
    {
      "content": "Example workflow",
      "pos": [
        1830,
        1846
      ]
    },
    {
      "content": "The workflow you will implement by following the instructions in this document contains two actions.",
      "pos": [
        1848,
        1948
      ]
    },
    {
      "content": "Actions are definitions for tasks, such as running Hive, Sqoop, MapReduce, or other process:",
      "pos": [
        1949,
        2041
      ]
    },
    {
      "content": "Workflow diagram",
      "pos": [
        2045,
        2061
      ]
    },
    {
      "content": "A Hive action runs a HiveQL script to extract records from the <bpt id=\"p1\">**</bpt>hivesampletable<ept id=\"p1\">**</ept> included with HDInsight.",
      "pos": [
        2089,
        2196
      ]
    },
    {
      "content": "Each row of data describes a visit from a specific mobile device.",
      "pos": [
        2197,
        2262
      ]
    },
    {
      "content": "The record format appears similar to the following:",
      "pos": [
        2263,
        2314
      ]
    },
    {
      "content": "The Hive script used in this document counts the total visits for each platform (such as Android or iPhone,) and stores the counts to a new Hive table.",
      "pos": [
        2703,
        2854
      ]
    },
    {
      "pos": [
        2860,
        2943
      ],
      "content": "For more information about Hive, see <bpt id=\"p1\">[</bpt>Use Hive with HDInsight<ept id=\"p1\">][hdinsight-use-hive]</ept>."
    },
    {
      "content": "A Sqoop action exports the contents of the new Hive table to a table in an Azure SQL database.",
      "pos": [
        2949,
        3043
      ]
    },
    {
      "content": "For more information about Sqoop, see <bpt id=\"p1\">[</bpt>Use Hadoop Sqoop with HDInsight<ept id=\"p1\">][hdinsight-use-sqoop]</ept>.",
      "pos": [
        3044,
        3137
      ]
    },
    {
      "pos": [
        3141,
        3297
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> For supported Oozie versions on HDInsight clusters, see <bpt id=\"p1\">[</bpt>What's new in the Hadoop cluster versions provided by HDInsight?<ept id=\"p1\">][hdinsight-versions]</ept>."
    },
    {
      "content": "Create the working directory",
      "pos": [
        3301,
        3329
      ]
    },
    {
      "content": "Oozie expects resources required for a job to be stored in the same directory.",
      "pos": [
        3331,
        3409
      ]
    },
    {
      "content": "This example uses <bpt id=\"p1\">**</bpt>wasb:///tutorials/useoozie<ept id=\"p1\">**</ept>.",
      "pos": [
        3410,
        3459
      ]
    },
    {
      "content": "Use the following command to create this directory, and the data directory that will hold the new Hive table created by this workflow:",
      "pos": [
        3460,
        3594
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The <ph id=\"ph2\">`-p`</ph> parameter caused all directories in the path to be created if they do not already exist.",
      "pos": [
        3648,
        3758
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>data<ept id=\"p1\">**</ept> directory will be used to hold data used by the <bpt id=\"p2\">**</bpt>useooziewf.hql<ept id=\"p2\">**</ept> script.",
      "pos": [
        3759,
        3846
      ]
    },
    {
      "content": "Also run the following command, which ensures that Oozie can impersonate your user account when running Hive and Sqoop jobs.",
      "pos": [
        3848,
        3972
      ]
    },
    {
      "content": "Replace <bpt id=\"p1\">**</bpt>USERNAME<ept id=\"p1\">**</ept> with your login name:",
      "pos": [
        3973,
        4015
      ]
    },
    {
      "content": "If you receive an error that the user is already a member of users, you can just ignore it.",
      "pos": [
        4050,
        4141
      ]
    },
    {
      "content": "Add a database driver",
      "pos": [
        4145,
        4166
      ]
    },
    {
      "content": "Since this workflow uses Sqoop to export data to SQL Database, you must provide a copy of the JDBC driver used to talk to SQL Database.",
      "pos": [
        4168,
        4303
      ]
    },
    {
      "content": "Use the following command to copy it to the working directory:",
      "pos": [
        4304,
        4366
      ]
    },
    {
      "content": "If your workflow used other resources, such as a jar containing a MapReduce application, you would need to add these as well.",
      "pos": [
        4476,
        4601
      ]
    },
    {
      "content": "Define the Hive query",
      "pos": [
        4605,
        4626
      ]
    },
    {
      "content": "Use the following steps to create a HiveQL script that defines a query, which will be used in an Oozie workflow later in this document.",
      "pos": [
        4628,
        4763
      ]
    },
    {
      "content": "Use SSH to connect to the Linux-based HDInsight cluster:",
      "pos": [
        4768,
        4824
      ]
    },
    {
      "pos": [
        4832,
        4980
      ],
      "content": "<bpt id=\"p1\">**</bpt>Linux, Unix or OS X clients<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X or Unix<ept id=\"p2\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        4988,
        5115
      ],
      "content": "<bpt id=\"p1\">**</bpt>Windows clients<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p2\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "content": "Use the following command to create a new file:",
      "pos": [
        5120,
        5167
      ]
    },
    {
      "content": "Once the nano editor opens, use the following as the contents of the file:",
      "pos": [
        5201,
        5275
      ]
    },
    {
      "content": "There are two variables used in the script:",
      "pos": [
        5642,
        5685
      ]
    },
    {
      "pos": [
        5693,
        5763
      ],
      "content": "<bpt id=\"p1\">**</bpt>${hiveTableName}<ept id=\"p1\">**</ept>: will contain the name of the table to be created"
    },
    {
      "pos": [
        5770,
        5856
      ],
      "content": "<bpt id=\"p1\">**</bpt>${hiveDataFolder}<ept id=\"p1\">**</ept>: will contain the location to store the data files for the table"
    },
    {
      "content": "The workflow definition file (workflow.xml in this tutorial) passes these values to this HiveQL script at run time.",
      "pos": [
        5862,
        5977
      ]
    },
    {
      "content": "Press Ctrl-X to exit the editor.",
      "pos": [
        5982,
        6014
      ]
    },
    {
      "content": "When prompted, select <bpt id=\"p1\">**</bpt>Y<ept id=\"p1\">**</ept> to save the file, then use <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept> to use the <bpt id=\"p3\">**</bpt>useooziewf.hql<ept id=\"p3\">**</ept> file name.",
      "pos": [
        6015,
        6120
      ]
    },
    {
      "pos": [
        6125,
        6228
      ],
      "content": "Use the following commands to copy <bpt id=\"p1\">**</bpt>useooziewf.hql<ept id=\"p1\">**</ept> to <bpt id=\"p2\">**</bpt>wasb:///tutorials/useoozie/useooziewf.hql<ept id=\"p2\">**</ept>:"
    },
    {
      "content": "These commands store the <bpt id=\"p1\">**</bpt>useooziewf.hql<ept id=\"p1\">**</ept> file on the Azure Storage account associated with this cluster, which will preserve the file even if the cluster is deleted.",
      "pos": [
        6318,
        6486
      ]
    },
    {
      "content": "This allows you to save money by deleting clusters when they aren't in use, while maintaining your jobs and workflows.",
      "pos": [
        6487,
        6605
      ]
    },
    {
      "content": "Define the workflow",
      "pos": [
        6609,
        6628
      ]
    },
    {
      "content": "Oozie workflows definitions are written in hPDL (a XML Process Definition Language).",
      "pos": [
        6630,
        6714
      ]
    },
    {
      "content": "Use the following steps to define the workflow:",
      "pos": [
        6715,
        6762
      ]
    },
    {
      "content": "Use the following statement to create and edit a new file:",
      "pos": [
        6767,
        6825
      ]
    },
    {
      "content": "Once the nano editor opens, enter the following as the file contents:",
      "pos": [
        6857,
        6926
      ]
    },
    {
      "content": "There are two actions defined in the workflow:",
      "pos": [
        8914,
        8960
      ]
    },
    {
      "pos": [
        8968,
        9056
      ],
      "content": "<bpt id=\"p1\">**</bpt>RunHiveScript<ept id=\"p1\">**</ept>: This is the start action, and runs the <bpt id=\"p2\">**</bpt>useooziewf.hql<ept id=\"p2\">**</ept> Hive script"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>RunSqoopExport<ept id=\"p1\">**</ept>: This exports the data created from the Hive script to SQL Database using Sqoop.",
      "pos": [
        9064,
        9163
      ]
    },
    {
      "content": "This will only run if the <bpt id=\"p1\">**</bpt>RunHiveScript<ept id=\"p1\">**</ept> action is successful.",
      "pos": [
        9164,
        9229
      ]
    },
    {
      "pos": [
        9241,
        9491
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> For more information about Oozie workflow and using workflow actions, see <bpt id=\"p1\">[</bpt>Apache Oozie 4.0 documentation<ept id=\"p1\">][apache-oozie-400]</ept> (for HDInsight version 3.0) or <bpt id=\"p2\">[</bpt>Apache Oozie 3.3.2 documentation<ept id=\"p2\">][apache-oozie-332]</ept> (for HDInsight version 2.1)."
    },
    {
      "pos": [
        9497,
        9651
      ],
      "content": "Note that the workflow has several entries, such as <ph id=\"ph1\">`${jobTracker}`</ph>, that will be replaced by values you use in the job definition later in this document."
    },
    {
      "content": "Also note the <ph id=\"ph1\">`&lt;archive&gt;sqljdbc4.jar&lt;/arcive&gt;`</ph> entry in the Sqoop section.",
      "pos": [
        9657,
        9731
      ]
    },
    {
      "content": "This instructs Oozie to make this archive available for Sqoop when this action runs.",
      "pos": [
        9732,
        9816
      ]
    },
    {
      "pos": [
        9821,
        9875
      ],
      "content": "Use Ctrl-X, then <bpt id=\"p1\">**</bpt>Y<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept> to save the file."
    },
    {
      "pos": [
        9880,
        9987
      ],
      "content": "Use the following command to copy the <bpt id=\"p1\">**</bpt>workflow.xml<ept id=\"p1\">**</ept> file to <bpt id=\"p2\">**</bpt>wasb:///tutorials/useoozie/workflow.xml<ept id=\"p2\">**</ept>:"
    },
    {
      "content": "Create the database",
      "pos": [
        10078,
        10097
      ]
    },
    {
      "content": "The following steps create the Azure SQL Database that data will be exported to.",
      "pos": [
        10099,
        10179
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Before performing these steps you must <bpt id=\"p1\">[</bpt>install and configure the Azure CLI<ept id=\"p1\">](xplat-cli.md)</ept>.",
      "pos": [
        10183,
        10292
      ]
    },
    {
      "content": "Installing the CLI and following the steps to create a database can be performed either from the HDInsight cluster or your local workstation.",
      "pos": [
        10293,
        10434
      ]
    },
    {
      "content": "Use the following command to create a new Azure SQL Database server:",
      "pos": [
        10439,
        10507
      ]
    },
    {
      "pos": [
        10584,
        10648
      ],
      "content": "For exmaple, <ph id=\"ph1\">`azure sql server create admin password \"West US\"`</ph>."
    },
    {
      "content": "When the command completes, you will receive a response similar to the following:",
      "pos": [
        10654,
        10735
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> Note the server name returned by this command (<bpt id=\"p1\">**</bpt>i1qwc540ts<ept id=\"p1\">**</ept> in the example above.) This is the short name of the SQL Database server that was created.",
      "pos": [
        10913,
        11083
      ]
    },
    {
      "content": "The fully qualified domain name (FQDN) is <bpt id=\"p1\">**</bpt>&amp;lt;shortname&amp;gt;.database.windows.net<ept id=\"p1\">**</ept>.",
      "pos": [
        11084,
        11169
      ]
    },
    {
      "content": "For the example above, the FQDN would be <bpt id=\"p1\">**</bpt>i1qwc540ts.database.windows.net<ept id=\"p1\">**</ept>.",
      "pos": [
        11170,
        11247
      ]
    },
    {
      "pos": [
        11252,
        11346
      ],
      "content": "Use the following command to create a database named <bpt id=\"p1\">**</bpt>oozietest<ept id=\"p1\">**</ept> on the SQL Database server:"
    },
    {
      "content": "This will return an \"OK\" message when it completes.",
      "pos": [
        11443,
        11494
      ]
    },
    {
      "pos": [
        11502,
        11680
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If you receive an error indicating that you do not have access, you may need to add the system's IP address to the SQL Database firewall using the following command:"
    },
    {
      "content": "Create the table",
      "pos": [
        11789,
        11805
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> There are many ways to connect to SQL Database to create a table.",
      "pos": [
        11809,
        11887
      ]
    },
    {
      "content": "The following steps use <bpt id=\"p1\">[</bpt>FreeTDS<ept id=\"p1\">](http://www.freetds.org/)</ept> from the HDInsight cluster.",
      "pos": [
        11888,
        11974
      ]
    },
    {
      "content": "Use the following command to install FreeTDS on the HDInsight cluster:",
      "pos": [
        11979,
        12049
      ]
    },
    {
      "content": "Once FreeTDS has been installed, use the following command to connect to the SQL Database server you created previously:",
      "pos": [
        12121,
        12241
      ]
    },
    {
      "content": "You will receive output similar to the following:",
      "pos": [
        12365,
        12414
      ]
    },
    {
      "pos": [
        12583,
        12629
      ],
      "content": "At the <ph id=\"ph1\">`1&gt;`</ph> prompt, enter the following lines:"
    },
    {
      "content": "When the <ph id=\"ph1\">`GO`</ph> statement is entered, the previous statements will be evaluated.",
      "pos": [
        12854,
        12932
      ]
    },
    {
      "content": "This will create a new table named <bpt id=\"p1\">**</bpt>mobiledata<ept id=\"p1\">**</ept> that will be written to by Sqoop.",
      "pos": [
        12933,
        13016
      ]
    },
    {
      "content": "Use the following to verify that the table has been created:",
      "pos": [
        13022,
        13082
      ]
    },
    {
      "content": "You should see output similar to the following:",
      "pos": [
        13148,
        13195
      ]
    },
    {
      "pos": [
        13327,
        13384
      ],
      "content": "Enter <ph id=\"ph1\">`exit`</ph> at the <ph id=\"ph2\">`1&gt;`</ph> prompt to exit the tsql utility."
    },
    {
      "content": "Create the job definition",
      "pos": [
        13388,
        13413
      ]
    },
    {
      "content": "The job definition describes where to find the workflow.xml, as well as other files used by the workflow (such as useooziewf.hql.) It also defines the values for properties used within the workflow and associated files.",
      "pos": [
        13415,
        13634
      ]
    },
    {
      "content": "Use the following command to get the full WASB address to default storage.",
      "pos": [
        13639,
        13713
      ]
    },
    {
      "content": "This will be used in the configuration file in a moment:",
      "pos": [
        13714,
        13770
      ]
    },
    {
      "content": "This should return information similar to the following:",
      "pos": [
        13857,
        13913
      ]
    },
    {
      "pos": [
        14035,
        14150
      ],
      "content": "Save the <bpt id=\"p1\">**</bpt>wasb://mycontainer@mystorageaccount.blob.core.windows.net<ept id=\"p1\">**</ept> value, as it will be used in the next steps."
    },
    {
      "content": "Use the following command to get FQDN of the cluster headnode.",
      "pos": [
        14155,
        14217
      ]
    },
    {
      "content": "This will be used for the JobTracker address for the cluster.",
      "pos": [
        14218,
        14279
      ]
    },
    {
      "content": "This will be used in the configuration file in a moment:",
      "pos": [
        14280,
        14336
      ]
    },
    {
      "content": "This will return information similar to the following:",
      "pos": [
        14363,
        14417
      ]
    },
    {
      "pos": [
        14483,
        14639
      ],
      "content": "The port used for the JobTracker is 8050, so the full address to use for the JobTracker will be <bpt id=\"p1\">**</bpt>headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:8050<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Use the following to create the Oozie job definition configuration:",
      "pos": [
        14644,
        14711
      ]
    },
    {
      "content": "Once the nano editor opens, use the following as the contents of the file:",
      "pos": [
        14738,
        14812
      ]
    },
    {
      "pos": [
        16634,
        16757
      ],
      "content": "Replace all instances of <bpt id=\"p1\">**</bpt>wasb://mycontainer@mystorageaccount.blob.core.windows.net<ept id=\"p1\">**</ept> with the value you received earlier."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.WARNING]</ph> You must use the full WASB path, with the container and storage account as part of the path.",
      "pos": [
        16765,
        16873
      ]
    },
    {
      "content": "Using the short format (wasb:///) will cause the RunHiveScript action to fail when the job is started.",
      "pos": [
        16874,
        16976
      ]
    },
    {
      "pos": [
        16984,
        17079
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>JOBTRACKERADDRESS<ept id=\"p1\">**</ept> with the JobTracker/ResourceManager address you received earlier."
    },
    {
      "pos": [
        17087,
        17155
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>YourName<ept id=\"p1\">**</ept> with your login name for the HDInsight cluster."
    },
    {
      "pos": [
        17163,
        17274
      ],
      "content": "Replace <bpt id=\"p1\">**</bpt>serverName<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>adminLogin<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>adminPassword<ept id=\"p3\">**</ept> with the information for your Azure SQL Database."
    },
    {
      "content": "Most of the information in this file is used to populate the values used in the workflow.xml or ooziewf.hql files (such as ${nameNode}.)",
      "pos": [
        17280,
        17416
      ]
    },
    {
      "pos": [
        17424,
        17566
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The <bpt id=\"p1\">**</bpt>oozie.wf.application.path<ept id=\"p1\">**</ept> entry defines where to find the workflow.xml file, which contains the workflow ran by this job."
    },
    {
      "pos": [
        17571,
        17625
      ],
      "content": "Use Ctrl-X, then <bpt id=\"p1\">**</bpt>Y<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept> to save the file."
    },
    {
      "content": "Submit and manage the job",
      "pos": [
        17629,
        17654
      ]
    },
    {
      "content": "The following steps use the Oozie command to submit and manage Oozie workflows on the cluster.",
      "pos": [
        17656,
        17750
      ]
    },
    {
      "content": "The Oozie command is a friendly interface over the <bpt id=\"p1\">[</bpt>Oozie REST API<ept id=\"p1\">](https://oozie.apache.org/docs/4.1.0/WebServicesAPI.html)</ept>.",
      "pos": [
        17751,
        17876
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph> When using the Oozie command, you must use the FQDN for the HDInsight headnode.",
      "pos": [
        17880,
        17977
      ]
    },
    {
      "content": "This FQDN is only accessible from the cluster, or if the cluster is on an Azure Virtual Network, from other machines on the same network.",
      "pos": [
        17978,
        18115
      ]
    },
    {
      "content": "Use the following to obtain the URL to the Oozie service:",
      "pos": [
        18120,
        18177
      ]
    },
    {
      "content": "This will return a value similar to the following:",
      "pos": [
        18268,
        18318
      ]
    },
    {
      "pos": [
        18454,
        18581
      ],
      "content": "The <bpt id=\"p1\">**</bpt>http://headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:11000/oozie<ept id=\"p1\">**</ept> portion is the URL to use with the Oozie command."
    },
    {
      "content": "Use the following to create an environment variable for the URL, so you don't have to type it for every command:",
      "pos": [
        18586,
        18698
      ]
    },
    {
      "content": "Replace the URL with the one you received earlier.",
      "pos": [
        18800,
        18850
      ]
    },
    {
      "content": "Use the following to submit the job:",
      "pos": [
        18855,
        18891
      ]
    },
    {
      "pos": [
        18940,
        19033
      ],
      "content": "This loads the job information from <bpt id=\"p1\">**</bpt>job.xml<ept id=\"p1\">**</ept> and submits it to Oozie, but does not run it."
    },
    {
      "content": "Once the command completes, it should return the ID of the job.",
      "pos": [
        19039,
        19102
      ]
    },
    {
      "content": "For example, <ph id=\"ph1\">`0000005-150622124850154-oozie-oozi-W`</ph>.",
      "pos": [
        19103,
        19155
      ]
    },
    {
      "content": "This will be used to manage the job.",
      "pos": [
        19156,
        19192
      ]
    },
    {
      "content": "View the status of the job using the following command.",
      "pos": [
        19197,
        19252
      ]
    },
    {
      "content": "Enter the job ID returned by the previous command:",
      "pos": [
        19253,
        19303
      ]
    },
    {
      "content": "This will return information similar to the following.",
      "pos": [
        19342,
        19396
      ]
    },
    {
      "pos": [
        20107,
        20208
      ],
      "content": "This job has a status of <ph id=\"ph1\">`PREP`</ph>, which indicates that it was submitted, but has not been started yet."
    },
    {
      "content": "Use the following to start the job:",
      "pos": [
        20213,
        20248
      ]
    },
    {
      "content": "If you check the status after this command, it will be in a running state, and information will be returned for the actions within the job.",
      "pos": [
        20286,
        20425
      ]
    },
    {
      "content": "Once the task completes successfully, you can verify that the data was generated and exported to the SQL Database table by using the following commands:",
      "pos": [
        20430,
        20582
      ]
    },
    {
      "pos": [
        20706,
        20746
      ],
      "content": "At the <ph id=\"ph1\">`1&gt;`</ph> prompt, enter the following:"
    },
    {
      "content": "You should receive information similar to the following:",
      "pos": [
        20797,
        20853
      ]
    },
    {
      "pos": [
        21068,
        21202
      ],
      "content": "For more information on the Oozie command, see <bpt id=\"p1\">[</bpt>Oozie Command Line Tool<ept id=\"p1\">](https://oozie.apache.org/docs/4.1.0/DG_CommandLineTool.html)</ept>."
    },
    {
      "content": "Oozie REST API",
      "pos": [
        21206,
        21220
      ]
    },
    {
      "content": "The Oozie REST API allow you to build your own tools that work with Oozie.",
      "pos": [
        21222,
        21296
      ]
    },
    {
      "content": "The following are HDInsight specific information about using the Oozie REST API:",
      "pos": [
        21297,
        21377
      ]
    },
    {
      "pos": [
        21381,
        21493
      ],
      "content": "<bpt id=\"p1\">**</bpt>URI<ept id=\"p1\">**</ept>: The REST API can be accessed from outside the cluster at <ph id=\"ph1\">`https://CLUSTERNAME.azurehdinsight.net/oozie`</ph>"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Authentication<ept id=\"p1\">**</ept>: You must authenticate to the API using the cluster HTTP account (admin,) and password.",
      "pos": [
        21497,
        21603
      ]
    },
    {
      "content": "For example:",
      "pos": [
        21604,
        21616
      ]
    },
    {
      "pos": [
        21704,
        21840
      ],
      "content": "For more information on using the Oozie REST API, see <bpt id=\"p1\">[</bpt>Oozie Web Services API<ept id=\"p1\">](https://oozie.apache.org/docs/4.1.0/WebServicesAPI.html)</ept>."
    },
    {
      "content": "Oozie Web UI",
      "pos": [
        21844,
        21856
      ]
    },
    {
      "content": "The Oozie Web UI provides a web-based view into the status of Oozie jobs on the cluster.",
      "pos": [
        21858,
        21946
      ]
    },
    {
      "content": "It allows you to view job status, the job definition, configuration, a graph of the actions in the job, and logs for the job.",
      "pos": [
        21947,
        22072
      ]
    },
    {
      "content": "You can also view details for actions within a job.",
      "pos": [
        22073,
        22124
      ]
    },
    {
      "content": "To access the Oozie Web UI, use the following steps:",
      "pos": [
        22126,
        22178
      ]
    },
    {
      "content": "Create an SSH tunnel to the HDInsight cluster.",
      "pos": [
        22183,
        22229
      ]
    },
    {
      "content": "For information on how to do this, see one of the following:",
      "pos": [
        22230,
        22290
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X",
      "pos": [
        22299,
        22369
      ]
    },
    {
      "content": "Use SSH with Linux-based Hadoop on HDInsight from Windows",
      "pos": [
        22426,
        22483
      ]
    },
    {
      "content": "Once a tunnel has been created, open the Ambari web UI in your web browser.",
      "pos": [
        22539,
        22614
      ]
    },
    {
      "content": "The URI for the Ambari site is <bpt id=\"p1\">**</bpt>https://CLUSTERNAME.azurehdinsight.net<ept id=\"p1\">**</ept>.",
      "pos": [
        22615,
        22689
      ]
    },
    {
      "content": "Replace <bpt id=\"p1\">**</bpt>CLUSTERNAME<ept id=\"p1\">**</ept> with the name of your Linux-based HDInsight cluster.",
      "pos": [
        22690,
        22766
      ]
    },
    {
      "pos": [
        22771,
        22872
      ],
      "content": "From the left side of the page, select <bpt id=\"p1\">**</bpt>Oozie<ept id=\"p1\">**</ept>, then <bpt id=\"p2\">**</bpt>Quick Links<ept id=\"p2\">**</ept>, and finally <bpt id=\"p3\">**</bpt>Oozie Web UI<ept id=\"p3\">**</ept>."
    },
    {
      "content": "image of the menus",
      "pos": [
        22880,
        22898
      ]
    },
    {
      "content": "The Oozie Web UI defaults to displaying running Workflow Jobs.",
      "pos": [
        22963,
        23025
      ]
    },
    {
      "content": "To see all workflow jobs, select <bpt id=\"p1\">**</bpt>All Jobs<ept id=\"p1\">**</ept>.",
      "pos": [
        23026,
        23072
      ]
    },
    {
      "content": "All jobs displayed",
      "pos": [
        23080,
        23098
      ]
    },
    {
      "content": "Select a job to view more information about the job.",
      "pos": [
        23157,
        23209
      ]
    },
    {
      "content": "Job info",
      "pos": [
        23217,
        23225
      ]
    },
    {
      "content": "From the Job Info tab, you can see basic job information, as well as the individual actions within the job.",
      "pos": [
        23282,
        23389
      ]
    },
    {
      "content": "Using the tabs at the top you can view the Job Definition, Job Configuration, access the Job Log or view a Directed Acyclic Graph (DAG) of the job.",
      "pos": [
        23390,
        23537
      ]
    },
    {
      "pos": [
        23545,
        23672
      ],
      "content": "<bpt id=\"p1\">**</bpt>Job Log<ept id=\"p1\">**</ept>: Select the <bpt id=\"p2\">**</bpt>GetLogs<ept id=\"p2\">**</ept> button to get all logs for the job, or use the <bpt id=\"p3\">**</bpt>Enter Search Filter<ept id=\"p3\">**</ept> field to filter logs"
    },
    {
      "content": "Job log",
      "pos": [
        23684,
        23691
      ]
    },
    {
      "pos": [
        23750,
        23838
      ],
      "content": "<bpt id=\"p1\">**</bpt>JobDAG<ept id=\"p1\">**</ept>: The DAG is a graphical overview of the data paths taken through the workflow"
    },
    {
      "content": "Job DAG",
      "pos": [
        23850,
        23857
      ]
    },
    {
      "content": "Selecting one of the actions from the <bpt id=\"p1\">**</bpt>Job Info<ept id=\"p1\">**</ept> tab will bring up information for the action.",
      "pos": [
        23913,
        24009
      ]
    },
    {
      "content": "For example, select the <bpt id=\"p1\">**</bpt>RunHiveScript<ept id=\"p1\">**</ept> action.",
      "pos": [
        24010,
        24059
      ]
    },
    {
      "content": "Action info",
      "pos": [
        24067,
        24078
      ]
    },
    {
      "pos": [
        24134,
        24272
      ],
      "content": "You can see details for the action, including a link to the <bpt id=\"p1\">**</bpt>Console URL<ept id=\"p1\">**</ept>, which can be used to view JobTracker information for the job."
    },
    {
      "content": "Scheduling jobs",
      "pos": [
        24276,
        24291
      ]
    },
    {
      "content": "The coordinator allows you to specify a start, end, and occurrance frequency for jobs so that they can be scheduled for certain times.",
      "pos": [
        24293,
        24427
      ]
    },
    {
      "content": "To define a schedule for the workflow, use the following steps:",
      "pos": [
        24429,
        24492
      ]
    },
    {
      "pos": [
        24497,
        24562
      ],
      "content": "Use the following to create a new file named <bpt id=\"p1\">**</bpt>coordinator.xml<ept id=\"p1\">**</ept>:"
    },
    {
      "content": "Use the following as the contents of the file:",
      "pos": [
        24598,
        24644
      ]
    },
    {
      "content": "Note that this uses <ph id=\"ph1\">`${...}`</ph> variables that will be replaced by values in the job definition.",
      "pos": [
        24993,
        25086
      ]
    },
    {
      "content": "The variables are:",
      "pos": [
        25087,
        25105
      ]
    },
    {
      "pos": [
        25113,
        25177
      ],
      "content": "<bpt id=\"p1\">**</bpt>${coordFrequency}<ept id=\"p1\">**</ept>: Time between running instances of the job"
    },
    {
      "pos": [
        25184,
        25221
      ],
      "content": "<bpt id=\"p1\">**</bpt>${coordStart}<ept id=\"p1\">**</ept>: The job start time"
    },
    {
      "pos": [
        25228,
        25261
      ],
      "content": "<bpt id=\"p1\">**</bpt>${coordEnd}<ept id=\"p1\">**</ept>: The job end time"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>${coordTimezone}<ept id=\"p1\">**</ept>: Coordinator jobs are in a fixed time zone with no daylight savings time (typically represented by using UTC).",
      "pos": [
        25268,
        25399
      ]
    },
    {
      "content": "This time zone is referred as the \"Oozie processing timezone\"",
      "pos": [
        25400,
        25461
      ]
    },
    {
      "pos": [
        25468,
        25511
      ],
      "content": "<bpt id=\"p1\">**</bpt>${wfPath}<ept id=\"p1\">**</ept>: The path to the workflow.xml"
    },
    {
      "pos": [
        25516,
        25570
      ],
      "content": "Use Ctrl-X, then <bpt id=\"p1\">**</bpt>Y<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept> to save the file."
    },
    {
      "content": "Use the following to copy it to the working directory for this job:",
      "pos": [
        25575,
        25642
      ]
    },
    {
      "pos": [
        25733,
        25782
      ],
      "content": "Use the following to modify the <bpt id=\"p1\">**</bpt>job.xml<ept id=\"p1\">**</ept> file:"
    },
    {
      "content": "Make the following changes:",
      "pos": [
        25810,
        25837
      ]
    },
    {
      "content": "Change <ph id=\"ph1\">`&lt;name&gt;oozie.wf.application.path&lt;/name&gt;`</ph> to <ph id=\"ph2\">`&lt;name&gt;oozie.coord.application.path&lt;/name&gt;`</ph>.",
      "pos": [
        25845,
        25940
      ]
    },
    {
      "content": "This instructs Oozie to run the coordinator file instead of the workflow file",
      "pos": [
        25941,
        26018
      ]
    },
    {
      "content": "Add the following, which will be sets a variable used in the coordinator.xml to point to the location of the workflow.xml:",
      "pos": [
        26026,
        26148
      ]
    },
    {
      "pos": [
        26352,
        26474
      ],
      "content": "Replace the values for <bpt id=\"p1\">**</bpt>mycontainer<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>mystorageaccount<ept id=\"p2\">**</ept> with the values used in other entries in the job.xml file."
    },
    {
      "content": "Add the following, which define the start, end, and frequency to use for the coordinator.xml file:",
      "pos": [
        26482,
        26580
      ]
    },
    {
      "content": "These set the start time to 12:00PM on June 25th, 2015, the end time to June 27th, 2015, and the interval for running this job to daily (the frequency is in minutes, so 24 hours x 60 minutes = 1440 minutes.) Finally, the timezone is set to UTC.",
      "pos": [
        27100,
        27344
      ]
    },
    {
      "pos": [
        27349,
        27403
      ],
      "content": "Use Ctrl-X, then <bpt id=\"p1\">**</bpt>Y<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Enter<ept id=\"p2\">**</ept> to save the file."
    },
    {
      "content": "To run the job, use the following command:",
      "pos": [
        27408,
        27450
      ]
    },
    {
      "content": "This will submit and start the job.",
      "pos": [
        27496,
        27531
      ]
    },
    {
      "pos": [
        27536,
        27655
      ],
      "content": "If you visit the Oozie Web UI and select the <bpt id=\"p1\">**</bpt>Coordinator Jobs<ept id=\"p1\">**</ept> tab, you should information similar to the following:"
    },
    {
      "content": "coordinator jobs tab",
      "pos": [
        27663,
        27683
      ]
    },
    {
      "pos": [
        27748,
        27824
      ],
      "content": "Note the <bpt id=\"p1\">**</bpt>Next Materialization<ept id=\"p1\">**</ept> entry; this is when the job will run next."
    },
    {
      "content": "Similar to the earlier workflow job, selecting the job entry in the web UI will display information on the job:",
      "pos": [
        27829,
        27940
      ]
    },
    {
      "content": "Coordinator job info",
      "pos": [
        27948,
        27968
      ]
    },
    {
      "content": "Note that this only shows successful runs of the job, not individual actions within the scheduled workflow.",
      "pos": [
        28037,
        28144
      ]
    },
    {
      "content": "To see that, select one of the <bpt id=\"p1\">**</bpt>Action<ept id=\"p1\">**</ept> entries.",
      "pos": [
        28145,
        28195
      ]
    },
    {
      "content": "This will display information similar to that retrieved for the earlier workflow job.",
      "pos": [
        28196,
        28281
      ]
    },
    {
      "content": "Action info",
      "pos": [
        28289,
        28300
      ]
    },
    {
      "content": "Troubleshooting",
      "pos": [
        28369,
        28384
      ]
    },
    {
      "content": "When troubleshooting problems with Oozie jobs, the Oozie UI is very helpful as it allows you to easily view both Oozie logs, as well as links to JobTracker logs for MapReduce tasks such as Hive queries.",
      "pos": [
        28386,
        28588
      ]
    },
    {
      "content": "In general, the pattern for troubleshooting should be:",
      "pos": [
        28589,
        28643
      ]
    },
    {
      "content": "View the job in Oozie Web UI.",
      "pos": [
        28648,
        28677
      ]
    },
    {
      "pos": [
        28682,
        28834
      ],
      "content": "If there is an error or failure for a specific action, select the action to see if the <bpt id=\"p1\">**</bpt>Error Message<ept id=\"p1\">**</ept> field provides more information on the failure."
    },
    {
      "content": "If available, use the URL from the action to view more details (such as JobTracker logs,) for the action.",
      "pos": [
        28839,
        28944
      ]
    },
    {
      "content": "The following are specific errors you may encounter, and how to resolve them.",
      "pos": [
        28946,
        29023
      ]
    },
    {
      "content": "JA009: Cannot initialize cluster",
      "pos": [
        29028,
        29060
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Symptoms<ept id=\"p1\">**</ept>: The job status will change to <bpt id=\"p2\">**</bpt>SUSPENDED<ept id=\"p2\">**</ept>.",
      "pos": [
        29062,
        29120
      ]
    },
    {
      "content": "Details for the job will show the RunHiveScript status as <bpt id=\"p1\">**</bpt>START_MANUAL<ept id=\"p1\">**</ept>.",
      "pos": [
        29121,
        29196
      ]
    },
    {
      "content": "Selecting the action will reveal the following error message:",
      "pos": [
        29197,
        29258
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Cause<ept id=\"p1\">**</ept>: The WASB addresses used in the <bpt id=\"p2\">**</bpt>job.xml<ept id=\"p2\">**</ept> file do not contain the storage container or storage account name.",
      "pos": [
        29339,
        29459
      ]
    },
    {
      "content": "The WASB address format must be <ph id=\"ph1\">`wasb://containername@storageaccountname.blob.core.windows.net`</ph>.",
      "pos": [
        29460,
        29556
      ]
    },
    {
      "pos": [
        29558,
        29616
      ],
      "content": "<bpt id=\"p1\">**</bpt>Resolution<ept id=\"p1\">**</ept>: Change the WASB addresses used by the job."
    },
    {
      "content": "JA002: Oozie is not allowed to impersonate &amp;lt;USER&gt;",
      "pos": [
        29621,
        29673
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Symptoms<ept id=\"p1\">**</ept>: The job status will change to <bpt id=\"p2\">**</bpt>SUSPENDED<ept id=\"p2\">**</ept>.",
      "pos": [
        29675,
        29733
      ]
    },
    {
      "content": "Details for the job will show the RunHiveScript status as <bpt id=\"p1\">**</bpt>START_MANUAL<ept id=\"p1\">**</ept>.",
      "pos": [
        29734,
        29809
      ]
    },
    {
      "content": "Selecting the action will reveal the following error message:",
      "pos": [
        29810,
        29871
      ]
    },
    {
      "pos": [
        29934,
        30034
      ],
      "content": "<bpt id=\"p1\">**</bpt>Cause<ept id=\"p1\">**</ept>: Current permission settings do not allow Oozie to impersonate the specified user account."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Resolution<ept id=\"p1\">**</ept>: Oozie is allowed to impersonate users in the <bpt id=\"p2\">**</bpt>users<ept id=\"p2\">**</ept> group.",
      "pos": [
        30036,
        30113
      ]
    },
    {
      "content": "Use the <ph id=\"ph1\">`groups USERNAME`</ph> to see the groups that the user account is a member of.",
      "pos": [
        30114,
        30195
      ]
    },
    {
      "content": "If the user is not a member of the <bpt id=\"p1\">**</bpt>users<ept id=\"p1\">**</ept> group, use the following command to add the user to the group:",
      "pos": [
        30196,
        30303
      ]
    },
    {
      "pos": [
        30340,
        30451
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> It may take several minutes before HDInsight recognizes that the user has been added to the group."
    },
    {
      "content": "Launcher ERROR (Sqoop)",
      "pos": [
        30456,
        30478
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Symptoms<ept id=\"p1\">**</ept>: The job status will changed to <bpt id=\"p2\">**</bpt>KILLED<ept id=\"p2\">**</ept>.",
      "pos": [
        30480,
        30536
      ]
    },
    {
      "content": "Details for the job will show the RunSqoopExport status as <bpt id=\"p1\">**</bpt>ERROR<ept id=\"p1\">**</ept>.",
      "pos": [
        30537,
        30606
      ]
    },
    {
      "content": "Selecting the action will reveal the following error message:",
      "pos": [
        30607,
        30668
      ]
    },
    {
      "pos": [
        30768,
        30855
      ],
      "content": "<bpt id=\"p1\">**</bpt>Cause<ept id=\"p1\">**</ept>: Sqoop is unable to load the database driver required to access the database."
    },
    {
      "pos": [
        30857,
        31015
      ],
      "content": "<bpt id=\"p1\">**</bpt>Resolution<ept id=\"p1\">**</ept>: When using Sqoop from an Oozie job, you must include the database driver with the other resources (such as the workflow.xml,) used by the job."
    },
    {
      "pos": [
        31017,
        31142
      ],
      "content": "You must also reference the archive containing the database driver from the <ph id=\"ph1\">`&lt;sqoop&gt;...&lt;/sqoop&gt;`</ph> section of the workflow.xml."
    },
    {
      "content": "For example, for the job in this document, you would use the following steps:",
      "pos": [
        31144,
        31221
      ]
    },
    {
      "content": "Copy the sqljdbc4.jar file to the /tutorials/useoozie directory:",
      "pos": [
        31226,
        31290
      ]
    },
    {
      "pos": [
        31408,
        31484
      ],
      "content": "Modify the workflow.xml to add the following on a new line above <ph id=\"ph1\">`&lt;/sqoop&gt;`</ph>:"
    },
    {
      "content": "Next steps",
      "pos": [
        31529,
        31539
      ]
    },
    {
      "content": "In this tutorial, you learned how to define an Oozie workflow and how to run an Oozie job.",
      "pos": [
        31540,
        31630
      ]
    },
    {
      "content": "To learn more about working with HDInsight, see the following articles:",
      "pos": [
        31631,
        31702
      ]
    },
    {
      "content": "Use time-based Oozie Coordinator with HDInsight",
      "pos": [
        31707,
        31754
      ]
    },
    {
      "content": "Upload data for Hadoop jobs in HDInsight",
      "pos": [
        31793,
        31833
      ]
    },
    {
      "content": "Use Sqoop with Hadoop in HDInsight",
      "pos": [
        31861,
        31895
      ]
    },
    {
      "content": "Use Hive with Hadoop on HDInsight",
      "pos": [
        31921,
        31954
      ]
    },
    {
      "content": "Use Pig with Hadoop on HDInsight",
      "pos": [
        31979,
        32011
      ]
    },
    {
      "content": "Develop Java MapReduce programs for HDInsight",
      "pos": [
        32035,
        32080
      ]
    },
    {
      "content": "test",
      "pos": [
        34303,
        34307
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Use Hadoop Oozie in HDInsight | Microsoft Azure\"\n    description=\"Use Hadoop Oozie in HDInsight, a big data service. Learn how to define an Oozie workflow, and submit an Oozie job.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"07/24/2015\"\n    ms.author=\"larryfr\"/>\n\n\n# Use Oozie with Hadoop to define and run a workflow on Linux-based HDInsight (Preview)\n\n[AZURE.INCLUDE [oozie-selector](../../includes/hdinsight-oozie-selector.md)]\n\nLearn how to use Apache Oozie to define a workflow that uses Hive and Sqoop, and then run the workflow on a Linux-based HDInsight cluster.\n\nApache Oozie is a workflow/coordination system that manages Hadoop jobs. It is integrated with the Hadoop stack, and it supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop. It can also be used to schedule jobs that are specific to a system, like Java programs or shell scripts\n\n> [AZURE.NOTE] Another option for defining workflows with HDInsight is Azure Data Factory. To learn more about Azure Data Factory, see [Use Pig and Hive with Data Factory][azure-data-factory-pig-hive].\n\n##Prerequisites\n\nBefore you begin this tutorial, you must have the following:\n\n- **An Azure subscription**: See [Get Azure free trial](get-azure-free-trial-for-testing-hadoop-in-hdinsight.md).\n\n- **Azure CLI**: See [Install and Configure the Azure CLI](xplat-cli.md)\n\n- **An HDInsight cluster**: See [Get Started with HDInsight on Linux](hdinsight-hadoop-linux-tutorial-get-started.md)\n\n- **An Azure SQL database**: This will be created using the steps in this document\n\n##Example workflow\n\nThe workflow you will implement by following the instructions in this document contains two actions. Actions are definitions for tasks, such as running Hive, Sqoop, MapReduce, or other process:\n\n![Workflow diagram][img-workflow-diagram]\n\n1. A Hive action runs a HiveQL script to extract records from the **hivesampletable** included with HDInsight. Each row of data describes a visit from a specific mobile device. The record format appears similar to the following:\n\n        8       18:54:20        en-US   Android Samsung SCH-i500        California     United States    13.9204007      0       0\n        23      19:19:44        en-US   Android HTC     Incredible      Pennsylvania   United States    NULL    0       0\n        23      19:19:46        en-US   Android HTC     Incredible      Pennsylvania   United States    1.4757422       0       1\n\n    The Hive script used in this document counts the total visits for each platform (such as Android or iPhone,) and stores the counts to a new Hive table.\n\n    For more information about Hive, see [Use Hive with HDInsight][hdinsight-use-hive].\n\n2.  A Sqoop action exports the contents of the new Hive table to a table in an Azure SQL database. For more information about Sqoop, see [Use Hadoop Sqoop with HDInsight][hdinsight-use-sqoop].\n\n> [AZURE.NOTE] For supported Oozie versions on HDInsight clusters, see [What's new in the Hadoop cluster versions provided by HDInsight?][hdinsight-versions].\n\n##Create the working directory\n\nOozie expects resources required for a job to be stored in the same directory. This example uses **wasb:///tutorials/useoozie**. Use the following command to create this directory, and the data directory that will hold the new Hive table created by this workflow:\n\n    hadoop fs -mkdir -p /tutorials/useoozie/data\n\n> [AZURE.NOTE] The `-p` parameter caused all directories in the path to be created if they do not already exist. The **data** directory will be used to hold data used by the **useooziewf.hql** script.\n\nAlso run the following command, which ensures that Oozie can impersonate your user account when running Hive and Sqoop jobs. Replace **USERNAME** with your login name:\n\n    sudo adduser USERNAME users\n\nIf you receive an error that the user is already a member of users, you can just ignore it.\n\n##Add a database driver\n\nSince this workflow uses Sqoop to export data to SQL Database, you must provide a copy of the JDBC driver used to talk to SQL Database. Use the following command to copy it to the working directory:\n\n    hadoop fs -copyFromLocal /usr/share/java/sqljdbc_4.1/enu/sqljdbc4.jar /tutorials/useoozie/sqljdbc4.jar\n\nIf your workflow used other resources, such as a jar containing a MapReduce application, you would need to add these as well.\n\n##Define the Hive query\n\nUse the following steps to create a HiveQL script that defines a query, which will be used in an Oozie workflow later in this document.\n\n1. Use SSH to connect to the Linux-based HDInsight cluster:\n\n    * **Linux, Unix or OS X clients**: See [Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X or Unix](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n    * **Windows clients**: See [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n2. Use the following command to create a new file:\n\n        nano useooziewf.hql\n\n1. Once the nano editor opens, use the following as the contents of the file:\n\n        DROP TABLE ${hiveTableName};\n        CREATE EXTERNAL TABLE ${hiveTableName}(deviceplatform string, count string) ROW FORMAT DELIMITED\n        FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE LOCATION '${hiveDataFolder}';\n        INSERT OVERWRITE TABLE ${hiveTableName} SELECT deviceplatform, COUNT(*) as count FROM hivesampletable GROUP BY deviceplatform;\n\n    There are two variables used in the script:\n\n    - **${hiveTableName}**: will contain the name of the table to be created\n    - **${hiveDataFolder}**: will contain the location to store the data files for the table\n\n    The workflow definition file (workflow.xml in this tutorial) passes these values to this HiveQL script at run time.\n\n2. Press Ctrl-X to exit the editor. When prompted, select **Y** to save the file, then use **Enter** to use the **useooziewf.hql** file name.\n\n3. Use the following commands to copy **useooziewf.hql** to **wasb:///tutorials/useoozie/useooziewf.hql**:\n\n        hadoop fs -copyFromLocal useooziewf.hql /tutorials/useoozie/useooziewf.hql\n\n    These commands store the **useooziewf.hql** file on the Azure Storage account associated with this cluster, which will preserve the file even if the cluster is deleted. This allows you to save money by deleting clusters when they aren't in use, while maintaining your jobs and workflows.\n\n##Define the workflow\n\nOozie workflows definitions are written in hPDL (a XML Process Definition Language). Use the following steps to define the workflow:\n\n1. Use the following statement to create and edit a new file:\n\n        nano workflow.xml\n\n1. Once the nano editor opens, enter the following as the file contents:\n\n        <workflow-app name=\"useooziewf\" xmlns=\"uri:oozie:workflow:0.2\">\n          <start to = \"RunHiveScript\"/>\n          <action name=\"RunHiveScript\">\n            <hive xmlns=\"uri:oozie:hive-action:0.2\">\n              <job-tracker>${jobTracker}</job-tracker>\n              <name-node>${nameNode}</name-node>\n              <configuration>\n                <property>\n                  <name>mapred.job.queue.name</name>\n                  <value>${queueName}</value>\n                </property>\n              </configuration>\n              <script>${hiveScript}</script>\n              <param>hiveTableName=${hiveTableName}</param>\n              <param>hiveDataFolder=${hiveDataFolder}</param>\n            </hive>\n            <ok to=\"RunSqoopExport\"/>\n            <error to=\"fail\"/>\n          </action>\n          <action name=\"RunSqoopExport\">\n            <sqoop xmlns=\"uri:oozie:sqoop-action:0.2\">\n              <job-tracker>${jobTracker}</job-tracker>\n              <name-node>${nameNode}</name-node>\n              <configuration>\n                <property>\n                  <name>mapred.compress.map.output</name>\n                  <value>true</value>\n                </property>\n              </configuration>\n              <arg>export</arg>\n              <arg>--connect</arg>\n              <arg>${sqlDatabaseConnectionString}</arg>\n              <arg>--table</arg>\n              <arg>${sqlDatabaseTableName}</arg>\n              <arg>--export-dir</arg>\n              <arg>${hiveDataFolder}</arg>\n              <arg>-m</arg>\n              <arg>1</arg>\n              <arg>--input-fields-terminated-by</arg>\n              <arg>\"\\t\"</arg>\n              <archive>sqljdbc4.jar</archive>\n              </sqoop>\n            <ok to=\"end\"/>\n            <error to=\"fail\"/>\n          </action>\n          <kill name=\"fail\">\n            <message>Job failed, error message[${wf:errorMessage(wf:lastErrorNode())}] </message>\n          </kill>\n          <end name=\"end\"/>\n        </workflow-app>\n\n    There are two actions defined in the workflow:\n\n    - **RunHiveScript**: This is the start action, and runs the **useooziewf.hql** Hive script\n\n    - **RunSqoopExport**: This exports the data created from the Hive script to SQL Database using Sqoop. This will only run if the **RunHiveScript** action is successful.\n\n        > [AZURE.NOTE] For more information about Oozie workflow and using workflow actions, see [Apache Oozie 4.0 documentation][apache-oozie-400] (for HDInsight version 3.0) or [Apache Oozie 3.3.2 documentation][apache-oozie-332] (for HDInsight version 2.1).\n\n    Note that the workflow has several entries, such as `${jobTracker}`, that will be replaced by values you use in the job definition later in this document.\n\n    Also note the `<archive>sqljdbc4.jar</arcive>` entry in the Sqoop section. This instructs Oozie to make this archive available for Sqoop when this action runs.\n\n2. Use Ctrl-X, then **Y** and **Enter** to save the file.\n\n3. Use the following command to copy the **workflow.xml** file to **wasb:///tutorials/useoozie/workflow.xml**:\n\n        hadoop fs -copyFromLocal workflow.xml wasb:///tutorials/useoozie/workflow.xml\n\n##Create the database\n\nThe following steps create the Azure SQL Database that data will be exported to.\n\n> [AZURE.IMPORTANT] Before performing these steps you must [install and configure the Azure CLI](xplat-cli.md). Installing the CLI and following the steps to create a database can be performed either from the HDInsight cluster or your local workstation.\n\n1. Use the following command to create a new Azure SQL Database server:\n\n        azure sql server create <adminLogin> <adminPassword> <region>\n\n    For exmaple, `azure sql server create admin password \"West US\"`.\n\n    When the command completes, you will receive a response similar to the following:\n\n        info:    Executing command sql server create\n        + Creating SQL Server\n        data:    Server Name i1qwc540ts\n        info:    sql server create command OK\n\n    > [AZURE.IMPORTANT] Note the server name returned by this command (**i1qwc540ts** in the example above.) This is the short name of the SQL Database server that was created. The fully qualified domain name (FQDN) is **&lt;shortname&gt;.database.windows.net**. For the example above, the FQDN would be **i1qwc540ts.database.windows.net**.\n\n2. Use the following command to create a database named **oozietest** on the SQL Database server:\n\n        azure sql db create [options] <serverName> oozietest <adminLogin> <adminPassword>\n\n    This will return an \"OK\" message when it completes.\n\n    > [AZURE.NOTE] If you receive an error indicating that you do not have access, you may need to add the system's IP address to the SQL Database firewall using the following command:\n    >\n    > `sql firewallrule create [options] <serverName> <ruleName> <startIPAddress> <endIPAddress>`\n\n###Create the table\n\n> [AZURE.NOTE] There are many ways to connect to SQL Database to create a table. The following steps use [FreeTDS](http://www.freetds.org/) from the HDInsight cluster.\n\n3. Use the following command to install FreeTDS on the HDInsight cluster:\n\n        sudo apt-get --assume-yes install freetds-dev freetds-bin\n\n4. Once FreeTDS has been installed, use the following command to connect to the SQL Database server you created previously:\n\n        TDSVER=8.0 tsql -H <serverName>.database.windows.net -U <adminLogin> -P <adminPassword> -p 1433 -D oozietest\n\n    You will receive output similar to the following:\n\n        locale is \"en_US.UTF-8\"\n        locale charset is \"UTF-8\"\n        using default charset \"UTF-8\"\n        Default database being set to oozietest\n        1>\n\n5. At the `1>` prompt, enter the following lines:\n\n        CREATE TABLE [dbo].[mobiledata](\n        [deviceplatform] [nvarchar](50),\n        [count] [bigint])\n        GO\n        CREATE CLUSTERED INDEX mobiledata_clustered_index on mobiledata(deviceplatform)\n        GO\n\n    When the `GO` statement is entered, the previous statements will be evaluated. This will create a new table named **mobiledata** that will be written to by Sqoop.\n\n    Use the following to verify that the table has been created:\n\n        SELECT * FROM information_schema.tables\n        GO\n\n    You should see output similar to the following:\n\n        TABLE_CATALOG   TABLE_SCHEMA    TABLE_NAME      TABLE_TYPE\n        oozietest       dbo     mobiledata      BASE TABLE\n\n8. Enter `exit` at the `1>` prompt to exit the tsql utility.\n\n##Create the job definition\n\nThe job definition describes where to find the workflow.xml, as well as other files used by the workflow (such as useooziewf.hql.) It also defines the values for properties used within the workflow and associated files.\n\n1. Use the following command to get the full WASB address to default storage. This will be used in the configuration file in a moment:\n\n        sed -n '/<name>fs.default/,/<\\/value>/p' /etc/hadoop/conf/core-site.xml\n\n    This should return information similar to the following:\n\n        <name>fs.defaultFS</name>\n        <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net</value>\n\n    Save the **wasb://mycontainer@mystorageaccount.blob.core.windows.net** value, as it will be used in the next steps.\n\n2. Use the following command to get FQDN of the cluster headnode. This will be used for the JobTracker address for the cluster. This will be used in the configuration file in a moment:\n\n        hostname -f\n\n    This will return information similar to the following:\n\n        headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net\n\n    The port used for the JobTracker is 8050, so the full address to use for the JobTracker will be **headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:8050**.\n\n1. Use the following to create the Oozie job definition configuration:\n\n        nano job.xml\n\n2. Once the nano editor opens, use the following as the contents of the file:\n\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <configuration>\n\n          <property>\n            <name>nameNode</name>\n            <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net</value>\n          </property>\n\n          <property>\n            <name>jobTracker</name>\n            <value>JOBTRACKERADDRESS</value>\n          </property>\n\n          <property>\n            <name>queueName</name>\n            <value>default</value>\n          </property>\n\n          <property>\n            <name>oozie.use.system.libpath</name>\n            <value>true</value>\n          </property>\n\n          <property>\n            <name>hiveScript</name>\n            <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net/tutorials/useoozie/useooziewf.hql</value>\n          </property>\n\n          <property>\n            <name>hiveTableName</name>\n            <value>mobilecount</value>\n          </property>\n\n          <property>\n            <name>hiveDataFolder</name>\n            <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net/tutorials/useoozie/data</value>\n          </property>\n\n          <property>\n            <name>sqlDatabaseConnectionString</name>\n            <value>\"jdbc:sqlserver://serverName.database.windows.net;user=adminLogin;password=adminPassword;database=oozietest\"</value>\n          </property>\n\n          <property>\n            <name>sqlDatabaseTableName</name>\n            <value>mobiledata</value>\n          </property>\n\n          <property>\n            <name>user.name</name>\n            <value>YourName</value>\n          </property>\n\n          <property>\n            <name>oozie.wf.application.path</name>\n            <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net/tutorials/useoozie</value>\n          </property>\n        </configuration>\n\n    * Replace all instances of **wasb://mycontainer@mystorageaccount.blob.core.windows.net** with the value you received earlier.\n\n    > [AZURE.WARNING] You must use the full WASB path, with the container and storage account as part of the path. Using the short format (wasb:///) will cause the RunHiveScript action to fail when the job is started.\n\n    * Replace **JOBTRACKERADDRESS** with the JobTracker/ResourceManager address you received earlier.\n\n    * Replace **YourName** with your login name for the HDInsight cluster.\n\n    * Replace **serverName**, **adminLogin**, and **adminPassword** with the information for your Azure SQL Database.\n\n    Most of the information in this file is used to populate the values used in the workflow.xml or ooziewf.hql files (such as ${nameNode}.)\n\n    > [AZURE.NOTE] The **oozie.wf.application.path** entry defines where to find the workflow.xml file, which contains the workflow ran by this job.\n\n2. Use Ctrl-X, then **Y** and **Enter** to save the file.\n\n##Submit and manage the job\n\nThe following steps use the Oozie command to submit and manage Oozie workflows on the cluster. The Oozie command is a friendly interface over the [Oozie REST API](https://oozie.apache.org/docs/4.1.0/WebServicesAPI.html).\n\n> [AZURE.IMPORTANT] When using the Oozie command, you must use the FQDN for the HDInsight headnode. This FQDN is only accessible from the cluster, or if the cluster is on an Azure Virtual Network, from other machines on the same network.\n\n1. Use the following to obtain the URL to the Oozie service:\n\n        sed -n '/<name>oozie.base.url/,/<\\/value>/p' /etc/oozie/conf/oozie-site.xml\n\n    This will return a value similar to the following:\n\n        <name>oozie.base.url</name>\n        <value>http://headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:11000/oozie</value>\n\n    The **http://headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:11000/oozie** portion is the URL to use with the Oozie command.\n\n2. Use the following to create an environment variable for the URL, so you don't have to type it for every command:\n\n        export OOZIE_URL=http://headnode0.CLUSTERNAME-ssh.j7.internal.cloudapp.net:11000/oozie\n\n    Replace the URL with the one you received earlier.\n\n3. Use the following to submit the job:\n\n        oozie job -config job.xml -submit\n\n    This loads the job information from **job.xml** and submits it to Oozie, but does not run it.\n\n    Once the command completes, it should return the ID of the job. For example, `0000005-150622124850154-oozie-oozi-W`. This will be used to manage the job.\n\n4. View the status of the job using the following command. Enter the job ID returned by the previous command:\n\n        oozie job -info <JOBID>\n\n    This will return information similar to the following.\n\n        Job ID : 0000005-150622124850154-oozie-oozi-W\n        ------------------------------------------------------------------------------------------------------------------------------------\n        Workflow Name : useooziewf\n        App Path      : wasb:///tutorials/useoozie\n        Status        : PREP\n        Run           : 0\n        User          : USERNAME\n        Group         : -\n        Created       : 2015-06-22 15:06 GMT\n        Started       : -\n        Last Modified : 2015-06-22 15:06 GMT\n        Ended         : -\n        CoordAction ID: -\n        ------------------------------------------------------------------------------------------------------------------------------------\n\n    This job has a status of `PREP`, which indicates that it was submitted, but has not been started yet.\n\n4. Use the following to start the job:\n\n        oozie job -start JOBID\n\n    If you check the status after this command, it will be in a running state, and information will be returned for the actions within the job.\n\n5. Once the task completes successfully, you can verify that the data was generated and exported to the SQL Database table by using the following commands:\n\n        TDSVER=8.0 tsql -H <serverName>.database.windows.net -U <adminLogin> -P <adminPassword> -p 1433 -D oozietest\n\n    At the `1>` prompt, enter the following:\n\n        SELECT * FROM mobiledata\n        GO\n\n    You should receive information similar to the following:\n\n        deviceplatform  count\n        Android 31591\n        iPhone OS       22731\n        proprietary development 3\n        RIM OS  3464\n        Unknown 213\n        Windows Phone   1791\n        (6 rows affected)\n\nFor more information on the Oozie command, see [Oozie Command Line Tool](https://oozie.apache.org/docs/4.1.0/DG_CommandLineTool.html).\n\n##Oozie REST API\n\nThe Oozie REST API allow you to build your own tools that work with Oozie. The following are HDInsight specific information about using the Oozie REST API:\n\n* **URI**: The REST API can be accessed from outside the cluster at `https://CLUSTERNAME.azurehdinsight.net/oozie`\n\n* **Authentication**: You must authenticate to the API using the cluster HTTP account (admin,) and password. For example:\n\n        curl -u admin:PASSWORD https://CLUSTERNAME.azurehdinsight.net/oozie/versions\n\nFor more information on using the Oozie REST API, see [Oozie Web Services API](https://oozie.apache.org/docs/4.1.0/WebServicesAPI.html).\n\n##Oozie Web UI\n\nThe Oozie Web UI provides a web-based view into the status of Oozie jobs on the cluster. It allows you to view job status, the job definition, configuration, a graph of the actions in the job, and logs for the job. You can also view details for actions within a job.\n\nTo access the Oozie Web UI, use the following steps:\n\n1. Create an SSH tunnel to the HDInsight cluster. For information on how to do this, see one of the following:\n\n    * [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md#tunnel)\n\n    * [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md#tunnel)\n\n2. Once a tunnel has been created, open the Ambari web UI in your web browser. The URI for the Ambari site is **https://CLUSTERNAME.azurehdinsight.net**. Replace **CLUSTERNAME** with the name of your Linux-based HDInsight cluster.\n\n3. From the left side of the page, select **Oozie**, then **Quick Links**, and finally **Oozie Web UI**.\n\n    ![image of the menus](./media/hdinsight-use-oozie-linux-mac/ooziewebuisteps.png)\n\n4. The Oozie Web UI defaults to displaying running Workflow Jobs. To see all workflow jobs, select **All Jobs**.\n\n    ![All jobs displayed](./media/hdinsight-use-oozie-linux-mac/ooziejobs.png)\n\n5. Select a job to view more information about the job.\n\n    ![Job info](./media/hdinsight-use-oozie-linux-mac/jobinfo.png)\n\n6. From the Job Info tab, you can see basic job information, as well as the individual actions within the job. Using the tabs at the top you can view the Job Definition, Job Configuration, access the Job Log or view a Directed Acyclic Graph (DAG) of the job.\n\n    * **Job Log**: Select the **GetLogs** button to get all logs for the job, or use the **Enter Search Filter** field to filter logs\n\n        ![Job log](./media/hdinsight-use-oozie-linux-mac/joblog.png)\n\n    * **JobDAG**: The DAG is a graphical overview of the data paths taken through the workflow\n\n        ![Job DAG](./media/hdinsight-use-oozie-linux-mac/jobdag.png)\n\n7. Selecting one of the actions from the **Job Info** tab will bring up information for the action. For example, select the **RunHiveScript** action.\n\n    ![Action info](./media/hdinsight-use-oozie-linux-mac/action.png)\n\n8. You can see details for the action, including a link to the **Console URL**, which can be used to view JobTracker information for the job.\n\n##Scheduling jobs\n\nThe coordinator allows you to specify a start, end, and occurrance frequency for jobs so that they can be scheduled for certain times.\n\nTo define a schedule for the workflow, use the following steps:\n\n1. Use the following to create a new file named **coordinator.xml**:\n\n        nano coordinator.xml\n\n    Use the following as the contents of the file:\n\n        <coordinator-app name=\"my_coord_app\" frequency=\"${coordFrequency}\" start=\"${coordStart}\" end=\"${coordEnd}\" timezone=\"${coordTimezone}\" xmlns=\"uri:oozie:coordinator:0.4\">\n          <action>\n            <workflow>\n              <app-path>${workflowPath}</app-path>\n            </workflow>\n          </action>\n        </coordinator-app>\n\n    Note that this uses `${...}` variables that will be replaced by values in the job definition. The variables are:\n\n    * **${coordFrequency}**: Time between running instances of the job\n    * **${coordStart}**: The job start time\n    * **${coordEnd}**: The job end time\n    * **${coordTimezone}**: Coordinator jobs are in a fixed time zone with no daylight savings time (typically represented by using UTC). This time zone is referred as the \"Oozie processing timezone\"\n    * **${wfPath}**: The path to the workflow.xml\n\n2. Use Ctrl-X, then **Y** and **Enter** to save the file.\n\n3. Use the following to copy it to the working directory for this job:\n\n        hadoop fs -copyFromLocal coordinator.xml /tutorials/useoozie/coordinator.xml\n\n4. Use the following to modify the **job.xml** file:\n\n        nano job.xml\n\n    Make the following changes:\n\n    * Change `<name>oozie.wf.application.path</name>` to `<name>oozie.coord.application.path</name>`. This instructs Oozie to run the coordinator file instead of the workflow file\n\n    * Add the following, which will be sets a variable used in the coordinator.xml to point to the location of the workflow.xml:\n\n            <property>\n              <name>workflowPath</name>\n              <value>wasb://mycontainer@mystorageaccount.blob.core.windows.net/tutorials/useoozie</value>\n            </property>\n\n        Replace the values for **mycontainer** and **mystorageaccount** with the values used in other entries in the job.xml file.\n\n    * Add the following, which define the start, end, and frequency to use for the coordinator.xml file:\n\n            <property>\n              <name>coordStart</name>\n              <value>2015-06-25T12:00Z</value>\n            </property>\n\n            <property>\n              <name>coordEnd</name>\n              <value>2015-06-27T12:00Z</value>\n            </property>\n\n            <property>\n              <name>coordFrequency</name>\n              <value>1440</value>\n            </property>\n\n            <property>\n              <name>coordTimezone</name>\n              <value>UTC</value>\n            </property>\n\n        These set the start time to 12:00PM on June 25th, 2015, the end time to June 27th, 2015, and the interval for running this job to daily (the frequency is in minutes, so 24 hours x 60 minutes = 1440 minutes.) Finally, the timezone is set to UTC.\n\n5. Use Ctrl-X, then **Y** and **Enter** to save the file.\n\n6. To run the job, use the following command:\n\n        oozie job -config job.xml -run\n\n    This will submit and start the job.\n\n7. If you visit the Oozie Web UI and select the **Coordinator Jobs** tab, you should information similar to the following:\n\n    ![coordinator jobs tab](./media/hdinsight-use-oozie-linux-mac/coordinatorjob.png)\n\n    Note the **Next Materialization** entry; this is when the job will run next.\n\n8. Similar to the earlier workflow job, selecting the job entry in the web UI will display information on the job:\n\n    ![Coordinator job info](./media/hdinsight-use-oozie-linux-mac/coordinatorjobinfo.png)\n\n    Note that this only shows successful runs of the job, not individual actions within the scheduled workflow. To see that, select one of the **Action** entries. This will display information similar to that retrieved for the earlier workflow job.\n\n    ![Action info](./media/hdinsight-use-oozie-linux-mac/coordinatoractionjob.png)\n\n##Troubleshooting\n\nWhen troubleshooting problems with Oozie jobs, the Oozie UI is very helpful as it allows you to easily view both Oozie logs, as well as links to JobTracker logs for MapReduce tasks such as Hive queries. In general, the pattern for troubleshooting should be:\n\n1. View the job in Oozie Web UI.\n\n2. If there is an error or failure for a specific action, select the action to see if the **Error Message** field provides more information on the failure.\n\n3. If available, use the URL from the action to view more details (such as JobTracker logs,) for the action.\n\nThe following are specific errors you may encounter, and how to resolve them.\n\n###JA009: Cannot initialize cluster\n\n**Symptoms**: The job status will change to **SUSPENDED**. Details for the job will show the RunHiveScript status as **START_MANUAL**. Selecting the action will reveal the following error message:\n\n    JA009: Cannot initialize Cluster. Please check your configuration for map\n\n**Cause**: The WASB addresses used in the **job.xml** file do not contain the storage container or storage account name. The WASB address format must be `wasb://containername@storageaccountname.blob.core.windows.net`.\n\n**Resolution**: Change the WASB addresses used by the job.\n\n###JA002: Oozie is not allowed to impersonate &lt;USER>\n\n**Symptoms**: The job status will change to **SUSPENDED**. Details for the job will show the RunHiveScript status as **START_MANUAL**. Selecting the action will reveal the following error message:\n\n    JA002: User: oozie is not allowed to impersonate <USER>\n\n**Cause**: Current permission settings do not allow Oozie to impersonate the specified user account.\n\n**Resolution**: Oozie is allowed to impersonate users in the **users** group. Use the `groups USERNAME` to see the groups that the user account is a member of. If the user is not a member of the **users** group, use the following command to add the user to the group:\n\n    sudo adduser USERNAME users\n\n> [AZURE.NOTE] It may take several minutes before HDInsight recognizes that the user has been added to the group.\n\n###Launcher ERROR (Sqoop)\n\n**Symptoms**: The job status will changed to **KILLED**. Details for the job will show the RunSqoopExport status as **ERROR**. Selecting the action will reveal the following error message:\n\n    Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.SqoopMain], exit code [1]\n\n**Cause**: Sqoop is unable to load the database driver required to access the database.\n\n**Resolution**: When using Sqoop from an Oozie job, you must include the database driver with the other resources (such as the workflow.xml,) used by the job.\n\nYou must also reference the archive containing the database driver from the `<sqoop>...</sqoop>` section of the workflow.xml.\n\nFor example, for the job in this document, you would use the following steps:\n\n1. Copy the sqljdbc4.jar file to the /tutorials/useoozie directory:\n\n         hadoop fs -copyFromLocal /usr/share/java/sqljdbc_4.1/enu/sqljdbc4.jar /tutorials/useoozie/sqljdbc4.jar\n\n2. Modify the workflow.xml to add the following on a new line above `</sqoop>`:\n\n        <archive>sqljdbc4.jar</archive>\n\n##Next steps\nIn this tutorial, you learned how to define an Oozie workflow and how to run an Oozie job. To learn more about working with HDInsight, see the following articles:\n\n- [Use time-based Oozie Coordinator with HDInsight][hdinsight-oozie-coordinator-time]\n- [Upload data for Hadoop jobs in HDInsight][hdinsight-upload-data]\n- [Use Sqoop with Hadoop in HDInsight][hdinsight-use-sqoop]\n- [Use Hive with Hadoop on HDInsight][hdinsight-use-hive]\n- [Use Pig with Hadoop on HDInsight][hdinsight-use-pig]\n- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]\n\n\n[hdinsight-cmdlets-download]: http://go.microsoft.com/fwlink/?LinkID=325563\n\n\n\n[azure-data-factory-pig-hive]: data-factory-pig-hive-activities.md\n[hdinsight-oozie-coordinator-time]: hdinsight-use-oozie-coordinator-time.md\n[hdinsight-versions]:  hdinsight-component-versioning.md\n[hdinsight-storage]: hdinsight-use-blob-storage.md\n[hdinsight-get-started]: hdinsight-get-started.md\n\n\n[hdinsight-use-sqoop]: hdinsight-use-sqoop-mac-linux.md\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-storage]: hdinsight-use-blob-storage.md\n[hdinsight-get-started-emulator]: hdinsight-get-started-emulator.md\n\n[hdinsight-develop-streaming-jobs]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n[hdinsight-develop-mapreduce]: hdinsight-develop-deploy-java-mapreduce.md\n\n[sqldatabase-create-configue]: sql-database-create-configure.md\n[sqldatabase-get-started]: sql-database-get-started.md\n\n[azure-create-storageaccount]: storage-create-storage-account.md\n\n[apache-hadoop]: http://hadoop.apache.org/\n[apache-oozie-400]: http://oozie.apache.org/docs/4.0.0/\n[apache-oozie-332]: http://oozie.apache.org/docs/3.3.2/\n\n[powershell-download]: http://azure.microsoft.com/downloads/\n[powershell-about-profiles]: http://go.microsoft.com/fwlink/?LinkID=113729\n[powershell-install-configure]: powershell-install-configure.md\n[powershell-start]: http://technet.microsoft.com/library/hh847889.aspx\n[powershell-script]: https://technet.microsoft.com/en-us/library/ee176961.aspx\n\n[cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\n\n[img-workflow-diagram]: ./media/hdinsight-use-oozie/HDI.UseOozie.Workflow.Diagram.png\n[img-preparation-output]: ./media/hdinsight-use-oozie/HDI.UseOozie.Preparation.Output1.png\n[img-runworkflow-output]: ./media/hdinsight-use-oozie/HDI.UseOozie.RunWF.Output.png\n\n[technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\n\ntest\n"
}