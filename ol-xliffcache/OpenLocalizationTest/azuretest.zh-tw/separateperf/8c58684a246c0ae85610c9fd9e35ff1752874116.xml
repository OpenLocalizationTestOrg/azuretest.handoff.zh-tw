{
  "nodes": [
    {
      "content": "Monitor and manage Azure Data Factory pipelines",
      "pos": [
        28,
        75
      ]
    },
    {
      "content": "Learn how to use Azure Management Portal and Azure PowerShell to monitor and manage Azure data factories and pipelines you have created.",
      "pos": [
        95,
        231
      ]
    },
    {
      "content": "Monitor and manage Azure Data Factory pipelines",
      "pos": [
        559,
        606
      ]
    },
    {
      "content": "The Data Factory service provides reliable and complete view of your storage, processing, and data movement services.",
      "pos": [
        607,
        724
      ]
    },
    {
      "content": "It helps you quickly assess end-to-end data pipeline health, pinpoint issues, and take corrective action if needed.",
      "pos": [
        725,
        840
      ]
    },
    {
      "content": "You can also visually track data lineage and the relationships between your data across any of your sources, and see a full historical accounting of job execution, system health, and dependencies from a single monitoring dashboard.",
      "pos": [
        841,
        1072
      ]
    },
    {
      "content": "This article describes how to monitor, manage and debug your pipelines.",
      "pos": [
        1074,
        1145
      ]
    },
    {
      "content": "It also provides information on how to create alerts and get notified on failures.",
      "pos": [
        1146,
        1228
      ]
    },
    {
      "content": "Understand pipelines and activity states",
      "pos": [
        1233,
        1273
      ]
    },
    {
      "content": "Using the Azure Preview Portal, you can view your data factory as a diagram, view activities in a pipeline, view input and output datasets, and more.",
      "pos": [
        1274,
        1423
      ]
    },
    {
      "content": "This section also provides how a slice transitions from one state to another state.",
      "pos": [
        1424,
        1507
      ]
    },
    {
      "content": "Navigate to your data factory",
      "pos": [
        1516,
        1545
      ]
    },
    {
      "pos": [
        1550,
        1613
      ],
      "content": "Sign-in to the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](http://portal.azure.com)</ept>."
    },
    {
      "pos": [
        1618,
        1669
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Browse All<ept id=\"p1\">**</ept> and select <bpt id=\"p2\">**</bpt>Data Factories<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Browse All -&gt; Data factories",
      "pos": [
        1681,
        1709
      ]
    },
    {
      "pos": [
        1792,
        1862
      ],
      "content": "You should see all the data factories in the <bpt id=\"p1\">**</bpt>Data factories<ept id=\"p1\">**</ept> blade."
    },
    {
      "pos": [
        1867,
        2021
      ],
      "content": "In the Data factories blade, select the data factory you are interested in and you should see the home page (<bpt id=\"p1\">**</bpt>Data factory<ept id=\"p1\">**</ept> blade) for the data factory."
    },
    {
      "content": "Data factory blade",
      "pos": [
        2029,
        2047
      ]
    },
    {
      "content": "Diagram view of your data factory",
      "pos": [
        2125,
        2158
      ]
    },
    {
      "content": "The Diagram View of a data factory provides a single pane of glass to monitor and manage the data factory and its assets.",
      "pos": [
        2159,
        2280
      ]
    },
    {
      "pos": [
        2282,
        2381
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Diagram<ept id=\"p1\">**</ept> on the data factory home page above to see the diagram view of your data factory."
    },
    {
      "content": "Diagram view",
      "pos": [
        2385,
        2397
      ]
    },
    {
      "content": "You can zoom in, zoom out, zoom to fit, zoom to 100%, lock the layout of the diagram, automaticaly position pipelines and tables, and show lineage (show upstream and downstream items of selected items).",
      "pos": [
        2464,
        2666
      ]
    },
    {
      "content": "Activities inside a pipeline",
      "pos": [
        2674,
        2702
      ]
    },
    {
      "content": "Right-click on the pipeline and click <bpt id=\"p1\">**</bpt>Open pipeline<ept id=\"p1\">**</ept> to see all activities in the pipeline along with input and output datasets for the activities.",
      "pos": [
        2707,
        2857
      ]
    },
    {
      "content": "This is useful when your pipeline comprises of more than 1 Activity and you want to understand the operational lineage of a single pipeline.",
      "pos": [
        2858,
        2998
      ]
    },
    {
      "content": "Open pipeline menu",
      "pos": [
        3006,
        3024
      ]
    },
    {
      "content": "In the following example, you see two activities in the pipeline with their inputs and outputs.",
      "pos": [
        3101,
        3196
      ]
    },
    {
      "content": "The activity titled <bpt id=\"p1\">**</bpt>JoinData<ept id=\"p1\">**</ept> of type HDInsight Hive Activity and <bpt id=\"p2\">**</bpt>EgressDataAzure<ept id=\"p2\">**</ept> of type Copy Activity are in this sample pipeline.",
      "pos": [
        3197,
        3336
      ]
    },
    {
      "content": "Activities inside a pipeline",
      "pos": [
        3349,
        3377
      ]
    },
    {
      "content": "You can navigate back to Data Factory home page by clicking Data factory link in the breadcrumb in the top-left corner.",
      "pos": [
        3461,
        3580
      ]
    },
    {
      "content": "Navigate back to data factory",
      "pos": [
        3588,
        3617
      ]
    },
    {
      "content": "View state of each activity inside a pipeline",
      "pos": [
        3705,
        3750
      ]
    },
    {
      "content": "You can view the current state of an activity by viewing the status of any of the datasets produced by the activity.",
      "pos": [
        3751,
        3867
      ]
    },
    {
      "pos": [
        3870,
        4051
      ],
      "content": "For example: in the following example, the <bpt id=\"p1\">**</bpt>BlobPartitionHiveActivity<ept id=\"p1\">**</ept> ran successfully and produced a dataset named <bpt id=\"p2\">**</bpt>PartitionedProductsUsageTable<ept id=\"p2\">**</ept> which is in <bpt id=\"p3\">**</bpt>Ready<ept id=\"p3\">**</ept> state."
    },
    {
      "content": "State of pipeline",
      "pos": [
        4055,
        4072
      ]
    },
    {
      "content": "Double clicking the <bpt id=\"p1\">**</bpt>PartitionedProductsUsageTable<ept id=\"p1\">**</ept> in the diagram view will showcase all the slices produced by different activity runs inside a pipeline.",
      "pos": [
        4144,
        4301
      ]
    },
    {
      "content": "You can see that the <bpt id=\"p1\">**</bpt>BlobPartitionHiveActivity<ept id=\"p1\">**</ept> ran successfully every month for last 8 months and produced the slices in <bpt id=\"p2\">**</bpt>Ready<ept id=\"p2\">**</ept> state.",
      "pos": [
        4302,
        4443
      ]
    },
    {
      "content": "The dataset slices in data factory can have one of the following status:",
      "pos": [
        4445,
        4517
      ]
    },
    {
      "content": "Status",
      "pos": [
        4519,
        4525
      ]
    },
    {
      "content": "Sub status",
      "pos": [
        4528,
        4538
      ]
    },
    {
      "content": "Description",
      "pos": [
        4541,
        4552
      ]
    },
    {
      "content": "Waiting",
      "pos": [
        4587,
        4594
      ]
    },
    {
      "content": "ScheduledTime",
      "pos": [
        4597,
        4610
      ]
    },
    {
      "content": "DatasetDependencies",
      "pos": [
        4615,
        4634
      ]
    },
    {
      "content": "ComputeResources",
      "pos": [
        4639,
        4655
      ]
    },
    {
      "content": "ConcurrencyLimit",
      "pos": [
        4660,
        4676
      ]
    },
    {
      "content": "ActivityResume",
      "pos": [
        4681,
        4695
      ]
    },
    {
      "content": "Retry",
      "pos": [
        4700,
        4705
      ]
    },
    {
      "content": "Validation",
      "pos": [
        4710,
        4720
      ]
    },
    {
      "content": "ValidationRetry",
      "pos": [
        4725,
        4740
      ]
    },
    {
      "content": "Waiting for pre-conditions to be met before executing.",
      "pos": [
        4743,
        4797
      ]
    },
    {
      "content": "Refer to the sub status to figure out what the slice is waiting for.",
      "pos": [
        4798,
        4866
      ]
    },
    {
      "content": "In-Progress",
      "pos": [
        4867,
        4878
      ]
    },
    {
      "content": "Starting",
      "pos": [
        4881,
        4889
      ]
    },
    {
      "content": "Configuring",
      "pos": [
        4894,
        4905
      ]
    },
    {
      "content": "Allocating Resources",
      "pos": [
        4910,
        4930
      ]
    },
    {
      "content": "Running",
      "pos": [
        4935,
        4942
      ]
    },
    {
      "content": "Validating",
      "pos": [
        4947,
        4957
      ]
    },
    {
      "content": "Currently, the activity is executing and producing/validating the data for a specific slice.",
      "pos": [
        4960,
        5052
      ]
    },
    {
      "content": "Failed",
      "pos": [
        5053,
        5059
      ]
    },
    {
      "content": "Slice processing failed.",
      "pos": [
        5064,
        5088
      ]
    },
    {
      "content": "Refer to error logs to figure out what caused the failure",
      "pos": [
        5089,
        5146
      ]
    },
    {
      "content": "Ready",
      "pos": [
        5147,
        5152
      ]
    },
    {
      "content": "Slice processing succeeded.",
      "pos": [
        5157,
        5184
      ]
    },
    {
      "content": "Slice is ready for consumption.",
      "pos": [
        5185,
        5216
      ]
    },
    {
      "content": "Skip",
      "pos": [
        5217,
        5221
      ]
    },
    {
      "content": "Do not process this slice",
      "pos": [
        5226,
        5251
      ]
    },
    {
      "pos": [
        5253,
        5359
      ],
      "content": "You can view the details about a slice by clicking a slice entry in the <bpt id=\"p1\">**</bpt>Recently Updated Slices<ept id=\"p1\">**</ept> blade."
    },
    {
      "content": "Slice details",
      "pos": [
        5363,
        5376
      ]
    },
    {
      "pos": [
        5445,
        5549
      ],
      "content": "If the slice has been executed multiple times, you will see multiple rows in the <bpt id=\"p1\">**</bpt>Activity runs<ept id=\"p1\">**</ept> list."
    },
    {
      "content": "Activity runs for a slice",
      "pos": [
        5553,
        5578
      ]
    },
    {
      "content": "You can view details about an activity run by clicking the run entry in the <bpt id=\"p1\">**</bpt>Activity runs<ept id=\"p1\">**</ept> list.",
      "pos": [
        5658,
        5757
      ]
    },
    {
      "content": "This will showcase all the log files along with an error message if any.",
      "pos": [
        5758,
        5830
      ]
    },
    {
      "content": "This is very useful to view and debug logs without having to leave your data factory.",
      "pos": [
        5831,
        5916
      ]
    },
    {
      "content": "Activity run details",
      "pos": [
        5920,
        5940
      ]
    },
    {
      "content": "If the slice is not in the <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state, you can see the upstream slices that are not ready and are blocking the current slice from executing in the <bpt id=\"p2\">**</bpt>Upstream slices that are not ready<ept id=\"p2\">**</ept> list.",
      "pos": [
        6015,
        6211
      ]
    },
    {
      "content": "This is very useful when your slice is in <bpt id=\"p1\">**</bpt>Waiting<ept id=\"p1\">**</ept> state and you want to understand the upstream dependencies on which the slice is waiting.",
      "pos": [
        6212,
        6355
      ]
    },
    {
      "content": "Upstream slices not ready",
      "pos": [
        6359,
        6384
      ]
    },
    {
      "content": "Dataset State Diagram",
      "pos": [
        6468,
        6489
      ]
    },
    {
      "content": "Once you deploy a data factory and the pipelines have a valid active period, the dataset slices transition from one state to another.",
      "pos": [
        6490,
        6623
      ]
    },
    {
      "content": "Currently the slice status follows the following state diagram:",
      "pos": [
        6624,
        6687
      ]
    },
    {
      "content": "State diagram",
      "pos": [
        6691,
        6704
      ]
    },
    {
      "content": "The dataset state transition flow in data factory involves the following:",
      "pos": [
        6772,
        6845
      ]
    },
    {
      "content": "Waiting-&gt; In-Progress/In-Progress (Validating) -&gt; Ready/Failed",
      "pos": [
        6846,
        6908
      ]
    },
    {
      "content": "The slices start in a <bpt id=\"p1\">**</bpt>Waiting<ept id=\"p1\">**</ept> state for pre-conditions to be met before executing.",
      "pos": [
        6910,
        6996
      ]
    },
    {
      "content": "Followed by this, the activity starts executing and the slice goes in <bpt id=\"p1\">**</bpt>In-Progress<ept id=\"p1\">**</ept> state.",
      "pos": [
        6997,
        7089
      ]
    },
    {
      "content": "The activity execution could be successful or fail and based on that the slice will go either in <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept>’ or <bpt id=\"p2\">**</bpt>Failed<ept id=\"p2\">**</ept> state.",
      "pos": [
        7090,
        7218
      ]
    },
    {
      "content": "The user can reset the slice to go back from <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>Failed<ept id=\"p2\">**</ept> state to <bpt id=\"p3\">**</bpt>Waiting<ept id=\"p3\">**</ept> state.",
      "pos": [
        7221,
        7317
      ]
    },
    {
      "content": "The user can also mark the slice state to <bpt id=\"p1\">**</bpt>Skip<ept id=\"p1\">**</ept> which will prevent the activity from executing and not process the slice.",
      "pos": [
        7318,
        7442
      ]
    },
    {
      "content": "Manage pipelines",
      "pos": [
        7448,
        7464
      ]
    },
    {
      "content": "You can manage your pipelines using Azure PowerShell.",
      "pos": [
        7465,
        7518
      ]
    },
    {
      "content": "For example, you can pause and resume pipelines by running Azure PowerShell cmdlets.",
      "pos": [
        7519,
        7603
      ]
    },
    {
      "content": "Pause and resume pipelines",
      "pos": [
        7610,
        7636
      ]
    },
    {
      "content": "You can pause/suspend pipelines using the <bpt id=\"p1\">**</bpt>Suspend-AzureDataFactoryPipeline<ept id=\"p1\">**</ept> Powershell cmdlet.",
      "pos": [
        7637,
        7734
      ]
    },
    {
      "content": "This is useful when you have figured out an issue with your data and don’t want to run your pipelines to process data any further until the issue is fixed.",
      "pos": [
        7735,
        7890
      ]
    },
    {
      "pos": [
        7892,
        8083
      ],
      "content": "For example: in the below screen shot, an issue has been identified with the <bpt id=\"p1\">**</bpt>PartitionProductsUsagePipeline<ept id=\"p1\">**</ept> in <bpt id=\"p2\">**</bpt>productrecgamalbox1dev<ept id=\"p2\">**</ept> data factory and we want to suspend the pipeline."
    },
    {
      "content": "Pipeline to be suspended",
      "pos": [
        8087,
        8111
      ]
    },
    {
      "pos": [
        8190,
        8277
      ],
      "content": "Run the following PowerShell command to suspend the <bpt id=\"p1\">**</bpt>PartitionProductsUsagePipeline<ept id=\"p1\">**</ept>."
    },
    {
      "content": "For example:",
      "pos": [
        8392,
        8404
      ]
    },
    {
      "pos": [
        8545,
        8702
      ],
      "content": "Once the issue has been fixed with the <bpt id=\"p1\">**</bpt>PartitionProductsUsagePipeline<ept id=\"p1\">**</ept>, the suspended pipeline can be resumed by running the following PowerShell command."
    },
    {
      "content": "For example:",
      "pos": [
        8816,
        8828
      ]
    },
    {
      "content": "Debug pipelines",
      "pos": [
        8972,
        8987
      ]
    },
    {
      "content": "Azure Data Factory provides rich capabilities via Azure portal and Azure PowerShell to debug and troubleshoot pipelines.",
      "pos": [
        8988,
        9108
      ]
    },
    {
      "content": "Find errors in a pipeline",
      "pos": [
        9114,
        9139
      ]
    },
    {
      "content": "If the activity run fails in a pipeline, the dataset produced by the pipeline is in an error state due to the failure.",
      "pos": [
        9140,
        9258
      ]
    },
    {
      "content": "You can debug and troubleshoot errors in Azure Data Factory using the following mechanisms.",
      "pos": [
        9259,
        9350
      ]
    },
    {
      "content": "Use Azure Portal to debug an error:",
      "pos": [
        9357,
        9392
      ]
    },
    {
      "pos": [
        9398,
        9471
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>With errors<ept id=\"p1\">**</ept> on <bpt id=\"p2\">**</bpt>Datasets<ept id=\"p2\">**</ept> tile on the data factory home page."
    },
    {
      "content": "Datasets tile with error",
      "pos": [
        9483,
        9507
      ]
    },
    {
      "pos": [
        9590,
        9672
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Datasets with errors<ept id=\"p1\">**</ept> blade, click the table that you are interested in."
    },
    {
      "content": "Datasets with errors blade",
      "pos": [
        9680,
        9706
      ]
    },
    {
      "pos": [
        9790,
        9875
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>TABLE<ept id=\"p1\">**</ept> blade, click on the problem slice with <bpt id=\"p2\">**</bpt>STATUS<ept id=\"p2\">**</ept> set to <bpt id=\"p3\">**</bpt>Failed<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Table blade with problem slice",
      "pos": [
        9883,
        9913
      ]
    },
    {
      "pos": [
        9993,
        10057
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade, click the activity run that failed."
    },
    {
      "content": "Dataslice with an error",
      "pos": [
        10069,
        10092
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>ACTIVITY RUN DETAILS<ept id=\"p1\">**</ept> blade, you can download the files associated with the HDInsight processing.",
      "pos": [
        10170,
        10277
      ]
    },
    {
      "content": "Click Download for Status/stderr to download the error log file that contains details about the error.",
      "pos": [
        10278,
        10380
      ]
    },
    {
      "content": "Activity run details blade with error",
      "pos": [
        10388,
        10425
      ]
    },
    {
      "content": "Use the PowerShell to debug an error",
      "pos": [
        10518,
        10554
      ]
    },
    {
      "pos": [
        10559,
        10587
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        10592,
        10692
      ],
      "content": "Switch to <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode as the Data Factory cmdlets are available only in this mode."
    },
    {
      "content": "Run <bpt id=\"p1\">**</bpt>Get-AzureDataFactorySlice<ept id=\"p1\">**</ept> command to see the slices and their statuses.",
      "pos": [
        10744,
        10823
      ]
    },
    {
      "content": "You should see a slice with the status: <bpt id=\"p1\">**</bpt>Failed<ept id=\"p1\">**</ept>.",
      "pos": [
        10824,
        10875
      ]
    },
    {
      "content": "For example:",
      "pos": [
        11106,
        11118
      ]
    },
    {
      "pos": [
        11413,
        11509
      ],
      "content": "Now, run the <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryRun<ept id=\"p1\">**</ept> cmdlet to get details about activity run for the slice."
    },
    {
      "content": "For example:",
      "pos": [
        11727,
        11739
      ]
    },
    {
      "content": "You should see the output with details about the error (similar to the following):",
      "pos": [
        12075,
        12157
      ]
    },
    {
      "pos": [
        13288,
        13456
      ],
      "content": "You can run <bpt id=\"p1\">**</bpt>Save-AzureDataFactoryLog<ept id=\"p1\">**</ept> cmdlet with Id value you see from the above output and download the log files using the <bpt id=\"p2\">**</bpt>-DownloadLogsoption<ept id=\"p2\">**</ept> for the cmdlet."
    },
    {
      "content": "Save-AzureDataFactoryLog -ResourceGroupName \"ADF\" -DataFactoryName \"LogProcessingFactory\" -Id \"841b77c9-d56c-48d1-99a3-8c16c3e77d39\" -DownloadLogs -Output \"C:\\Test\"",
      "pos": [
        13462,
        13626
      ]
    },
    {
      "content": "Re-run failures in a pipeline",
      "pos": [
        13632,
        13661
      ]
    },
    {
      "content": "Using Azure Portal",
      "pos": [
        13667,
        13685
      ]
    },
    {
      "pos": [
        13687,
        13851
      ],
      "content": "Once you troubleshoot and debug failures in a pipeline, you can re-run failures by navigating to the error slice and clicking the <bpt id=\"p1\">**</bpt>Run<ept id=\"p1\">**</ept> button on the command bar."
    },
    {
      "content": "Rerun a failed slice",
      "pos": [
        13855,
        13875
      ]
    },
    {
      "content": "In case the slice has failed validation due to a policy failure (for ex: data not available), you can fix the failure and validate again by clicking the <bpt id=\"p1\">**</bpt>Validate<ept id=\"p1\">**</ept> button on the command bar.",
      "pos": [
        13941,
        14133
      ]
    },
    {
      "content": "<ph id=\"ph1\">![</ph>Fix errors and validate<ph id=\"ph2\">](./media/data-factory-monitor-manage-pipelines/fix-error-and-validate.png)</ph>",
      "pos": [
        14134,
        14234
      ]
    },
    {
      "content": "Using Azure PowerShell",
      "pos": [
        14240,
        14262
      ]
    },
    {
      "content": "You can re-run failures by using the ‘Set-AzureDataFactorySliceStatus’ cmdlet.",
      "pos": [
        14264,
        14342
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Example:<ept id=\"p1\">**</ept>",
      "pos": [
        14612,
        14624
      ]
    },
    {
      "content": "The following example sets the status of all slices for the table 'DAWikiAggregatedData' to 'PendingExecution' in the Azure data factory 'WikiADF'.",
      "pos": [
        14625,
        14772
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Note:<ept id=\"p1\">**</ept> The UpdateType is set to UpstreamInPipeline, which means that status of each slice for the table and all the dependent (upstream) tables which are used as input tables for activities in the pipeline is set to \"PendingExecution\".",
      "pos": [
        14774,
        15012
      ]
    },
    {
      "content": "Other possible value for this parameter is \"Individual\".",
      "pos": [
        15013,
        15069
      ]
    },
    {
      "content": "Create alerts",
      "pos": [
        15316,
        15329
      ]
    },
    {
      "content": "Azure logs user events when an Azure resource (e.g. data factory) is created, updated or deleted.",
      "pos": [
        15330,
        15427
      ]
    },
    {
      "content": "You can create alerts on these events.",
      "pos": [
        15428,
        15466
      ]
    },
    {
      "content": "Data Factory allows you to capture various metrics and create alerts on metrics.",
      "pos": [
        15467,
        15547
      ]
    },
    {
      "content": "We recommend that you use events for real time monitoring and metrics for historical purposes.",
      "pos": [
        15548,
        15642
      ]
    },
    {
      "content": "Alerts on events",
      "pos": [
        15649,
        15665
      ]
    },
    {
      "content": "Azure events provide useful insights into what is happening in your Azure resources.",
      "pos": [
        15666,
        15750
      ]
    },
    {
      "content": "Azure logs user events when an Azure resource (e.g. data factory) is created, updated or deleted.",
      "pos": [
        15751,
        15848
      ]
    },
    {
      "content": "When using the Azure Data Factory, events are generated when:",
      "pos": [
        15849,
        15910
      ]
    },
    {
      "content": "Azure Data Factory is created/updated/deleted.",
      "pos": [
        15914,
        15960
      ]
    },
    {
      "content": "Data processing (called as Runs) has started/completed.",
      "pos": [
        15963,
        16018
      ]
    },
    {
      "content": "When an on-demand HDInsight cluster is created and removed.",
      "pos": [
        16021,
        16080
      ]
    },
    {
      "content": "You can create alerts on these user events and configure them to send email notifications to the administrator and co-administrators of the subscription.",
      "pos": [
        16082,
        16235
      ]
    },
    {
      "content": "In addition, you can specify additional email addresses of users who need to receive email notifications when the conditions are met.",
      "pos": [
        16236,
        16369
      ]
    },
    {
      "content": "This is very useful when you want to get notified on failures and don’t want to continuously monitor your data factory.",
      "pos": [
        16370,
        16489
      ]
    },
    {
      "content": "Specifying an alert definition:",
      "pos": [
        16496,
        16527
      ]
    },
    {
      "content": "To specify an alert definition, you create a JSON file describing the operations that you want to be alerted on.",
      "pos": [
        16528,
        16640
      ]
    },
    {
      "content": "In the example below, the alert will send an email notification for the RunFinished operation.",
      "pos": [
        16641,
        16735
      ]
    },
    {
      "content": "To be specific, an email notification is sent when a run in the data factory has completed and the run has failed (Status = FailedExecution).",
      "pos": [
        16736,
        16877
      ]
    },
    {
      "pos": [
        18409,
        18524
      ],
      "content": "From the above JSON definition, <bpt id=\"p1\">**</bpt>subStatus<ept id=\"p1\">**</ept> can be removed if you don’t want to be alerted on a specific failure."
    },
    {
      "content": "The following table provides the list of available operations and statuses (and sub-statuses).",
      "pos": [
        18526,
        18620
      ]
    },
    {
      "content": "Operation name",
      "pos": [
        18622,
        18636
      ]
    },
    {
      "content": "Status",
      "pos": [
        18639,
        18645
      ]
    },
    {
      "content": "Sub status",
      "pos": [
        18648,
        18658
      ]
    },
    {
      "content": "RunStarted",
      "pos": [
        18696,
        18706
      ]
    },
    {
      "content": "Started",
      "pos": [
        18709,
        18716
      ]
    },
    {
      "content": "Starting",
      "pos": [
        18719,
        18727
      ]
    },
    {
      "content": "RunFinished",
      "pos": [
        18728,
        18739
      ]
    },
    {
      "content": "Failed / Succeeded",
      "pos": [
        18742,
        18760
      ]
    },
    {
      "content": "FailedResourceAllocation",
      "pos": [
        18766,
        18790
      ]
    },
    {
      "content": "Succeeded",
      "pos": [
        18797,
        18806
      ]
    },
    {
      "content": "FailedExecution",
      "pos": [
        18813,
        18828
      ]
    },
    {
      "content": "TimedOut",
      "pos": [
        18835,
        18843
      ]
    },
    {
      "content": "&lt;Canceled/p&gt;",
      "pos": [
        18850,
        18862
      ]
    },
    {
      "content": "FailedValidation",
      "pos": [
        18865,
        18881
      ]
    },
    {
      "content": "Abandoned",
      "pos": [
        18888,
        18897
      ]
    },
    {
      "content": "SliceOnTime",
      "pos": [
        18902,
        18913
      ]
    },
    {
      "content": "In Progress",
      "pos": [
        18916,
        18927
      ]
    },
    {
      "content": "Ontime",
      "pos": [
        18930,
        18936
      ]
    },
    {
      "content": "SliceDelayed",
      "pos": [
        18937,
        18949
      ]
    },
    {
      "content": "In Progress",
      "pos": [
        18952,
        18963
      ]
    },
    {
      "content": "Late",
      "pos": [
        18966,
        18970
      ]
    },
    {
      "content": "OnDemandClusterCreateStarted",
      "pos": [
        18971,
        18999
      ]
    },
    {
      "content": "Started",
      "pos": [
        19002,
        19009
      ]
    },
    {
      "content": "OnDemandClusterCreateSuccessful",
      "pos": [
        19010,
        19041
      ]
    },
    {
      "content": "Succeeded",
      "pos": [
        19044,
        19053
      ]
    },
    {
      "content": "OnDemandClusterDeleted",
      "pos": [
        19054,
        19076
      ]
    },
    {
      "content": "Succeeded",
      "pos": [
        19079,
        19088
      ]
    },
    {
      "content": "Deploying the Alert",
      "pos": [
        19095,
        19114
      ]
    },
    {
      "pos": [
        19116,
        19242
      ],
      "content": "To deploy the alert, use the Azure PowerShell cmdlet: <bpt id=\"p1\">**</bpt>New-AzureResourceGroupDeployment<ept id=\"p1\">**</ept>, as shown in the following example:"
    },
    {
      "content": "Once the resource group deployment has completed successfully, you will see the following messages:",
      "pos": [
        19352,
        19451
      ]
    },
    {
      "content": "Retrieving the list of Azure Resource Group Deployments",
      "pos": [
        20130,
        20185
      ]
    },
    {
      "pos": [
        20186,
        20341
      ],
      "content": "To retrieve the list of deployed Azure Resource Group deployments, use the cmdlet: <bpt id=\"p1\">**</bpt>Get-AzureResourceGroupDeployment<ept id=\"p1\">**</ept>, as shown in the following example:"
    },
    {
      "content": "Troubleshooting User Events",
      "pos": [
        20675,
        20702
      ]
    },
    {
      "pos": [
        20703,
        20866
      ],
      "content": "You can see all the events generated after clicking on the <bpt id=\"p1\">**</bpt>Operations<ept id=\"p1\">**</ept> tile, and alerts can be setup on any of these operations visible on the <bpt id=\"p2\">**</bpt>Events<ept id=\"p2\">**</ept> blade:"
    },
    {
      "content": "Operations",
      "pos": [
        20870,
        20880
      ]
    },
    {
      "content": "To see the alerts setup using Power shell, you can run the following command, and see all the alerts created.",
      "pos": [
        20945,
        21054
      ]
    },
    {
      "content": "This will show the alerts set up for both metrics and events with resource type as <bpt id=\"p1\">**</bpt>microsoft.insights/alertrules<ept id=\"p1\">**</ept>.",
      "pos": [
        21056,
        21173
      ]
    },
    {
      "content": "If you see the alert generation events on the portal blade but you don't receive email notifications, check whether e-mail address specified is set to receive emails from external senders.",
      "pos": [
        21786,
        21974
      ]
    },
    {
      "content": "The alert e-mails may have been blocked by your e-mail settings.",
      "pos": [
        21975,
        22039
      ]
    },
    {
      "content": "Alerts on Metrics",
      "pos": [
        22045,
        22062
      ]
    },
    {
      "content": "Data Factory allows you to capture various metrics and create alerts on metrics.",
      "pos": [
        22063,
        22143
      ]
    },
    {
      "content": "You can monitor and create alerts on the following metrics for the slices in your data factory.",
      "pos": [
        22144,
        22239
      ]
    },
    {
      "content": "Failed Runs",
      "pos": [
        22244,
        22255
      ]
    },
    {
      "content": "Successful Runs",
      "pos": [
        22258,
        22273
      ]
    },
    {
      "content": "These metrics are very useful and allow users to get an overview of overall failed and successful runs in their data factory.",
      "pos": [
        22275,
        22400
      ]
    },
    {
      "content": "Metrics are emitted every time there is a slice run.",
      "pos": [
        22401,
        22453
      ]
    },
    {
      "content": "On top of the hour, these metrics are aggregated and pushed to your storage account.",
      "pos": [
        22454,
        22538
      ]
    },
    {
      "content": "Therefore, to enable metrics, you will need to setup a storage account.",
      "pos": [
        22539,
        22610
      ]
    },
    {
      "content": "Enabling Metrics:",
      "pos": [
        22617,
        22634
      ]
    },
    {
      "content": "To enable metrics, please click on the following from Data Factory blade:",
      "pos": [
        22635,
        22708
      ]
    },
    {
      "pos": [
        22710,
        22783
      ],
      "content": "<bpt id=\"p1\">**</bpt>Monitoring<ept id=\"p1\">**</ept> -&gt; <bpt id=\"p2\">**</bpt>Metric<ept id=\"p2\">**</ept> -&gt; <bpt id=\"p3\">**</bpt>Diagnostic settings<ept id=\"p3\">**</ept> -&gt; <bpt id=\"p4\">**</bpt>Diagnostic<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        22785,
        22867
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Diagnostic<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>On<ept id=\"p2\">**</ept> and select the storage account and save."
    },
    {
      "content": "Enable metrics",
      "pos": [
        22871,
        22885
      ]
    },
    {
      "content": "Once saved, it may to take up to one hour for the metrics to be visible on the monitoring blade, because metrics aggregation happens hourly.",
      "pos": [
        22954,
        23094
      ]
    },
    {
      "content": "Setting up alert on Metrics:",
      "pos": [
        23101,
        23129
      ]
    },
    {
      "content": "To set up alerting on Metrics, click on the following from Data factory blade:",
      "pos": [
        23131,
        23209
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Monitoring<ept id=\"p1\">**</ept> -&gt; <bpt id=\"p2\">**</bpt>Metric<ept id=\"p2\">**</ept> -&gt; <bpt id=\"p3\">**</bpt>Add alert<ept id=\"p3\">**</ept> -&gt; <bpt id=\"p4\">**</bpt>Add an alert rule<ept id=\"p4\">**</ept>.",
      "pos": [
        23210,
        23281
      ]
    },
    {
      "pos": [
        23283,
        23351
      ],
      "content": "Fill in the details for alert rule, specify emails and click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Setting up alerts on metrics",
      "pos": [
        23356,
        23384
      ]
    },
    {
      "content": "Once done, you should see a new alert rule enabled on Alert rules tile as following:",
      "pos": [
        23467,
        23551
      ]
    },
    {
      "content": "Alert rules enabled",
      "pos": [
        23555,
        23574
      ]
    },
    {
      "content": "Congratulations!",
      "pos": [
        23647,
        23663
      ]
    },
    {
      "content": "You have set up your first alert on Metrics.",
      "pos": [
        23664,
        23708
      ]
    },
    {
      "content": "Now you should get notifications every time alert rule matches in the given time window.",
      "pos": [
        23709,
        23797
      ]
    },
    {
      "content": "Alert notifications:",
      "pos": [
        23803,
        23823
      ]
    },
    {
      "content": "Once the setup rule matches the condition, you should get an alert activated email.",
      "pos": [
        23824,
        23907
      ]
    },
    {
      "content": "Once the issue is resolved and the alert condition doesn’t match any more, you will get alert resolved email.",
      "pos": [
        23908,
        24017
      ]
    },
    {
      "content": "This behavior is different than events where a notification will be sent on each and every failure for which alert rule qualify.",
      "pos": [
        24019,
        24147
      ]
    },
    {
      "content": "Deploying alerts using PowerShell",
      "pos": [
        24153,
        24186
      ]
    },
    {
      "content": "You can deploy alerts for metrics in the same way as you do for events.",
      "pos": [
        24187,
        24258
      ]
    },
    {
      "content": "Alert definition:",
      "pos": [
        24263,
        24280
      ]
    },
    {
      "content": "Replace subscriptionId, resourceGroupName, and dataFactoryName in the above sample with appropriate values.",
      "pos": [
        26375,
        26482
      ]
    },
    {
      "pos": [
        26484,
        26525
      ],
      "content": "<bpt id=\"p1\">*</bpt>metricName<ept id=\"p1\">*</ept> as of now supports 2 values:"
    },
    {
      "content": "FailedRuns",
      "pos": [
        26528,
        26538
      ]
    },
    {
      "content": "SuccessfulRuns",
      "pos": [
        26541,
        26555
      ]
    },
    {
      "content": "Deploying the alert:",
      "pos": [
        26559,
        26579
      ]
    },
    {
      "pos": [
        26583,
        26709
      ],
      "content": "To deploy the alert, use the Azure PowerShell cmdlet: <bpt id=\"p1\">**</bpt>New-AzureResourceGroupDeployment<ept id=\"p1\">**</ept>, as shown in the following example:"
    },
    {
      "content": "You should see following message after successful deployment:",
      "pos": [
        26816,
        26877
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        27405,
        27418
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        27419,
        27476
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-monitor-manage-pipelines.md)</ept>.",
      "pos": [
        27477,
        27625
      ]
    },
    {
      "content": "test",
      "pos": [
        27626,
        27630
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Monitor and manage Azure Data Factory pipelines\" \n    description=\"Learn how to use Azure Management Portal and Azure PowerShell to monitor and manage Azure data factories and pipelines you have created.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/27/2015\" \n    ms.author=\"spelluru\"/>\n\n# Monitor and manage Azure Data Factory pipelines\nThe Data Factory service provides reliable and complete view of your storage, processing, and data movement services. It helps you quickly assess end-to-end data pipeline health, pinpoint issues, and take corrective action if needed. You can also visually track data lineage and the relationships between your data across any of your sources, and see a full historical accounting of job execution, system health, and dependencies from a single monitoring dashboard.\n\nThis article describes how to monitor, manage and debug your pipelines. It also provides information on how to create alerts and get notified on failures.\n\n## Understand pipelines and activity states\nUsing the Azure Preview Portal, you can view your data factory as a diagram, view activities in a pipeline, view input and output datasets, and more. This section also provides how a slice transitions from one state to another state.   \n\n### Navigate to your data factory\n1.  Sign-in to the [Azure Preview Portal](http://portal.azure.com).\n2.  Click **Browse All** and select **Data Factories**.\n    \n    ![Browse All -> Data factories](./media/data-factory-monitor-manage-pipelines/browseall-data-factories.png)\n\n    You should see all the data factories in the **Data factories** blade. \n4. In the Data factories blade, select the data factory you are interested in and you should see the home page (**Data factory** blade) for the data factory.\n\n    ![Data factory blade](./media/data-factory-monitor-manage-pipelines/data-factory-blade.png)\n\n#### Diagram view of your data factory\nThe Diagram View of a data factory provides a single pane of glass to monitor and manage the data factory and its assets.\n\nClick **Diagram** on the data factory home page above to see the diagram view of your data factory.\n\n![Diagram view](./media/data-factory-monitor-manage-pipelines/diagram-view.png)\n\nYou can zoom in, zoom out, zoom to fit, zoom to 100%, lock the layout of the diagram, automaticaly position pipelines and tables, and show lineage (show upstream and downstream items of selected items).\n \n\n### Activities inside a pipeline \n1. Right-click on the pipeline and click **Open pipeline** to see all activities in the pipeline along with input and output datasets for the activities. This is useful when your pipeline comprises of more than 1 Activity and you want to understand the operational lineage of a single pipeline.\n\n    ![Open pipeline menu](./media/data-factory-monitor-manage-pipelines/open-pipeline-menu.png)  \n2. In the following example, you see two activities in the pipeline with their inputs and outputs. The activity titled **JoinData** of type HDInsight Hive Activity and **EgressDataAzure** of type Copy Activity are in this sample pipeline. \n    \n    ![Activities inside a pipeline](./media/data-factory-monitor-manage-pipelines/activities-inside-pipeline.png) \n3. You can navigate back to Data Factory home page by clicking Data factory link in the breadcrumb in the top-left corner.\n\n    ![Navigate back to data factory](./media/data-factory-monitor-manage-pipelines/navigate-back-to-data-factory.png)\n\n### View state of each activity inside a pipeline\nYou can view the current state of an activity by viewing the status of any of the datasets produced by the activity. \n\nFor example: in the following example, the **BlobPartitionHiveActivity** ran successfully and produced a dataset named **PartitionedProductsUsageTable** which is in **Ready** state.\n\n![State of pipeline](./media/data-factory-monitor-manage-pipelines/state-of-pipeline.png)\n\nDouble clicking the **PartitionedProductsUsageTable** in the diagram view will showcase all the slices produced by different activity runs inside a pipeline. You can see that the **BlobPartitionHiveActivity** ran successfully every month for last 8 months and produced the slices in **Ready** state.\n\nThe dataset slices in data factory can have one of the following status:\n\nStatus | Sub status | Description\n------ | ---------- | -----------\nWaiting | ScheduledTime<br/>DatasetDependencies<br/>ComputeResources<br/>ConcurrencyLimit<br/>ActivityResume<br/>Retry<br/>Validation<br/>ValidationRetry | Waiting for pre-conditions to be met before executing. Refer to the sub status to figure out what the slice is waiting for.\nIn-Progress | Starting<br/>Configuring<br/>Allocating Resources<br/>Running<br/>Validating | Currently, the activity is executing and producing/validating the data for a specific slice.\nFailed | | Slice processing failed. Refer to error logs to figure out what caused the failure\nReady | | Slice processing succeeded. Slice is ready for consumption.\nSkip | | Do not process this slice\n\nYou can view the details about a slice by clicking a slice entry in the **Recently Updated Slices** blade.\n\n![Slice details](./media/data-factory-monitor-manage-pipelines/slice-details.png)\n \nIf the slice has been executed multiple times, you will see multiple rows in the **Activity runs** list.\n\n![Activity runs for a slice](./media/data-factory-monitor-manage-pipelines/activity-runs-for-a-slice.png)\n\nYou can view details about an activity run by clicking the run entry in the **Activity runs** list. This will showcase all the log files along with an error message if any. This is very useful to view and debug logs without having to leave your data factory.\n\n![Activity run details](./media/data-factory-monitor-manage-pipelines/activity-run-details.png)\n\nIf the slice is not in the **Ready** state, you can see the upstream slices that are not ready and are blocking the current slice from executing in the **Upstream slices that are not ready** list. This is very useful when your slice is in **Waiting** state and you want to understand the upstream dependencies on which the slice is waiting.\n\n![Upstream slices not ready](./media/data-factory-monitor-manage-pipelines/upstream-slices-not-ready.png)\n\n### Dataset State Diagram\nOnce you deploy a data factory and the pipelines have a valid active period, the dataset slices transition from one state to another. Currently the slice status follows the following state diagram:\n\n![State diagram](./media/data-factory-monitor-manage-pipelines/state-diagram.png)\n\nThe dataset state transition flow in data factory involves the following:\nWaiting-> In-Progress/In-Progress (Validating) -> Ready/Failed\n\nThe slices start in a **Waiting** state for pre-conditions to be met before executing. Followed by this, the activity starts executing and the slice goes in **In-Progress** state. The activity execution could be successful or fail and based on that the slice will go either in **Ready**’ or **Failed** state. \n\nThe user can reset the slice to go back from **Ready** or **Failed** state to **Waiting** state. The user can also mark the slice state to **Skip** which will prevent the activity from executing and not process the slice.\n\n\n## Manage pipelines\nYou can manage your pipelines using Azure PowerShell. For example, you can pause and resume pipelines by running Azure PowerShell cmdlets. \n\n### Pause and resume pipelines\nYou can pause/suspend pipelines using the **Suspend-AzureDataFactoryPipeline** Powershell cmdlet. This is useful when you have figured out an issue with your data and don’t want to run your pipelines to process data any further until the issue is fixed.\n\nFor example: in the below screen shot, an issue has been identified with the **PartitionProductsUsagePipeline** in **productrecgamalbox1dev** data factory and we want to suspend the pipeline.\n\n![Pipeline to be suspended](./media/data-factory-monitor-manage-pipelines/pipeline-to-be-suspended.png)\n\nRun the following PowerShell command to suspend the **PartitionProductsUsagePipeline**.\n\n    Suspend-AzureDataFactoryPipeline [-ResourceGroupName] <String> [-DataFactoryName] <String> [-Name] <String>\n\nFor example:\n\n    Suspend-AzureDataFactoryPipeline -ResourceGroupName ADF -DataFactoryName productrecgamalbox1dev -Name PartitionProductsUsagePipeline \n\nOnce the issue has been fixed with the **PartitionProductsUsagePipeline**, the suspended pipeline can be resumed by running the following PowerShell command.\n\n    Resume-AzureDataFactoryPipeline [-ResourceGroupName] <String> [-DataFactoryName] <String> [-Name] <String>\n\nFor example:\n\n    Resume-AzureDataFactoryPipeline -ResourceGroupName ADF -DataFactoryName productrecgamalbox1dev -Name PartitionProductsUsagePipeline \n\n\n## Debug pipelines\nAzure Data Factory provides rich capabilities via Azure portal and Azure PowerShell to debug and troubleshoot pipelines.\n\n### Find errors in a pipeline\nIf the activity run fails in a pipeline, the dataset produced by the pipeline is in an error state due to the failure. You can debug and troubleshoot errors in Azure Data Factory using the following mechanisms.\n\n#### Use Azure Portal to debug an error:\n\n1.  Click **With errors** on **Datasets** tile on the data factory home page.\n    \n    ![Datasets tile with error](./media/data-factory-monitor-manage-pipelines/datasets-tile-with-errors.png)\n2.  In the **Datasets with errors** blade, click the table that you are interested in.\n\n    ![Datasets with errors blade](./media/data-factory-monitor-manage-pipelines/datasets-with-errors-blade.png)\n3.  In the **TABLE** blade, click on the problem slice with **STATUS** set to **Failed**.\n\n    ![Table blade with problem slice](./media/data-factory-monitor-manage-pipelines/table-blade-with-error.png)\n4.  In the **DATA SLICE** blade, click the activity run that failed.\n    \n    ![Dataslice with an error](./media/data-factory-monitor-manage-pipelines/dataslice-with-error.png)\n5.  In the **ACTIVITY RUN DETAILS** blade, you can download the files associated with the HDInsight processing. Click Download for Status/stderr to download the error log file that contains details about the error.\n\n    ![Activity run details blade with error](./media/data-factory-monitor-manage-pipelines/activity-run-details-with-error.png)  \n\n#### Use the PowerShell to debug an error\n1.  Launch **Azure PowerShell**.\n2.  Switch to **AzureResourceManager** mode as the Data Factory cmdlets are available only in this mode.\n\n        switch-azuremode AzureResourceManager\n3.  Run **Get-AzureDataFactorySlice** command to see the slices and their statuses. You should see a slice with the status: **Failed**.\n\n        Get-AzureDataFactorySlice [-ResourceGroupName] <String> [-DataFactoryName] <String> [-TableName] <String> [-StartDateTime] <DateTime> [[-EndDateTime] <DateTime> ] [-Profile <AzureProfile> ] [ <CommonParameters>]\n    \n    For example:\n\n\n        Get-AzureDataFactorySlice -ResourceGroupName ADF -DataFactoryName LogProcessingFactory -TableName EnrichedGameEventsTable -StartDateTime 2014-05-04 20:00:00\n\n    Replace **StartDateTime** with the StartDateTime value you specified for the Set-AzureDataFactoryPipelineActivePeriod.\n4. Now, run the **Get-AzureDataFactoryRun** cmdlet to get details about activity run for the slice.\n\n        Get-AzureDataFactoryRun [-ResourceGroupName] <String> [-\n        DataFactoryName] <String> [-TableName] <String> [-StartDateTime] \n        <DateTime> [-Profile <AzureProfile> ] [ <CommonParameters>]\n    \n    For example:\n\n\n        Get-AzureDataFactoryRun -ResourceGroupName ADF -DataFactoryName LogProcessingFactory -TableName EnrichedGameEventsTable -StartDateTime \"5/5/2014 12:00:00 AM\"\n\n    The value of StartDateTime is the start time for the error/problem slice you noted from the previous step. The date-time should be enclosed in double quotes.\n5.  You should see the output with details about the error (similar to the following):\n\n        Id                      : 841b77c9-d56c-48d1-99a3-8c16c3e77d39\n        ResourceGroupName       : ADF\n        DataFactoryName         : LogProcessingFactory3\n        TableName               : EnrichedGameEventsTable\n        ProcessingStartTime     : 10/10/2014 3:04:52 AM\n        ProcessingEndTime       : 10/10/2014 3:06:49 AM\n        PercentComplete         : 0\n        DataSliceStart          : 5/5/2014 12:00:00 AM\n        DataSliceEnd            : 5/6/2014 12:00:00 AM\n        Status                  : FailedExecution\n        Timestamp               : 10/10/2014 3:04:52 AM\n        RetryAttempt            : 0\n        Properties              : {}\n        ErrorMessage            : Pig script failed with exit code '5'. See wasb://     adfjobs@spestore.blob.core.windows.net/PigQuery\n                                        Jobs/841b77c9-d56c-48d1-99a3-\n                    8c16c3e77d39/10_10_2014_03_04_53_277/Status/stderr' for\n                    more details.\n        ActivityName            : PigEnrichLogs\n        PipelineName            : EnrichGameLogsPipeline\n        Type                    :\n    \n    \n6.  You can run **Save-AzureDataFactoryLog** cmdlet with Id value you see from the above output and download the log files using the **-DownloadLogsoption** for the cmdlet.\n\n    Save-AzureDataFactoryLog -ResourceGroupName \"ADF\" -DataFactoryName \"LogProcessingFactory\" -Id \"841b77c9-d56c-48d1-99a3-8c16c3e77d39\" -DownloadLogs -Output \"C:\\Test\"\n\n\n## Re-run failures in a pipeline\n\n### Using Azure Portal\n\nOnce you troubleshoot and debug failures in a pipeline, you can re-run failures by navigating to the error slice and clicking the **Run** button on the command bar.\n\n![Rerun a failed slice](./media/data-factory-monitor-manage-pipelines/rerun-slice.png)\n\nIn case the slice has failed validation due to a policy failure (for ex: data not available), you can fix the failure and validate again by clicking the **Validate** button on the command bar.\n![Fix errors and validate](./media/data-factory-monitor-manage-pipelines/fix-error-and-validate.png)\n\n### Using Azure PowerShell\n\nYou can re-run failures by using the ‘Set-AzureDataFactorySliceStatus’ cmdlet.\n\n    Set-AzureDataFactorySliceStatus [-ResourceGroupName] <String> [-DataFactoryName] <String> [-TableName] <String> [-StartDateTime] <DateTime> [[-EndDateTime] <DateTime> ] [-Status] <String> [[-UpdateType] <String> ] [-Profile <AzureProfile> ] [ <CommonParameters>]\n\n**Example:**\nThe following example sets the status of all slices for the table 'DAWikiAggregatedData' to 'PendingExecution' in the Azure data factory 'WikiADF'.\n\n**Note:** The UpdateType is set to UpstreamInPipeline, which means that status of each slice for the table and all the dependent (upstream) tables which are used as input tables for activities in the pipeline is set to \"PendingExecution\". Other possible value for this parameter is \"Individual\".\n\n    Set-AzureDataFactorySliceStatus -ResourceGroupName ADF -DataFactoryName WikiADF -TableName DAWikiAggregatedData -Status PendingExecution -UpdateType UpstreamInPipeline -StartDateTime 2014-05-21T16:00:00 -EndDateTime 2014-05-21T20:00:00\n\n\n## Create alerts\nAzure logs user events when an Azure resource (e.g. data factory) is created, updated or deleted. You can create alerts on these events. Data Factory allows you to capture various metrics and create alerts on metrics. We recommend that you use events for real time monitoring and metrics for historical purposes. \n\n### Alerts on events\nAzure events provide useful insights into what is happening in your Azure resources. Azure logs user events when an Azure resource (e.g. data factory) is created, updated or deleted. When using the Azure Data Factory, events are generated when:\n\n- Azure Data Factory is created/updated/deleted.\n- Data processing (called as Runs) has started/completed.\n- When an on-demand HDInsight cluster is created and removed.\n\nYou can create alerts on these user events and configure them to send email notifications to the administrator and co-administrators of the subscription. In addition, you can specify additional email addresses of users who need to receive email notifications when the conditions are met. This is very useful when you want to get notified on failures and don’t want to continuously monitor your data factory.\n\n#### Specifying an alert definition:\nTo specify an alert definition, you create a JSON file describing the operations that you want to be alerted on. In the example below, the alert will send an email notification for the RunFinished operation. To be specific, an email notification is sent when a run in the data factory has completed and the run has failed (Status = FailedExecution).\n\n    {\n        \"contentVersion\": \"1.0.0.0\",\n         \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#\",\n        \"parameters\": {},\n        \"resources\": \n        [\n            {\n                \"name\": \"ADFAlertsSlice\",\n                \"type\": \"microsoft.insights/alertrules\",\n                \"apiVersion\": \"2014-04-01\",\n                \"location\": \"East US\",\n                \"properties\": \n                {\n                    \"name\": \"ADFAlertsSlice\",\n                    \"description\": \"One or more of the data slices for the Azure Data Factory has failed processing.\",\n                    \"isEnabled\": true,\n                    \"condition\": \n                    {\n                        \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.ManagementEventRuleCondition\",\n                        \"dataSource\": \n                        {\n                            \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleManagementEventDataSource\",\n                            \"operationName\": \"RunFinished\",\n                            \"status\": \"Failed\",\n                                \"subStatus\": \"FailedExecution\"   \n                        }\n                    },\n                    \"action\": \n                    {\n                        \"odata.type\": \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",\n                        \"customEmails\": [ \"<your alias>@contoso.com\" ]\n                    }\n                }\n            }\n        ]\n    }\n\nFrom the above JSON definition, **subStatus** can be removed if you don’t want to be alerted on a specific failure.\n\nThe following table provides the list of available operations and statuses (and sub-statuses).\n\nOperation name | Status | Sub status\n-------------- | ------ | ----------\nRunStarted | Started | Starting\nRunFinished | Failed / Succeeded | <p>FailedResourceAllocation</p><p>Succeeded</p><p>FailedExecution</p><p>TimedOut</p><p><Canceled/p><p>FailedValidation</p><p>Abandoned</p>\nSliceOnTime | In Progress | Ontime\nSliceDelayed | In Progress | Late\nOnDemandClusterCreateStarted | Started\nOnDemandClusterCreateSuccessful | Succeeded\nOnDemandClusterDeleted | Succeeded\n\n#### Deploying the Alert \nTo deploy the alert, use the Azure PowerShell cmdlet: **New-AzureResourceGroupDeployment**, as shown in the following example:\n\n    New-AzureResourceGroupDeployment -ResourceGroupName adf     -TemplateFile .\\ADFAlertFailedSlice.json  \n\nOnce the resource group deployment has completed successfully, you will see the following messages:\n\n    VERBOSE: 7:00:48 PM - Template is valid.\n    WARNING: 7:00:48 PM - The StorageAccountName parameter is no longer used and will be removed in a future release.\n    Please update scripts to remove this parameter.\n    VERBOSE: 7:00:49 PM - Create template deployment 'ADFAlertFailedSlice'.\n    VERBOSE: 7:00:57 PM - Resource microsoft.insights/alertrules 'ADFAlertsSlice' provisioning status is succeeded\n    \n    DeploymentName    : ADFAlertFailedSlice\n    ResourceGroupName : adf\n    ProvisioningState : Succeeded\n    Timestamp         : 10/11/2014 2:01:00 AM\n    Mode              : Incremental\n    TemplateLink      :\n    Parameters        :\n    Outputs           :\n\n#### Retrieving the list of Azure Resource Group Deployments\nTo retrieve the list of deployed Azure Resource Group deployments, use the cmdlet: **Get-AzureResourceGroupDeployment**, as shown in the following example:\n\n    Get-AzureResourceGroupDeployment -ResourceGroupName adf\n    \n    DeploymentName    : ADFAlertFailedSlice\n    ResourceGroupName : adf\n    ProvisioningState : Succeeded\n    Timestamp         : 10/11/2014 2:01:00 AM\n    Mode              : Incremental\n    TemplateLink      :\n    Parameters        :\n    Outputs           :\n\n\n#### Troubleshooting User Events\nYou can see all the events generated after clicking on the **Operations** tile, and alerts can be setup on any of these operations visible on the **Events** blade:\n\n![Operations](./media/data-factory-monitor-manage-pipelines/operations.png)\n\nTo see the alerts setup using Power shell, you can run the following command, and see all the alerts created.  This will show the alerts set up for both metrics and events with resource type as **microsoft.insights/alertrules**.\n\n    Get-AzureResourceGroup -Name $resourceGroupName\n\n    ResourceGroupName : mdwevent\n    Location          : westus\n    ProvisioningState : Succeeded\n    Resources         :\n                    Name                  Type                                 Location\n                    ====================  ===================================  ========\n                    abhieventtest1        Microsoft.DataFactory/dataFactories  westus\n                    abhieventtest2        Microsoft.DataFactory/dataFactories  westus\n                    FailedValidationRuns  microsoft.insights/alertrules        eastus\n\n\nIf you see the alert generation events on the portal blade but you don't receive email notifications, check whether e-mail address specified is set to receive emails from external senders. The alert e-mails may have been blocked by your e-mail settings.\n\n### Alerts on Metrics\nData Factory allows you to capture various metrics and create alerts on metrics. You can monitor and create alerts on the following metrics for the slices in your data factory.\n \n- Failed Runs\n- Successful Runs\n\nThese metrics are very useful and allow users to get an overview of overall failed and successful runs in their data factory. Metrics are emitted every time there is a slice run. On top of the hour, these metrics are aggregated and pushed to your storage account. Therefore, to enable metrics, you will need to setup a storage account.\n\n#### Enabling Metrics:\nTo enable metrics, please click on the following from Data Factory blade:\n\n**Monitoring** -> **Metric** -> **Diagnostic settings** -> **Diagnostic**\n\nOn the **Diagnostic** blade, click **On** and select the storage account and save.\n\n![Enable metrics](./media/data-factory-monitor-manage-pipelines/enable-metrics.png)\n\nOnce saved, it may to take up to one hour for the metrics to be visible on the monitoring blade, because metrics aggregation happens hourly.\n\n\n### Setting up alert on Metrics:\n\nTo set up alerting on Metrics, click on the following from Data factory blade:\n**Monitoring** -> **Metric** -> **Add alert** -> **Add an alert rule**.\n\nFill in the details for alert rule, specify emails and click **OK**.\n\n\n![Setting up alerts on metrics](./media/data-factory-monitor-manage-pipelines/setting-up-alerts-on-metrics.png)\n\nOnce done, you should see a new alert rule enabled on Alert rules tile as following:\n\n![Alert rules enabled](./media/data-factory-monitor-manage-pipelines/alert-rule-enabled.png)\n\nCongratulations! You have set up your first alert on Metrics. Now you should get notifications every time alert rule matches in the given time window.\n\n### Alert notifications:\nOnce the setup rule matches the condition, you should get an alert activated email. Once the issue is resolved and the alert condition doesn’t match any more, you will get alert resolved email.\n\nThis behavior is different than events where a notification will be sent on each and every failure for which alert rule qualify.\n\n### Deploying alerts using PowerShell\nYou can deploy alerts for metrics in the same way as you do for events. \n\n**Alert definition:**\n\n    {\n        \"contentVersion\" : \"1.0.0.0\",\n        \"$schema\" : \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#\",\n        \"parameters\" : {},\n        \"resources\" : [\n        {\n                \"name\" : \"FailedRunsGreaterThan5\",\n                \"type\" : \"microsoft.insights/alertrules\",\n                \"apiVersion\" : \"2014-04-01\",\n                \"location\" : \"East US\",\n                \"properties\" : {\n                    \"name\" : \"FailedRunsGreaterThan5\",\n                    \"description\" : \"Failed Runs greater than 5\",\n                    \"isEnabled\" : true,\n                    \"condition\" : {\n                        \"$type\" : \"Microsoft.WindowsAzure.Management.Monitoring.Alerts.Models.ThresholdRuleCondition, Microsoft.WindowsAzure.Management.Mon.Client\",\n                        \"odata.type\" : \"Microsoft.Azure.Management.Insights.Models.ThresholdRuleCondition\",\n                        \"dataSource\" : {\n                            \"$type\" : \"Microsoft.WindowsAzure.Management.Monitoring.Alerts.Models.RuleMetricDataSource, Microsoft.WindowsAzure.Management.Mon.Client\",\n                            \"odata.type\" : \"Microsoft.Azure.Management.Insights.Models.RuleMetricDataSource\",\n                            \"resourceUri\" : \"/SUBSCRIPTIONS/<subscriptionId>/RESOURCEGROUPS/<resourceGroupName\n    >/PROVIDERS/MICROSOFT.DATAFACTORY/DATAFACTORIES/<dataFactoryName>\",\n                            \"metricName\" : \"FailedRuns\"\n                        },\n                        \"threshold\" : 5.0,\n                        \"windowSize\" : \"PT3H\",\n                        \"timeAggregation\" : \"Total\"\n                    },\n                    \"action\" : {\n                        \"$type\" : \"Microsoft.WindowsAzure.Management.Monitoring.Alerts.Models.RuleEmailAction, Microsoft.WindowsAzure.Management.Mon.Client\",\n                        \"odata.type\" : \"Microsoft.Azure.Management.Insights.Models.RuleEmailAction\",\n                        \"customEmails\" : [\"abhinav.gpt@live.com\"]\n                    }\n                }\n            }\n        ]\n    }\n \nReplace subscriptionId, resourceGroupName, and dataFactoryName in the above sample with appropriate values.\n\n*metricName* as of now supports 2 values:\n- FailedRuns\n- SuccessfulRuns\n\n**Deploying the alert:**\n\nTo deploy the alert, use the Azure PowerShell cmdlet: **New-AzureResourceGroupDeployment**, as shown in the following example:\n\n    New-AzureResourceGroupDeployment -ResourceGroupName adf -TemplateFile .\\FailedRunsGreaterThan5.json\n\nYou should see following message after successful deployment:\n\n    VERBOSE: 12:52:47 PM - Template is valid.\n    VERBOSE: 12:52:48 PM - Create template deployment 'FailedRunsGreaterThan5'.\n    VERBOSE: 12:52:55 PM - Resource microsoft.insights/alertrules 'FailedRunsGreaterThan5' provisioning status is succeeded\n    \n    \n    DeploymentName    : FailedRunsGreaterThan5\n    ResourceGroupName : adf\n    ProvisioningState : Succeeded\n    Timestamp         : 7/27/2015 7:52:56 PM\n    Mode              : Incremental\n    TemplateLink      :\n    Parameters        :\n    Outputs           \n\n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-monitor-manage-pipelines.md).\ntest\n"
}