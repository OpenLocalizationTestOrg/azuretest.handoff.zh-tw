{
  "nodes": [
    {
      "content": "Azure Data Factory - Terminology",
      "pos": [
        28,
        60
      ]
    },
    {
      "content": "This article introduces you to the terminology used in creating data factories using the Azure Data Factory service",
      "pos": [
        80,
        195
      ]
    },
    {
      "content": "Azure Data Factory - Terminology",
      "pos": [
        522,
        554
      ]
    },
    {
      "content": "Data factory",
      "pos": [
        559,
        571
      ]
    },
    {
      "content": "An <bpt id=\"p1\">**</bpt>Azure data factory<ept id=\"p1\">**</ept> has one or more pipelines that process data in linked data stores (Azure Storage, Azure SQL Database, on-premises SQL Server etc...) by using linked compute services such as Azure HDInsight.",
      "pos": [
        572,
        788
      ]
    },
    {
      "content": "An Azure data factory itself does not contain data within it; The data is rather stored in data stores mentioned above.",
      "pos": [
        789,
        908
      ]
    },
    {
      "content": "Linked Service",
      "pos": [
        915,
        929
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>linked service<ept id=\"p1\">**</ept> is a service that is linked to an Azure data factory.",
      "pos": [
        930,
        1004
      ]
    },
    {
      "content": "A linked service can be one of the following:",
      "pos": [
        1005,
        1050
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>data storage<ept id=\"p1\">**</ept> service such as Azure Storage, Azure SQL Database or on-premises SQL Server database.",
      "pos": [
        1054,
        1158
      ]
    },
    {
      "content": "A data store is a container of input/output data sets.",
      "pos": [
        1159,
        1213
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>compute<ept id=\"p1\">**</ept> service such as Azure HDInsight, Azure Machine Learning, and Azure Batch.",
      "pos": [
        1220,
        1307
      ]
    },
    {
      "content": "A compute service process the input data and produces the output data.",
      "pos": [
        1308,
        1378
      ]
    },
    {
      "content": "Data set",
      "pos": [
        1385,
        1393
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>data set<ept id=\"p1\">**</ept> is a named view of data.",
      "pos": [
        1394,
        1433
      ]
    },
    {
      "content": "The data being described can vary from simple bytes, semi-structured data like CSV files all the way up to relational tables or even models.",
      "pos": [
        1434,
        1574
      ]
    },
    {
      "content": "A  Data Factory <bpt id=\"p1\">**</bpt>table<ept id=\"p1\">**</ept> is a data set that has a schema and is rectangular.",
      "pos": [
        1575,
        1652
      ]
    },
    {
      "content": "After creating a linked service in a data store that refers to a data store, you define data sets that represent input/output data that is stored in the data store.",
      "pos": [
        1653,
        1817
      ]
    },
    {
      "content": "Pipeline",
      "pos": [
        1823,
        1831
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>pipeline<ept id=\"p1\">**</ept> in an Azure data factory processes data in linked storage services by using linked compute services.",
      "pos": [
        1832,
        1947
      ]
    },
    {
      "content": "It contains a sequence of activities where each activity performing a specific processing operation.",
      "pos": [
        1948,
        2048
      ]
    },
    {
      "content": "For example, a <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> copies data from a source storage to a destination storage and <bpt id=\"p2\">**</bpt>HDInsight Activity<ept id=\"p2\">**</ept> use an Azure HDInsight cluster to process data using Hive queries or Pig scripts.",
      "pos": [
        2049,
        2249
      ]
    },
    {
      "content": "A data factory can have one or more pipelines.",
      "pos": [
        2250,
        2296
      ]
    },
    {
      "content": "Typical steps for creating an Azure Data Factory instance are:",
      "pos": [
        2299,
        2361
      ]
    },
    {
      "pos": [
        2366,
        2392
      ],
      "content": "Create a <bpt id=\"p1\">**</bpt>data factory<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        2396,
        2463
      ],
      "content": "Create a <bpt id=\"p1\">**</bpt>linked service<ept id=\"p1\">**</ept> for each data store or compute service."
    },
    {
      "pos": [
        2467,
        2504
      ],
      "content": "Create input and output <bpt id=\"p1\">**</bpt>datasets<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        2508,
        2530
      ],
      "content": "Create a <bpt id=\"p1\">**</bpt>pipeline<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Activity",
      "pos": [
        2535,
        2543
      ]
    },
    {
      "content": "A data processing step in a pipeline that takes one or more input datasets and produces one or more output datasets.",
      "pos": [
        2544,
        2660
      ]
    },
    {
      "content": "Activities run in an execution environment (for example: Azure HDInsight cluster) and read/write data to a data store associated/linked with the Azure data factory.",
      "pos": [
        2662,
        2826
      ]
    },
    {
      "content": "Azure Data Factory service supports the following activities in a pipeline:",
      "pos": [
        2829,
        2904
      ]
    },
    {
      "pos": [
        2909,
        2983
      ],
      "content": "<bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> copies the data from a data store to another data store."
    },
    {
      "pos": [
        2988,
        3100
      ],
      "content": "<bpt id=\"p1\">**</bpt>HDInsight Activity<ept id=\"p1\">**</ept> processes data by running Hive/Pig scripts or MapReduce programs on an HDInsight cluster."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure Machine Learning Batch Scoring Activity<ept id=\"p1\">**</ept> invokes the Azure Machine Learning batch scoring API.",
      "pos": [
        3105,
        3208
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning<ept id=\"p1\">][azure-ml-adf]</ept> for details.",
      "pos": [
        3209,
        3321
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Stored Procedure Activity<ept id=\"p1\">**</ept> invokes a stored procedure in an Azure SQL Database.",
      "pos": [
        3325,
        3407
      ]
    },
    {
      "content": "See the <bpt id=\"p1\">[</bpt>Stored Procedure Activity<ept id=\"p1\">][msdn-stored-procedure-activity]</ept> on MSDN Library for details.",
      "pos": [
        3408,
        3504
      ]
    },
    {
      "content": "Slice",
      "pos": [
        3511,
        3516
      ]
    },
    {
      "content": "A table in an Azure data factory is composed of slices.",
      "pos": [
        3517,
        3572
      ]
    },
    {
      "content": "The width of a slice is determined by the schedule – hourly/daily.",
      "pos": [
        3573,
        3639
      ]
    },
    {
      "content": "When the schedule is “hourly”, a slice is produced hourly with in the start time and end time of a pipeline.",
      "pos": [
        3640,
        3748
      ]
    },
    {
      "content": "For example, if the pipeline start date-time is 03/03/2015 06:00:00 (6 AM) and end date-time is 03-03/2015 09:00:00 (9 AM on the same day), three data slices are produced, a slice for each 1 hour interval: 6-7 AM, 7-8 AM, and 8-9 AM.",
      "pos": [
        3749,
        3982
      ]
    },
    {
      "content": "Slices provide the ability to work with a subset of overall data for a specific time window (for example: the slice that is produced for the duration (hour): 1:00 PM to 2:00 PM).",
      "pos": [
        3988,
        4166
      ]
    },
    {
      "content": "Activity run for a slice",
      "pos": [
        4172,
        4196
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>run<ept id=\"p1\">**</ept> or an activity run is a unit of processing for a slice.",
      "pos": [
        4197,
        4264
      ]
    },
    {
      "content": "There could be one or more runs for a slice in case of retries or if you rerun your slice in case of a failure.",
      "pos": [
        4265,
        4376
      ]
    },
    {
      "content": "A slice is identified by its start time.",
      "pos": [
        4377,
        4417
      ]
    },
    {
      "content": "Data Management Gateway",
      "pos": [
        4421,
        4444
      ]
    },
    {
      "content": "Microsoft <bpt id=\"p1\">**</bpt>Data Management Gateway<ept id=\"p1\">**</ept> is software that connects on-premises data sources to cloud services for consumption.",
      "pos": [
        4445,
        4568
      ]
    },
    {
      "content": "You must have at least one gateway installed in your corporate environment and register it with the Azure Data Factory portal before adding on-premises data sources as linked services.",
      "pos": [
        4569,
        4753
      ]
    },
    {
      "content": "Data Hub",
      "pos": [
        4758,
        4766
      ]
    },
    {
      "content": "A <bpt id=\"p1\">**</bpt>Data Hub<ept id=\"p1\">**</ept> is a logical container for data storage and compute services.",
      "pos": [
        4767,
        4843
      ]
    },
    {
      "content": "For example, a Hadoop cluster with HDFS as storage and Hive/Pig as compute (processing) is a data hub.",
      "pos": [
        4844,
        4946
      ]
    },
    {
      "content": "Similarly, an enterprise data warehouse (EDW) can be modeled as a data hub (database as storage, stored procedures and/or ETL tool as compute services).",
      "pos": [
        4947,
        5099
      ]
    },
    {
      "content": "Pipelines use data stores and run on compute resources in a data hub.",
      "pos": [
        5101,
        5170
      ]
    },
    {
      "content": "Only HDInsight hub is supported at this moment.",
      "pos": [
        5171,
        5218
      ]
    },
    {
      "content": "The Data Hub allows a data factory to be divided into logical or domain specific groupings, such as the “West US Azure Hub” which manages all of the linked services (data stores and compute) and pipelines focused in the West US data center, or the “Sales EDW Hub” which manages all the linked services and pipelines concerned with populating and processing data for the Sales Enterprise Data Warehouse.",
      "pos": [
        5220,
        5622
      ]
    },
    {
      "content": "An important characteristic of Hub is that a pipeline runs on a single hub.",
      "pos": [
        5624,
        5699
      ]
    },
    {
      "content": "This means that when defining a pipeline, all of the linked services referenced by tables or activities within that pipeline must have the same hub name as the pipeline itself.",
      "pos": [
        5700,
        5876
      ]
    },
    {
      "content": "If the HubName property is not specified for a linked service, the linked service is placed in the “Default” Hub.",
      "pos": [
        5877,
        5990
      ]
    },
    {
      "content": "See Also",
      "pos": [
        5995,
        6003
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">][adf-intro]</ept>.",
      "pos": [
        6008,
        6056
      ]
    },
    {
      "content": "This article provides an overview of the Azure Data Factory service, the value it provides, and the scenarios it supports.",
      "pos": [
        6057,
        6179
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Get started with Data Factory<ept id=\"p1\">][datafactory-getstarted]</ept>.",
      "pos": [
        6183,
        6239
      ]
    },
    {
      "content": "This article provides an end-to-end tutorial that shows you how to create a sample Azure data factory that copies data from an Azure blob to an Azure SQL database.",
      "pos": [
        6240,
        6403
      ]
    },
    {
      "content": "test",
      "pos": [
        8201,
        8205
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Azure Data Factory - Terminology\" \n    description=\"This article introduces you to the terminology used in creating data factories using the Azure Data Factory service\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/25/2015\" \n    ms.author=\"spelluru\"/>\n\n#Azure Data Factory - Terminology\n\n## Data factory\nAn **Azure data factory** has one or more pipelines that process data in linked data stores (Azure Storage, Azure SQL Database, on-premises SQL Server etc...) by using linked compute services such as Azure HDInsight. An Azure data factory itself does not contain data within it; The data is rather stored in data stores mentioned above.  \n\n## Linked Service\nA **linked service** is a service that is linked to an Azure data factory. A linked service can be one of the following:\n\n- A **data storage** service such as Azure Storage, Azure SQL Database or on-premises SQL Server database. A data store is a container of input/output data sets.    \n- A **compute** service such as Azure HDInsight, Azure Machine Learning, and Azure Batch. A compute service process the input data and produces the output data.  \n\n## Data set\nA **data set** is a named view of data. The data being described can vary from simple bytes, semi-structured data like CSV files all the way up to relational tables or even models. A  Data Factory **table** is a data set that has a schema and is rectangular. After creating a linked service in a data store that refers to a data store, you define data sets that represent input/output data that is stored in the data store. \n\n\n##Pipeline\nA **pipeline** in an Azure data factory processes data in linked storage services by using linked compute services. It contains a sequence of activities where each activity performing a specific processing operation. For example, a **Copy Activity** copies data from a source storage to a destination storage and **HDInsight Activity** use an Azure HDInsight cluster to process data using Hive queries or Pig scripts. A data factory can have one or more pipelines. \n\nTypical steps for creating an Azure Data Factory instance are:\n\n1. Create a **data factory**.\n2. Create a **linked service** for each data store or compute service.\n3. Create input and output **datasets**.\n4. Create a **pipeline**. \n\n##Activity\nA data processing step in a pipeline that takes one or more input datasets and produces one or more output datasets.  Activities run in an execution environment (for example: Azure HDInsight cluster) and read/write data to a data store associated/linked with the Azure data factory. \n\nAzure Data Factory service supports the following activities in a pipeline: \n\n- **Copy Activity** copies the data from a data store to another data store.  \n- **HDInsight Activity** processes data by running Hive/Pig scripts or MapReduce programs on an HDInsight cluster.  \n- **Azure Machine Learning Batch Scoring Activity** invokes the Azure Machine Learning batch scoring API. See [Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning][azure-ml-adf] for details. \n- **Stored Procedure Activity** invokes a stored procedure in an Azure SQL Database. See the [Stored Procedure Activity][msdn-stored-procedure-activity] on MSDN Library for details.   \n\n##Slice\nA table in an Azure data factory is composed of slices. The width of a slice is determined by the schedule – hourly/daily. When the schedule is “hourly”, a slice is produced hourly with in the start time and end time of a pipeline. For example, if the pipeline start date-time is 03/03/2015 06:00:00 (6 AM) and end date-time is 03-03/2015 09:00:00 (9 AM on the same day), three data slices are produced, a slice for each 1 hour interval: 6-7 AM, 7-8 AM, and 8-9 AM.    \n\nSlices provide the ability to work with a subset of overall data for a specific time window (for example: the slice that is produced for the duration (hour): 1:00 PM to 2:00 PM). \n\n## Activity run for a slice\nThe **run** or an activity run is a unit of processing for a slice. There could be one or more runs for a slice in case of retries or if you rerun your slice in case of a failure. A slice is identified by its start time.\n\n##Data Management Gateway\nMicrosoft **Data Management Gateway** is software that connects on-premises data sources to cloud services for consumption. You must have at least one gateway installed in your corporate environment and register it with the Azure Data Factory portal before adding on-premises data sources as linked services.\n \n##Data Hub\nA **Data Hub** is a logical container for data storage and compute services. For example, a Hadoop cluster with HDFS as storage and Hive/Pig as compute (processing) is a data hub. Similarly, an enterprise data warehouse (EDW) can be modeled as a data hub (database as storage, stored procedures and/or ETL tool as compute services).  Pipelines use data stores and run on compute resources in a data hub. Only HDInsight hub is supported at this moment.\n\nThe Data Hub allows a data factory to be divided into logical or domain specific groupings, such as the “West US Azure Hub” which manages all of the linked services (data stores and compute) and pipelines focused in the West US data center, or the “Sales EDW Hub” which manages all the linked services and pipelines concerned with populating and processing data for the Sales Enterprise Data Warehouse.\n\nAn important characteristic of Hub is that a pipeline runs on a single hub. This means that when defining a pipeline, all of the linked services referenced by tables or activities within that pipeline must have the same hub name as the pipeline itself. If the HubName property is not specified for a linked service, the linked service is placed in the “Default” Hub.\n\n## See Also\n\n1. [Introduction to Azure Data Factory][adf-intro]. This article provides an overview of the Azure Data Factory service, the value it provides, and the scenarios it supports.\n2. [Get started with Data Factory][datafactory-getstarted]. This article provides an end-to-end tutorial that shows you how to create a sample Azure data factory that copies data from an Azure blob to an Azure SQL database.\n\n\n[Power-Query-Azure-Table]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azuretable-storage-HA104122607.aspx\n[Power-Query-Azure-Blob]: http://office.microsoft.com/en-001/excel-help/connect-to-microsoft-azure-blob-storage-HA104113447.aspx\n[Power-Query-Azure-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-microsoft-azure-sql-database-HA104019809.aspx\n[Power-Query-OnPrem-SQL]: http://office.microsoft.com/en-001/excel-help/connect-to-a-sql-server-database-HA104019808.aspx\n\n[adf-faq]: data-factory-faq.md\n[adf-intro]: data-factory-introduction.md\n[azure-ml-adf]: data-factory-create-predictive-pipelines.md\n[adf-common-scenarios]: data-factory-common-scenarios.md\n[create-factory-using-dotnet-sdk]: data-factory-create-data-factories-programmatically.md\n[data-factory-editor]: data-factory-editor.md\n[create-data-factory-using-powershell]: data-factory-monitor-manage-using-powershell.md\n\n[adf-powershell-reference]: https://msdn.microsoft.com/library/dn820234.aspx \n\n\n[msdn-stored-procedure-activity]: https://msdn.microsoft.com/library/dn912649.aspx\n[msdn-class-library-reference]: https://msdn.microsoft.com/library/dn883654.aspx\n[msdn-rest-api-reference]: https://msdn.microsoft.com/library/dn906738.aspx\n\n[adf-tutorial]: data-factory-tutorial.md\n[datafactory-getstarted]: data-factory-get-started.md\n\n[image-data-factory-introduction-traditional-ETL]: ./media/data-factory-introduction/TraditionalETL.PNG\n\n[image-data-factory-introduction-todays-diverse-processing-landspace]:./media/data-factory-introduction/TodaysDiverseDataProcessingLandscape.PNG\n\n[image-data-factory-application-model]:./media/data-factory-introduction/DataFactoryApplicationModel.png\n\n[image-data-factory-data-flow]:./media/data-factory-introduction/DataFactoryDataFlow.png\n\n\n\n \ntest\n"
}