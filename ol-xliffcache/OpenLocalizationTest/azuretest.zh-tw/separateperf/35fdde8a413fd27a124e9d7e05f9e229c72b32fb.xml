{
  "nodes": [
    {
      "content": "Move data to and from Azure Blob | Azure Data Factory",
      "pos": [
        28,
        81
      ]
    },
    {
      "content": "Learn how to move data to/from Azure Blob Storage using Azure Data Factory",
      "pos": [
        101,
        175
      ]
    },
    {
      "content": "Move data to and from Azure Blob using Azure Data Factory",
      "pos": [
        503,
        560
      ]
    },
    {
      "content": "This article outlines how you can use the Copy Activity in an Azure data factory to move data to Azure Blob from another data store and move data from another data store to Azure Blob.",
      "pos": [
        561,
        745
      ]
    },
    {
      "content": "This article builds on the <bpt id=\"p1\">[</bpt>data movement activities<ept id=\"p1\">](data-factory-data-movement-activities.md)</ept> article which presents a general overview of data movement with the copy activity and the supported data store combinations.",
      "pos": [
        746,
        966
      ]
    },
    {
      "content": "Sample: Copy data from Azure Blob to Azure SQL",
      "pos": [
        971,
        1017
      ]
    },
    {
      "content": "The sample below shows:",
      "pos": [
        1018,
        1041
      ]
    },
    {
      "pos": [
        1047,
        1164
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>AzureSqlDatabase<ept id=\"p1\">](data-factory-azure-sql-connector.md#azure-sql-linked-service-properties)</ept>."
    },
    {
      "pos": [
        1169,
        1251
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>AzureStorage<ept id=\"p1\">](#azure-storage-linked-service-properties)</ept>."
    },
    {
      "pos": [
        1256,
        1365
      ],
      "content": "An input <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>AzureBlob<ept id=\"p2\">](#azure-blob-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        1370,
        1518
      ],
      "content": "An output <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>AzureSqlTable<ept id=\"p2\">](data-factory-azure-sql-connector.md#azure-sql-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        1523,
        1748
      ],
      "content": "A <bpt id=\"p1\">[</bpt>pipeline<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> with a Copy activity that uses <bpt id=\"p2\">[</bpt>BlobSource<ept id=\"p2\">](#azure-blob-copy-activity-type-properties)</ept> and <bpt id=\"p3\">[</bpt>SqlSink<ept id=\"p3\">](data-factory-azure-sql-connector.md#azure-sql-copy-activity-type-properties)</ept>."
    },
    {
      "content": "The sample copies data belonging to a time series from an Azure blob to a table in an Azure SQL database every hour.",
      "pos": [
        1750,
        1866
      ]
    },
    {
      "content": "The JSON properties used in these samples are described in sections following the samples.",
      "pos": [
        1867,
        1957
      ]
    },
    {
      "content": "Azure SQL linked service:",
      "pos": [
        1962,
        1987
      ]
    },
    {
      "content": "Azure Storage linked service:",
      "pos": [
        2367,
        2396
      ]
    },
    {
      "content": "Azure Blob input dataset:",
      "pos": [
        2666,
        2691
      ]
    },
    {
      "content": "Data is picked up from a new blob every hour (frequency: hour, interval: 1).",
      "pos": [
        2695,
        2771
      ]
    },
    {
      "content": "The folder path and file name for the blob are dynamically evaluated based on the start time of the slice that is being processed.",
      "pos": [
        2772,
        2902
      ]
    },
    {
      "content": "The folder path uses year, month, and day part of the start time and file name uses the hour part of the start time.",
      "pos": [
        2903,
        3019
      ]
    },
    {
      "content": "“external”: “true” setting informs the Data Factory service that this table is external to the data factory and not produced by an activity in the data factory.",
      "pos": [
        3020,
        3180
      ]
    },
    {
      "content": "Azure SQL output dataset:",
      "pos": [
        4805,
        4830
      ]
    },
    {
      "content": "The sample copies data to a table named “MyTable” in an Azure SQL database.",
      "pos": [
        4834,
        4909
      ]
    },
    {
      "content": "You should create the table in your Azure SQL database with the same number of columns as you expect the Blob CSV file to contain.",
      "pos": [
        4910,
        5040
      ]
    },
    {
      "content": "New rows are added to the table every hour.",
      "pos": [
        5041,
        5084
      ]
    },
    {
      "content": "Pipeline with a Copy activity:",
      "pos": [
        5419,
        5449
      ]
    },
    {
      "content": "The pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour.",
      "pos": [
        5453,
        5588
      ]
    },
    {
      "content": "In the pipeline JSON definition, the <bpt id=\"p1\">**</bpt>source<ept id=\"p1\">**</ept> type is set to <bpt id=\"p2\">**</bpt>BlobSource<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>sink<ept id=\"p3\">**</ept> type is set to <bpt id=\"p4\">**</bpt>SqlSink<ept id=\"p4\">**</ept>.",
      "pos": [
        5589,
        5707
      ]
    },
    {
      "content": "Sample: Copy data from Azure SQL to Azure Blob",
      "pos": [
        6857,
        6903
      ]
    },
    {
      "content": "The sample below shows:",
      "pos": [
        6904,
        6927
      ]
    },
    {
      "pos": [
        6933,
        7050
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>AzureSqlDatabase<ept id=\"p1\">](data-factory-azure-sql-connector.md#azure-sql-linked-service-properties)</ept>."
    },
    {
      "pos": [
        7055,
        7137
      ],
      "content": "A linked service of type <bpt id=\"p1\">[</bpt>AzureStorage<ept id=\"p1\">](#azure-storage-linked-service-properties)</ept>."
    },
    {
      "pos": [
        7142,
        7289
      ],
      "content": "An input <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>AzureSqlTable<ept id=\"p2\">](data-factory-azure-sql-connector.md#azure-sql-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        7294,
        7404
      ],
      "content": "An output <bpt id=\"p1\">[</bpt>dataset<ept id=\"p1\">](data-factory-create-datasets.md)</ept> of type <bpt id=\"p2\">[</bpt>AzureBlob<ept id=\"p2\">](#azure-blob-dataset-type-properties)</ept>."
    },
    {
      "pos": [
        7409,
        7632
      ],
      "content": "A <bpt id=\"p1\">[</bpt>pipeline<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> with Copy activity that uses <bpt id=\"p2\">[</bpt>SqlSource<ept id=\"p2\">](data-factory-azure-sql-connector.md#azure-sql-copy-activity-type-properties)</ept> and <bpt id=\"p3\">[</bpt>BlobSink<ept id=\"p3\">](#azure-blob-copy-activity-type-properties)</ept>."
    },
    {
      "content": "The sample copies data belonging to a time series from a table in Azure SQL database to a blob every hour.",
      "pos": [
        7635,
        7741
      ]
    },
    {
      "content": "The JSON properties used in these samples are described in sections following the samples.",
      "pos": [
        7742,
        7832
      ]
    },
    {
      "content": "Azure SQL linked service:",
      "pos": [
        7837,
        7862
      ]
    },
    {
      "content": "Azure Storage linked service:",
      "pos": [
        8242,
        8271
      ]
    },
    {
      "content": "Azure SQL input dataset:",
      "pos": [
        8541,
        8565
      ]
    },
    {
      "content": "The sample assumes you have created a table “MyTable” in Azure SQL and it contains a column called “timestampcolumn” for time series data.",
      "pos": [
        8569,
        8707
      ]
    },
    {
      "content": "Setting “external”: ”true” and specifying externalData policy informs the Azure Data Factory service that this is a table that is external to the data factory and not produced by an activity in the data factory.",
      "pos": [
        8710,
        8921
      ]
    },
    {
      "content": "Azure Blob output dataset:",
      "pos": [
        9457,
        9483
      ]
    },
    {
      "content": "Data is written to a new blob every hour (frequency: hour, interval: 1).",
      "pos": [
        9487,
        9559
      ]
    },
    {
      "content": "The folder path for the blob is dynamically evaluated based on the start time of the slice that is being processed.",
      "pos": [
        9560,
        9675
      ]
    },
    {
      "content": "The folder path uses year, month, day, and hours parts of the start time.",
      "pos": [
        9676,
        9749
      ]
    },
    {
      "content": "Pipeline with the Copy activity:",
      "pos": [
        11151,
        11183
      ]
    },
    {
      "content": "The pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour.",
      "pos": [
        11187,
        11322
      ]
    },
    {
      "content": "In the pipeline JSON definition, the <bpt id=\"p1\">**</bpt>source<ept id=\"p1\">**</ept> type is set to <bpt id=\"p2\">**</bpt>SqlSource<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>sink<ept id=\"p3\">**</ept> type is set to <bpt id=\"p4\">**</bpt>BlobSink<ept id=\"p4\">**</ept>.",
      "pos": [
        11323,
        11441
      ]
    },
    {
      "content": "The SQL query specified for the <bpt id=\"p1\">**</bpt>SqlReaderQuery<ept id=\"p1\">**</ept> property selects the data in the past hour to copy.",
      "pos": [
        11442,
        11544
      ]
    },
    {
      "content": "Azure Storage Linked Service properties",
      "pos": [
        13170,
        13209
      ]
    },
    {
      "content": "You can link an Azure storage account to an Azure data factory using an Azure Storage linked service.",
      "pos": [
        13211,
        13312
      ]
    },
    {
      "content": "The following table provides description for JSON elements specific to Azure Storage linked service.",
      "pos": [
        13313,
        13413
      ]
    },
    {
      "content": "Property",
      "pos": [
        13417,
        13425
      ]
    },
    {
      "content": "Description",
      "pos": [
        13428,
        13439
      ]
    },
    {
      "content": "Required",
      "pos": [
        13442,
        13450
      ]
    },
    {
      "content": "type",
      "pos": [
        13493,
        13497
      ]
    },
    {
      "pos": [
        13500,
        13550
      ],
      "content": "The type property must be set to: <bpt id=\"p1\">**</bpt>AzureStorage<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Yes",
      "pos": [
        13553,
        13556
      ]
    },
    {
      "content": "connectionString",
      "pos": [
        13561,
        13577
      ]
    },
    {
      "content": "Specify information needed to connect to Azure storage for the connectionString property.",
      "pos": [
        13580,
        13669
      ]
    },
    {
      "content": "You can get the connectionString for the Azure storage from the Azure Portal.",
      "pos": [
        13670,
        13747
      ]
    },
    {
      "content": "Yes",
      "pos": [
        13750,
        13753
      ]
    },
    {
      "content": "Azure Blob Dataset type properties",
      "pos": [
        13760,
        13794
      ]
    },
    {
      "content": "For a full list of JSON sections &amp; properties available for defining datasets, see the <bpt id=\"p1\">[</bpt>Creating datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept> article.",
      "pos": [
        13796,
        13944
      ]
    },
    {
      "content": "Sections like structure, availability, and policy of a dataset JSON are similar for all dataset types (Azure SQL, Azure blob, Azure table, etc...).",
      "pos": [
        13945,
        14092
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>typeProperties<ept id=\"p1\">**</ept> section is different for each type of dataset and provides information about the location, format etc. of the data in the data store.",
      "pos": [
        14094,
        14250
      ]
    },
    {
      "content": "The typeProperties section for dataset of type <bpt id=\"p1\">**</bpt>AzureBlob<ept id=\"p1\">**</ept> dataset has the following properties.",
      "pos": [
        14251,
        14349
      ]
    },
    {
      "content": "Property",
      "pos": [
        14353,
        14361
      ]
    },
    {
      "content": "Description",
      "pos": [
        14364,
        14375
      ]
    },
    {
      "content": "Required",
      "pos": [
        14378,
        14386
      ]
    },
    {
      "content": "folderPath",
      "pos": [
        14430,
        14440
      ]
    },
    {
      "content": "Path to the container and folder in the blob storage.",
      "pos": [
        14443,
        14496
      ]
    },
    {
      "content": "Example: myblobcontainer\\myblobfolder\\",
      "pos": [
        14497,
        14535
      ]
    },
    {
      "content": "Yes",
      "pos": [
        14538,
        14541
      ]
    },
    {
      "content": "fileName",
      "pos": [
        14546,
        14554
      ]
    },
    {
      "content": "Name of the blob.",
      "pos": [
        14560,
        14577
      ]
    },
    {
      "content": "fileName is optional.",
      "pos": [
        14578,
        14599
      ]
    },
    {
      "content": "If you specify a filename, the activity (including Copy) works on the specific Blob.",
      "pos": [
        14607,
        14691
      ]
    },
    {
      "content": "When fileName is not specified Copy will include all Blobs in the folderPath for input dataset.",
      "pos": [
        14698,
        14793
      ]
    },
    {
      "content": "When fileName is not specified for an output dataset, the name of the generated file would be in the following this format: Data.",
      "pos": [
        14800,
        14929
      ]
    },
    {
      "content": ".txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt",
      "pos": [
        14935,
        15001
      ]
    },
    {
      "content": "No",
      "pos": [
        15008,
        15010
      ]
    },
    {
      "content": "partitionedBy",
      "pos": [
        15015,
        15028
      ]
    },
    {
      "content": "partitionedBy is an optional property.",
      "pos": [
        15031,
        15069
      ]
    },
    {
      "content": "You can use it to specify a dynamic folderPath and filename for time series data.",
      "pos": [
        15070,
        15151
      ]
    },
    {
      "content": "For example, folderPath can be parameterized for every hour of data.",
      "pos": [
        15152,
        15220
      ]
    },
    {
      "content": "See the Leverage partitionedBy prperty section below for details and examples.",
      "pos": [
        15221,
        15299
      ]
    },
    {
      "content": "No",
      "pos": [
        15302,
        15304
      ]
    },
    {
      "content": "format",
      "pos": [
        15307,
        15313
      ]
    },
    {
      "content": "Two formats types are supported: <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>AvroFormat<ept id=\"p2\">**</ept>.",
      "pos": [
        15316,
        15380
      ]
    },
    {
      "content": "You need to set the type property under format to either of these values.",
      "pos": [
        15381,
        15454
      ]
    },
    {
      "content": "When the format is TextFormat you can specify additional optional properties for format.",
      "pos": [
        15455,
        15543
      ]
    },
    {
      "content": "See the <bpt id=\"p1\">[</bpt>Specifying TextFormat<ept id=\"p1\">](#specifying-textformat)</ept> section below for more details.",
      "pos": [
        15544,
        15631
      ]
    },
    {
      "content": "No",
      "pos": [
        15634,
        15636
      ]
    },
    {
      "content": "compression",
      "pos": [
        15639,
        15650
      ]
    },
    {
      "content": "Specify the type and level of compression for the data.",
      "pos": [
        15653,
        15708
      ]
    },
    {
      "content": "Supported types are: GZip, Deflate, and BZip2 and supported levels are: Optimal and Fastest.",
      "pos": [
        15709,
        15801
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Compression support<ept id=\"p1\">](#compression-support)</ept> section for more details.",
      "pos": [
        15802,
        15875
      ]
    },
    {
      "content": "No",
      "pos": [
        15879,
        15881
      ]
    },
    {
      "content": "Leveraging partitionedBy property",
      "pos": [
        15889,
        15922
      ]
    },
    {
      "pos": [
        15923,
        16179
      ],
      "content": "As mentioned above, you can specify a dynamic folderPath and filename for time series data with the <bpt id=\"p1\">**</bpt>partitionedBy<ept id=\"p1\">**</ept> section, Data Factory macros and the system variables: SliceStart and SliceEnd, which indicate start and end times for a given data slice."
    },
    {
      "pos": [
        16181,
        16392
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Creating Datasets<ept id=\"p1\">](data-factory-create-datasets.md)</ept> and <bpt id=\"p2\">[</bpt>Scheduling &amp; Execution<ept id=\"p2\">](data-factory-scheduling-and-execution.md)</ept> articles to understand more details on time series datasets, scheduling and slices."
    },
    {
      "content": "Sample 1",
      "pos": [
        16399,
        16407
      ]
    },
    {
      "content": "In the above example {Slice} is replaced with the value of Data Factory system variable SliceStart in the format (YYYYMMDDHH) specified.",
      "pos": [
        16616,
        16752
      ]
    },
    {
      "content": "The SliceStart refers to start time of the slice.",
      "pos": [
        16753,
        16802
      ]
    },
    {
      "content": "The folderPath is different for each slice.",
      "pos": [
        16803,
        16846
      ]
    },
    {
      "content": "For example: wikidatagateway/wikisampledataout/2014100103 or wikidatagateway/wikisampledataout/2014100104",
      "pos": [
        16847,
        16952
      ]
    },
    {
      "content": "Sample 2",
      "pos": [
        16959,
        16967
      ]
    },
    {
      "content": "In the above example, year, month, day, and time of SliceStart are extracted into separate variables that are used by folderPath and fileName properties.",
      "pos": [
        17512,
        17665
      ]
    },
    {
      "content": "Specifying TextFormat",
      "pos": [
        17671,
        17692
      ]
    },
    {
      "pos": [
        17694,
        17814
      ],
      "content": "If the format is set to <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>, you can specify the following <bpt id=\"p2\">**</bpt>optional<ept id=\"p2\">**</ept> properties in the <bpt id=\"p3\">**</bpt>Format<ept id=\"p3\">**</ept> section."
    },
    {
      "content": "Property",
      "pos": [
        17818,
        17826
      ]
    },
    {
      "content": "Description",
      "pos": [
        17829,
        17840
      ]
    },
    {
      "content": "Required",
      "pos": [
        17843,
        17851
      ]
    },
    {
      "content": "columnDelimiter",
      "pos": [
        17894,
        17909
      ]
    },
    {
      "content": "The character(s) used as a column separator in a file.This tag is optional.",
      "pos": [
        17912,
        17987
      ]
    },
    {
      "content": "The default value is comma (,).",
      "pos": [
        17988,
        18019
      ]
    },
    {
      "content": "No",
      "pos": [
        18022,
        18024
      ]
    },
    {
      "content": "rowDelimiter",
      "pos": [
        18029,
        18041
      ]
    },
    {
      "content": "The character(s) used as a raw separator in file.",
      "pos": [
        18044,
        18093
      ]
    },
    {
      "content": "This tag is optional.",
      "pos": [
        18094,
        18115
      ]
    },
    {
      "content": "The default value is any of the following: [“\\r\\n”, “\\r”,” \\n”].",
      "pos": [
        18116,
        18180
      ]
    },
    {
      "content": "No",
      "pos": [
        18183,
        18185
      ]
    },
    {
      "content": "escapeChar",
      "pos": [
        18190,
        18200
      ]
    },
    {
      "content": "The special character used to escape column delimiter shown in content.",
      "pos": [
        18206,
        18277
      ]
    },
    {
      "content": "This tag is optional.",
      "pos": [
        18278,
        18299
      ]
    },
    {
      "content": "No default value.",
      "pos": [
        18300,
        18317
      ]
    },
    {
      "content": "You must specify no more than one character for this property.",
      "pos": [
        18318,
        18380
      ]
    },
    {
      "content": "For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: “Hello, world”), you can define ‘$’ as the escape character and use string “Hello$, world” in the source.",
      "pos": [
        18387,
        18607
      ]
    },
    {
      "content": "Note that you cannot specify both escapeChar and quoteChar for a table.",
      "pos": [
        18614,
        18685
      ]
    },
    {
      "content": "No",
      "pos": [
        18692,
        18694
      ]
    },
    {
      "content": "quoteChar",
      "pos": [
        18700,
        18709
      ]
    },
    {
      "content": "The special character is used to quote the string value.",
      "pos": [
        18715,
        18771
      ]
    },
    {
      "content": "The column and row delimiters inside of the quote characters would be treated as part of the string value.",
      "pos": [
        18772,
        18878
      ]
    },
    {
      "content": "This tag is optional.",
      "pos": [
        18879,
        18900
      ]
    },
    {
      "content": "No default value.",
      "pos": [
        18901,
        18918
      ]
    },
    {
      "content": "You must specify no more than one character for this property.",
      "pos": [
        18919,
        18981
      ]
    },
    {
      "content": "For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: &lt;Hello, world&gt;), you can define ‘\"’ as the quote character and use string &lt;\"Hello, world\"&gt; in the source.",
      "pos": [
        18988,
        19208
      ]
    },
    {
      "content": "This property is applicable to both input and output tables.",
      "pos": [
        19209,
        19269
      ]
    },
    {
      "content": "Note that you cannot specify both escapeChar and quoteChar for a table.",
      "pos": [
        19276,
        19347
      ]
    },
    {
      "content": "No",
      "pos": [
        19354,
        19356
      ]
    },
    {
      "content": "nullValue",
      "pos": [
        19361,
        19370
      ]
    },
    {
      "content": "The character(s) used to represent null value in blob file content.",
      "pos": [
        19376,
        19443
      ]
    },
    {
      "content": "This tag is optional.",
      "pos": [
        19444,
        19465
      ]
    },
    {
      "content": "The default value is “\\N”.",
      "pos": [
        19466,
        19492
      ]
    },
    {
      "content": "For example, based on above sample, “NaN” in blob will be translated as null value while copied into e.g. SQL Server.",
      "pos": [
        19499,
        19616
      ]
    },
    {
      "content": "No",
      "pos": [
        19623,
        19625
      ]
    },
    {
      "content": "encodingName",
      "pos": [
        19630,
        19642
      ]
    },
    {
      "content": "Specify the encoding name.",
      "pos": [
        19645,
        19671
      ]
    },
    {
      "content": "For the list of valid encoding names, see: <bpt id=\"p1\">[</bpt>Encoding.EncodingName Property<ept id=\"p1\">](https://msdn.microsoft.com/library/system.text.encoding.aspx)</ept>.",
      "pos": [
        19672,
        19810
      ]
    },
    {
      "content": "For example: windows-1250 or shift_jis.",
      "pos": [
        19811,
        19850
      ]
    },
    {
      "content": "The default value is: UTF-8.",
      "pos": [
        19851,
        19879
      ]
    },
    {
      "content": "No",
      "pos": [
        19882,
        19884
      ]
    },
    {
      "content": "Samples",
      "pos": [
        19894,
        19901
      ]
    },
    {
      "content": "The following sample shows some of the format properties for TextFormat.",
      "pos": [
        19902,
        19974
      ]
    },
    {
      "content": "To use an escapeChar instead of quoteChar, replace the line with quoteChar with the following:",
      "pos": [
        20294,
        20388
      ]
    },
    {
      "content": "Specifying AvroFormat",
      "pos": [
        20418,
        20439
      ]
    },
    {
      "content": "If the format is set to AvroFormat, you do not need to specify any properties in the Format section within the typeProperties section.",
      "pos": [
        20440,
        20574
      ]
    },
    {
      "content": "Example:",
      "pos": [
        20575,
        20583
      ]
    },
    {
      "pos": [
        20642,
        20780
      ],
      "content": "To use Avro format in a Hive table, you can refer to <bpt id=\"p1\">[</bpt>Apache Hive’s tutorial<ept id=\"p1\">](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe)</ept>."
    },
    {
      "content": "Azure Blob Copy Activity type properties",
      "pos": [
        20874,
        20914
      ]
    },
    {
      "content": "For a full list of sections &amp; properties available for defining activities, see the <bpt id=\"p1\">[</bpt>Creating Pipelines<ept id=\"p1\">](data-factory-create-pipelines.md)</ept> article.",
      "pos": [
        20917,
        21064
      ]
    },
    {
      "content": "Properties like name, description, input and output tables, various policies etc are available for all types of activities.",
      "pos": [
        21065,
        21188
      ]
    },
    {
      "content": "Properties available in the typeProperties section of the activity on the other hand vary with each activity type and in case of Copy activity they vary depending on the types of sources and sinks",
      "pos": [
        21190,
        21386
      ]
    },
    {
      "pos": [
        21388,
        21471
      ],
      "content": "<bpt id=\"p1\">**</bpt>BlobSource<ept id=\"p1\">**</ept> supports the following properties in the <bpt id=\"p2\">**</bpt>typeProperties<ept id=\"p2\">**</ept> section:"
    },
    {
      "content": "Property",
      "pos": [
        21475,
        21483
      ]
    },
    {
      "content": "Description",
      "pos": [
        21486,
        21497
      ]
    },
    {
      "content": "Allowed values",
      "pos": [
        21500,
        21514
      ]
    },
    {
      "content": "Required",
      "pos": [
        21517,
        21525
      ]
    },
    {
      "content": "treatEmptyAsNull",
      "pos": [
        21586,
        21602
      ]
    },
    {
      "content": "Specifies whether to treat null or empty string as null value.",
      "pos": [
        21605,
        21667
      ]
    },
    {
      "content": "TRUE",
      "pos": [
        21670,
        21674
      ]
    },
    {
      "content": "FALSE",
      "pos": [
        21679,
        21684
      ]
    },
    {
      "content": "No",
      "pos": [
        21687,
        21689
      ]
    },
    {
      "content": "skipHeaderLineCount",
      "pos": [
        21694,
        21713
      ]
    },
    {
      "content": "Indicate how many lines need be skipped.",
      "pos": [
        21716,
        21756
      ]
    },
    {
      "content": "It is applicable only when input dataset is using <bpt id=\"p1\">**</bpt>TextFormat<ept id=\"p1\">**</ept>.",
      "pos": [
        21757,
        21822
      ]
    },
    {
      "content": "Integer from 0 to Max.",
      "pos": [
        21825,
        21847
      ]
    },
    {
      "content": "No",
      "pos": [
        21850,
        21852
      ]
    },
    {
      "pos": [
        21858,
        21932
      ],
      "content": "<bpt id=\"p1\">**</bpt>BlobSink<ept id=\"p1\">**</ept> supports the following properties <bpt id=\"p2\">**</bpt>typeProperties<ept id=\"p2\">**</ept> section:"
    },
    {
      "content": "Property",
      "pos": [
        21936,
        21944
      ]
    },
    {
      "content": "Description",
      "pos": [
        21947,
        21958
      ]
    },
    {
      "content": "Allowed values",
      "pos": [
        21961,
        21975
      ]
    },
    {
      "content": "Required",
      "pos": [
        21978,
        21986
      ]
    },
    {
      "content": "blobWriterAddHeader",
      "pos": [
        22046,
        22065
      ]
    },
    {
      "content": "Specifies whether to add header of column definitions.",
      "pos": [
        22068,
        22122
      ]
    },
    {
      "content": "TRUE",
      "pos": [
        22125,
        22129
      ]
    },
    {
      "content": "FALSE (default)",
      "pos": [
        22134,
        22149
      ]
    },
    {
      "content": "No",
      "pos": [
        22152,
        22154
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        22502,
        22515
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        22516,
        22573
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-azure-blob-connector.md)</ept>.",
      "pos": [
        22574,
        22718
      ]
    },
    {
      "content": "test",
      "pos": [
        22732,
        22736
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Move data to and from Azure Blob | Azure Data Factory\" \n    description=\"Learn how to move data to/from Azure Blob Storage using Azure Data Factory\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/26/2015\" \n    ms.author=\"spelluru\"/>\n\n# Move data to and from Azure Blob using Azure Data Factory\nThis article outlines how you can use the Copy Activity in an Azure data factory to move data to Azure Blob from another data store and move data from another data store to Azure Blob. This article builds on the [data movement activities](data-factory-data-movement-activities.md) article which presents a general overview of data movement with the copy activity and the supported data store combinations.\n\n## Sample: Copy data from Azure Blob to Azure SQL\nThe sample below shows:\n\n1.  A linked service of type [AzureSqlDatabase](data-factory-azure-sql-connector.md#azure-sql-linked-service-properties).\n2.  A linked service of type [AzureStorage](#azure-storage-linked-service-properties).\n3.  An input [dataset](data-factory-create-datasets.md) of type [AzureBlob](#azure-blob-dataset-type-properties).\n4.  An output [dataset](data-factory-create-datasets.md) of type [AzureSqlTable](data-factory-azure-sql-connector.md#azure-sql-dataset-type-properties).\n4.  A [pipeline](data-factory-create-pipelines.md) with a Copy activity that uses [BlobSource](#azure-blob-copy-activity-type-properties) and [SqlSink](data-factory-azure-sql-connector.md#azure-sql-copy-activity-type-properties).\n\nThe sample copies data belonging to a time series from an Azure blob to a table in an Azure SQL database every hour. The JSON properties used in these samples are described in sections following the samples. \n\n**Azure SQL linked service:**\n\n    {\n      \"name\": \"AzureSqlLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureSqlDatabase\",\n        \"typeProperties\": {\n          \"connectionString\": \"Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30\"\n        }\n      }\n    }\n\n**Azure Storage linked service:**\n\n    {\n      \"name\": \"StorageLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureStorage\",\n        \"typeProperties\": {\n          \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n        }\n      }\n    }\n\n**Azure Blob input dataset:**\n\nData is picked up from a new blob every hour (frequency: hour, interval: 1). The folder path and file name for the blob are dynamically evaluated based on the start time of the slice that is being processed. The folder path uses year, month, and day part of the start time and file name uses the hour part of the start time. “external”: “true” setting informs the Data Factory service that this table is external to the data factory and not produced by an activity in the data factory.\n\n    {\n      \"name\": \"AzureBlobInput\",\n      \"properties\": {\n        \"type\": \"AzureBlob\",\n        \"linkedServiceName\": \"StorageLinkedService\",\n        \"typeProperties\": {\n          \"folderPath\": \"mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}\",\n          \"fileName\": \"{Hour}.csv\",\n          \"partitionedBy\": [\n            {\n              \"name\": \"Year\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"yyyy\"\n              }\n            },\n            {\n              \"name\": \"Month\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%M\"\n              }\n            },\n            {\n              \"name\": \"Day\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%d\"\n              }\n            },\n            {\n              \"name\": \"Hour\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%H\"\n              }\n            }\n          ],\n          \"format\": {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \",\",\n            \"rowDelimiter\": \"\\n\"\n          }\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n**Azure SQL output dataset:**\n\nThe sample copies data to a table named “MyTable” in an Azure SQL database. You should create the table in your Azure SQL database with the same number of columns as you expect the Blob CSV file to contain. New rows are added to the table every hour.\n\n    {\n      \"name\": \"AzureSqlOutput\",\n      \"properties\": {\n        \"type\": \"AzureSqlTable\",\n        \"linkedServiceName\": \"AzureSqlLinkedService\",\n        \"typeProperties\": {\n          \"tableName\": \"MyOutputTable\"\n        },\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        }\n      }\n    }\n\n**Pipeline with a Copy activity:**\n\nThe pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour. In the pipeline JSON definition, the **source** type is set to **BlobSource** and **sink** type is set to **SqlSink**. \n\n    {  \n        \"name\":\"SamplePipeline\",\n        \"properties\":{  \n        \"start\":\"2014-06-01T18:00:00\",\n        \"end\":\"2014-06-01T19:00:00\",\n        \"description\":\"pipeline with copy activity\",\n        \"activities\":[  \n          {\n            \"name\": \"AzureBlobtoSQL\",\n            \"description\": \"Copy Activity\",\n            \"type\": \"Copy\",\n            \"inputs\": [\n              {\n                \"name\": \"AzureBlobInput\"\n              }\n            ],\n            \"outputs\": [\n              {\n                \"name\": \"AzureSqlOutput\"\n              }\n            ],\n            \"typeProperties\": {\n              \"source\": {\n                \"type\": \"BlobSource\",\n                \"blobColumnSeparators\": \",\"\n              },\n              \"sink\": {\n                \"type\": \"SqlSink\"\n              }\n            },\n           \"scheduler\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            },\n            \"policy\": {\n              \"concurrency\": 1,\n              \"executionPriorityOrder\": \"OldestFirst\",\n              \"retry\": 0,\n              \"timeout\": \"01:00:00\"\n            }\n          }\n          ]\n       }\n    }\n\n## Sample: Copy data from Azure SQL to Azure Blob\nThe sample below shows:\n\n1.  A linked service of type [AzureSqlDatabase](data-factory-azure-sql-connector.md#azure-sql-linked-service-properties).\n2.  A linked service of type [AzureStorage](#azure-storage-linked-service-properties).\n3.  An input [dataset](data-factory-create-datasets.md) of type [AzureSqlTable](data-factory-azure-sql-connector.md#azure-sql-dataset-type-properties).\n4.  An output [dataset](data-factory-create-datasets.md) of type [AzureBlob](#azure-blob-dataset-type-properties).\n4.  A [pipeline](data-factory-create-pipelines.md) with Copy activity that uses [SqlSource](data-factory-azure-sql-connector.md#azure-sql-copy-activity-type-properties) and [BlobSink](#azure-blob-copy-activity-type-properties).\n\n\nThe sample copies data belonging to a time series from a table in Azure SQL database to a blob every hour. The JSON properties used in these samples are described in sections following the samples. \n\n**Azure SQL linked service:**\n\n    {\n      \"name\": \"AzureSqlLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureSqlDatabase\",\n        \"typeProperties\": {\n          \"connectionString\": \"Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30\"\n        }\n      }\n    }\n\n**Azure Storage linked service:**\n\n    {\n      \"name\": \"StorageLinkedService\",\n      \"properties\": {\n        \"type\": \"AzureStorage\",\n        \"typeProperties\": {\n          \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n        }\n      }\n    }\n\n**Azure SQL input dataset:**\n\nThe sample assumes you have created a table “MyTable” in Azure SQL and it contains a column called “timestampcolumn” for time series data. \n\nSetting “external”: ”true” and specifying externalData policy informs the Azure Data Factory service that this is a table that is external to the data factory and not produced by an activity in the data factory.\n\n    {\n      \"name\": \"AzureSqlInput\",\n      \"properties\": {\n        \"type\": \"AzureSqlTable\",\n        \"linkedServiceName\": \"AzureSqlLinkedService\",\n        \"typeProperties\": {\n          \"tableName\": \"MyTable\"\n        },\n        \"external\": true,\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        },\n        \"policy\": {\n          \"externalData\": {\n            \"retryInterval\": \"00:01:00\",\n            \"retryTimeout\": \"00:10:00\",\n            \"maximumRetry\": 3\n          }\n        }\n      }\n    }\n\n**Azure Blob output dataset:**\n\nData is written to a new blob every hour (frequency: hour, interval: 1). The folder path for the blob is dynamically evaluated based on the start time of the slice that is being processed. The folder path uses year, month, day, and hours parts of the start time. \n    \n    {\n      \"name\": \"AzureBlobOutput\",\n      \"properties\": {\n        \"type\": \"AzureBlob\",\n        \"linkedServiceName\": \"StorageLinkedService\",\n        \"typeProperties\": {\n          \"folderPath\": \"mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}/hourno={Hour}\",\n          \"partitionedBy\": [\n            {\n              \"name\": \"Year\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"yyyy\"\n              }\n            },\n            {\n              \"name\": \"Month\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%M\"\n              }\n            },\n            {\n              \"name\": \"Day\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%d\"\n              }\n            },\n            {\n              \"name\": \"Hour\",\n              \"value\": {\n                \"type\": \"DateTime\",\n                \"date\": \"SliceStart\",\n                \"format\": \"%H\"\n              }\n            }\n          ],\n          \"format\": {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \"\\t\",\n            \"rowDelimiter\": \"\\n\"\n          }\n        },\n        \"availability\": {\n          \"frequency\": \"Hour\",\n          \"interval\": 1\n        }\n      }\n    }\n\n**Pipeline with the Copy activity:**\n\nThe pipeline contains a Copy Activity that is configured to use the above input and output datasets and is scheduled to run every hour. In the pipeline JSON definition, the **source** type is set to **SqlSource** and **sink** type is set to **BlobSink**. The SQL query specified for the **SqlReaderQuery** property selects the data in the past hour to copy.\n\n\n    {  \n        \"name\":\"SamplePipeline\",\n        \"properties\":{  \n            \"start\":\"2014-06-01T18:00:00\",\n            \"end\":\"2014-06-01T19:00:00\",\n            \"description\":\"pipeline for copy activity\",\n            \"activities\":[  \n                {\n                    \"name\": \"AzureSQLtoBlob\",\n                    \"description\": \"copy activity\",\n                    \"type\": \"Copy\",\n                    \"inputs\": [\n                      {\n                        \"name\": \"AzureSQLInput\"\n                      }\n                    ],\n                    \"outputs\": [\n                      {\n                        \"name\": \"AzureBlobOutput\"\n                      }\n                    ],\n                    \"typeProperties\": {\n                        \"source\": {\n                            \"type\": \"SqlSource\",\n                            \"SqlReaderQuery\": \"$$Text.Format('select * from MyTable where timestampcolumn >= \\\\'{0:yyyy-MM-dd HH:mm}\\\\' AND timestampcolumn < \\\\'{1:yyyy-MM-dd HH:mm}\\\\'', WindowStart, WindowEnd)\"\n                        },\n                        \"sink\": {\n                            \"type\": \"BlobSink\"\n                        }\n                    },\n                    \"scheduler\": {\n                        \"frequency\": \"Hour\",\n                        \"interval\": 1\n                    },\n                    \"policy\": {\n                        \"concurrency\": 1,\n                        \"executionPriorityOrder\": \"OldestFirst\",\n                        \"retry\": 0,\n                        \"timeout\": \"01:00:00\"\n                    }\n                }\n             ]\n        }\n    }\n\n## Azure Storage Linked Service properties\n\nYou can link an Azure storage account to an Azure data factory using an Azure Storage linked service. The following table provides description for JSON elements specific to Azure Storage linked service.\n\n| Property | Description | Required |\n| -------- | ----------- | -------- |\n| type | The type property must be set to: **AzureStorage** | Yes |\n| connectionString | Specify information needed to connect to Azure storage for the connectionString property. You can get the connectionString for the Azure storage from the Azure Portal. | Yes |\n\n## Azure Blob Dataset type properties\n\nFor a full list of JSON sections & properties available for defining datasets, see the [Creating datasets](data-factory-create-datasets.md) article. Sections like structure, availability, and policy of a dataset JSON are similar for all dataset types (Azure SQL, Azure blob, Azure table, etc...).\n\nThe **typeProperties** section is different for each type of dataset and provides information about the location, format etc. of the data in the data store. The typeProperties section for dataset of type **AzureBlob** dataset has the following properties.\n\n| Property | Description | Required |\n| -------- | ----------- | -------- | \n| folderPath | Path to the container and folder in the blob storage. Example: myblobcontainer\\myblobfolder\\ | Yes |\n| fileName | <p>Name of the blob. fileName is optional. </p><p>If you specify a filename, the activity (including Copy) works on the specific Blob.</p><p>When fileName is not specified Copy will include all Blobs in the folderPath for input dataset.</p><p>When fileName is not specified for an output dataset, the name of the generated file would be in the following this format: Data.<Guid>.txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt</p> | No |\n| partitionedBy | partitionedBy is an optional property. You can use it to specify a dynamic folderPath and filename for time series data. For example, folderPath can be parameterized for every hour of data. See the Leverage partitionedBy prperty section below for details and examples. | No\n| format | Two formats types are supported: **TextFormat**, **AvroFormat**. You need to set the type property under format to either of these values. When the format is TextFormat you can specify additional optional properties for format. See the [Specifying TextFormat](#specifying-textformat) section below for more details. | No\n| compression | Specify the type and level of compression for the data. Supported types are: GZip, Deflate, and BZip2 and supported levels are: Optimal and Fastest. See [Compression support](#compression-support) section for more details.  | No |\n\n### Leveraging partitionedBy property\nAs mentioned above, you can specify a dynamic folderPath and filename for time series data with the **partitionedBy** section, Data Factory macros and the system variables: SliceStart and SliceEnd, which indicate start and end times for a given data slice.\n\nSee [Creating Datasets](data-factory-create-datasets.md) and [Scheduling & Execution](data-factory-scheduling-and-execution.md) articles to understand more details on time series datasets, scheduling and slices.\n\n#### Sample 1\n\n    \"folderPath\": \"wikidatagateway/wikisampledataout/{Slice}\",\n    \"partitionedBy\": \n    [\n        { \"name\": \"Slice\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyyMMddHH\" } },\n    ],\n\nIn the above example {Slice} is replaced with the value of Data Factory system variable SliceStart in the format (YYYYMMDDHH) specified. The SliceStart refers to start time of the slice. The folderPath is different for each slice. For example: wikidatagateway/wikisampledataout/2014100103 or wikidatagateway/wikisampledataout/2014100104\n\n#### Sample 2\n\n    \"folderPath\": \"wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}\",\n    \"fileName\": \"{Hour}.csv\",\n    \"partitionedBy\": \n     [\n        { \"name\": \"Year\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyy\" } },\n        { \"name\": \"Month\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"MM\" } }, \n        { \"name\": \"Day\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"dd\" } }, \n        { \"name\": \"Hour\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"hh\" } } \n    ],\n\nIn the above example, year, month, day, and time of SliceStart are extracted into separate variables that are used by folderPath and fileName properties.\n\n### Specifying TextFormat\n\nIf the format is set to **TextFormat**, you can specify the following **optional** properties in the **Format** section.\n\n| Property | Description | Required |\n| -------- | ----------- | -------- |\n| columnDelimiter | The character(s) used as a column separator in a file.This tag is optional. The default value is comma (,). | No |\n| rowDelimiter | The character(s) used as a raw separator in file. This tag is optional. The default value is any of the following: [“\\r\\n”, “\\r”,” \\n”]. | No |\n| escapeChar | <p>The special character used to escape column delimiter shown in content. This tag is optional. No default value. You must specify no more than one character for this property.</p><p>For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: “Hello, world”), you can define ‘$’ as the escape character and use string “Hello$, world” in the source.</p><p>Note that you cannot specify both escapeChar and quoteChar for a table.</p> | No | \n| quoteChar | <p>The special character is used to quote the string value. The column and row delimiters inside of the quote characters would be treated as part of the string value. This tag is optional. No default value. You must specify no more than one character for this property.</p><p>For example, if you have comma (,) as the column delimiter but you want have comma character in the text (example: <Hello, world>), you can define ‘\"’ as the quote character and use string <\"Hello, world\"> in the source. This property is applicable to both input and output tables.</p><p>Note that you cannot specify both escapeChar and quoteChar for a table.</p> | No |\n| nullValue | <p>The character(s) used to represent null value in blob file content. This tag is optional. The default value is “\\N”.</p><p>For example, based on above sample, “NaN” in blob will be translated as null value while copied into e.g. SQL Server.</p> | No |\n| encodingName | Specify the encoding name. For the list of valid encoding names, see: [Encoding.EncodingName Property](https://msdn.microsoft.com/library/system.text.encoding.aspx). For example: windows-1250 or shift_jis. The default value is: UTF-8. | No | \n\n#### Samples\nThe following sample shows some of the format properties for TextFormat.\n\n    \"typeProperties\":\n    {\n        \"folderPath\": \"mycontainer/myfolder\",\n        \"fileName\": \"myblobname\"\n        \"format\":\n        {\n            \"type\": \"TextFormat\",\n            \"columnDelimiter\": \",\",\n            \"rowDelimiter\": \";\",\n            \"quoteChar\": \"\\\"\",\n            \"NullValue\": \"NaN\"\n        }\n    },\n\nTo use an escapeChar instead of quoteChar, replace the line with quoteChar with the following:\n\n    \"escapeChar\": \"$\",\n\n### Specifying AvroFormat\nIf the format is set to AvroFormat, you do not need to specify any properties in the Format section within the typeProperties section. Example:\n\n    \"format\":\n    {\n        \"type\": \"AvroFormat\",\n    }\n\nTo use Avro format in a Hive table, you can refer to [Apache Hive’s tutorial](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe).\n\n[AZURE.INCLUDE [data-factory-compression](../../includes/data-factory-compression.md)]\n\n\n## Azure Blob Copy Activity type properties  \nFor a full list of sections & properties available for defining activities, see the [Creating Pipelines](data-factory-create-pipelines.md) article. Properties like name, description, input and output tables, various policies etc are available for all types of activities.\n\nProperties available in the typeProperties section of the activity on the other hand vary with each activity type and in case of Copy activity they vary depending on the types of sources and sinks\n\n**BlobSource** supports the following properties in the **typeProperties** section:\n\n| Property | Description | Allowed values | Required |\n| -------- | ----------- | -------------- | -------- | \n| treatEmptyAsNull | Specifies whether to treat null or empty string as null value. | TRUE<br/>FALSE | No |\n| skipHeaderLineCount | Indicate how many lines need be skipped. It is applicable only when input dataset is using **TextFormat**. | Integer from 0 to Max. | No | \n\n\n**BlobSink** supports the following properties **typeProperties** section:\n\n| Property | Description | Allowed values | Required |\n| -------- | ----------- | -------------- | -------- |\n| blobWriterAddHeader | Specifies whether to add header of column definitions. | TRUE<br/>FALSE (default) | No |\n\n\n[AZURE.INCLUDE [data-factory-structure-for-rectangualr-datasets](../../includes/data-factory-structure-for-rectangualr-datasets.md)]\n\n[AZURE.INCLUDE [data-factory-type-conversion-sample](../../includes/data-factory-type-conversion-sample.md)]\n\n[AZURE.INCLUDE [data-factory-column-mapping](../../includes/data-factory-column-mapping.md)]\n\n\n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-azure-blob-connector.md).\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest\n"
}