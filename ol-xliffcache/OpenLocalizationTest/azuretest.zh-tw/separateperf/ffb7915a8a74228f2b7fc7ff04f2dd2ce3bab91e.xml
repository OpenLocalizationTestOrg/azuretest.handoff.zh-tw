{
  "nodes": [
    {
      "content": "Create predictive pipelines using Azure Machine Learning Bach Execution activity | Microsoft Azure",
      "pos": [
        28,
        126
      ]
    },
    {
      "content": "Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning",
      "pos": [
        146,
        249
      ]
    },
    {
      "content": "Create predictive pipelines using Azure Machine Learning Batch Execution activity",
      "pos": [
        577,
        658
      ]
    },
    {
      "content": "Overview",
      "pos": [
        665,
        673
      ]
    },
    {
      "pos": [
        677,
        904
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> See <bpt id=\"p1\">[</bpt>Introduction to Azure Data Factory<ept id=\"p1\">](data-factory-introduction.md)</ept> and <bpt id=\"p2\">[</bpt>Build your first pipeline<ept id=\"p2\">](data-factory-build-your-first-pipeline.md)</ept> articles to quickly get started with the Azure Data Factory service."
    },
    {
      "content": "Azure Data Factory enables you to easily create pipelines that leverage a published <bpt id=\"p1\">[</bpt>Azure Machine Learning<ept id=\"p1\">][azure-machine-learning]</ept> web service for predictive analytics.",
      "pos": [
        906,
        1076
      ]
    },
    {
      "content": "Using Azure Data Factory, you can make use of big data pipelines (e.g. Pig and Hive) to process the data that you have ingested from various data sources, and use the Azure Machine Learning web services to make predictions on the data in batch.",
      "pos": [
        1077,
        1321
      ]
    },
    {
      "content": "You use Azure Data Factory to orchestrate  data movement and processing, and then perform batch execution using Azure Machine Learning.",
      "pos": [
        1324,
        1459
      ]
    },
    {
      "content": "To achieve this, you will need to do the following:",
      "pos": [
        1460,
        1511
      ]
    },
    {
      "content": "Create an Azure Machne Learning linked service.",
      "pos": [
        1516,
        1563
      ]
    },
    {
      "content": "You will require the following:",
      "pos": [
        1564,
        1595
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Request URI<ept id=\"p1\">**</ept> for the Batch Execution API.",
      "pos": [
        1603,
        1647
      ]
    },
    {
      "content": "You can find the Request URI by clicking on the <bpt id=\"p1\">**</bpt>BATCH EXECUTION<ept id=\"p1\">**</ept> link in the web services page (shown below).",
      "pos": [
        1648,
        1760
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>API key<ept id=\"p1\">**</ept> for the published Azure Machine Learning web service.",
      "pos": [
        1768,
        1833
      ]
    },
    {
      "content": "You can find the API key by clicking on the web service that you have published.",
      "pos": [
        1834,
        1914
      ]
    },
    {
      "pos": [
        1920,
        1963
      ],
      "content": "Use the <bpt id=\"p1\">**</bpt>AzureMLBatchExecution<ept id=\"p1\">**</ept> activity."
    },
    {
      "content": "Machine Learning Dashboard",
      "pos": [
        1971,
        1997
      ]
    },
    {
      "content": "Batch URI",
      "pos": [
        2083,
        2092
      ]
    },
    {
      "content": "Scenario: Experiments using Web service inputs/outputs that refer to data in Azure Blob Storage",
      "pos": [
        2169,
        2264
      ]
    },
    {
      "content": "In this scenario, the Azure Machine Learning Web service makes predictions using data from a file in an Azure blob storage and stores the prediction results in the blob storage.",
      "pos": [
        2265,
        2442
      ]
    },
    {
      "content": "The following JSON defines a Azure Data Factory pipeline with an AzureMLBatchExecution activity.",
      "pos": [
        2443,
        2539
      ]
    },
    {
      "content": "The activity has the dataset <bpt id=\"p1\">**</bpt>DecisionTreeInputBlob<ept id=\"p1\">**</ept> as input and <bpt id=\"p2\">**</bpt>DecisionTreeResultBlob<ept id=\"p2\">**</ept> as the output.",
      "pos": [
        2540,
        2649
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>DecisionTreeInputBlob<ept id=\"p1\">**</ept> is passed as an input to the Web service by using the <bpt id=\"p2\">**</bpt>webServiceInput<ept id=\"p2\">**</ept> JSON property and <bpt id=\"p3\">**</bpt>DecisionTreeResultBlob<ept id=\"p3\">**</ept> as an output to the Web service by using the <bpt id=\"p4\">**</bpt>webServiceOutputs<ept id=\"p4\">**</ept> JSON property.",
      "pos": [
        2650,
        2880
      ]
    },
    {
      "content": "Only datasets that are inputs/outputs for the activity can be passed as Web service inputs and outputs.",
      "pos": [
        2881,
        2984
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Only inputs and outputs of the AzureMLBatchExecution activity can be passed as parameters to the Web service.",
      "pos": [
        4134,
        4256
      ]
    },
    {
      "content": "For example, in the above JSON snippet, DecisionTreeInputBlob is an input to the AzureMLBatchExecution activity, which is passed as an input to the Web service via webServiceInput parameter.",
      "pos": [
        4257,
        4447
      ]
    },
    {
      "content": "Example",
      "pos": [
        4456,
        4463
      ]
    },
    {
      "content": "This example uses Azure Storage to hold both the input and output data.",
      "pos": [
        4465,
        4536
      ]
    },
    {
      "pos": [
        4539,
        4810
      ],
      "content": "We recommend that you go through the <bpt id=\"p1\">[</bpt>Build your first pipeline with Data Factory<ept id=\"p1\">][adf-build-1st-pipeline]</ept> tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, datasets, pipeline) in this example."
    },
    {
      "content": "Create a <bpt id=\"p1\">**</bpt>linked service<ept id=\"p1\">**</ept> for your <bpt id=\"p2\">**</bpt>Azure Storage<ept id=\"p2\">**</ept>.",
      "pos": [
        4820,
        4875
      ]
    },
    {
      "content": "If the input and output files will be in different storage accounts, you will need two linked services.",
      "pos": [
        4876,
        4979
      ]
    },
    {
      "content": "Here is a JSON example:",
      "pos": [
        4980,
        5003
      ]
    },
    {
      "content": "Create the <bpt id=\"p1\">**</bpt>input<ept id=\"p1\">**</ept> Azure Data Factory <bpt id=\"p2\">**</bpt>dataset<ept id=\"p2\">**</ept>.",
      "pos": [
        5302,
        5354
      ]
    },
    {
      "content": "Note that unlike some other Data Factory datasets, these must both contain both <bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>fileName<ept id=\"p2\">**</ept> values.",
      "pos": [
        5355,
        5474
      ]
    },
    {
      "content": "You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files.",
      "pos": [
        5475,
        5600
      ]
    },
    {
      "content": "You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice.",
      "pos": [
        5601,
        5755
      ]
    },
    {
      "content": "In that case, you would not include the <bpt id=\"p1\">**</bpt>external<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>externalData<ept id=\"p2\">**</ept> settings shown in the example below, and your DecisionTreeInputBlob would be the output dataset of a different Activity.",
      "pos": [
        5756,
        5950
      ]
    },
    {
      "content": "Your input csv file must have the column header row.",
      "pos": [
        6752,
        6804
      ]
    },
    {
      "content": "If you are using the <bpt id=\"p1\">**</bpt>Copy Activity<ept id=\"p1\">**</ept> to create/move the csv into the blob storage, you should set the sink property <bpt id=\"p2\">**</bpt>blobWriterAddHeader<ept id=\"p2\">**</ept> to <bpt id=\"p3\">**</bpt>true<ept id=\"p3\">**</ept>.",
      "pos": [
        6805,
        6959
      ]
    },
    {
      "content": "For example:",
      "pos": [
        6960,
        6972
      ]
    },
    {
      "pos": [
        7106,
        7289
      ],
      "content": "If the csv file does not have the header row, you may see the following error: <bpt id=\"p1\">**</bpt>Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Create the <bpt id=\"p1\">**</bpt>output<ept id=\"p1\">**</ept> Azure Data Factory <bpt id=\"p2\">**</bpt>dataset<ept id=\"p2\">**</ept>.",
      "pos": [
        7293,
        7346
      ]
    },
    {
      "content": "This example uses partitioning to create a unique output path for each slice execution.",
      "pos": [
        7347,
        7434
      ]
    },
    {
      "content": "Without this, the activity would overwrite the file.",
      "pos": [
        7435,
        7487
      ]
    },
    {
      "pos": [
        8618,
        8733
      ],
      "content": "Create a <bpt id=\"p1\">**</bpt>linked service<ept id=\"p1\">**</ept> of type: <bpt id=\"p2\">**</bpt>AzureMLLinkedService<ept id=\"p2\">**</ept>, providing the API key and model batch execution URL."
    },
    {
      "content": "Finally, author a pipeline containing an <bpt id=\"p1\">**</bpt>AzureMLBatchExecution<ept id=\"p1\">**</ept> Activity.",
      "pos": [
        9031,
        9107
      ]
    },
    {
      "content": "It will get the location of the input file from your input datasets, call the Azure Machine Learning batch execution API, and copy the batch execution output to the blob given in your output dataset.",
      "pos": [
        9108,
        9307
      ]
    },
    {
      "pos": [
        9316,
        9413
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> AzureMLBatchExecution activity can have zero or more inputs and one or more outputs."
    },
    {
      "content": "Both <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> datetimes must be in <bpt id=\"p3\">[</bpt>ISO format<ept id=\"p3\">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.",
      "pos": [
        10716,
        10816
      ]
    },
    {
      "content": "For example: 2014-10-14T16:32:41Z.",
      "pos": [
        10817,
        10851
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> time is optional.",
      "pos": [
        10852,
        10881
      ]
    },
    {
      "content": "If you do not specify value for the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> property, it is calculated as \"<bpt id=\"p2\">**</bpt>start + 48 hours<ept id=\"p2\">**</ept>\".",
      "pos": [
        10882,
        10979
      ]
    },
    {
      "content": "To run the pipeline indefinitely, specify <bpt id=\"p1\">**</bpt>9999-09-09<ept id=\"p1\">**</ept> as the value for the <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> property.",
      "pos": [
        10980,
        11075
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.",
      "pos": [
        11076,
        11191
      ]
    },
    {
      "pos": [
        11199,
        11280
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Specifying input for the AzureMLBatchExecution activity is optional."
    },
    {
      "content": "Scenario: Experiments using Reader/Writer Modules to refer to data in various storages",
      "pos": [
        11286,
        11372
      ]
    },
    {
      "content": "Another common scenario when creating Azure ML experiments is to use Reader and Writer modules.",
      "pos": [
        11374,
        11469
      ]
    },
    {
      "content": "The reader module is used to load data into an experiment and the writer module is to save data from your experiments.",
      "pos": [
        11470,
        11588
      ]
    },
    {
      "content": "For details about reader and writer modules, see <bpt id=\"p1\">[</bpt>Reader<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn905997.aspx)</ept> and <bpt id=\"p2\">[</bpt>Writer<ept id=\"p2\">](https://msdn.microsoft.com/library/azure/dn905984.aspx)</ept> topics on MSDN Library.",
      "pos": [
        11589,
        11795
      ]
    },
    {
      "content": "When using the reader and writer modules, it is good practice to use a Web service parameter for each property of these reader/writer modules.",
      "pos": [
        11802,
        11944
      ]
    },
    {
      "content": "These web parameters enable you to configure the values during runtime.",
      "pos": [
        11945,
        12016
      ]
    },
    {
      "content": "For example, you could create an experiment with a reader module that uses an Azure SQL Database: XXX.database.windows.net.",
      "pos": [
        12017,
        12140
      ]
    },
    {
      "content": "After the web service has been deployed, you want to enable the consumers of the web service to specify another Azure SQL Server called YYY.database.windows.net.",
      "pos": [
        12141,
        12302
      ]
    },
    {
      "content": "You can use a Web service parameter to allow this value to be configured.",
      "pos": [
        12303,
        12376
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Web service input and output are different from Web service parameters.",
      "pos": [
        12380,
        12464
      ]
    },
    {
      "content": "In the first scenario, you have seen how an input and output can be specified for an Azure ML Web service.",
      "pos": [
        12465,
        12571
      ]
    },
    {
      "content": "In this scenario, you will pass parameters for a Web service that correspond to properties of reader/writer modules.",
      "pos": [
        12572,
        12688
      ]
    },
    {
      "content": "Let's look at a scenario for using Web service parameters.",
      "pos": [
        12691,
        12749
      ]
    },
    {
      "content": "You have a deployed Azure Machine Learning web service that is using a reader module to read data from one of the Azure Machine Learning supported data sources (for example: Azure SQL Database).",
      "pos": [
        12750,
        12944
      ]
    },
    {
      "content": "After the batch execution is performed, the results are written using a Writer module (Azure SQL Database).",
      "pos": [
        12945,
        13052
      ]
    },
    {
      "content": "No web service inputs and outputs are defined in the experiments.",
      "pos": [
        13054,
        13119
      ]
    },
    {
      "content": "In this case, we recommend that you setup the relevant Web service parameters for the reader and writer modules.",
      "pos": [
        13120,
        13232
      ]
    },
    {
      "content": "This will allow the reader/writer modules to be configured when using the AzureMLBatchExecution activity.",
      "pos": [
        13233,
        13338
      ]
    },
    {
      "content": "You specify Web service parameters in the <bpt id=\"p1\">**</bpt>globalParameters<ept id=\"p1\">**</ept> section in the activity JSON as follows.",
      "pos": [
        13339,
        13442
      ]
    },
    {
      "pos": [
        13584,
        13759
      ],
      "content": "You can also use <bpt id=\"p1\">[</bpt>Data Factory Functions<ept id=\"p1\">](https://msdn.microsoft.com/library/dn835056.aspx)</ept> in passing values for the Web service parameters as shown in the following example:"
    },
    {
      "pos": [
        13985,
        14145
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service."
    },
    {
      "content": "Using a Reader module to read data from multiple files in Azure Blob",
      "pos": [
        14152,
        14220
      ]
    },
    {
      "content": "Big data pipelines (Pig, Hive, etc...) can produce one or more output files with no extensions.",
      "pos": [
        14221,
        14316
      ]
    },
    {
      "content": "For example, when you specify an external Hive table, the data for the external Hive table can be stored in Azure blob storage with the following name 000000_0.",
      "pos": [
        14317,
        14477
      ]
    },
    {
      "content": "You can use the reader module in an experiment to read multiple files, and use them for predictions.",
      "pos": [
        14478,
        14578
      ]
    },
    {
      "content": "When using the reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input.",
      "pos": [
        14581,
        14690
      ]
    },
    {
      "content": "The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight.",
      "pos": [
        14691,
        14831
      ]
    },
    {
      "content": "The reader module allows you to read files (with no extensions) by configuring the <bpt id=\"p1\">**</bpt>Path to container, directory or blob<ept id=\"p1\">**</ept> property of the reader module to point to the container/folder that contains the files as shown below.",
      "pos": [
        14832,
        15058
      ]
    },
    {
      "content": "Note, the asterisk (i.e. \\*) <bpt id=\"p1\">**</bpt>specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\\*)<ept id=\"p1\">**</ept> will be read as part of the experiment.",
      "pos": [
        15059,
        15232
      ]
    },
    {
      "content": "Azure Blob properties",
      "pos": [
        15236,
        15257
      ]
    },
    {
      "content": "Example",
      "pos": [
        15341,
        15348
      ]
    },
    {
      "content": "Pipeline with AzureMLBatchExecution activity with Web Service Parameters",
      "pos": [
        15355,
        15427
      ]
    },
    {
      "content": "In the above JSON example:",
      "pos": [
        16873,
        16899
      ]
    },
    {
      "content": "The deployed Azure Machine Learning Web service uses a reader and a writer module to read/write data from/to an Azure SQL Database.",
      "pos": [
        16903,
        17034
      ]
    },
    {
      "content": "This Web service exposes the following four parameters:  Database server name, Database name, Server user account name, and Server user account password.",
      "pos": [
        17035,
        17188
      ]
    },
    {
      "content": "Both <bpt id=\"p1\">**</bpt>start<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> datetimes must be in <bpt id=\"p3\">[</bpt>ISO format<ept id=\"p3\">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.",
      "pos": [
        17193,
        17293
      ]
    },
    {
      "content": "For example: 2014-10-14T16:32:41Z.",
      "pos": [
        17294,
        17328
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> time is optional.",
      "pos": [
        17329,
        17358
      ]
    },
    {
      "content": "If you do not specify value for the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> property, it is calculated as \"<bpt id=\"p2\">**</bpt>start + 48 hours<ept id=\"p2\">**</ept>\".",
      "pos": [
        17359,
        17456
      ]
    },
    {
      "content": "To run the pipeline indefinitely, specify <bpt id=\"p1\">**</bpt>9999-09-09<ept id=\"p1\">**</ept> as the value for the <bpt id=\"p2\">**</bpt>end<ept id=\"p2\">**</ept> property.",
      "pos": [
        17457,
        17552
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>JSON Scripting Reference<ept id=\"p1\">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.",
      "pos": [
        17553,
        17668
      ]
    },
    {
      "content": "Frequently asked questions",
      "pos": [
        17675,
        17701
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Q:<ept id=\"p1\">**</ept> I am using the AzureMLBatchScoring activity.",
      "pos": [
        17703,
        17754
      ]
    },
    {
      "content": "Should I switch to using the AzureMLBatchExecution Activity?",
      "pos": [
        17755,
        17815
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>A:<ept id=\"p1\">**</ept> Yes.",
      "pos": [
        17817,
        17828
      ]
    },
    {
      "content": "If you are using the AzureMLBatchScoring activity to integrate with Azure Machine Learning, we recommend that you use the latest AzureMLBatchExecution activity.",
      "pos": [
        17829,
        17989
      ]
    },
    {
      "content": "We are deprecating the AzureMLBatchScoring activity, and it will be removed in a future release.",
      "pos": [
        17990,
        18086
      ]
    },
    {
      "content": "The AzureMLBatchExecution activity is introduced in the August 2015 release of Azure SDK and Azure PowerShell.",
      "pos": [
        18088,
        18198
      ]
    },
    {
      "content": "The AzureMLBatchExecution activity does not require an input (if input dependencies are not needed).",
      "pos": [
        18200,
        18300
      ]
    },
    {
      "content": "It also allows you to be explicit about whether you want to use the Web service input and Web service output or not use them.",
      "pos": [
        18301,
        18426
      ]
    },
    {
      "content": "If you do choose to use Web service input/output, it enables you to specify the relevant Azure Blob datasets to be used.In addition, it allows you to clearly specify the values for the web service parameters that are provided by the web service.",
      "pos": [
        18427,
        18672
      ]
    },
    {
      "pos": [
        18674,
        18853
      ],
      "content": "If you want to continue using the AzureMLBatchScoring activity, please refer to <bpt id=\"p1\">[</bpt>Azure ML Batch Scoring Activity<ept id=\"p1\">](data-factory-create-predictive-pipelines.md)</ept> article for details."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Q:<ept id=\"p1\">**</ept> I have multiple files that are generated by my big data pipelines.",
      "pos": [
        18856,
        18929
      ]
    },
    {
      "content": "Can I use the AzureMLBatchExecution Activity to work on all the files?",
      "pos": [
        18930,
        19000
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>A:<ept id=\"p1\">**</ept> Yes.",
      "pos": [
        19002,
        19013
      ]
    },
    {
      "content": "See the <bpt id=\"p1\">**</bpt>Using a Reader module to read data from multiple files in Azure Blob<ept id=\"p1\">**</ept> section for details.",
      "pos": [
        19014,
        19115
      ]
    },
    {
      "content": "test",
      "pos": [
        19279,
        19283
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Create predictive pipelines using Azure Machine Learning Bach Execution activity | Microsoft Azure\" \n    description=\"Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"08/24/2015\" \n    ms.author=\"spelluru\"/>\n\n# Create predictive pipelines using Azure Machine Learning Batch Execution activity   \n## Overview\n\n> [AZURE.NOTE] See [Introduction to Azure Data Factory](data-factory-introduction.md) and [Build your first pipeline](data-factory-build-your-first-pipeline.md) articles to quickly get started with the Azure Data Factory service.\n\nAzure Data Factory enables you to easily create pipelines that leverage a published [Azure Machine Learning][azure-machine-learning] web service for predictive analytics. Using Azure Data Factory, you can make use of big data pipelines (e.g. Pig and Hive) to process the data that you have ingested from various data sources, and use the Azure Machine Learning web services to make predictions on the data in batch. \n\nYou use Azure Data Factory to orchestrate  data movement and processing, and then perform batch execution using Azure Machine Learning. To achieve this, you will need to do the following:\n\n1. Create an Azure Machne Learning linked service. You will require the following:\n    1. **Request URI** for the Batch Execution API. You can find the Request URI by clicking on the **BATCH EXECUTION** link in the web services page (shown below).\n    1. **API key** for the published Azure Machine Learning web service. You can find the API key by clicking on the web service that you have published. \n 2. Use the **AzureMLBatchExecution** activity.\n\n    ![Machine Learning Dashboard](./media/data-factory-azure-ml-batch-execution-activity/AzureMLDashboard.png)\n\n    ![Batch URI](./media/data-factory-azure-ml-batch-execution-activity/batch-uri.png)\n\n\n## Scenario: Experiments using Web service inputs/outputs that refer to data in Azure Blob Storage\nIn this scenario, the Azure Machine Learning Web service makes predictions using data from a file in an Azure blob storage and stores the prediction results in the blob storage. The following JSON defines a Azure Data Factory pipeline with an AzureMLBatchExecution activity. The activity has the dataset **DecisionTreeInputBlob** as input and **DecisionTreeResultBlob** as the output. The **DecisionTreeInputBlob** is passed as an input to the Web service by using the **webServiceInput** JSON property and **DecisionTreeResultBlob** as an output to the Web service by using the **webServiceOutputs** JSON property. Only datasets that are inputs/outputs for the activity can be passed as Web service inputs and outputs.    \n\n\n    {\n      \"name\": \"PredictivePipeline\",\n      \"properties\": {\n        \"description\": \"use AzureML model\",\n        \"activities\": [\n          {\n            \"name\": \"MLActivity\",\n            \"type\": \"AzureMLBatchExecution\",\n            \"description\": \"prediction analysis on batch input\",\n            \"inputs\": [\n              {\n                \"name\": \"DecisionTreeInputBlob\"\n              }\n            ],\n            \"outputs\": [\n              {\n                \"name\": \"DecisionTreeResultBlob\"\n              }\n            ],\n            \"linkedServiceName\": \"MyAzureMLLinkedService\",\n            \"typeProperties\":\n            {\n                \"webServiceInput\": \"DecisionTreeInputBlob \",\n                \"webServiceOutputs\": {\n                    \"output1\": \"DecisionTreeResultBlob \"\n                }                \n            },\n            \"policy\": {\n              \"concurrency\": 3,\n              \"executionPriorityOrder\": \"NewestFirst\",\n              \"retry\": 1,\n              \"timeout\": \"02:00:00\"\n            }\n          }\n        ],\n        \"start\": \"2015-02-13T00:00:00Z\",\n        \"end\": \"2015-02-14T00:00:00Z\"\n      }\n    }\n\n> [AZURE.NOTE] Only inputs and outputs of the AzureMLBatchExecution activity can be passed as parameters to the Web service. For example, in the above JSON snippet, DecisionTreeInputBlob is an input to the AzureMLBatchExecution activity, which is passed as an input to the Web service via webServiceInput parameter.   \n\n### Example\n\nThis example uses Azure Storage to hold both the input and output data. \n\nWe recommend that you go through the [Build your first pipeline with Data Factory][adf-build-1st-pipeline] tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, datasets, pipeline) in this example.   \n \n\n1. Create a **linked service** for your **Azure Storage**. If the input and output files will be in different storage accounts, you will need two linked services. Here is a JSON example:\n\n        {\n          \"name\": \"StorageLinkedService\",\n          \"properties\": {\n            \"type\": \"AzureStorage\",\n            \"typeProperties\": {\n              \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=[acctName];AccountKey=[acctKey]\"\n            }\n          }\n        }\n\n2. Create the **input** Azure Data Factory **dataset**. Note that unlike some other Data Factory datasets, these must both contain both **folderPath** and **fileName** values. You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files. You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice. In that case, you would not include the **external** and **externalData** settings shown in the example below, and your DecisionTreeInputBlob would be the output dataset of a different Activity.\n\n        {\n          \"name\": \"DecisionTreeInputBlob\",\n          \"properties\": {\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"azuremltesting/input\",\n              \"fileName\": \"in.csv\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"external\": true,\n            \"availability\": {\n              \"frequency\": \"Day\",\n              \"interval\": 1\n            },\n            \"policy\": {\n              \"externalData\": {\n                \"retryInterval\": \"00:01:00\",\n                \"retryTimeout\": \"00:10:00\",\n                \"maximumRetry\": 3\n              }\n            }\n          }\n        }\n    \n    Your input csv file must have the column header row. If you are using the **Copy Activity** to create/move the csv into the blob storage, you should set the sink property **blobWriterAddHeader** to **true**. For example:\n    \n         sink: \n         {\n             \"type\": \"BlobSink\",     \n             \"blobWriterAddHeader\": true \n         }\n     \n    If the csv file does not have the header row, you may see the following error: **Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1**.\n3. Create the **output** Azure Data Factory **dataset**. This example uses partitioning to create a unique output path for each slice execution. Without this, the activity would overwrite the file.\n\n        {\n          \"name\": \"DecisionTreeResultBlob\",\n          \"properties\": {\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"azuremltesting/scored/{folderpart}/\",\n              \"fileName\": \"{filepart}result.csv\",\n              \"partitionedBy\": [\n                {\n                  \"name\": \"folderpart\",\n                  \"value\": {\n                    \"type\": \"DateTime\",\n                    \"date\": \"SliceStart\",\n                    \"format\": \"yyyyMMdd\"\n                  }\n                },\n                {\n                  \"name\": \"filepart\",\n                  \"value\": {\n                    \"type\": \"DateTime\",\n                    \"date\": \"SliceStart\",\n                    \"format\": \"HHmmss\"\n                  }\n                }\n              ],\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"availability\": {\n              \"frequency\": \"Day\",\n              \"interval\": 15\n            }\n          }\n        }\n\n4. Create a **linked service** of type: **AzureMLLinkedService**, providing the API key and model batch execution URL.\n        \n        {\n          \"name\": \"MyAzureMLLinkedService\",\n          \"properties\": {\n            \"type\": \"AzureML\",\n            \"typeProperties\": {\n              \"mlEndpoint\": \"https://[batch execution endpoint]/jobs\",\n              \"apiKey\": \"[apikey]\"\n            }\n          }\n        }\n5. Finally, author a pipeline containing an **AzureMLBatchExecution** Activity. It will get the location of the input file from your input datasets, call the Azure Machine Learning batch execution API, and copy the batch execution output to the blob given in your output dataset. \n\n    > [AZURE.NOTE] AzureMLBatchExecution activity can have zero or more inputs and one or more outputs.\n\n        {\n          \"name\": \"PredictivePipeline\",\n          \"properties\": {\n            \"description\": \"use AzureML model\",\n            \"activities\": [\n              {\n                \"name\": \"MLActivity\",\n                \"type\": \"AzureMLBatchExecution\",\n                \"description\": \"prediction analysis on batch input\",\n                \"inputs\": [\n                  {\n                    \"name\": \"DecisionTreeInputBlob\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"DecisionTreeResultBlob\"\n                  }\n                ],\n                \"linkedServiceName\": \"MyAzureMLLinkedService\",\n                \"typeProperties\":\n                {\n                    \"webServiceInput\": \"DecisionTreeInputBlob \",\n                    \"webServiceOutputs\": {\n                        \"output1\": \"DecisionTreeResultBlob \"\n                    }                \n                },\n                \"policy\": {\n                  \"concurrency\": 3,\n                  \"executionPriorityOrder\": \"NewestFirst\",\n                  \"retry\": 1,\n                  \"timeout\": \"02:00:00\"\n                }\n              }\n            ],\n            \"start\": \"2015-02-13T00:00:00Z\",\n            \"end\": \"2015-02-14T00:00:00Z\"\n          }\n        }\n\n    Both **start** and **end** datetimes must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-10-14T16:32:41Z. The **end** time is optional. If you do not specify value for the **end** property, it is calculated as \"**start + 48 hours**\". To run the pipeline indefinitely, specify **9999-09-09** as the value for the **end** property. See [JSON Scripting Reference](https://msdn.microsoft.com/library/dn835050.aspx) for details about JSON properties.\n\n    > [AZURE.NOTE] Specifying input for the AzureMLBatchExecution activity is optional. \n\n## Scenario: Experiments using Reader/Writer Modules to refer to data in various storages\n\nAnother common scenario when creating Azure ML experiments is to use Reader and Writer modules. The reader module is used to load data into an experiment and the writer module is to save data from your experiments. For details about reader and writer modules, see [Reader](https://msdn.microsoft.com/library/azure/dn905997.aspx) and [Writer](https://msdn.microsoft.com/library/azure/dn905984.aspx) topics on MSDN Library.     \n\nWhen using the reader and writer modules, it is good practice to use a Web service parameter for each property of these reader/writer modules. These web parameters enable you to configure the values during runtime. For example, you could create an experiment with a reader module that uses an Azure SQL Database: XXX.database.windows.net. After the web service has been deployed, you want to enable the consumers of the web service to specify another Azure SQL Server called YYY.database.windows.net. You can use a Web service parameter to allow this value to be configured.\n\n> [AZURE.NOTE] Web service input and output are different from Web service parameters. In the first scenario, you have seen how an input and output can be specified for an Azure ML Web service. In this scenario, you will pass parameters for a Web service that correspond to properties of reader/writer modules. \n\nLet's look at a scenario for using Web service parameters. You have a deployed Azure Machine Learning web service that is using a reader module to read data from one of the Azure Machine Learning supported data sources (for example: Azure SQL Database). After the batch execution is performed, the results are written using a Writer module (Azure SQL Database).  No web service inputs and outputs are defined in the experiments. In this case, we recommend that you setup the relevant Web service parameters for the reader and writer modules. This will allow the reader/writer modules to be configured when using the AzureMLBatchExecution activity. You specify Web service parameters in the **globalParameters** section in the activity JSON as follows. \n\n\n    \"typeProperties\": {\n        \"globalParameters\": {\n            \"Param 1\": \"Value 1\",\n            \"Param 2\": \"Value 2\"\n        }\n    }\n\nYou can also use [Data Factory Functions](https://msdn.microsoft.com/library/dn835056.aspx) in passing values for the Web service parameters as shown in the following example:\n\n    \"typeProperties\": {\n        \"globalParameters\": {\n           \"Database query\": \"$$Text.Format('SELECT * FROM myTable WHERE timeColumn = \\\\'{0:yyyy-MM-dd HH:mm:ss}\\\\'', Time.AddHours(WindowStart, 0))\"\n        }\n    }\n \n> [AZURE.NOTE] The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service. \n\n### Using a Reader module to read data from multiple files in Azure Blob\nBig data pipelines (Pig, Hive, etc...) can produce one or more output files with no extensions. For example, when you specify an external Hive table, the data for the external Hive table can be stored in Azure blob storage with the following name 000000_0. You can use the reader module in an experiment to read multiple files, and use them for predictions. \n\nWhen using the reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input. The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight. The reader module allows you to read files (with no extensions) by configuring the **Path to container, directory or blob** property of the reader module to point to the container/folder that contains the files as shown below. Note, the asterisk (i.e. \\*) **specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\\*)** will be read as part of the experiment.\n\n![Azure Blob properties](./media/data-factory-create-predictive-pipelines/azure-blob-properties.png)\n\n\n### Example \n#### Pipeline with AzureMLBatchExecution activity with Web Service Parameters\n\n    {\n      \"name\": \"MLWithSqlReaderSqlWriter\",\n      \"properties\": {\n        \"description\": \"Azure ML model with sql azure reader/writer\",\n        \"activities\": [\n          {\n            \"name\": \"MLSqlReaderSqlWriterActivity\",\n            \"type\": \"AzureMLBatchExecution\",\n            \"description\": \"test\",\n            \"inputs\": [\n              {\n                \"name\": \"MLSqlInput\"\n              }\n            ],\n            \"outputs\": [\n              {\n                \"name\": \"MLSqlOutput\"\n              }\n            ],\n            \"linkedServiceName\": \"MLSqlReaderSqlWriterDecisionTreeModel\",\n            \"typeProperties\":\n            {\n                \"webServiceInput\": \"MLSqlInput\",\n                \"webServiceOutputs\": {\n                    \"output1\": \"MLSqlOutput\"\n                }\n                \"globalParameters\": {\n                    \"Database server name\": \"<myserver>.database.windows.net\",\n                    \"Database name\": \"<database>\",\n                    \"Server user account name\": \"<user name>\",\n                    \"Server user account password\": \"<password>\"\n                }              \n            },\n            \"policy\": {\n              \"concurrency\": 1,\n              \"executionPriorityOrder\": \"NewestFirst\",\n              \"retry\": 1,\n              \"timeout\": \"02:00:00\"\n            },\n          }\n        ],\n        \"start\": \"2015-02-13T00:00:00Z\",\n        \"end\": \"2015-02-14T00:00:00Z\"\n      }\n    }\n \nIn the above JSON example:\n\n- The deployed Azure Machine Learning Web service uses a reader and a writer module to read/write data from/to an Azure SQL Database. This Web service exposes the following four parameters:  Database server name, Database name, Server user account name, and Server user account password.  \n- Both **start** and **end** datetimes must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-10-14T16:32:41Z. The **end** time is optional. If you do not specify value for the **end** property, it is calculated as \"**start + 48 hours**\". To run the pipeline indefinitely, specify **9999-09-09** as the value for the **end** property. See [JSON Scripting Reference](https://msdn.microsoft.com/library/dn835050.aspx) for details about JSON properties.\n \n\n## Frequently asked questions\n\n**Q:** I am using the AzureMLBatchScoring activity. Should I switch to using the AzureMLBatchExecution Activity?\n\n**A:** Yes. If you are using the AzureMLBatchScoring activity to integrate with Azure Machine Learning, we recommend that you use the latest AzureMLBatchExecution activity. We are deprecating the AzureMLBatchScoring activity, and it will be removed in a future release.\n\nThe AzureMLBatchExecution activity is introduced in the August 2015 release of Azure SDK and Azure PowerShell.\n\nThe AzureMLBatchExecution activity does not require an input (if input dependencies are not needed). It also allows you to be explicit about whether you want to use the Web service input and Web service output or not use them. If you do choose to use Web service input/output, it enables you to specify the relevant Azure Blob datasets to be used.In addition, it allows you to clearly specify the values for the web service parameters that are provided by the web service.\n\nIf you want to continue using the AzureMLBatchScoring activity, please refer to [Azure ML Batch Scoring Activity](data-factory-create-predictive-pipelines.md) article for details.\n\n\n**Q:** I have multiple files that are generated by my big data pipelines. Can I use the AzureMLBatchExecution Activity to work on all the files?\n\n**A:** Yes. See the **Using a Reader module to read data from multiple files in Azure Blob** section for details. \n\n\n\n\n\n\n\n\n[adf-build-1st-pipeline]: data-factory-build-your-first-pipeline.md\n\n[azure-machine-learning]: http://azure.microsoft.com/services/machine-learning/\n\n\n \n\ntest\n"
}