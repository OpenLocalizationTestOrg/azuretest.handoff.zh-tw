{
  "nodes": [
    {
      "content": "Move data between on-premises and cloud using Azure Data Factory",
      "pos": [
        28,
        92
      ]
    },
    {
      "content": "Learn about moving data between on-premises and cloud using Data Management Gateway and Azure Data Factory.",
      "pos": [
        112,
        219
      ]
    },
    {
      "content": "Move Data Between On-premises Sources and Cloud with Data Management Gateway",
      "pos": [
        547,
        623
      ]
    },
    {
      "content": "One of the challenges for modern data integration is to seamlessly move data to and from on-premises to cloud.",
      "pos": [
        624,
        734
      ]
    },
    {
      "content": "Data factory makes this integration seamless with data management gateway.",
      "pos": [
        735,
        809
      ]
    },
    {
      "content": "Data factory management gateway  is an agent you can install on-premises to enable hybrid pipelines.",
      "pos": [
        810,
        910
      ]
    },
    {
      "content": "This article provides an overview of integrating on-premises data stores with cloud data stores and cloud processing using data factory.",
      "pos": [
        912,
        1048
      ]
    },
    {
      "content": "This article builds on the <bpt id=\"p1\">[</bpt>Data Movement Activities<ept id=\"p1\">](data-factory-data-movement-activities.md)</ept> article and other data factory core concepts articles.",
      "pos": [
        1049,
        1199
      ]
    },
    {
      "content": "The following overview assumes you are familiar with data factory concepts like pipelines, activities, datasets and the copy activity.",
      "pos": [
        1200,
        1334
      ]
    },
    {
      "content": "The data gateway provides the following capabilities:",
      "pos": [
        1336,
        1389
      ]
    },
    {
      "content": "Model on-premises data sources and cloud data sources within the same data factory and move data.",
      "pos": [
        1395,
        1492
      ]
    },
    {
      "content": "Have a single pane of glass for monitoring and management with visibility into gateway status with data factory cloud dashboard.",
      "pos": [
        1497,
        1625
      ]
    },
    {
      "content": "Manage access to on-premises data sources securely.",
      "pos": [
        1630,
        1681
      ]
    },
    {
      "content": "No changes required to corporate firewall.",
      "pos": [
        1689,
        1731
      ]
    },
    {
      "content": "Gateway only makes outbound HTTP based connections to open internet.",
      "pos": [
        1732,
        1800
      ]
    },
    {
      "content": "Encrypt credentials for your on-premises data stores with your certificate.",
      "pos": [
        1808,
        1883
      ]
    },
    {
      "content": "Move data efficiently – data is transferred in parallel, resilient to intermittent network issues with auto retry logic.",
      "pos": [
        1888,
        2008
      ]
    },
    {
      "content": "Considerations for using Data Management Gateway",
      "pos": [
        2013,
        2061
      ]
    },
    {
      "pos": [
        2066,
        2292
      ],
      "content": "A single instance of Data Management Gateway can be used for multiple on-premises data sources, but note that <bpt id=\"p1\">**</bpt>a single gateway instance is tied to only one Azure data factory<ept id=\"p1\">**</ept> and cannot be shared with another data factory."
    },
    {
      "content": "You can have <bpt id=\"p1\">**</bpt>only one instance of Data Management Gateway<ept id=\"p1\">**</ept> installed on a single machine.",
      "pos": [
        2297,
        2389
      ]
    },
    {
      "content": "Suppose, you have two data factories that need to access on-premises data sources, you need to install gateways on two on-premises computers where each gateway tied to a separate data factory.",
      "pos": [
        2390,
        2582
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>gateway does not need to be on the same machine as the data source<ept id=\"p1\">**</ept>, but staying closer to the data source reduces the time for the gateway to connect to the data source.",
      "pos": [
        2587,
        2764
      ]
    },
    {
      "content": "We recommend that you install the gateway on a machine that is different from the one that hosts on-premises data source so that the gateway does not compete for resources with data source.",
      "pos": [
        2765,
        2954
      ]
    },
    {
      "content": "You can have <bpt id=\"p1\">**</bpt>multiple gateways on different machines connecting to the same on-premises data source<ept id=\"p1\">**</ept>.",
      "pos": [
        2959,
        3063
      ]
    },
    {
      "content": "For example, you may have two gateways serving two data factories but the same on-premises data source is registered with both the data factories.",
      "pos": [
        3064,
        3210
      ]
    },
    {
      "pos": [
        3215,
        3385
      ],
      "content": "If you already have a gateway installed on your computer serving a <bpt id=\"p1\">**</bpt>Power BI<ept id=\"p1\">**</ept> scenario, please install a <bpt id=\"p2\">**</bpt>separate gateway for Azure Data Factory<ept id=\"p2\">**</ept> on another machine."
    },
    {
      "pos": [
        3390,
        3450
      ],
      "content": "You must <bpt id=\"p1\">**</bpt>use the gateway even when you use ExpressRoute<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        3456,
        3673
      ],
      "content": "You should treat your data source as an on-premises data source (that is behind a firewall) even when you use <bpt id=\"p1\">**</bpt>ExpressRoute<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>use the gateway<ept id=\"p2\">**</ept> to establish connectivity between the service and the data source."
    },
    {
      "content": "Gateway installation - prerequisites",
      "pos": [
        3679,
        3715
      ]
    },
    {
      "pos": [
        3720,
        3838
      ],
      "content": "The supported <bpt id=\"p1\">**</bpt>Operating System<ept id=\"p1\">**</ept> versions are Windows 7, Windows 8/8.1, Windows Server 2008 R2, Windows Server 2012."
    },
    {
      "pos": [
        3843,
        3953
      ],
      "content": "The recommended <bpt id=\"p1\">**</bpt>configuration<ept id=\"p1\">**</ept> for the gateway machine is at least 2 GHz, 4 cores, 8 GB RAM and 80 GB disk."
    },
    {
      "content": "If the host machine hibernates, the gateway won’t be able to respond to data requests.",
      "pos": [
        3958,
        4044
      ]
    },
    {
      "content": "Therefore, configure an appropriate <bpt id=\"p1\">**</bpt>power plan<ept id=\"p1\">**</ept> on the computer before installing the gateway.",
      "pos": [
        4045,
        4142
      ]
    },
    {
      "content": "The gateway installation prompts a message if the machine is configured to hibernate.",
      "pos": [
        4143,
        4228
      ]
    },
    {
      "content": "Due to the fact that copy activity runs happen on a specific frequency, the resource usage (CPU, memory) on the machine also follows the same pattern with peak and idle times.",
      "pos": [
        4230,
        4405
      ]
    },
    {
      "content": "Resource utilization also depends heavily on the amount of data being moved.",
      "pos": [
        4406,
        4482
      ]
    },
    {
      "content": "When multiple copy jobs are in progress you will observe resource usage go up during peak times.",
      "pos": [
        4483,
        4579
      ]
    },
    {
      "content": "While above is the minimum configuration it is always better to have a configuration with more resources than the min configuration described above depending on your specific load for data movement.",
      "pos": [
        4580,
        4778
      ]
    },
    {
      "content": "Installation",
      "pos": [
        4783,
        4795
      ]
    },
    {
      "content": "Data Management Gateway can be installed by downloading an MSI setup package from the Microsoft Download Center.",
      "pos": [
        4796,
        4908
      ]
    },
    {
      "content": "The MSI can also be used to upgrade existing Data Management Gateway to the latest version, with all settings preserved.",
      "pos": [
        4910,
        5030
      ]
    },
    {
      "content": "You can find the link to the MSI package from Azure portal by following the step by step walkthrough below.",
      "pos": [
        5031,
        5138
      ]
    },
    {
      "content": "Installation Best practices:",
      "pos": [
        5144,
        5172
      ]
    },
    {
      "content": "Configure power plan on the host machine for the gateway so that the machine does not hibernate.",
      "pos": [
        5177,
        5273
      ]
    },
    {
      "content": "If the host machine hibernates, the gateway won’t be able to respond to data requests.",
      "pos": [
        5274,
        5360
      ]
    },
    {
      "content": "You should backup the certificate associated with the gateway.",
      "pos": [
        5365,
        5427
      ]
    },
    {
      "content": "Installation Troubleshooting:",
      "pos": [
        5433,
        5462
      ]
    },
    {
      "content": "If your company uses a firewall or proxy server, additional steps may be required in case Data Management Gateway cannot connect to Microsoft cloud services.",
      "pos": [
        5463,
        5620
      ]
    },
    {
      "content": "Looking at Gateway logs with Event Viewer:",
      "pos": [
        5628,
        5670
      ]
    },
    {
      "content": "Gaetway configuration manager application shows status for gateway like “Disconnected” or “Connecting”.",
      "pos": [
        5672,
        5775
      ]
    },
    {
      "content": "For more detailed information you can look at gateway logs in Windows event logs.",
      "pos": [
        5777,
        5858
      ]
    },
    {
      "content": "You can find them by using Windows <bpt id=\"p1\">**</bpt>Event Viewer<ept id=\"p1\">**</ept> under <bpt id=\"p2\">**</bpt>Application and Services Logs<ept id=\"p2\">**</ept> &gt; <bpt id=\"p3\">**</bpt>Data Management Gateway<ept id=\"p3\">**</ept> While troubleshooting gateway related issues look for error level events in the event viewer.",
      "pos": [
        5859,
        6074
      ]
    },
    {
      "content": "Possible symptoms for firewall related issues:",
      "pos": [
        6082,
        6128
      ]
    },
    {
      "content": "When you try to register the gateway, you receive the following error: \"Failed to register the gateway key.",
      "pos": [
        6133,
        6240
      ]
    },
    {
      "content": "Before trying to register the gateway key again, confirm that the Data Management Gateway is in a connected state and the Data Management Gateway Host Service is Started.\"",
      "pos": [
        6241,
        6412
      ]
    },
    {
      "content": "When you open Configuration Manager, you see status as “Disconnected” or “Connecting”.",
      "pos": [
        6416,
        6502
      ]
    },
    {
      "content": "When viewing Windows event logs, under “Event Viewer” &gt; “Application and Services Logs” &gt; “Data Management Gateway” you see error messages such as “Unable to connect to the remote server” or “A component of Data Management Gateway has become unresponsive and will restart automatically.",
      "pos": [
        6503,
        6789
      ]
    },
    {
      "content": "Component name: Gateway.”",
      "pos": [
        6790,
        6815
      ]
    },
    {
      "content": "These are caused by the improper configuration of the firewall or proxy server, which blocks Data Management Gateway from connecting to cloud services to authenticate itself.",
      "pos": [
        6817,
        6991
      ]
    },
    {
      "content": "The two firewalls that are possibly in scope are: corporate firewall running on the central router of the organization, and Windows firewall configured as a daemon on the local machine where the gateway is installed.",
      "pos": [
        6993,
        7209
      ]
    },
    {
      "content": "Here are the some considerations:",
      "pos": [
        7210,
        7243
      ]
    },
    {
      "content": "There is no need to change the inbound policy for corporate firewall.",
      "pos": [
        7247,
        7316
      ]
    },
    {
      "content": "Both corporate firewall and Windows firewall should enable outbound rule for TCP ports: 80, 440, and from 9305 to 9354.",
      "pos": [
        7319,
        7438
      ]
    },
    {
      "content": "These are used by Microsoft Azure Service Bus to establish connection between the cloud services and Data Management Gateway.",
      "pos": [
        7439,
        7564
      ]
    },
    {
      "content": "The MSI setup will automatically configure Windows firewall rules for inbound ports for the gateway machine (see ports and security considerations section above).",
      "pos": [
        7566,
        7728
      ]
    },
    {
      "content": "But the setup assumes the above mentioned outbound ports are allowed by default on the local machine and corporate firewall.",
      "pos": [
        7730,
        7854
      ]
    },
    {
      "content": "You need to enable these outbound ports if that is not the case.",
      "pos": [
        7855,
        7919
      ]
    },
    {
      "content": "If you have replaced the Windows firewall with a third party firewall, these ports might need to be opened manually.",
      "pos": [
        7920,
        8036
      ]
    },
    {
      "content": "If your company uses a proxy server, then you need to add Microsoft Azure to the whitelist.",
      "pos": [
        8039,
        8130
      ]
    },
    {
      "content": "You can download a list of valid Microsoft Azure IP addresses from the <bpt id=\"p1\">[</bpt>Microsoft Download Center<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsazure/dn175718.aspx)</ept>.",
      "pos": [
        8131,
        8292
      ]
    },
    {
      "content": "Using the Data Gateway – Step by Step Walkthrough",
      "pos": [
        8297,
        8346
      ]
    },
    {
      "content": "In this walkthrough, you create a data factory with a pipeline that moves data from an on-premises SQL Server database to an Azure blob.",
      "pos": [
        8347,
        8483
      ]
    },
    {
      "content": "Step 1: Create an Azure data factory",
      "pos": [
        8490,
        8526
      ]
    },
    {
      "content": "In this step, you use the Azure Management Portal to create an Azure Data Factory instance named <bpt id=\"p1\">**</bpt>ADFTutorialOnPremDF<ept id=\"p1\">**</ept>.",
      "pos": [
        8527,
        8648
      ]
    },
    {
      "content": "You can also create a data factory by using Azure Data Factory cmdlets.",
      "pos": [
        8649,
        8720
      ]
    },
    {
      "pos": [
        8727,
        8953
      ],
      "content": "After logging into the <bpt id=\"p1\">[</bpt>Azure Preview Portal<ept id=\"p1\">](https://portal.azure.com)</ept>, click <bpt id=\"p2\">**</bpt>NEW<ept id=\"p2\">**</ept> from the bottom-left corner, select <bpt id=\"p3\">**</bpt>Data analytics<ept id=\"p3\">**</ept> in the <bpt id=\"p4\">**</bpt>Create<ept id=\"p4\">**</ept> blade, and click <bpt id=\"p5\">**</bpt>Data Factory<ept id=\"p5\">**</ept> on the <bpt id=\"p6\">**</bpt>Data analytics<ept id=\"p6\">**</ept> blade."
    },
    {
      "content": "New-&gt;DataFactory",
      "pos": [
        8961,
        8977
      ]
    },
    {
      "pos": [
        9066,
        9100
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade:"
    },
    {
      "pos": [
        9108,
        9155
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>ADFTutorialOnPremDF<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>name<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>RESOURCE GROUP NAME<ept id=\"p1\">**</ept> and select <bpt id=\"p2\">**</bpt>ADFTutorialResourceGroup<ept id=\"p2\">**</ept>.",
      "pos": [
        9163,
        9233
      ]
    },
    {
      "content": "You can select an existing resource group or create a new one.",
      "pos": [
        9234,
        9296
      ]
    },
    {
      "content": "To create a new resource group:",
      "pos": [
        9297,
        9328
      ]
    },
    {
      "pos": [
        9340,
        9378
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Create a new resource group<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        9390,
        9488
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Create resource group blade<ept id=\"p1\">**</ept>, enter a <bpt id=\"p2\">**</bpt>name<ept id=\"p2\">**</ept> for the resource group, and click <bpt id=\"p3\">**</bpt>OK<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        9493,
        9570
      ],
      "content": "Note that <bpt id=\"p1\">**</bpt>Add to Startboard<ept id=\"p1\">**</ept> is checked on the <bpt id=\"p2\">**</bpt>New data factory<ept id=\"p2\">**</ept> blade."
    },
    {
      "content": "Add to Startboard",
      "pos": [
        9578,
        9595
      ]
    },
    {
      "pos": [
        9698,
        9750
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>New data factory<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept>."
    },
    {
      "content": "The name of the Azure data factory must be globally unique.",
      "pos": [
        9756,
        9815
      ]
    },
    {
      "content": "If you receive the error: <bpt id=\"p1\">**</bpt>Data factory name “ADFTutorialOnPremDF” is not available<ept id=\"p1\">**</ept>, change the name of the data factory (for example, yournameADFTutorialOnPremDF) and try creating again.",
      "pos": [
        9816,
        10006
      ]
    },
    {
      "content": "Use this name in place of ADFTutorialOnPremDF while performing remaining steps in this tutorial.",
      "pos": [
        10007,
        10103
      ]
    },
    {
      "content": "Look for notifications from the creation process in the <bpt id=\"p1\">**</bpt>NOTIFICATIONS<ept id=\"p1\">**</ept> hub on the left.",
      "pos": [
        10110,
        10200
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> to close the <bpt id=\"p2\">**</bpt>NOTIFCATIONS<ept id=\"p2\">**</ept> blade if it is open.",
      "pos": [
        10201,
        10263
      ]
    },
    {
      "content": "NOTIFICATIONS hub",
      "pos": [
        10271,
        10288
      ]
    },
    {
      "pos": [
        10379,
        10462
      ],
      "content": "After creation is complete, you will see the <bpt id=\"p1\">**</bpt>Data Factory<ept id=\"p1\">**</ept> blade as shown below:"
    },
    {
      "content": "Data Factory Home Page",
      "pos": [
        10470,
        10492
      ]
    },
    {
      "content": "Step 2: Create a data management gateway",
      "pos": [
        10586,
        10626
      ]
    },
    {
      "pos": [
        10631,
        10716
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Data Factory<ept id=\"p1\">**</ept> blade for <bpt id=\"p2\">**</bpt>ADFTutorialOnPremDF<ept id=\"p2\">**</ept>, click <bpt id=\"p3\">**</bpt>Linked Services<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Data Factory Home Page",
      "pos": [
        10725,
        10747
      ]
    },
    {
      "pos": [
        10835,
        10894
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>+ Data gateway<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Linked Services - Add a Gateway button",
      "pos": [
        10902,
        10940
      ]
    },
    {
      "pos": [
        11043,
        11132
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Create<ept id=\"p1\">**</ept> blade, enter <bpt id=\"p2\">**</bpt>adftutorialgateway<ept id=\"p2\">**</ept> for the <bpt id=\"p3\">**</bpt>name<ept id=\"p3\">**</ept>, and click <bpt id=\"p4\">**</bpt>OK<ept id=\"p4\">**</ept>."
    },
    {
      "content": "Create Gateway blade",
      "pos": [
        11144,
        11164
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Configure<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Install directly on this computer<ept id=\"p2\">**</ept>.",
      "pos": [
        11256,
        11328
      ]
    },
    {
      "content": "This will download the installation package for the gateway, install, configure, and register the gateway on the computer.",
      "pos": [
        11329,
        11451
      ]
    },
    {
      "pos": [
        11461,
        11551
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Please use Internet Explorer or a Microsoft ClickOnce compatible web browser."
    },
    {
      "content": "Gateway - Configure blade",
      "pos": [
        11559,
        11584
      ]
    },
    {
      "content": "This is the easiest way (one-click) to download, install, configure, and register the gateway in one single step.",
      "pos": [
        11680,
        11793
      ]
    },
    {
      "content": "You can see the <bpt id=\"p1\">**</bpt>Microsoft Data Management Gateway Configuration Manager<ept id=\"p1\">**</ept> application is installed on your computer.",
      "pos": [
        11794,
        11912
      ]
    },
    {
      "content": "You can also find the executable <bpt id=\"p1\">**</bpt>ConfigManager.exe<ept id=\"p1\">**</ept> in the folder: <bpt id=\"p2\">**</bpt>C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\Shared<ept id=\"p2\">**</ept>.",
      "pos": [
        11913,
        12049
      ]
    },
    {
      "pos": [
        12055,
        12213
      ],
      "content": "You can also download and install gateway manually by using the links in this blade and register it using the key shown in the <bpt id=\"p1\">**</bpt>REGISTER WITH KEY<ept id=\"p1\">**</ept> text box."
    },
    {
      "pos": [
        12223,
        12351
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Data Management Gateway<ept id=\"p1\">](#DMG)</ept> section for details about the gateway including best practices and important considerations."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> You must be an administrator on the local computer to install and configure the Data Management Gateway successfully.",
      "pos": [
        12358,
        12488
      ]
    },
    {
      "content": "You can add additional users to the Data Management Gateway Users local Windows group.",
      "pos": [
        12489,
        12575
      ]
    },
    {
      "content": "The members of this group will be able to use the Data Management Gateway Configuration Manager tool to configure the gateway.",
      "pos": [
        12576,
        12702
      ]
    },
    {
      "content": "Click the <bpt id=\"p1\">**</bpt>NOTIFICATIONS<ept id=\"p1\">**</ept> hub on the left.",
      "pos": [
        12708,
        12752
      ]
    },
    {
      "content": "Wait until you see <bpt id=\"p1\">**</bpt>Express setup for 'adftutorialgateway' succeeded<ept id=\"p1\">**</ept> message in the <bpt id=\"p2\">**</bpt>Notifications<ept id=\"p2\">**</ept> blade.",
      "pos": [
        12753,
        12864
      ]
    },
    {
      "content": "Express setup succeeded",
      "pos": [
        12872,
        12895
      ]
    },
    {
      "pos": [
        12985,
        13065
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>Create<ept id=\"p2\">**</ept> blade and then on the <bpt id=\"p3\">**</bpt>New data gateway<ept id=\"p3\">**</ept> blade."
    },
    {
      "pos": [
        13069,
        13237
      ],
      "content": "Close the <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> blade (by pressing <bpt id=\"p2\">**</bpt>X<ept id=\"p2\">**</ept> button in the top-right corner) and reopen the <bpt id=\"p3\">**</bpt>Linked Services<ept id=\"p3\">**</ept> blade to see the latest status of the gateway."
    },
    {
      "pos": [
        13242,
        13299
      ],
      "content": "Confirm that the <bpt id=\"p1\">**</bpt>status<ept id=\"p1\">**</ept> of the gateway is <bpt id=\"p2\">**</bpt>Online<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Gateway status",
      "pos": [
        13308,
        13322
      ]
    },
    {
      "pos": [
        13404,
        13501
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Microsoft Data Management Gateway Configuration Manager<ept id=\"p1\">**</ept> application  on your computer."
    },
    {
      "content": "Gateway Configuration Manager",
      "pos": [
        13509,
        13538
      ]
    },
    {
      "content": "Wait until the values are set as follows :",
      "pos": [
        13635,
        13677
      ]
    },
    {
      "pos": [
        13685,
        13837
      ],
      "content": "If the Service <bpt id=\"p1\">**</bpt>status<ept id=\"p1\">**</ept> is not set to <bpt id=\"p2\">**</bpt>Started<ept id=\"p2\">**</ept>, click <bpt id=\"p3\">**</bpt>Start service<ept id=\"p3\">**</ept> to start the service and wait for a minute for the other fields to refresh."
    },
    {
      "pos": [
        13845,
        13895
      ],
      "content": "<bpt id=\"p1\">**</bpt>Gateway name<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>adftutorialgateway<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13903,
        13954
      ],
      "content": "<bpt id=\"p1\">**</bpt>Instance name<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>adftutorialgateway<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        13962,
        14010
      ],
      "content": "<bpt id=\"p1\">**</bpt>Gateway key status<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>Registered<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        14018,
        14142
      ],
      "content": "The status bar the bottom displays <bpt id=\"p1\">**</bpt>Connected to Data Management Gateway Cloud Service<ept id=\"p1\">**</ept> along with a <bpt id=\"p2\">**</bpt>green check mark<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Switch to <bpt id=\"p1\">**</bpt>Certificates<ept id=\"p1\">**</ept>.",
      "pos": [
        14151,
        14178
      ]
    },
    {
      "content": "The certificate specified on this tab is used to encrypt/decrypt credentials for the on-premises data store that you specify on the portal.",
      "pos": [
        14179,
        14318
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Change<ept id=\"p1\">**</ept> to use your own certificate instead.",
      "pos": [
        14319,
        14372
      ]
    },
    {
      "content": "By default, the gateway uses the certificate that is auto-generated by the Data Factory service.",
      "pos": [
        14373,
        14469
      ]
    },
    {
      "content": "Gateway certificate configuration",
      "pos": [
        14477,
        14510
      ]
    },
    {
      "pos": [
        14597,
        14701
      ],
      "content": "In the portal, on the <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> blade, confirm that the <bpt id=\"p2\">**</bpt>status<ept id=\"p2\">**</ept> of the gateway is <bpt id=\"p3\">**</bpt>Good<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Step 2: Create linked services",
      "pos": [
        14709,
        14739
      ]
    },
    {
      "content": "In this step, you will create two linked services: <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>SqlServerLinkedService<ept id=\"p2\">**</ept>.",
      "pos": [
        14741,
        14848
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>SqlServerLinkedService<ept id=\"p1\">**</ept> links an on-premises SQL Server database and the <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> linked service links an Azure blob store to the <bpt id=\"p3\">**</bpt>ADFTutorialDataFactory<ept id=\"p3\">**</ept>.",
      "pos": [
        14849,
        15029
      ]
    },
    {
      "content": "You will create a pipeline later in this walkthrough that copies data from the on-premises SQL Server database to the Azure blob store.",
      "pos": [
        15030,
        15165
      ]
    },
    {
      "content": "Add a linked service to an on-premises SQL Server database",
      "pos": [
        15173,
        15231
      ]
    },
    {
      "pos": [
        15236,
        15314
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>New data store<ept id=\"p2\">**</ept> on the command bar."
    },
    {
      "pos": [
        15319,
        15369
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>SqlServerLinkedService<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>name<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        15375,
        15435
      ],
      "content": "Click arrow next to the <bpt id=\"p1\">**</bpt>Type<ept id=\"p1\">**</ept>, and select <bpt id=\"p2\">**</bpt>SQL Server<ept id=\"p2\">**</ept>."
    },
    {
      "content": "Create new data store",
      "pos": [
        15443,
        15464
      ]
    },
    {
      "pos": [
        15546,
        15598
      ],
      "content": "You should more settings below the <bpt id=\"p1\">**</bpt>Type<ept id=\"p1\">**</ept> setting."
    },
    {
      "pos": [
        15603,
        15673
      ],
      "content": "For the <bpt id=\"p1\">**</bpt>Data gateway<ept id=\"p1\">**</ept> setting, select the gateway you just created."
    },
    {
      "content": "SQL Server settings",
      "pos": [
        15682,
        15701
      ]
    },
    {
      "pos": [
        15788,
        15854
      ],
      "content": "Enter the name of your database server for the <bpt id=\"p1\">**</bpt>Server<ept id=\"p1\">**</ept> setting."
    },
    {
      "pos": [
        15859,
        15919
      ],
      "content": "Enter the name of the database for the <bpt id=\"p1\">**</bpt>Database<ept id=\"p1\">**</ept> setting."
    },
    {
      "pos": [
        15924,
        15960
      ],
      "content": "Click arrow next to <bpt id=\"p1\">**</bpt>Credentials<ept id=\"p1\">**</ept>."
    },
    {
      "content": "Credentials blade",
      "pos": [
        15968,
        15985
      ]
    },
    {
      "pos": [
        16071,
        16141
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Credentials<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Click here to set credentials<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        16146,
        16206
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Setting Credentials<ept id=\"p1\">**</ept> dialog box, do the following:"
    },
    {
      "content": "Setting credentials dialog",
      "pos": [
        16214,
        16240
      ]
    },
    {
      "pos": [
        16338,
        16437
      ],
      "content": "Select <bpt id=\"p1\">**</bpt>authentication<ept id=\"p1\">**</ept> that you want the Data Factory service to use to connect to the database."
    },
    {
      "pos": [
        16447,
        16530
      ],
      "content": "Enter name of the user who has access to the database for the <bpt id=\"p1\">**</bpt>USERNAME<ept id=\"p1\">**</ept> setting."
    },
    {
      "pos": [
        16540,
        16597
      ],
      "content": "Enter password for the user for the <bpt id=\"p1\">**</bpt>PASSWORD<ept id=\"p1\">**</ept> setting."
    },
    {
      "pos": [
        16608,
        16645
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> to close the dialog box."
    },
    {
      "pos": [
        16650,
        16698
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> to close the <bpt id=\"p2\">**</bpt>Credentials<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        16703,
        16748
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> on the <bpt id=\"p2\">**</bpt>New data store<ept id=\"p2\">**</ept> blade."
    },
    {
      "pos": [
        16756,
        16991
      ],
      "content": "Confirm that the status for **SqlServerLinkedService** is set to Online in the Linked Services blade.\n ![SQL Server linked service status](./media/data-factory-move-data-between-onprem-and-cloud/sql-server-linked-service-status.png)",
      "leadings": [
        "",
        "   "
      ],
      "nodes": [
        {
          "content": "Confirm that the status for <bpt id=\"p1\">**</bpt>SqlServerLinkedService<ept id=\"p1\">**</ept> is set to Online in the Linked Services blade.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "<ph id=\"ph1\"> ![</ph>SQL Server linked service status<ph id=\"ph2\">](./media/data-factory-move-data-between-onprem-and-cloud/sql-server-linked-service-status.png)</ph>",
          "pos": [
            102,
            232
          ]
        }
      ]
    },
    {
      "pos": [
        16993,
        17118
      ],
      "content": "See <bpt id=\"p1\">[</bpt>Setting Credentials and Security<ept id=\"p1\">](#setting-credentials-and-security)</ept> section for more details about setting credentials."
    },
    {
      "content": "Add a linked service for an Azure storage account",
      "pos": [
        17127,
        17176
      ]
    },
    {
      "pos": [
        17182,
        17255
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Linked Services<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>New datastore<ept id=\"p2\">**</ept> on the toolbar."
    },
    {
      "pos": [
        17260,
        17314
      ],
      "content": "Enter <bpt id=\"p1\">**</bpt>StorageLinkedService<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>Name<ept id=\"p2\">**</ept> field."
    },
    {
      "pos": [
        17319,
        17377
      ],
      "content": "Click arrow next to <bpt id=\"p1\">**</bpt>Type<ept id=\"p1\">**</ept> and select <bpt id=\"p2\">**</bpt>Azure Storage<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        17381,
        17472
      ],
      "content": "You should see new fields: <bpt id=\"p1\">**</bpt>Account name<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Account key<ept id=\"p2\">**</ept> under <bpt id=\"p3\">**</bpt>Type<ept id=\"p3\">**</ept> setting now."
    },
    {
      "pos": [
        17477,
        17547
      ],
      "content": "Enter the name of your Azure storage account for the <bpt id=\"p1\">**</bpt>Account name<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        17551,
        17620
      ],
      "content": "Enter the key for your Azure storage account for the <bpt id=\"p1\">**</bpt>Account key<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        17625,
        17662
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> to close the dialog box."
    },
    {
      "content": "Step 3: Create input and output datasets",
      "pos": [
        17671,
        17711
      ]
    },
    {
      "content": "In this step, you will create input and output datasets that represent input and output data for the copy operation (On-premises SQL Server database =&gt; Azure blob storage).",
      "pos": [
        17712,
        17884
      ]
    },
    {
      "content": "Before creating datasets or tables (rectangular datasets), you need to do the following (detailed steps follows the list):",
      "pos": [
        17885,
        18007
      ]
    },
    {
      "pos": [
        18011,
        18168
      ],
      "content": "Create a table named <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> in the SQL Server Database you added as a linked service to the data factory and insert couple of sample entries into the table."
    },
    {
      "pos": [
        18171,
        18301
      ],
      "content": "Create a blob container named <bpt id=\"p1\">**</bpt>adftutorial<ept id=\"p1\">**</ept> in the Azure blob storage account you added as a linked service to the data factory."
    },
    {
      "content": "Prepare On-premises SQL Server for the tutorial",
      "pos": [
        18307,
        18354
      ]
    },
    {
      "pos": [
        18359,
        18538
      ],
      "content": "In the database you specified for the on-premises SQL Server linked service (<bpt id=\"p1\">**</bpt>SqlServerLinkedService<ept id=\"p1\">**</ept>), use the following SQL script to create the <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept> table in the database."
    },
    {
      "content": "Insert some sample into the table:",
      "pos": [
        18767,
        18801
      ]
    },
    {
      "content": "Create input table",
      "pos": [
        18906,
        18924
      ]
    },
    {
      "pos": [
        18930,
        19040
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Author and deploy<ept id=\"p2\">**</ept> tile to launch the <bpt id=\"p3\">**</bpt>Editor<ept id=\"p3\">**</ept> for the data factory."
    },
    {
      "content": "Author and Deploy Tile",
      "pos": [
        19048,
        19070
      ]
    },
    {
      "pos": [
        19156,
        19260
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New dataset<ept id=\"p2\">**</ept> on the command bar, and click <bpt id=\"p3\">**</bpt>On-premises SQL<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Replace the JSON in the right pane with the following text:",
      "pos": [
        19266,
        19325
      ]
    },
    {
      "content": "Note the following:",
      "pos": [
        19957,
        19976
      ]
    },
    {
      "pos": [
        19989,
        20027
      ],
      "content": "<bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>SqlServerTable<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        20034,
        20066
      ],
      "content": "<bpt id=\"p1\">**</bpt>tableName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>emp<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        20073,
        20180
      ],
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>SqlServerLinkedService<ept id=\"p2\">**</ept> (you had created this linked service in Step 2)."
    },
    {
      "content": "For an input table that is not generated by another pipeline in Azure Data Factory, you must set <bpt id=\"p1\">**</bpt>external<ept id=\"p1\">**</ept> to <bpt id=\"p2\">**</bpt>true<ept id=\"p2\">**</ept>.",
      "pos": [
        20187,
        20309
      ]
    },
    {
      "content": "It denotes the input data is produced external to the Azure Data Factory service.",
      "pos": [
        20310,
        20391
      ]
    },
    {
      "content": "You can optionally specify any external data policies using the <bpt id=\"p1\">**</bpt>externalData<ept id=\"p1\">**</ept> element in the <bpt id=\"p2\">**</bpt>Policy<ept id=\"p2\">**</ept> section.",
      "pos": [
        20392,
        20507
      ]
    },
    {
      "content": "See [JSON Scripting Reference][json-script-reference] for details about JSON properties.",
      "pos": [
        20517,
        20605
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the dataset (table is a rectangular dataset).",
      "pos": [
        20610,
        20701
      ]
    },
    {
      "content": "Confirm that you see a message on the title bar that says <bpt id=\"p1\">**</bpt>TABLE DEPLOYED SUCCESSFULLY<ept id=\"p1\">**</ept>.",
      "pos": [
        20702,
        20792
      ]
    },
    {
      "content": "Create output table",
      "pos": [
        20800,
        20819
      ]
    },
    {
      "pos": [
        20825,
        20932
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>Data Factory Editor<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New dataset<ept id=\"p2\">**</ept> on the command bar, and click <bpt id=\"p3\">**</bpt>Azure Blob storage<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Replace the JSON in the right pane with the following text:",
      "pos": [
        20937,
        20996
      ]
    },
    {
      "content": "Note the following:",
      "pos": [
        21523,
        21542
      ]
    },
    {
      "pos": [
        21555,
        21588
      ],
      "content": "<bpt id=\"p1\">**</bpt>type<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>AzureBlob<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        21595,
        21700
      ],
      "content": "<bpt id=\"p1\">**</bpt>linkedServiceName<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>StorageLinkedService<ept id=\"p2\">**</ept> (you had created this linked service in Step 2)."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>adftutorial/outfromonpremdf<ept id=\"p2\">**</ept> where outfromonpremdf is the folder in the adftutorial container.",
      "pos": [
        21707,
        21829
      ]
    },
    {
      "content": "You just need to create the <bpt id=\"p1\">**</bpt>adftutorial<ept id=\"p1\">**</ept> container.",
      "pos": [
        21830,
        21884
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>availability<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>hourly<ept id=\"p2\">**</ept> (<bpt id=\"p3\">**</bpt>frequency<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>hour<ept id=\"p4\">**</ept> and <bpt id=\"p5\">**</bpt>interval<ept id=\"p5\">**</ept> set to <bpt id=\"p6\">**</bpt>1<ept id=\"p6\">**</ept>).",
      "pos": [
        21891,
        21995
      ]
    },
    {
      "content": "The Data Factory service will generate an output data slice every hour in the <bpt id=\"p1\">**</bpt>emp<ept id=\"p1\">**</ept> table in the Azure SQL Database.",
      "pos": [
        21997,
        22115
      ]
    },
    {
      "content": "if you don't specify a <bpt id=\"p1\">**</bpt>fileName<ept id=\"p1\">**</ept> for an <bpt id=\"p2\">**</bpt>input table<ept id=\"p2\">**</ept>, all files/blobs from the input folder (<bpt id=\"p3\">**</bpt>folderPath<ept id=\"p3\">**</ept>) are considered as inputs.",
      "pos": [
        22122,
        22262
      ]
    },
    {
      "content": "If you specify a fileName in the JSON, only the specified file/blob is considered asn input.",
      "pos": [
        22263,
        22355
      ]
    },
    {
      "content": "See the sample files in the [tutorial][adf-tutorial] for examples.",
      "pos": [
        22356,
        22422
      ]
    },
    {
      "content": "If you do not specify a <bpt id=\"p1\">**</bpt>fileName<ept id=\"p1\">**</ept> for an <bpt id=\"p2\">**</bpt>output table<ept id=\"p2\">**</ept>, the generated files in the <bpt id=\"p3\">**</bpt>folderPath<ept id=\"p3\">**</ept> are named in the following format: Data.",
      "pos": [
        22429,
        22573
      ]
    },
    {
      "content": ".txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).",
      "pos": [
        22579,
        22648
      ]
    },
    {
      "content": "To set <bpt id=\"p1\">**</bpt>folderPath<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>fileName<ept id=\"p2\">**</ept> dynamically based on the <bpt id=\"p3\">**</bpt>SliceStart<ept id=\"p3\">**</ept> time, use the partitionedBy property.",
      "pos": [
        22654,
        22770
      ]
    },
    {
      "content": "In the following example, folderPath uses Year, Month, and Day from from the SliceStart (start time of the slice being processed) and fileName uses Hour from the SliceStart.",
      "pos": [
        22771,
        22944
      ]
    },
    {
      "content": "For example, if a slice is being produced for 2014-10-20T08:00:00, the folderName is set to wikidatagateway/wikisampledataout/2014/10/20 and the fileName is set to 08.csv.",
      "pos": [
        22945,
        23116
      ]
    },
    {
      "content": "See [JSON Scripting Reference][json-script-reference] for details about JSON properties.",
      "pos": [
        23704,
        23792
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the dataset (table is a rectangular dataset).",
      "pos": [
        23798,
        23889
      ]
    },
    {
      "content": "Confirm that you see a message on the title bar that says <bpt id=\"p1\">**</bpt>TABLE DEPLOYED SUCCESSFULLY<ept id=\"p1\">**</ept>.",
      "pos": [
        23890,
        23980
      ]
    },
    {
      "content": "Step 4: Create and run a pipeline",
      "pos": [
        23989,
        24022
      ]
    },
    {
      "pos": [
        24023,
        24165
      ],
      "content": "In this step, you create a <bpt id=\"p1\">**</bpt>pipeline<ept id=\"p1\">**</ept> with one <bpt id=\"p2\">**</bpt>Copy Activity<ept id=\"p2\">**</ept> that uses <bpt id=\"p3\">**</bpt>EmpOnPremSQLTable<ept id=\"p3\">**</ept> as input and <bpt id=\"p4\">**</bpt>OutputBlobTable<ept id=\"p4\">**</ept> as output."
    },
    {
      "pos": [
        24171,
        24281
      ],
      "content": "In the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>Author and deploy<ept id=\"p2\">**</ept> tile to launch the <bpt id=\"p3\">**</bpt>Editor<ept id=\"p3\">**</ept> for the data factory."
    },
    {
      "content": "Author and Deploy Tile",
      "pos": [
        24289,
        24311
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>New pipeline<ept id=\"p1\">**</ept> on the command bar.",
      "pos": [
        24398,
        24440
      ]
    },
    {
      "content": "If you do not see the button, click <bpt id=\"p1\">**</bpt>... (ellipsis)<ept id=\"p1\">**</ept> to expand the command bar.",
      "pos": [
        24441,
        24522
      ]
    },
    {
      "content": "Replace the JSON in the right pane with the following text:",
      "pos": [
        24527,
        24586
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>Deploy<ept id=\"p1\">**</ept> on the command bar to deploy the dataset (table is a rectangular dataset).",
      "pos": [
        27265,
        27356
      ]
    },
    {
      "content": "Confirm that you see a message on the title bar that says <bpt id=\"p1\">**</bpt>PIPELINE DEPLOYED SUCCESSFULLY<ept id=\"p1\">**</ept>.",
      "pos": [
        27357,
        27450
      ]
    },
    {
      "content": "Now, close the <bpt id=\"p1\">**</bpt>Editor<ept id=\"p1\">**</ept> blade by clicking <bpt id=\"p2\">**</bpt>X<ept id=\"p2\">**</ept>.",
      "pos": [
        27456,
        27506
      ]
    },
    {
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> again to close the ADFTutorialDataFactory blade with the toolbar and tree view.",
      "pos": [
        27507,
        27598
      ]
    },
    {
      "content": "If you see <bpt id=\"p1\">**</bpt>your unsaved edits will be discarded<ept id=\"p1\">**</ept> message, click <bpt id=\"p2\">**</bpt>OK<ept id=\"p2\">**</ept>.",
      "pos": [
        27599,
        27673
      ]
    },
    {
      "pos": [
        27677,
        27758
      ],
      "content": "You should be back to the <bpt id=\"p1\">**</bpt>DATA FACTORY<ept id=\"p1\">**</ept> blade for the <bpt id=\"p2\">**</bpt>ADFTutorialOnPremDF<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        27760,
        27901
      ],
      "content": "<bpt id=\"p1\">**</bpt>Congratulations!<ept id=\"p1\">**</ept> You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline."
    },
    {
      "content": "View the data factory in a Diagram View",
      "pos": [
        27908,
        27947
      ]
    },
    {
      "content": "In the <bpt id=\"p1\">**</bpt>Azure Preview Portal<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Diagram<ept id=\"p2\">**</ept> tile on the home page for the <bpt id=\"p3\">**</bpt>ADFTutorialOnPremDF<ept id=\"p3\">**</ept> data factory.",
      "pos": [
        27952,
        28070
      ]
    },
    {
      "content": ":",
      "pos": [
        28071,
        28072
      ]
    },
    {
      "content": "Diagram Link",
      "pos": [
        28080,
        28092
      ]
    },
    {
      "content": "You should see the diagram similar to the following:",
      "pos": [
        28177,
        28229
      ]
    },
    {
      "content": "Diagram View",
      "pos": [
        28237,
        28249
      ]
    },
    {
      "content": "You can zoom in, zoom out, zoom to 100%, zoom to fit, automatically position pipelines and tables, and show lineage information (highlights upstream and downstream items of selected items).",
      "pos": [
        28335,
        28524
      ]
    },
    {
      "content": "You can double-blick on an object (input/output table or pipeline) to see properties for it.",
      "pos": [
        28526,
        28618
      ]
    },
    {
      "content": "Step 5: Monitor the datasets and pipelines",
      "pos": [
        28625,
        28667
      ]
    },
    {
      "content": "In this step, you will use the Azure Portal to monitor what’s going on in an Azure data factory.",
      "pos": [
        28668,
        28764
      ]
    },
    {
      "content": "You can also use PowerShell cmdlets to monitor datasets and pipelines.",
      "pos": [
        28765,
        28835
      ]
    },
    {
      "content": "For details about monitoring, see <bpt id=\"p1\">[</bpt>Monitor and Manage Pipelines<ept id=\"p1\">](monitor-manage-pipelines.md)</ept>.",
      "pos": [
        28836,
        28930
      ]
    },
    {
      "pos": [
        28935,
        28995
      ],
      "content": "Navigate to <bpt id=\"p1\">**</bpt>Azure Preview Portal<ept id=\"p1\">**</ept> (if you have closed it)"
    },
    {
      "pos": [
        28999,
        29119
      ],
      "content": "If the blade for <bpt id=\"p1\">**</bpt>ADFTutorialOnPremDF<ept id=\"p1\">**</ept> is not open, open it by clicking <bpt id=\"p2\">**</bpt>ADFTutorialOnPremDF<ept id=\"p2\">**</ept> on the <bpt id=\"p3\">**</bpt>Startboard<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        29123,
        29215
      ],
      "content": "You should see the <bpt id=\"p1\">**</bpt>count<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>names<ept id=\"p2\">**</ept> of tables and pipeline you created on this blade."
    },
    {
      "content": "Data Factory Home Page",
      "pos": [
        29223,
        29245
      ]
    },
    {
      "pos": [
        29329,
        29358
      ],
      "content": "Now, click <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> tile."
    },
    {
      "pos": [
        29362,
        29421
      ],
      "content": "On the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> blade, click the <bpt id=\"p2\">**</bpt>EmpOnPremSQLTable<ept id=\"p2\">**</ept>."
    },
    {
      "content": "EmpOnPremSQLTable slices",
      "pos": [
        29429,
        29453
      ]
    },
    {
      "content": "Notice that the data slices up to the current time have already been produced and they are <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept>.",
      "pos": [
        29546,
        29647
      ]
    },
    {
      "content": "It is because you have inserted the data in the SQL Server database and it is there all the time.",
      "pos": [
        29648,
        29745
      ]
    },
    {
      "content": "Confirm that no slices show up in the <bpt id=\"p1\">**</bpt>Problem slices<ept id=\"p1\">**</ept> section at the bottom.",
      "pos": [
        29746,
        29825
      ]
    },
    {
      "pos": [
        30599,
        30657
      ],
      "content": "Now, In the <bpt id=\"p1\">**</bpt>Datasets<ept id=\"p1\">**</ept> blade, click <bpt id=\"p2\">**</bpt>OutputBlobTable<ept id=\"p2\">**</ept>."
    },
    {
      "content": "![OputputBlobTable slices][image-data-factory-output-blobtable-slices]",
      "pos": [
        30663,
        30733
      ]
    },
    {
      "content": "Confirm that slices up to the current time are produced and <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept>.",
      "pos": [
        30737,
        30807
      ]
    },
    {
      "content": "Wait until the statuses of slices up to the current time are set to <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept>.",
      "pos": [
        30808,
        30886
      ]
    },
    {
      "pos": [
        30890,
        30972
      ],
      "content": "Click on any data slice from the list and you should see the <bpt id=\"p1\">**</bpt>DATA SLICE<ept id=\"p1\">**</ept> blade."
    },
    {
      "content": "Data Slice Blade",
      "pos": [
        30980,
        30996
      ]
    },
    {
      "pos": [
        31074,
        31270
      ],
      "content": "If the slice is not in the <bpt id=\"p1\">**</bpt>Ready<ept id=\"p1\">**</ept> state, you can see the upstream slices that are not Ready and are blocking the current slice from executing in the <bpt id=\"p2\">**</bpt>Upstream slices that are not ready<ept id=\"p2\">**</ept> list."
    },
    {
      "pos": [
        31276,
        31366
      ],
      "content": "Click on the <bpt id=\"p1\">**</bpt>activity run<ept id=\"p1\">**</ept> from the list at the bottom to see <bpt id=\"p2\">**</bpt>activity run details<ept id=\"p2\">**</ept>."
    },
    {
      "content": "![Activity Run Details blade][image-data-factory-activity-run-details]",
      "pos": [
        31372,
        31442
      ]
    },
    {
      "pos": [
        31448,
        31493
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>X<ept id=\"p1\">**</ept> to close all the blades until you"
    },
    {
      "pos": [
        31499,
        31558
      ],
      "content": "get back to the home blade for the <bpt id=\"p1\">**</bpt>ADFTutorialOnPremDF<ept id=\"p1\">**</ept>."
    },
    {
      "pos": [
        31563,
        31704
      ],
      "content": "(optional) Click <bpt id=\"p1\">**</bpt>Pipelines<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>ADFTutorialOnPremDF<ept id=\"p2\">**</ept>, and drill through input tables (<bpt id=\"p3\">**</bpt>Consumed<ept id=\"p3\">**</ept>) or output tables (<bpt id=\"p4\">**</bpt>Produced<ept id=\"p4\">**</ept>)."
    },
    {
      "pos": [
        31709,
        31775
      ],
      "content": "Use tools such as <bpt id=\"p1\">**</bpt>Azure Storage Explorer<ept id=\"p1\">**</ept> to verify the output."
    },
    {
      "content": "Azure Storage Explorer",
      "pos": [
        31783,
        31805
      ]
    },
    {
      "content": "Setting Credentials and Security",
      "pos": [
        31900,
        31932
      ]
    },
    {
      "content": "If you access the portal from a machine that is different from the gateway machine, you must make sure that the Credentials Manager application can connect to the gateway machine.",
      "pos": [
        31933,
        32112
      ]
    },
    {
      "content": "If the application cannot reach the gateway machine, it will not allow you to set credentials for the data source and to test connection to the data source.",
      "pos": [
        32113,
        32269
      ]
    },
    {
      "content": "When you use the “Setting Credentials” application launched from Azure Portal to set credentials for an on-premises data source, the portal encrypts the credentials with the certificate you specified in the Certificate tab of the Data Management Gateway Configuration Manager on the gateway machine.",
      "pos": [
        32272,
        32571
      ]
    },
    {
      "content": "If you are looking for an API based approach for encrypting the credentials you can  use the <bpt id=\"p1\">[</bpt>New-AzureDataFactoryEncryptValue<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn834940.aspx)</ept> PowerShell cmdlet to encrypt credentials.",
      "pos": [
        32574,
        32799
      ]
    },
    {
      "content": "The cmdlet uses the certificate that gateway is configured to use to encrypt the credentials.",
      "pos": [
        32800,
        32893
      ]
    },
    {
      "content": "You can the encrypted credentials returned by this cmdlet and add it to EncryptedCredential element of the connectionString in the JSON file that you will use with the <bpt id=\"p1\">[</bpt>New-AzureDataFactoryLinkedService<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/dn820246.aspx)</ept> cmdlet or in the JSON snippet in the Data Factory Editor in the portal.",
      "pos": [
        32894,
        33225
      ]
    },
    {
      "pos": [
        33380,
        33524
      ],
      "content": "<bpt id=\"p1\">**</bpt>Note:<ept id=\"p1\">**</ept> If you use the “Setting Credentials” application it automatically sets the encrypted credentials in the linked service as shown above."
    },
    {
      "content": "There is one more approach for setting credentials using Data Factory Editor.",
      "pos": [
        33526,
        33603
      ]
    },
    {
      "content": "If you create a SQL Server linked service by using the editor and you enter credentials in plain text, the credentials are encrypted using a certificate that the Data Factory service owns, NOT the certificate that gateway is configured to use.",
      "pos": [
        33604,
        33847
      ]
    },
    {
      "content": "While this approach might be a little faster in some cases it is less secure.",
      "pos": [
        33848,
        33925
      ]
    },
    {
      "content": "Therefore, we recommend that you follow this approach only for development/testing purposes.",
      "pos": [
        33926,
        34018
      ]
    },
    {
      "content": "Creating and registering a gateway using Azure PowerShell",
      "pos": [
        34025,
        34082
      ]
    },
    {
      "content": "This section describes how to create and register a gateway using Azure PowerShell cmdlets.",
      "pos": [
        34084,
        34175
      ]
    },
    {
      "pos": [
        34181,
        34231
      ],
      "content": "Launch <bpt id=\"p1\">**</bpt>Azure PowerShell<ept id=\"p1\">**</ept> in administrator mode."
    },
    {
      "content": "The Azure Data Factory cmdlets are available in the <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode.",
      "pos": [
        34236,
        34318
      ]
    },
    {
      "content": "Execute the following command to switch to the <bpt id=\"p1\">**</bpt>AzureResourceManager<ept id=\"p1\">**</ept> mode.",
      "pos": [
        34319,
        34396
      ]
    },
    {
      "pos": [
        34454,
        34540
      ],
      "content": "Use the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryGateway<ept id=\"p1\">**</ept> cmdlet to create a logical gateway as follows:"
    },
    {
      "pos": [
        34681,
        34712
      ],
      "content": "<bpt id=\"p1\">**</bpt>Example command and output<ept id=\"p1\">**</ept>:"
    },
    {
      "pos": [
        35229,
        35388
      ],
      "content": "Use the <bpt id=\"p1\">**</bpt>New-AzureDataFactoryGatewayKey<ept id=\"p1\">**</ept> cmdlet to generate a registration key for the newly created gateway, and store the key in a local variable <bpt id=\"p2\">**</bpt>$Key<ept id=\"p2\">**</ept>:"
    },
    {
      "content": "Example command output:",
      "pos": [
        35526,
        35549
      ]
    },
    {
      "pos": [
        35685,
        36021
      ],
      "content": "In Azure PowerShell, switch to the folder: *<bpt id=\"p1\">*</bpt>C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\PowerShellScript\\*<ept id=\"p1\">*</ept> and Run <bpt id=\"p2\">**</bpt>RegisterGateway.ps1<ept id=\"p2\">**</ept> script associated with the local variable <bpt id=\"p3\">**</bpt>$Key<ept id=\"p3\">**</ept> as shown in the following command to register the client agent installed on your machine with the logical gateway you create earlier."
    },
    {
      "content": "You can use the <bpt id=\"p1\">**</bpt>Get-AzureDataFactoryGateway<ept id=\"p1\">**</ept> cmdlet to get the list of Gateways in your data factory.",
      "pos": [
        36132,
        36236
      ]
    },
    {
      "content": "When the <bpt id=\"p1\">**</bpt>Status<ept id=\"p1\">**</ept> shows <bpt id=\"p2\">**</bpt>online<ept id=\"p2\">**</ept>, it means your gateway is ready to use.",
      "pos": [
        36237,
        36313
      ]
    },
    {
      "content": "You can remove a gateway using the <bpt id=\"p1\">**</bpt>Remove-AzureDataFactoryGateway<ept id=\"p1\">**</ept> cmdlet and update description for a gateway using the <bpt id=\"p2\">**</bpt>Set-AzureDataFactoryGateway<ept id=\"p2\">**</ept> cmdlets.",
      "pos": [
        36410,
        36574
      ]
    },
    {
      "content": "For syntax and other details about these cmdlets, see Data Factory Cmdlet Reference.",
      "pos": [
        36575,
        36659
      ]
    },
    {
      "content": "Data flow for copy Using Data Management Gateway",
      "pos": [
        36667,
        36715
      ]
    },
    {
      "content": "When you use a copy activity in a data pipeline to ingest on-premises data to cloud for further processing, or export result data in the cloud back to an on-premises data store, the copy activity internally uses a gateway to transfer data from on-premises data source to cloud and vice versa.",
      "pos": [
        36716,
        37008
      ]
    },
    {
      "content": "Here high level data flow for and summary of steps for copy with data gateway:",
      "pos": [
        37010,
        37088
      ]
    },
    {
      "content": "Data flow using gateway",
      "pos": [
        37091,
        37114
      ]
    },
    {
      "pos": [
        37206,
        37399
      ],
      "content": "Data developer creates a new gateway for an Azure Data Factory using either the <bpt id=\"p1\">[</bpt>Azure Portal<ept id=\"p1\">](http://portal.azure.com)</ept> or <bpt id=\"p2\">[</bpt>PowerShell Cmdlet<ept id=\"p2\">](https://msdn.microsoft.com/library/dn820234.aspx)</ept>."
    },
    {
      "content": "Data developer uses “Linked services” panel to define a new linked service for an on-premises data store with the gateway.",
      "pos": [
        37405,
        37527
      ]
    },
    {
      "content": "As part of setting up the linked service data developer uses the Setting Credentials application as show in the step by step walkthrough to specify authentication types and credentials.",
      "pos": [
        37528,
        37713
      ]
    },
    {
      "content": "The Setting Credentials application dialog will communicate with the data store to test connection and the gateway to save credentials.",
      "pos": [
        37715,
        37850
      ]
    },
    {
      "content": "Gateway will encrypt the credentials with the certificate associated with the gateway (supplied by data developer), before saving the credentials in the cloud.",
      "pos": [
        37855,
        38014
      ]
    },
    {
      "content": "Data factory movement service communicates with the gateway for scheduling &amp; management of jobs via a control channel that uses a shared Azure service bus queue.",
      "pos": [
        38019,
        38180
      ]
    },
    {
      "content": "When copy activity job needs to be kicked off, data factory queues up the request along with credential information.",
      "pos": [
        38181,
        38297
      ]
    },
    {
      "content": "Gateway kicks off the job after polling the queue.",
      "pos": [
        38298,
        38348
      ]
    },
    {
      "content": "The gateway decrypts the credentials with the same certificate and then connects to the on-premises data store with the proper authentication type.",
      "pos": [
        38353,
        38500
      ]
    },
    {
      "content": "The gateway copies data from the on-premises store to a cloud storage, or from a cloud storage to an on-premises data store depending on how the Copy Activity is configured in the data pipeline.",
      "pos": [
        38505,
        38699
      ]
    },
    {
      "content": "Note: For this step the gateway directly communicates with cloud based storage service (e.g. Azure Blob, Azure SQL etc) over secure (HTTPS) channel.",
      "pos": [
        38700,
        38848
      ]
    },
    {
      "content": "Ports and security considerations",
      "pos": [
        38854,
        38887
      ]
    },
    {
      "content": "As mentioned above in the step by step walkthrough there are multiple ways of setting credentials for on-prem data stores with data factory.",
      "pos": [
        38892,
        39032
      ]
    },
    {
      "content": "The port considerations vary for these options.",
      "pos": [
        39033,
        39080
      ]
    },
    {
      "content": "Using the <bpt id=\"p1\">**</bpt>Setting Credentials<ept id=\"p1\">**</ept> App: The Data Management Gateway installation program by default opens <bpt id=\"p2\">**</bpt>8050<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>8051<ept id=\"p3\">**</ept> ports on the local windows firewall for the gateway machine.",
      "pos": [
        39089,
        39276
      ]
    },
    {
      "content": "These ports are used by the Setting Credentials application to relay the credentials to the gateway.",
      "pos": [
        39277,
        39377
      ]
    },
    {
      "content": "These ports are only opened for the machine on local windows firewall.",
      "pos": [
        39378,
        39448
      ]
    },
    {
      "content": "They cannot be reached from internet and you do not need have these opened in the corporate wide firewall.",
      "pos": [
        39449,
        39555
      ]
    },
    {
      "content": "Using the <bpt id=\"p1\">[</bpt>New-AzureDataFactoryEncryptValue<ept id=\"p1\">](https://msdn.microsoft.com/library/dn834940.aspx)</ept> powershell commandlet: a.",
      "pos": [
        39564,
        39684
      ]
    },
    {
      "content": "If you are using powershell command to encrypt the credentials and as a result you do not want gateway installation to open the inbound ports on gateway machine in windows firewall you can do that by using the following command during installation:",
      "pos": [
        39688,
        39936
      ]
    },
    {
      "pos": [
        40011,
        40248
      ],
      "content": "If you are using the <bpt id=\"p1\">**</bpt>Setting Credentials<ept id=\"p1\">**</ept> application you must launch it on a computer that is able to connect to the Data Management Gateway to be able to set credentials for the data source and to test connection to the data source."
    },
    {
      "content": "When copying data from/to an on-premises SQL Server database to/from an Azure SQL database, ensure the following:",
      "pos": [
        40253,
        40366
      ]
    },
    {
      "pos": [
        40378,
        40469
      ],
      "content": "Firewall on the gateway machine allows outgoing TCP communication on <bpt id=\"p1\">**</bpt>TCP<ept id=\"p1\">**</ept> port <bpt id=\"p2\">**</bpt>1433<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        40478,
        40655
      ],
      "content": "Configure <bpt id=\"p1\">[</bpt>Azure SQL firewall settings<ept id=\"p1\">](https://msdn.microsoft.com/library/azure/jj553530.aspx)</ept> to add the <bpt id=\"p2\">**</bpt>IP address of the gateway<ept id=\"p2\">**</ept> machine to the <bpt id=\"p3\">**</bpt>allowed IP addresses<ept id=\"p3\">**</ept>."
    },
    {
      "content": "When copying data to/from on-premises SQL Server to any destination and the gateway and SQL Server machines are different, do the following: <bpt id=\"p1\">[</bpt>configure Windows Firewall<ept id=\"p1\">](https://msdn.microsoft.com/library/ms175043.aspx)</ept> on the SQL Server machine so that the gateway can access the database via ports that the SQL Server instance listens on.",
      "pos": [
        40660,
        41000
      ]
    },
    {
      "content": "For the default instance, it is port 1433.",
      "pos": [
        41001,
        41043
      ]
    },
    {
      "content": "Send Feedback",
      "pos": [
        41048,
        41061
      ]
    },
    {
      "content": "We would really appreciate your feedback on this article.",
      "pos": [
        41062,
        41119
      ]
    },
    {
      "content": "Please take a few minutes to submit your feedback via <bpt id=\"p1\">[</bpt>email<ept id=\"p1\">](mailto:adfdocfeedback@microsoft.com?subject=data-factory-move-data-between-onprem-and-cloud.md)</ept>.",
      "pos": [
        41120,
        41278
      ]
    },
    {
      "content": "test",
      "pos": [
        41284,
        41288
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Move data between on-premises and cloud using Azure Data Factory\" \n    description=\"Learn about moving data between on-premises and cloud using Data Management Gateway and Azure Data Factory.\" \n    services=\"data-factory\" \n    documentationCenter=\"\" \n    authors=\"spelluru\" \n    manager=\"jhubbard\" \n    editor=\"monicar\"/>\n\n<tags \n    ms.service=\"data-factory\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/29/2015\" \n    ms.author=\"spelluru\"/>\n\n# Move Data Between On-premises Sources and Cloud with Data Management Gateway\nOne of the challenges for modern data integration is to seamlessly move data to and from on-premises to cloud. Data factory makes this integration seamless with data management gateway. Data factory management gateway  is an agent you can install on-premises to enable hybrid pipelines.\n\nThis article provides an overview of integrating on-premises data stores with cloud data stores and cloud processing using data factory. This article builds on the [Data Movement Activities](data-factory-data-movement-activities.md) article and other data factory core concepts articles. The following overview assumes you are familiar with data factory concepts like pipelines, activities, datasets and the copy activity.\n\nThe data gateway provides the following capabilities:\n\n1.  Model on-premises data sources and cloud data sources within the same data factory and move data.\n2.  Have a single pane of glass for monitoring and management with visibility into gateway status with data factory cloud dashboard.\n3.  Manage access to on-premises data sources securely.\n    1. No changes required to corporate firewall. Gateway only makes outbound HTTP based connections to open internet.\n    2. Encrypt credentials for your on-premises data stores with your certificate.\n4.  Move data efficiently – data is transferred in parallel, resilient to intermittent network issues with auto retry logic.\n\n## Considerations for using Data Management Gateway\n1.  A single instance of Data Management Gateway can be used for multiple on-premises data sources, but note that **a single gateway instance is tied to only one Azure data factory** and cannot be shared with another data factory.\n2.  You can have **only one instance of Data Management Gateway** installed on a single machine. Suppose, you have two data factories that need to access on-premises data sources, you need to install gateways on two on-premises computers where each gateway tied to a separate data factory.\n3.  The **gateway does not need to be on the same machine as the data source**, but staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the gateway on a machine that is different from the one that hosts on-premises data source so that the gateway does not compete for resources with data source.\n4.  You can have **multiple gateways on different machines connecting to the same on-premises data source**. For example, you may have two gateways serving two data factories but the same on-premises data source is registered with both the data factories.\n5.  If you already have a gateway installed on your computer serving a **Power BI** scenario, please install a **separate gateway for Azure Data Factory** on another machine.\n6.  You must **use the gateway even when you use ExpressRoute**. \n7.  You should treat your data source as an on-premises data source (that is behind a firewall) even when you use **ExpressRoute** and **use the gateway** to establish connectivity between the service and the data source. \n\n## Gateway installation - prerequisites\n1.  The supported **Operating System** versions are Windows 7, Windows 8/8.1, Windows Server 2008 R2, Windows Server 2012.\n2.  The recommended **configuration** for the gateway machine is at least 2 GHz, 4 cores, 8 GB RAM and 80 GB disk.\n3.  If the host machine hibernates, the gateway won’t be able to respond to data requests. Therefore, configure an appropriate **power plan** on the computer before installing the gateway. The gateway installation prompts a message if the machine is configured to hibernate.\n\nDue to the fact that copy activity runs happen on a specific frequency, the resource usage (CPU, memory) on the machine also follows the same pattern with peak and idle times. Resource utilization also depends heavily on the amount of data being moved. When multiple copy jobs are in progress you will observe resource usage go up during peak times. While above is the minimum configuration it is always better to have a configuration with more resources than the min configuration described above depending on your specific load for data movement.\n\n## Installation\nData Management Gateway can be installed by downloading an MSI setup package from the Microsoft Download Center.  The MSI can also be used to upgrade existing Data Management Gateway to the latest version, with all settings preserved. You can find the link to the MSI package from Azure portal by following the step by step walkthrough below.\n\n### Installation Best practices:\n1.  Configure power plan on the host machine for the gateway so that the machine does not hibernate. If the host machine hibernates, the gateway won’t be able to respond to data requests.\n2.  You should backup the certificate associated with the gateway.\n\n### Installation Troubleshooting:\nIf your company uses a firewall or proxy server, additional steps may be required in case Data Management Gateway cannot connect to Microsoft cloud services. \n\n#### Looking at Gateway logs with Event Viewer:\n\nGaetway configuration manager application shows status for gateway like “Disconnected” or “Connecting”.\n\nFor more detailed information you can look at gateway logs in Windows event logs. You can find them by using Windows **Event Viewer** under **Application and Services Logs** > **Data Management Gateway** While troubleshooting gateway related issues look for error level events in the event viewer.\n\n\n#### Possible symptoms for firewall related issues:\n\n1. When you try to register the gateway, you receive the following error: \"Failed to register the gateway key. Before trying to register the gateway key again, confirm that the Data Management Gateway is in a connected state and the Data Management Gateway Host Service is Started.\"\n2. When you open Configuration Manager, you see status as “Disconnected” or “Connecting”. When viewing Windows event logs, under “Event Viewer” > “Application and Services Logs” > “Data Management Gateway” you see error messages such as “Unable to connect to the remote server” or “A component of Data Management Gateway has become unresponsive and will restart automatically. Component name: Gateway.”\n\nThese are caused by the improper configuration of the firewall or proxy server, which blocks Data Management Gateway from connecting to cloud services to authenticate itself.\n\nThe two firewalls that are possibly in scope are: corporate firewall running on the central router of the organization, and Windows firewall configured as a daemon on the local machine where the gateway is installed. Here are the some considerations:\n\n- There is no need to change the inbound policy for corporate firewall.\n- Both corporate firewall and Windows firewall should enable outbound rule for TCP ports: 80, 440, and from 9305 to 9354. These are used by Microsoft Azure Service Bus to establish connection between the cloud services and Data Management Gateway.\n\nThe MSI setup will automatically configure Windows firewall rules for inbound ports for the gateway machine (see ports and security considerations section above).\n\nBut the setup assumes the above mentioned outbound ports are allowed by default on the local machine and corporate firewall. You need to enable these outbound ports if that is not the case. If you have replaced the Windows firewall with a third party firewall, these ports might need to be opened manually. \n\nIf your company uses a proxy server, then you need to add Microsoft Azure to the whitelist. You can download a list of valid Microsoft Azure IP addresses from the [Microsoft Download Center](http://msdn.microsoft.com/library/windowsazure/dn175718.aspx).\n\n## Using the Data Gateway – Step by Step Walkthrough\nIn this walkthrough, you create a data factory with a pipeline that moves data from an on-premises SQL Server database to an Azure blob. \n\n### Step 1: Create an Azure data factory\nIn this step, you use the Azure Management Portal to create an Azure Data Factory instance named **ADFTutorialOnPremDF**. You can also create a data factory by using Azure Data Factory cmdlets. \n\n1.  After logging into the [Azure Preview Portal](https://portal.azure.com), click **NEW** from the bottom-left corner, select **Data analytics** in the **Create** blade, and click **Data Factory** on the **Data analytics** blade.\n\n    ![New->DataFactory](./media/data-factory-move-data-between-onprem-and-cloud/NewDataFactoryMenu.png) \n  \n6. In the **New data factory** blade:\n    1. Enter **ADFTutorialOnPremDF** for the **name**.\n    2. Click **RESOURCE GROUP NAME** and select **ADFTutorialResourceGroup**. You can select an existing resource group or create a new one. To create a new resource group:\n        1. Click **Create a new resource group**.\n        2. In the **Create resource group blade**, enter a **name** for the resource group, and click **OK**.\n\n7. Note that **Add to Startboard** is checked on the **New data factory** blade.\n\n    ![Add to Startboard](./media/data-factory-move-data-between-onprem-and-cloud/OnPremNewDataFactoryAddToStartboard.png)\n\n8. In the **New data factory** blade, click **Create**.\n\n    The name of the Azure data factory must be globally unique. If you receive the error: **Data factory name “ADFTutorialOnPremDF” is not available**, change the name of the data factory (for example, yournameADFTutorialOnPremDF) and try creating again. Use this name in place of ADFTutorialOnPremDF while performing remaining steps in this tutorial.  \n\n9. Look for notifications from the creation process in the **NOTIFICATIONS** hub on the left. Click **X** to close the **NOTIFCATIONS** blade if it is open.\n\n    ![NOTIFICATIONS hub](./media/data-factory-move-data-between-onprem-and-cloud/OnPremNotificationsHub.png)\n\n11. After creation is complete, you will see the **Data Factory** blade as shown below:\n\n    ![Data Factory Home Page](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDataFactoryHomePage.png)\n\n### Step 2: Create a data management gateway\n5.  On the **Data Factory** blade for **ADFTutorialOnPremDF**, click **Linked Services**. \n\n    ![Data Factory Home Page](./media/data-factory-move-data-between-onprem-and-cloud/DataFactoryHomePage.png)\n\n2.  On the **Linked Services** blade, click **+ Data gateway**.\n\n    ![Linked Services - Add a Gateway button](./media/data-factory-move-data-between-onprem-and-cloud/OnPremLinkedServicesAddGaewayButton.png)\n\n2. In the **Create** blade, enter **adftutorialgateway** for the **name**, and click **OK**.    \n\n    ![Create Gateway blade](./media/data-factory-move-data-between-onprem-and-cloud/OnPremCreateGatewayBlade.png)\n\n3. In the **Configure** blade, click **Install directly on this computer**. This will download the installation package for the gateway, install, configure, and register the gateway on the computer.  \n\n    > [AZURE.NOTE] Please use Internet Explorer or a Microsoft ClickOnce compatible web browser.\n\n    ![Gateway - Configure blade](./media/data-factory-move-data-between-onprem-and-cloud/OnPremGatewayConfigureBlade.png)\n\n    This is the easiest way (one-click) to download, install, configure, and register the gateway in one single step. You can see the **Microsoft Data Management Gateway Configuration Manager** application is installed on your computer. You can also find the executable **ConfigManager.exe** in the folder: **C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\Shared**.\n\n    You can also download and install gateway manually by using the links in this blade and register it using the key shown in the **REGISTER WITH KEY** text box.\n    \n    See [Data Management Gateway](#DMG) section for details about the gateway including best practices and important considerations.\n\n    >[AZURE.NOTE] You must be an administrator on the local computer to install and configure the Data Management Gateway successfully. You can add additional users to the Data Management Gateway Users local Windows group. The members of this group will be able to use the Data Management Gateway Configuration Manager tool to configure the gateway. \n\n4. Click the **NOTIFICATIONS** hub on the left. Wait until you see **Express setup for 'adftutorialgateway' succeeded** message in the **Notifications** blade.\n\n    ![Express setup succeeded](./media/data-factory-move-data-between-onprem-and-cloud/express-setup-succeeded.png)\n6. Click **OK** on the **Create** blade and then on the **New data gateway** blade.\n6. Close the **Linked Services** blade (by pressing **X** button in the top-right corner) and reopen the **Linked Services** blade to see the latest status of the gateway. \n7. Confirm that the **status** of the gateway is **Online**. \n\n    ![Gateway status](./media/data-factory-move-data-between-onprem-and-cloud/gateway-status.png)\n\n5. Launch **Microsoft Data Management Gateway Configuration Manager** application  on your computer.\n\n    ![Gateway Configuration Manager](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDMGConfigurationManager.png)\n\n6. Wait until the values are set as follows :\n    1. If the Service **status** is not set to **Started**, click **Start service** to start the service and wait for a minute for the other fields to refresh.\n    2. **Gateway name** is set to **adftutorialgateway**.\n    3. **Instance name** is set to **adftutorialgateway**.\n    4. **Gateway key status** is set to **Registered**.\n    5. The status bar the bottom displays **Connected to Data Management Gateway Cloud Service** along with a **green check mark**.\n    \n7. Switch to **Certificates**. The certificate specified on this tab is used to encrypt/decrypt credentials for the on-premises data store that you specify on the portal. Click **Change** to use your own certificate instead. By default, the gateway uses the certificate that is auto-generated by the Data Factory service.\n\n    ![Gateway certificate configuration](./media/data-factory-move-data-between-onprem-and-cloud/gateway-certificate.png)\n\n8. In the portal, on the **Linked Services** blade, confirm that the **status** of the gateway is **Good**. \n\n\n### Step 2: Create linked services \nIn this step, you will create two linked services: **StorageLinkedService** and **SqlServerLinkedService**. The **SqlServerLinkedService** links an on-premises SQL Server database and the **StorageLinkedService** linked service links an Azure blob store to the **ADFTutorialDataFactory**. You will create a pipeline later in this walkthrough that copies data from the on-premises SQL Server database to the Azure blob store. \n\n#### Add a linked service to an on-premises SQL Server database\n1.  In the **Linked Services** blade, click **New data store** on the command bar.\n2.  Enter **SqlServerLinkedService** for the **name**. \n2.  Click arrow next to the **Type**, and select **SQL Server**.\n\n    ![Create new data store](./media/data-factory-move-data-between-onprem-and-cloud/new-data-store.png)\n3.  You should more settings below the **Type** setting.\n4.  For the **Data gateway** setting, select the gateway you just created. \n\n    ![SQL Server settings](./media/data-factory-move-data-between-onprem-and-cloud/sql-server-settings.png)\n4.  Enter the name of your database server for the **Server** setting.\n5.  Enter the name of the database for the **Database** setting.\n6.  Click arrow next to **Credentials**.\n\n    ![Credentials blade](./media/data-factory-move-data-between-onprem-and-cloud/credentials-dialog.png)\n7.  In the **Credentials** blade, click **Click here to set credentials**.\n8.  In the **Setting Credentials** dialog box, do the following:\n\n    ![Setting credentials dialog](./media/data-factory-move-data-between-onprem-and-cloud/setting-credentials-dialog.png)\n    1.  Select **authentication** that you want the Data Factory service to use to connect to the database. \n    2.  Enter name of the user who has access to the database for the **USERNAME** setting. \n    3.  Enter password for the user for the **PASSWORD** setting.  \n    4.  Click **OK** to close the dialog box. \n4. Click **OK** to close the **Credentials** blade. \n5. Click **OK** on the **New data store** blade.    \n6. Confirm that the status for **SqlServerLinkedService** is set to Online in the Linked Services blade.\n    ![SQL Server linked service status](./media/data-factory-move-data-between-onprem-and-cloud/sql-server-linked-service-status.png)\n\nSee [Setting Credentials and Security](#setting-credentials-and-security) section for more details about setting credentials.  \n\n#### Add a linked service for an Azure storage account\n \n1. In the **Linked Services** blade, click **New datastore** on the toolbar. \n2. Enter **StorageLinkedService** for the **Name** field. \n3. Click arrow next to **Type** and select **Azure Storage**.\n4. You should see new fields: **Account name** and **Account key** under **Type** setting now. \n3. Enter the name of your Azure storage account for the **Account name**.\n4. Enter the key for your Azure storage account for the **Account key**. \n5. Click **OK** to close the dialog box. \n\n \n### Step 3: Create input and output datasets\nIn this step, you will create input and output datasets that represent input and output data for the copy operation (On-premises SQL Server database => Azure blob storage). Before creating datasets or tables (rectangular datasets), you need to do the following (detailed steps follows the list):\n\n- Create a table named **emp** in the SQL Server Database you added as a linked service to the data factory and insert couple of sample entries into the table.\n- Create a blob container named **adftutorial** in the Azure blob storage account you added as a linked service to the data factory.\n\n### Prepare On-premises SQL Server for the tutorial\n\n1. In the database you specified for the on-premises SQL Server linked service (**SqlServerLinkedService**), use the following SQL script to create the **emp** table in the database.\n\n\n        CREATE TABLE dbo.emp\n        (\n            ID int IDENTITY(1,1) NOT NULL, \n            FirstName varchar(50),\n            LastName varchar(50),\n            CONSTRAINT PK_emp PRIMARY KEY (ID)\n        )\n        GO\n \n\n2. Insert some sample into the table: \n\n\n        INSERT INTO emp VALUES ('John', 'Doe')\n        INSERT INTO emp VALUES ('Jane', 'Doe')\n\n\n\n### Create input table\n\n1.  In the **DATA FACTORY** blade, click **Author and deploy** tile to launch the **Editor** for the data factory.\n\n    ![Author and Deploy Tile](./media/data-factory-move-data-between-onprem-and-cloud/author-deploy-tile.png) \n1. In the **Data Factory Editor**, click **New dataset** on the command bar, and click **On-premises SQL**. \n2.  Replace the JSON in the right pane with the following text:    \n\n        {\n          \"name\": \"EmpOnPremSQLTable\",\n          \"properties\": {\n            \"type\": \"SqlServerTable\",\n            \"linkedServiceName\": \"SqlServerLinkedService\",\n            \"typeProperties\": {\n              \"tableName\": \"emp\"\n            },\n            \"external\": true,\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            },\n            \"policy\": {\n              \"externalData\": {\n                \"retryInterval\": \"00:01:00\",\n                \"retryTimeout\": \"00:10:00\",\n                \"maximumRetry\": 3\n              }\n            }\n          }\n        }\n\n    Note the following: \n    \n    - **type** is set to **SqlServerTable**.\n    - **tableName** is set to **emp**.\n    - **linkedServiceName** is set to **SqlServerLinkedService** (you had created this linked service in Step 2).\n    - For an input table that is not generated by another pipeline in Azure Data Factory, you must set **external** to **true**. It denotes the input data is produced external to the Azure Data Factory service. You can optionally specify any external data policies using the **externalData** element in the **Policy** section.    \n\n    See [JSON Scripting Reference][json-script-reference] for details about JSON properties.\n\n2. Click **Deploy** on the command bar to deploy the dataset (table is a rectangular dataset). Confirm that you see a message on the title bar that says **TABLE DEPLOYED SUCCESSFULLY**. \n\n\n### Create output table\n\n1.  In the **Data Factory Editor**, click **New dataset** on the command bar, and click **Azure Blob storage**.\n2.  Replace the JSON in the right pane with the following text: \n\n        {\n          \"name\": \"OutputBlobTable\",\n          \"properties\": {\n            \"type\": \"AzureBlob\",\n            \"linkedServiceName\": \"StorageLinkedService\",\n            \"typeProperties\": {\n              \"folderPath\": \"adftutorial/outfromonpremdf\",\n              \"format\": {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \",\"\n              }\n            },\n            \"availability\": {\n              \"frequency\": \"Hour\",\n              \"interval\": 1\n            }\n          }\n        }\n  \n    Note the following: \n    \n    - **type** is set to **AzureBlob**.\n    - **linkedServiceName** is set to **StorageLinkedService** (you had created this linked service in Step 2).\n    - **folderPath** is set to **adftutorial/outfromonpremdf** where outfromonpremdf is the folder in the adftutorial container. You just need to create the **adftutorial** container.\n    - The **availability** is set to **hourly** (**frequency** set to **hour** and **interval** set to **1**).  The Data Factory service will generate an output data slice every hour in the **emp** table in the Azure SQL Database. \n\n    if you don't specify a **fileName** for an **input table**, all files/blobs from the input folder (**folderPath**) are considered as inputs. If you specify a fileName in the JSON, only the specified file/blob is considered asn input. See the sample files in the [tutorial][adf-tutorial] for examples.\n \n    If you do not specify a **fileName** for an **output table**, the generated files in the **folderPath** are named in the following format: Data.<Guid>.txt (for example: : Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).\n\n    To set **folderPath** and **fileName** dynamically based on the **SliceStart** time, use the partitionedBy property. In the following example, folderPath uses Year, Month, and Day from from the SliceStart (start time of the slice being processed) and fileName uses Hour from the SliceStart. For example, if a slice is being produced for 2014-10-20T08:00:00, the folderName is set to wikidatagateway/wikisampledataout/2014/10/20 and the fileName is set to 08.csv. \n\n        \"folderPath\": \"wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}\",\n        \"fileName\": \"{Hour}.csv\",\n        \"partitionedBy\": \n        [\n            { \"name\": \"Year\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"yyyy\" } },\n            { \"name\": \"Month\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"MM\" } }, \n            { \"name\": \"Day\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"dd\" } }, \n            { \"name\": \"Hour\", \"value\": { \"type\": \"DateTime\", \"date\": \"SliceStart\", \"format\": \"hh\" } } \n        ],\n\n \n\n    See [JSON Scripting Reference][json-script-reference] for details about JSON properties.\n\n2.  Click **Deploy** on the command bar to deploy the dataset (table is a rectangular dataset). Confirm that you see a message on the title bar that says **TABLE DEPLOYED SUCCESSFULLY**.\n  \n\n### Step 4: Create and run a pipeline\nIn this step, you create a **pipeline** with one **Copy Activity** that uses **EmpOnPremSQLTable** as input and **OutputBlobTable** as output.\n\n1.  In the **DATA FACTORY** blade, click **Author and deploy** tile to launch the **Editor** for the data factory.\n\n    ![Author and Deploy Tile](./media/data-factory-move-data-between-onprem-and-cloud/author-deploy-tile.png) \n2.  Click **New pipeline** on the command bar. If you do not see the button, click **... (ellipsis)** to expand the command bar.\n2.  Replace the JSON in the right pane with the following text:   \n\n\n        {\n          \"name\": \"ADFTutorialPipelineOnPrem\",\n          \"properties\": {\n            \"description\": \"This pipeline has one Copy activity that copies data from an on-prem SQL to Azure blob\",\n            \"activities\": [\n              {\n                \"name\": \"CopyFromSQLtoBlob\",\n                \"description\": \"Copy data from on-prem SQL server to blob\",\n                \"type\": \"Copy\",\n                \"inputs\": [\n                  {\n                    \"name\": \"EmpOnPremSQLTable\"\n                  }\n                ],\n                \"outputs\": [\n                  {\n                    \"name\": \"OutputBlobTable\"\n                  }\n                ],\n                \"typeProperties\": {\n                  \"source\": {\n                    \"type\": \"SqlSource\",\n                    \"sqlReaderQuery\": \"select * from emp\"\n                  },\n                  \"sink\": {\n                    \"type\": \"BlobSink\"\n                  }\n                },\n                \"Policy\": {\n                  \"concurrency\": 1,\n                  \"executionPriorityOrder\": \"NewestFirst\",\n                  \"style\": \"StartOfInterval\",\n                  \"retry\": 0,\n                  \"timeout\": \"01:00:00\"\n                }\n              }\n            ],\n            \"start\": \"2015-02-13T00:00:00Z\",\n            \"end\": \"2015-02-14T00:00:00Z\",\n            \"isPaused\": false\n          }\n        }\n\n    Note the following:\n \n    - In the activities section, there is only activity whose **type** is set to **Copy**.\n    - **Input** for the activity is set to **EmpOnPremSQLTable** and **output** for the activity is set to **OutputBlobTable**.\n    - In the **transformation** section, **SqlSource** is specified as the **source type** and **BlobSink **is specified as the **sink type**.\n    - SQL query **select * from emp** is specified for the **sqlReaderQuery** property of **SqlSource**.\n\n    Replace the value of the **start** property with the current day and **end** value with the next day. Both start and end datetimes must be in [ISO format](http://en.wikipedia.org/wiki/ISO_8601). For example: 2014-10-14T16:32:41Z. The **end** time is optional, but we will use it in this tutorial. \n    \n    If you do not specify value for the **end** property, it is calculated as \"**start + 48 hours**\". To run the pipeline indefinitely, specify **9/9/9999** as the value for the **end** property. \n    \n    You are defining the time duration in which the data slices will be processed based on the **Availability** properties that were defined for each Azure Data Factory table.\n    \n    In the example above, there will be 24 data slices as each data slice is produced hourly.\n    \n2. Click **Deploy** on the command bar to deploy the dataset (table is a rectangular dataset). Confirm that you see a message on the title bar that says **PIPELINE DEPLOYED SUCCESSFULLY**.  \n5. Now, close the **Editor** blade by clicking **X**. Click **X** again to close the ADFTutorialDataFactory blade with the toolbar and tree view. If you see **your unsaved edits will be discarded** message, click **OK**.\n6. You should be back to the **DATA FACTORY** blade for the **ADFTutorialOnPremDF**.\n\n**Congratulations!** You have successfully created an Azure data factory, linked services, tables, and a pipeline and scheduled the pipeline.\n\n#### View the data factory in a Diagram View \n1. In the **Azure Preview Portal**, click **Diagram** tile on the home page for the **ADFTutorialOnPremDF** data factory. :\n\n    ![Diagram Link](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramLink.png)\n\n2. You should see the diagram similar to the following:\n\n    ![Diagram View](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramView.png)\n\n    You can zoom in, zoom out, zoom to 100%, zoom to fit, automatically position pipelines and tables, and show lineage information (highlights upstream and downstream items of selected items).  You can double-blick on an object (input/output table or pipeline) to see properties for it. \n\n### Step 5: Monitor the datasets and pipelines\nIn this step, you will use the Azure Portal to monitor what’s going on in an Azure data factory. You can also use PowerShell cmdlets to monitor datasets and pipelines. For details about monitoring, see [Monitor and Manage Pipelines](monitor-manage-pipelines.md).\n\n1. Navigate to **Azure Preview Portal** (if you have closed it)\n2. If the blade for **ADFTutorialOnPremDF** is not open, open it by clicking **ADFTutorialOnPremDF** on the **Startboard**.\n3. You should see the **count** and **names** of tables and pipeline you created on this blade.\n\n    ![Data Factory Home Page](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramView.png)\n4. Now, click **Datasets** tile.\n5. On the **Datasets** blade, click the **EmpOnPremSQLTable**.\n\n    ![EmpOnPremSQLTable slices](./media/data-factory-move-data-between-onprem-and-cloud/OnPremSQLTableSlicesBlade.png)\n\n6. Notice that the data slices up to the current time have already been produced and they are **Ready**. It is because you have inserted the data in the SQL Server database and it is there all the time. Confirm that no slices show up in the **Problem slices** section at the bottom.\n\n\n    Both **Recently updated slices** and **Recently failed slices** lists are sorted by the **LAST UPDATE TIME**. The update time of a slice is changed in the following situations. \n    \n\n    -  You update the status of the slice manually, for example, by using the **Set-AzureDataFactorySliceStatus** (or) by clicking **RUN** on the **SLICE** blade for the slice.\n    -  The slice changes status due to an execution (e.g. a run started, a run ended and failed, a run ended and succeeded, etc).\n \n    Click on the title of the lists or **... (ellipses)** to see the larger list of slices. Click **Filter** on the toolbar to filter the slices.  \n    \n    To view the data slices sorted by the slice start/end times instead, click **Data slices (by slice time)** tile.\n\n7. Now, In the **Datasets** blade, click **OutputBlobTable**.\n\n    ![OputputBlobTable slices][image-data-factory-output-blobtable-slices]\n8. Confirm that slices up to the current time are produced and **Ready**. Wait until the statuses of slices up to the current time are set to **Ready**.\n9. Click on any data slice from the list and you should see the **DATA SLICE** blade.\n\n    ![Data Slice Blade](./media/data-factory-move-data-between-onprem-and-cloud/DataSlice.png)\n\n    If the slice is not in the **Ready** state, you can see the upstream slices that are not Ready and are blocking the current slice from executing in the **Upstream slices that are not ready** list.\n\n10. Click on the **activity run** from the list at the bottom to see **activity run details**.\n\n    ![Activity Run Details blade][image-data-factory-activity-run-details]\n\n11. Click **X** to close all the blades until you \n12. get back to the home blade for the **ADFTutorialOnPremDF**.\n14. (optional) Click **Pipelines**, click **ADFTutorialOnPremDF**, and drill through input tables (**Consumed**) or output tables (**Produced**).\n15. Use tools such as **Azure Storage Explorer** to verify the output.\n\n    ![Azure Storage Explorer](./media/data-factory-move-data-between-onprem-and-cloud/OnPremAzureStorageExplorer.png)\n\n\n## Setting Credentials and Security\nIf you access the portal from a machine that is different from the gateway machine, you must make sure that the Credentials Manager application can connect to the gateway machine. If the application cannot reach the gateway machine, it will not allow you to set credentials for the data source and to test connection to the data source. \n\nWhen you use the “Setting Credentials” application launched from Azure Portal to set credentials for an on-premises data source, the portal encrypts the credentials with the certificate you specified in the Certificate tab of the Data Management Gateway Configuration Manager on the gateway machine. \n\nIf you are looking for an API based approach for encrypting the credentials you can  use the [New-AzureDataFactoryEncryptValue](https://msdn.microsoft.com/library/azure/dn834940.aspx) PowerShell cmdlet to encrypt credentials. The cmdlet uses the certificate that gateway is configured to use to encrypt the credentials. You can the encrypted credentials returned by this cmdlet and add it to EncryptedCredential element of the connectionString in the JSON file that you will use with the [New-AzureDataFactoryLinkedService](https://msdn.microsoft.com/library/azure/dn820246.aspx) cmdlet or in the JSON snippet in the Data Factory Editor in the portal. \n\n    \"connectionString\": \"Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=True;EncryptedCredential=<encrypted credential>\",\n\n**Note:** If you use the “Setting Credentials” application it automatically sets the encrypted credentials in the linked service as shown above.\n\nThere is one more approach for setting credentials using Data Factory Editor. If you create a SQL Server linked service by using the editor and you enter credentials in plain text, the credentials are encrypted using a certificate that the Data Factory service owns, NOT the certificate that gateway is configured to use. While this approach might be a little faster in some cases it is less secure. Therefore, we recommend that you follow this approach only for development/testing purposes. \n\n\n## Creating and registering a gateway using Azure PowerShell \nThis section describes how to create and register a gateway using Azure PowerShell cmdlets. \n\n1. Launch **Azure PowerShell** in administrator mode. \n2. The Azure Data Factory cmdlets are available in the **AzureResourceManager** mode. Execute the following command to switch to the **AzureResourceManager** mode.     \n\n        switch-azuremode AzureResourceManager\n\n\n2. Use the **New-AzureDataFactoryGateway** cmdlet to create a logical gateway as follows:\n\n        New-AzureDataFactoryGateway -Name <gatewayName> -DataFactoryName <dataFactoryName> -ResourceGroupName ADF –Description <desc>\n\n    **Example command and output**:\n\n\n        PS C:\\> New-AzureDataFactoryGateway -Name MyGateway -DataFactoryName $df -ResourceGroupName ADF –Description “gateway for walkthrough”\n\n        Name              : MyGateway\n        Description       : gateway for walkthrough\n        Version           :\n        Status            : NeedRegistration\n        VersionStatus     : None\n        CreateTime        : 9/28/2014 10:58:22\n        RegisterTime      :\n        LastConnectTime   :\n        ExpiryTime        :\n        ProvisioningState : Succeeded\n\n\n3. Use the **New-AzureDataFactoryGatewayKey** cmdlet to generate a registration key for the newly created gateway, and store the key in a local variable **$Key**:\n\n        New-AzureDataFactoryGatewayKey -GatewayName <gatewayname> -ResourceGroupName ADF -DataFactoryName <dataFactoryName>\n\n    \n    **Example command output:**\n\n\n        PS C:\\> $Key = New-AzureDataFactoryGatewayKey -GatewayName MyGateway -ResourceGroupName ADF -DataFactoryName $df \n\n    \n4. In Azure PowerShell, switch to the folder: **C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\PowerShellScript\\** and Run **RegisterGateway.ps1** script associated with the local variable **$Key** as shown in the following command to register the client agent installed on your machine with the logical gateway you create earlier.\n\n        PS C:\\> .\\RegisterGateway.ps1 $Key.GatewayKey\n        \n        Agent registration is successful!\n\n5. You can use the **Get-AzureDataFactoryGateway** cmdlet to get the list of Gateways in your data factory. When the **Status** shows **online**, it means your gateway is ready to use.\n\n        Get-AzureDataFactoryGateway -DataFactoryName <dataFactoryName> -ResourceGroupName ADF\n\nYou can remove a gateway using the **Remove-AzureDataFactoryGateway** cmdlet and update description for a gateway using the **Set-AzureDataFactoryGateway** cmdlets. For syntax and other details about these cmdlets, see Data Factory Cmdlet Reference.  \n\n\n## Data flow for copy Using Data Management Gateway\nWhen you use a copy activity in a data pipeline to ingest on-premises data to cloud for further processing, or export result data in the cloud back to an on-premises data store, the copy activity internally uses a gateway to transfer data from on-premises data source to cloud and vice versa.\n\nHere high level data flow for and summary of steps for copy with data gateway:\n![Data flow using gateway](./media/data-factory-move-data-between-onprem-and-cloud/data-flow-using-gateway.png)\n\n1.  Data developer creates a new gateway for an Azure Data Factory using either the [Azure Portal](http://portal.azure.com) or [PowerShell Cmdlet](https://msdn.microsoft.com/library/dn820234.aspx). \n2.  Data developer uses “Linked services” panel to define a new linked service for an on-premises data store with the gateway. As part of setting up the linked service data developer uses the Setting Credentials application as show in the step by step walkthrough to specify authentication types and credentials.  The Setting Credentials application dialog will communicate with the data store to test connection and the gateway to save credentials.\n3.  Gateway will encrypt the credentials with the certificate associated with the gateway (supplied by data developer), before saving the credentials in the cloud.\n4.  Data factory movement service communicates with the gateway for scheduling & management of jobs via a control channel that uses a shared Azure service bus queue. When copy activity job needs to be kicked off, data factory queues up the request along with credential information. Gateway kicks off the job after polling the queue.\n5.  The gateway decrypts the credentials with the same certificate and then connects to the on-premises data store with the proper authentication type.\n6.  The gateway copies data from the on-premises store to a cloud storage, or from a cloud storage to an on-premises data store depending on how the Copy Activity is configured in the data pipeline. Note: For this step the gateway directly communicates with cloud based storage service (e.g. Azure Blob, Azure SQL etc) over secure (HTTPS) channel.\n\n### Ports and security considerations\n\n1. As mentioned above in the step by step walkthrough there are multiple ways of setting credentials for on-prem data stores with data factory. The port considerations vary for these options. \n\n    - Using the **Setting Credentials** App: The Data Management Gateway installation program by default opens **8050** and **8051** ports on the local windows firewall for the gateway machine. These ports are used by the Setting Credentials application to relay the credentials to the gateway. These ports are only opened for the machine on local windows firewall. They cannot be reached from internet and you do not need have these opened in the corporate wide firewall.\n    2.  Using the [New-AzureDataFactoryEncryptValue](https://msdn.microsoft.com/library/dn834940.aspx) powershell commandlet: a.    If you are using powershell command to encrypt the credentials and as a result you do not want gateway installation to open the inbound ports on gateway machine in windows firewall you can do that by using the following command during installation:\n    \n            msiexec /q /i DataManagementGateway.msi NOFIREWALL=1\n3.  If you are using the **Setting Credentials** application you must launch it on a computer that is able to connect to the Data Management Gateway to be able to set credentials for the data source and to test connection to the data source.\n4.  When copying data from/to an on-premises SQL Server database to/from an Azure SQL database, ensure the following:   \n    -   Firewall on the gateway machine allows outgoing TCP communication on **TCP** port **1433**.\n    -   Configure [Azure SQL firewall settings](https://msdn.microsoft.com/library/azure/jj553530.aspx) to add the **IP address of the gateway** machine to the **allowed IP addresses**.\n5.  When copying data to/from on-premises SQL Server to any destination and the gateway and SQL Server machines are different, do the following: [configure Windows Firewall](https://msdn.microsoft.com/library/ms175043.aspx) on the SQL Server machine so that the gateway can access the database via ports that the SQL Server instance listens on. For the default instance, it is port 1433.\n\n## Send Feedback\nWe would really appreciate your feedback on this article. Please take a few minutes to submit your feedback via [email](mailto:adfdocfeedback@microsoft.com?subject=data-factory-move-data-between-onprem-and-cloud.md). \n\n\n\n\ntest\n"
}