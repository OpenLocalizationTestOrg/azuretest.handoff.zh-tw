{
  "nodes": [
    {
      "content": "Learn about Stream Analytics key concepts | Microsoft Azure",
      "pos": [
        28,
        87
      ]
    },
    {
      "content": "Learn key concepts of Azure Stream Analytics: Components of a stream analytics job, including supported inputs and outputs, job configuration, and metrics.",
      "pos": [
        107,
        262
      ]
    },
    {
      "content": "Stream Analytics key concepts: Guide to the basics of a stream analytics job",
      "pos": [
        679,
        755
      ]
    },
    {
      "content": "Azure Stream Analytics is a fully managed service providing low-latency, highly available, scalable, complex event processing over a data stream in the cloud.",
      "pos": [
        758,
        916
      ]
    },
    {
      "content": "Stream Analytics enables customers to set up streaming jobs to analyze data streams, and allows customers to drive near real-time analytics.",
      "pos": [
        917,
        1057
      ]
    },
    {
      "content": "This article explains the key concepts of a Stream Analytics job.",
      "pos": [
        1058,
        1123
      ]
    },
    {
      "content": "What can you do in Stream Analytics?",
      "pos": [
        1128,
        1164
      ]
    },
    {
      "content": "With Stream Analytics, you can:",
      "pos": [
        1165,
        1196
      ]
    },
    {
      "content": "Perform complex event processing on high-volume and high-velocity data streams.",
      "pos": [
        1200,
        1279
      ]
    },
    {
      "content": "Collect event data from globally distributed assets or equipment, such as connected cars or utility grids.",
      "pos": [
        1285,
        1391
      ]
    },
    {
      "content": "Process telemetry data for near real-time monitoring and diagnostics.",
      "pos": [
        1395,
        1464
      ]
    },
    {
      "content": "Capture and archive real-time events for future processing",
      "pos": [
        1468,
        1526
      ]
    },
    {
      "pos": [
        1528,
        1629
      ],
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Introduction to Azure Stream Analytics<ept id=\"p1\">](stream-analytics-introduction.md)</ept>."
    },
    {
      "content": "A Stream Analytics job includes all of the following:",
      "pos": [
        1632,
        1685
      ]
    },
    {
      "content": "One or more input sources",
      "pos": [
        1688,
        1713
      ]
    },
    {
      "content": "A query over an incoming data stream",
      "pos": [
        1716,
        1752
      ]
    },
    {
      "content": "An output target.",
      "pos": [
        1755,
        1772
      ]
    },
    {
      "content": "Inputs",
      "pos": [
        1782,
        1788
      ]
    },
    {
      "content": "Data stream",
      "pos": [
        1794,
        1805
      ]
    },
    {
      "content": "Each Stream Analytics job definition must contain at least one data stream input source to be consumed and transformed by the job.",
      "pos": [
        1807,
        1937
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Azure Blob storage<ept id=\"p1\">](http://azure.microsoft.com/documentation/services/storage/)</ept> and <bpt id=\"p2\">[</bpt>Azure Event Hubs<ept id=\"p2\">](http://azure.microsoft.com/services/event-hubs/)</ept> are supported as data stream input sources.",
      "pos": [
        1938,
        2134
      ]
    },
    {
      "content": "Event Hubs input sources are used to collect event streams from multiple different devices and services, while Blob storage can be used an input source for ingesting large amounts of data.",
      "pos": [
        2135,
        2323
      ]
    },
    {
      "content": "Because blobs do not stream data, Stream Analytics jobs over blobs will not be temporal in nature unless the records in the blob contain timestamps.",
      "pos": [
        2324,
        2472
      ]
    },
    {
      "content": "Reference data",
      "pos": [
        2478,
        2492
      ]
    },
    {
      "content": "Stream Analytics also supports a second type of input source: reference data.",
      "pos": [
        2493,
        2570
      ]
    },
    {
      "content": "This is auxiliary data used for performing correlation and lookups, and the data here is usually static or infrequently changing.",
      "pos": [
        2571,
        2700
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Azure Blob storage<ept id=\"p1\">](http://azure.microsoft.com/documentation/services/storage/)</ept> is the only supported input source for reference data.",
      "pos": [
        2701,
        2836
      ]
    },
    {
      "content": "Reference data source blobs are limited to 50MB in size.",
      "pos": [
        2837,
        2893
      ]
    },
    {
      "content": "To enable support for refreshing reference data the user needs to specify a list of blobs in the input configuration using the {date} and {time} tokens inside the path pattern.",
      "pos": [
        2895,
        3071
      ]
    },
    {
      "content": "The job will load the corresponding blob based on the date and time encoded in the blob names using UTC time zone.",
      "pos": [
        3072,
        3186
      ]
    },
    {
      "content": "For example if the job has a reference input configured in the portal with the path pattern such as: /sample/{date}/{time}/products.csv where the date format is “YYYY-MM-DD” and the time format is “HH:mm” than the job will pick up a file named /sample/2015-04-16/17:30/products.csv at 5:30 PM on April 16th 2015 UTC time zone (which is equivalent to 10:30 AM on April 16th 2015 using PST time zone).",
      "pos": [
        3188,
        3587
      ]
    },
    {
      "content": "Serialization",
      "pos": [
        3594,
        3607
      ]
    },
    {
      "content": "To ensure correct behavior of queries, Stream Analytics must be aware of the serialization format being used on incoming data streams.",
      "pos": [
        3608,
        3742
      ]
    },
    {
      "content": "Currently supported serialization formats are JSON, CSV, and Avro for data streams and CSV or JSON for reference data.",
      "pos": [
        3743,
        3861
      ]
    },
    {
      "content": "Generated properties",
      "pos": [
        3867,
        3887
      ]
    },
    {
      "content": "Depending on the input type used in the job, some additional fields with event metadata will be generated.",
      "pos": [
        3888,
        3994
      ]
    },
    {
      "content": "These fields can be queried against just like other input columns.",
      "pos": [
        3995,
        4061
      ]
    },
    {
      "content": "If an existing event has a field that has the same name as one of the properties below, it will be overwritten with the input metadata.",
      "pos": [
        4062,
        4197
      ]
    },
    {
      "content": "Property",
      "pos": [
        4257,
        4265
      ]
    },
    {
      "content": "Description",
      "pos": [
        4283,
        4294
      ]
    },
    {
      "content": "Blob",
      "pos": [
        4364,
        4368
      ]
    },
    {
      "content": "BlobName",
      "pos": [
        4395,
        4403
      ]
    },
    {
      "content": "The name of the input blob that the event came from.",
      "pos": [
        4421,
        4473
      ]
    },
    {
      "content": "EventProcessedUtcTime",
      "pos": [
        4510,
        4531
      ]
    },
    {
      "content": "The date and time that the blob record was processed.",
      "pos": [
        4549,
        4602
      ]
    },
    {
      "content": "BlobLastModifiedUtcTime",
      "pos": [
        4639,
        4662
      ]
    },
    {
      "content": "The date and time that the blob was last modified.",
      "pos": [
        4680,
        4730
      ]
    },
    {
      "content": "PartitionId",
      "pos": [
        4767,
        4778
      ]
    },
    {
      "content": "The zero-based partition ID for the input adapter.",
      "pos": [
        4796,
        4846
      ]
    },
    {
      "content": "Event Hub",
      "pos": [
        4911,
        4920
      ]
    },
    {
      "content": "EventProcessedUtcTime",
      "pos": [
        4942,
        4963
      ]
    },
    {
      "content": "The date and time that the event was processed.",
      "pos": [
        4981,
        5028
      ]
    },
    {
      "content": "EventEnqueuedUtcTime",
      "pos": [
        5065,
        5085
      ]
    },
    {
      "content": "The date and time that the event was received by Event Hubs.",
      "pos": [
        5103,
        5163
      ]
    },
    {
      "content": "PartitionId",
      "pos": [
        5200,
        5211
      ]
    },
    {
      "content": "The zero-based partition ID for the input adapter.",
      "pos": [
        5229,
        5279
      ]
    },
    {
      "content": "Partition(s) with slow or no input data",
      "pos": [
        5308,
        5347
      ]
    },
    {
      "content": "When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.",
      "pos": [
        5348,
        5584
      ]
    },
    {
      "content": "Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.",
      "pos": [
        5585,
        5903
      ]
    },
    {
      "content": "This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.",
      "pos": [
        5904,
        6027
      ]
    },
    {
      "content": "You can also decide to wait for only a limited time: ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.",
      "pos": [
        6030,
        6380
      ]
    },
    {
      "content": "This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.",
      "pos": [
        6381,
        6496
      ]
    },
    {
      "content": "Partition(s) with out of order events",
      "pos": [
        6501,
        6538
      ]
    },
    {
      "content": "When streaming job query uses the TIMESTAMP BY keyword, there are no guarantees about the order in which the events will arrive to input, Some events in the same input partition may be lagging, parameter ‘Maximum allowed disorder within an input’ causes the streaming job to act on events that are outside of the order tolerance, according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps.",
      "pos": [
        6539,
        6976
      ]
    },
    {
      "content": "Additional resources",
      "pos": [
        6982,
        7002
      ]
    },
    {
      "pos": [
        7003,
        7212
      ],
      "content": "For details on creating input sources, see <bpt id=\"p1\">[</bpt>Azure Event Hubs developer guide<ept id=\"p1\">](http://msdn.microsoft.com/library/azure/dn789972.aspx)</ept> and <bpt id=\"p2\">[</bpt>Use Azure Blob Storage<ept id=\"p2\">](../storage/storage-dotnet-how-to-use-blobs.md)</ept>."
    },
    {
      "content": "Query",
      "pos": [
        7221,
        7226
      ]
    },
    {
      "content": "The logic to filter, manipulate, and process incoming data is defined in the query of Stream Analytics jobs.",
      "pos": [
        7227,
        7335
      ]
    },
    {
      "content": "Queries are written in the Stream Analytics query language, an SQL-like language that is largely a subset of standard Transact-SQL syntax with some specific extensions for temporal queries.",
      "pos": [
        7336,
        7525
      ]
    },
    {
      "content": "Windowing",
      "pos": [
        7531,
        7540
      ]
    },
    {
      "content": "Windowing extensions allow aggregations and computations to be performed over subsets of events that fall within some period of time.",
      "pos": [
        7541,
        7674
      ]
    },
    {
      "content": "Windowing functions are invoked through the <bpt id=\"p1\">**</bpt>GROUP BY<ept id=\"p1\">**</ept> statement.",
      "pos": [
        7675,
        7742
      ]
    },
    {
      "content": "For example, the following query counts the events received per second:",
      "pos": [
        7743,
        7814
      ]
    },
    {
      "content": "Execution steps",
      "pos": [
        7900,
        7915
      ]
    },
    {
      "content": "For more complex queries, the standard SQL clause <bpt id=\"p1\">**</bpt>WITH<ept id=\"p1\">**</ept> can be used to specify a temporary named result set.",
      "pos": [
        7916,
        8027
      ]
    },
    {
      "content": "For example, this query uses <bpt id=\"p1\">**</bpt>WITH<ept id=\"p1\">**</ept> to perform a transformation with two execution steps:",
      "pos": [
        8028,
        8119
      ]
    },
    {
      "pos": [
        8361,
        8502
      ],
      "content": "To learn more about the query language, see <bpt id=\"p1\">[</bpt>Azure Stream Analytics Query Language Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkID=513299)</ept>."
    },
    {
      "content": "Output",
      "pos": [
        8508,
        8514
      ]
    },
    {
      "content": "The output target is where the results of the Stream Analytics job will be written to.",
      "pos": [
        8515,
        8601
      ]
    },
    {
      "content": "Results are written continuously to the output target as the job processes input events.",
      "pos": [
        8602,
        8690
      ]
    },
    {
      "content": "The following output targets are supported:",
      "pos": [
        8691,
        8734
      ]
    },
    {
      "content": "Azure Event Hubs - Choose Event Hubs as an output target for scenarios when multiple streaming pipelines need to be composed together, such as issuing commands back to devices.",
      "pos": [
        8738,
        8914
      ]
    },
    {
      "content": "Azure Blob storage - Use Blob storage for long-term archival of output or for storing data for later processing.",
      "pos": [
        8917,
        9029
      ]
    },
    {
      "content": "Azure Table storage - Azure Table storage is a structured data store with fewer constraints on the schema.",
      "pos": [
        9032,
        9138
      ]
    },
    {
      "content": "Entities with different schema and different types can be stored in the same Azure table.",
      "pos": [
        9139,
        9228
      ]
    },
    {
      "content": "Azure Table storage can be used to store data for persistence and efficient retrieval.",
      "pos": [
        9229,
        9315
      ]
    },
    {
      "content": "For more information, see <bpt id=\"p1\">[</bpt>Introduction to Azure Storage<ept id=\"p1\">](../storage/storage-introduction.md)</ept> and <bpt id=\"p2\">[</bpt>Designing a Scalable Partitioning Strategy for Azure Table Storage<ept id=\"p2\">](https://msdn.microsoft.com/library/azure/hh508997.aspx)</ept>.",
      "pos": [
        9316,
        9539
      ]
    },
    {
      "content": "Azure SQL Database - This output target is appropriate for data that is relational in nature or for applications that depend on content being hosted in a database.",
      "pos": [
        9542,
        9705
      ]
    },
    {
      "content": "Streaming Units",
      "pos": [
        9710,
        9725
      ]
    },
    {
      "content": "As part of providing a more predictable performance experience for customers, Azure Stream Analytics uses Streaming Units (SUs) to represent the resources and power to execute a job.",
      "pos": [
        9729,
        9911
      ]
    },
    {
      "content": "SUs provide a way to describe the relative event processing capacity based on a blended measure of CPU, memory, and read and write rates.",
      "pos": [
        9912,
        10049
      ]
    },
    {
      "content": "Each streaming unit corresponds to roughly 1MB/second of throughput.",
      "pos": [
        10050,
        10118
      ]
    },
    {
      "content": "Each Azure Stream Analytics job needs a minimum of one streaming unit, which is the default for all jobs.",
      "pos": [
        10121,
        10226
      ]
    },
    {
      "content": "To learn more about selecting the right number of SU’s for a job, see <bpt id=\"p1\">[</bpt>Scale Azure Stream Analytics jobs<ept id=\"p1\">](stream-analytics-scale-jobs.md)</ept>",
      "pos": [
        10227,
        10364
      ]
    },
    {
      "content": "Scale jobs",
      "pos": [
        10369,
        10379
      ]
    },
    {
      "content": "The SU % Utilization metric defined below, is an indicator for the need to scale an Azure Stream Analytics job.",
      "pos": [
        10381,
        10492
      ]
    },
    {
      "content": "High SU % Utilization may be a result of large window in a query, large events in input, large out of order tolerance window, or a combination of the above.",
      "pos": [
        10494,
        10650
      ]
    },
    {
      "content": "Partitioning the query, or breaking down the query into more steps, and adding more SUs from the Scale tab are both strategies to avoid such a condition.",
      "pos": [
        10651,
        10804
      ]
    },
    {
      "content": "You may observe a baseline resource utilization even without input events, because the system consumes certain amount of resource.",
      "pos": [
        10806,
        10936
      ]
    },
    {
      "content": "The amount of resource consumed by the system may also fluctuate over time.",
      "pos": [
        10937,
        11012
      ]
    },
    {
      "pos": [
        11014,
        11099
      ],
      "content": "For details, see <bpt id=\"p1\">[</bpt>Scale Azure Stream Analytics jobs<ept id=\"p1\">](stream-analytics-scale-jobs.md)</ept>."
    },
    {
      "content": "Monitor and troubleshoot jobs",
      "pos": [
        11105,
        11134
      ]
    },
    {
      "content": "Regional monitoring Storage account",
      "pos": [
        11140,
        11175
      ]
    },
    {
      "content": "To enable job monitoring, Stream Analytics requires you to designate an Azure Storage account for monitoring data in each region that contains Stream Analytics jobs.",
      "pos": [
        11177,
        11342
      ]
    },
    {
      "content": "This is configured at the time of job creation.",
      "pos": [
        11343,
        11390
      ]
    },
    {
      "content": "Metrics",
      "pos": [
        11398,
        11405
      ]
    },
    {
      "content": "The following metrics are available for monitoring the usage and performance of Stream Analytics jobs:",
      "pos": [
        11406,
        11508
      ]
    },
    {
      "content": "SU % Utilizaiton - An indicator of the relative event processing capacity for one or more of the query steps.",
      "pos": [
        11512,
        11621
      ]
    },
    {
      "content": "Should this indicator reach 80%, or above, there is high probability that event processing may be delayed or stopped making progress.",
      "pos": [
        11623,
        11756
      ]
    },
    {
      "content": "Errors - Number of error messages incurred by a Stream Analytics job.",
      "pos": [
        11759,
        11828
      ]
    },
    {
      "content": "Input events - Amount of data received by the Stream Analytics job, in terms of event count.",
      "pos": [
        11831,
        11923
      ]
    },
    {
      "content": "Output events - Amount of data sent by the Stream Analytics job to the output target, in terms of event count.",
      "pos": [
        11926,
        12036
      ]
    },
    {
      "content": "Out-of-order events - Number of events received out of order that were either dropped or given an adjusted timestamp, based on the out-of-order policy.",
      "pos": [
        12039,
        12190
      ]
    },
    {
      "content": "Data conversion errors - Number of data conversion errors incurred by a Stream Analytics job.",
      "pos": [
        12193,
        12286
      ]
    },
    {
      "content": "Operation logs",
      "pos": [
        12292,
        12306
      ]
    },
    {
      "content": "The best approach to debugging or troubleshooting a Stream Analytics job is through Azure operation logs.",
      "pos": [
        12307,
        12412
      ]
    },
    {
      "content": "Operation logs can be accessed in the <bpt id=\"p1\">**</bpt>Management Services<ept id=\"p1\">**</ept> section of the portal.",
      "pos": [
        12413,
        12497
      ]
    },
    {
      "content": "To inspect logs for your job, set <bpt id=\"p1\">**</bpt>Service Type<ept id=\"p1\">**</ept> to <bpt id=\"p2\">**</bpt>Stream Analytics<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>Service Name<ept id=\"p3\">**</ept> to the name of your job.",
      "pos": [
        12498,
        12618
      ]
    },
    {
      "content": "Manage jobs",
      "pos": [
        12624,
        12635
      ]
    },
    {
      "content": "Start and stop jobs",
      "pos": [
        12642,
        12661
      ]
    },
    {
      "content": "When starting a job, you're prompted to specify a <bpt id=\"p1\">**</bpt>Start Output<ept id=\"p1\">**</ept> value, which determines when this job will start producing resulting output.",
      "pos": [
        12662,
        12805
      ]
    },
    {
      "content": "If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required, in order to produce the first output event at the specified time.",
      "pos": [
        12806,
        13019
      ]
    },
    {
      "content": "There are three options: <bpt id=\"p1\">**</bpt>Job Start Time<ept id=\"p1\">**</ept>, <bpt id=\"p2\">**</bpt>Custom<ept id=\"p2\">**</ept>, and <bpt id=\"p3\">**</bpt>Last Stopped Time<ept id=\"p3\">**</ept>.",
      "pos": [
        13020,
        13103
      ]
    },
    {
      "content": "The default setting is <bpt id=\"p1\">**</bpt>Job Start Time<ept id=\"p1\">**</ept>.",
      "pos": [
        13104,
        13146
      ]
    },
    {
      "content": "For cases when a job has been stopped temporarily, the best practice is to choose <bpt id=\"p1\">**</bpt>Last Stopped Time<ept id=\"p1\">**</ept> for the <bpt id=\"p2\">**</bpt>Start Output<ept id=\"p2\">**</ept> value in order to resume the job from the last output time and avoid data loss.",
      "pos": [
        13147,
        13355
      ]
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>Custom<ept id=\"p1\">**</ept> option, you must specify a date and time.",
      "pos": [
        13356,
        13416
      ]
    },
    {
      "content": "This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time.",
      "pos": [
        13417,
        13566
      ]
    },
    {
      "content": "Configure jobs",
      "pos": [
        13573,
        13587
      ]
    },
    {
      "content": "You can adjust the following top-level settings for a Stream Analytics job:",
      "pos": [
        13588,
        13663
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Start output<ept id=\"p1\">**</ept> - Use this setting to specify when this job will start producing resulting output.",
      "pos": [
        13667,
        13766
      ]
    },
    {
      "content": "If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required in order to produce the first output event at the specified time.",
      "pos": [
        13767,
        13979
      ]
    },
    {
      "content": "There are two options, <bpt id=\"p1\">**</bpt>Job Start Time<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>Custom<ept id=\"p2\">**</ept>.",
      "pos": [
        13980,
        14037
      ]
    },
    {
      "content": "The default setting is <bpt id=\"p1\">**</bpt>Job Start Time<ept id=\"p1\">**</ept>.",
      "pos": [
        14038,
        14080
      ]
    },
    {
      "content": "For the <bpt id=\"p1\">**</bpt>Custom<ept id=\"p1\">**</ept> option, you must specify a date and time.",
      "pos": [
        14081,
        14141
      ]
    },
    {
      "content": "This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time, such as when a job was last stopped.",
      "pos": [
        14142,
        14328
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Out of order policy<ept id=\"p1\">**</ept> - Settings for handling events that do not arrive to the Stream Analytics job sequentially.",
      "pos": [
        14332,
        14447
      ]
    },
    {
      "content": "You can designate a time threshold to reorder events within by specifying a tolerance window and also determine an action to take on events outside this window: <bpt id=\"p1\">**</bpt>Drop<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>Adjust<ept id=\"p2\">**</ept>.",
      "pos": [
        14448,
        14632
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Drop<ept id=\"p1\">**</ept> will drop all events received out of order, and <bpt id=\"p2\">**</bpt>Adjust<ept id=\"p2\">**</ept> will change the System.",
      "pos": [
        14633,
        14724
      ]
    },
    {
      "content": "Timestamp of out-of-order events to the timestamp of the most recently received ordered event.",
      "pos": [
        14725,
        14819
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Late arrival policy<ept id=\"p1\">**</ept> - When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.",
      "pos": [
        14823,
        15085
      ]
    },
    {
      "content": "Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.",
      "pos": [
        15086,
        15404
      ]
    },
    {
      "content": "This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.",
      "pos": [
        15405,
        15528
      ]
    },
    {
      "content": "User can also decide to only wait for a limited time, ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.",
      "pos": [
        15529,
        15880
      ]
    },
    {
      "content": "This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.",
      "pos": [
        15881,
        15996
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Locale<ept id=\"p1\">**</ept> - Use this setting to specify the internationalization preference for the Stream Analytics job.",
      "pos": [
        15999,
        16105
      ]
    },
    {
      "content": "While timestamps of data are locale neutral, settings here impact how the job will parse, compare, and sort data.",
      "pos": [
        16106,
        16219
      ]
    },
    {
      "content": "For the preview release, only <bpt id=\"p1\">**</bpt>en-US<ept id=\"p1\">**</ept> is supported.",
      "pos": [
        16220,
        16273
      ]
    },
    {
      "content": "Status",
      "pos": [
        16279,
        16285
      ]
    },
    {
      "content": "The status of Stream Analytics jobs can be inspected in the Azure portal.",
      "pos": [
        16287,
        16360
      ]
    },
    {
      "content": "Running jobs can be in one of two states: <bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept>, or <bpt id=\"p2\">**</bpt>Degraded<ept id=\"p2\">**</ept>.",
      "pos": [
        16361,
        16432
      ]
    },
    {
      "content": "The definition for each of these states is below:",
      "pos": [
        16433,
        16482
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Running<ept id=\"p1\">**</ept> - The job is allocated, processing input, or waiting to process input.",
      "pos": [
        16486,
        16568
      ]
    },
    {
      "content": "If the job shows a Running state without producing output, it is likely that the data processing time window is large or the query logic is complicated.",
      "pos": [
        16569,
        16721
      ]
    },
    {
      "content": "Another reason may be that currently there isn't any data being sent to the job.",
      "pos": [
        16722,
        16802
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Degraded<ept id=\"p1\">**</ept> - This state indicates that a Stream Analytics job is encountering one of the following errors: input/output communication errors, query errors, or retry-able run-time errors.",
      "pos": [
        16805,
        16993
      ]
    },
    {
      "content": "To distinguish what type of error(s) the job is encountering, view the operation logs.",
      "pos": [
        16994,
        17080
      ]
    },
    {
      "content": "Get support",
      "pos": [
        17086,
        17097
      ]
    },
    {
      "pos": [
        17098,
        17241
      ],
      "content": "For further assistance, try our <bpt id=\"p1\">[</bpt>Azure Stream Analytics forum<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)</ept>."
    },
    {
      "content": "Next steps",
      "pos": [
        17248,
        17258
      ]
    },
    {
      "content": "Now that you're familiar with the key concepts of Stream Analytics, try:",
      "pos": [
        17260,
        17332
      ]
    },
    {
      "content": "Introduction to Azure Stream Analytics",
      "pos": [
        17337,
        17375
      ]
    },
    {
      "content": "Get started using Azure Stream Analytics",
      "pos": [
        17414,
        17454
      ]
    },
    {
      "content": "Scale Azure Stream Analytics jobs",
      "pos": [
        17492,
        17525
      ]
    },
    {
      "content": "Azure Stream Analytics Query Language Reference",
      "pos": [
        17562,
        17609
      ]
    },
    {
      "content": "Azure Stream Analytics Management REST API Reference",
      "pos": [
        17670,
        17722
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Learn about Stream Analytics key concepts | Microsoft Azure\" \n    description=\"Learn key concepts of Azure Stream Analytics: Components of a stream analytics job, including supported inputs and outputs, job configuration, and metrics.\" \n    keywords=\"event processing,data stream,key concepts,serialization\"  \n    services=\"stream-analytics\" \n    documentationCenter=\"\" \n    authors=\"jeffstokes72\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"stream-analytics\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.tgt_pltfrm=\"na\" \n    ms.workload=\"data-services\" \n    ms.date=\"08/19/2015\" \n    ms.author=\"jeffstok\" />\n\n\n# Stream Analytics key concepts: Guide to the basics of a stream analytics job \n\nAzure Stream Analytics is a fully managed service providing low-latency, highly available, scalable, complex event processing over a data stream in the cloud. Stream Analytics enables customers to set up streaming jobs to analyze data streams, and allows customers to drive near real-time analytics. This article explains the key concepts of a Stream Analytics job.\n\n## What can you do in Stream Analytics?\nWith Stream Analytics, you can:\n\n- Perform complex event processing on high-volume and high-velocity data streams.   \n- Collect event data from globally distributed assets or equipment, such as connected cars or utility grids. \n- Process telemetry data for near real-time monitoring and diagnostics. \n- Capture and archive real-time events for future processing\n\nFor more information, see [Introduction to Azure Stream Analytics](stream-analytics-introduction.md). \n\nA Stream Analytics job includes all of the following:\n- One or more input sources\n- A query over an incoming data stream\n- An output target.    \n\n\n## Inputs\n\n### Data stream\n\nEach Stream Analytics job definition must contain at least one data stream input source to be consumed and transformed by the job. [Azure Blob storage](http://azure.microsoft.com/documentation/services/storage/) and [Azure Event Hubs](http://azure.microsoft.com/services/event-hubs/) are supported as data stream input sources. Event Hubs input sources are used to collect event streams from multiple different devices and services, while Blob storage can be used an input source for ingesting large amounts of data. Because blobs do not stream data, Stream Analytics jobs over blobs will not be temporal in nature unless the records in the blob contain timestamps.\n\n### Reference data\nStream Analytics also supports a second type of input source: reference data. This is auxiliary data used for performing correlation and lookups, and the data here is usually static or infrequently changing. [Azure Blob storage](http://azure.microsoft.com/documentation/services/storage/) is the only supported input source for reference data. Reference data source blobs are limited to 50MB in size.\n\nTo enable support for refreshing reference data the user needs to specify a list of blobs in the input configuration using the {date} and {time} tokens inside the path pattern. The job will load the corresponding blob based on the date and time encoded in the blob names using UTC time zone.\n\nFor example if the job has a reference input configured in the portal with the path pattern such as: /sample/{date}/{time}/products.csv where the date format is “YYYY-MM-DD” and the time format is “HH:mm” than the job will pick up a file named /sample/2015-04-16/17:30/products.csv at 5:30 PM on April 16th 2015 UTC time zone (which is equivalent to 10:30 AM on April 16th 2015 using PST time zone).\n\n\n### Serialization\nTo ensure correct behavior of queries, Stream Analytics must be aware of the serialization format being used on incoming data streams. Currently supported serialization formats are JSON, CSV, and Avro for data streams and CSV or JSON for reference data.\n\n### Generated properties\nDepending on the input type used in the job, some additional fields with event metadata will be generated. These fields can be queried against just like other input columns. If an existing event has a field that has the same name as one of the properties below, it will be overwritten with the input metadata.\n\n<table border=\"1\">\n    <tr>\n        <th></th>\n        <th>Property</th>\n        <th>Description</th>\n    </tr>\n    <tr>\n        <td rowspan=\"4\" valign=\"top\"><strong>Blob</strong></td>\n        <td>BlobName</td>\n        <td>The name of the input blob that the event came from.</td>\n    </tr>\n    <tr>\n        <td>EventProcessedUtcTime</td>\n        <td>The date and time that the blob record was processed.</td>\n    </tr>\n    <tr>\n        <td>BlobLastModifiedUtcTime</td>\n        <td>The date and time that the blob was last modified.</td>\n    </tr>\n    <tr>\n        <td>PartitionId</td>\n        <td>The zero-based partition ID for the input adapter.</td>\n    </tr>\n    <tr>\n        <td rowspan=\"3\" valign=\"top\"><b>Event Hub</b></td>\n        <td>EventProcessedUtcTime</td>\n        <td>The date and time that the event was processed.</td>\n    </tr>\n    <tr>\n        <td>EventEnqueuedUtcTime</td>\n        <td>The date and time that the event was received by Event Hubs.</td>\n    </tr>\n    <tr>\n        <td>PartitionId</td>\n        <td>The zero-based partition ID for the input adapter.</td>\n    </tr>\n</table>\n\n###Partition(s) with slow or no input data\nWhen reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system. Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data. This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical. \n\nYou can also decide to wait for only a limited time: ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later. This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.\n\n###Partition(s) with out of order events\nWhen streaming job query uses the TIMESTAMP BY keyword, there are no guarantees about the order in which the events will arrive to input, Some events in the same input partition may be lagging, parameter ‘Maximum allowed disorder within an input’ causes the streaming job to act on events that are outside of the order tolerance, according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps.\n\n### Additional resources\nFor details on creating input sources, see [Azure Event Hubs developer guide](http://msdn.microsoft.com/library/azure/dn789972.aspx) and [Use Azure Blob Storage](../storage/storage-dotnet-how-to-use-blobs.md).  \n\n\n\n## Query\nThe logic to filter, manipulate, and process incoming data is defined in the query of Stream Analytics jobs. Queries are written in the Stream Analytics query language, an SQL-like language that is largely a subset of standard Transact-SQL syntax with some specific extensions for temporal queries.\n\n### Windowing\nWindowing extensions allow aggregations and computations to be performed over subsets of events that fall within some period of time. Windowing functions are invoked through the **GROUP BY** statement. For example, the following query counts the events received per second: \n\n    SELECT Count(*) \n    FROM Input1 \n    GROUP BY TumblingWindow(second, 1) \n\n### Execution steps\nFor more complex queries, the standard SQL clause **WITH** can be used to specify a temporary named result set. For example, this query uses **WITH** to perform a transformation with two execution steps:\n \n    WITH step1 AS ( \n        SELECT Avg(Reading) as avr \n        FROM temperatureInput1 \n        GROUP BY Building, TumblingWindow(hour, 1) \n    ) \n\n    SELECT Avg(avr) AS campus_Avg \n    FROM step1 \n    GROUP BY TumblingWindow (day, 1) \n\nTo learn more about the query language, see [Azure Stream Analytics Query Language Reference](http://go.microsoft.com/fwlink/?LinkID=513299). \n\n## Output\nThe output target is where the results of the Stream Analytics job will be written to. Results are written continuously to the output target as the job processes input events. The following output targets are supported:\n\n- Azure Event Hubs - Choose Event Hubs as an output target for scenarios when multiple streaming pipelines need to be composed together, such as issuing commands back to devices.\n- Azure Blob storage - Use Blob storage for long-term archival of output or for storing data for later processing.\n- Azure Table storage - Azure Table storage is a structured data store with fewer constraints on the schema. Entities with different schema and different types can be stored in the same Azure table. Azure Table storage can be used to store data for persistence and efficient retrieval. For more information, see [Introduction to Azure Storage](../storage/storage-introduction.md) and [Designing a Scalable Partitioning Strategy for Azure Table Storage](https://msdn.microsoft.com/library/azure/hh508997.aspx).\n- Azure SQL Database - This output target is appropriate for data that is relational in nature or for applications that depend on content being hosted in a database.\n\n## Streaming Units ##\nAs part of providing a more predictable performance experience for customers, Azure Stream Analytics uses Streaming Units (SUs) to represent the resources and power to execute a job. SUs provide a way to describe the relative event processing capacity based on a blended measure of CPU, memory, and read and write rates. Each streaming unit corresponds to roughly 1MB/second of throughput.  \nEach Azure Stream Analytics job needs a minimum of one streaming unit, which is the default for all jobs. To learn more about selecting the right number of SU’s for a job, see [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n\n## Scale jobs\n\nThe SU % Utilization metric defined below, is an indicator for the need to scale an Azure Stream Analytics job.  High SU % Utilization may be a result of large window in a query, large events in input, large out of order tolerance window, or a combination of the above. Partitioning the query, or breaking down the query into more steps, and adding more SUs from the Scale tab are both strategies to avoid such a condition.\n\nYou may observe a baseline resource utilization even without input events, because the system consumes certain amount of resource. The amount of resource consumed by the system may also fluctuate over time.\n\nFor details, see [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md).\n\n\n## Monitor and troubleshoot jobs\n\n### Regional monitoring Storage account\n\nTo enable job monitoring, Stream Analytics requires you to designate an Azure Storage account for monitoring data in each region that contains Stream Analytics jobs. This is configured at the time of job creation.  \n\n### Metrics\nThe following metrics are available for monitoring the usage and performance of Stream Analytics jobs:\n\n- SU % Utilizaiton - An indicator of the relative event processing capacity for one or more of the query steps.  Should this indicator reach 80%, or above, there is high probability that event processing may be delayed or stopped making progress.\n- Errors - Number of error messages incurred by a Stream Analytics job.\n- Input events - Amount of data received by the Stream Analytics job, in terms of event count.\n- Output events - Amount of data sent by the Stream Analytics job to the output target, in terms of event count.\n- Out-of-order events - Number of events received out of order that were either dropped or given an adjusted timestamp, based on the out-of-order policy.\n- Data conversion errors - Number of data conversion errors incurred by a Stream Analytics job.\n\n### Operation logs\nThe best approach to debugging or troubleshooting a Stream Analytics job is through Azure operation logs. Operation logs can be accessed in the **Management Services** section of the portal. To inspect logs for your job, set **Service Type** to **Stream Analytics** and **Service Name** to the name of your job.\n\n\n## Manage jobs \n\n### Start and stop jobs\nWhen starting a job, you're prompted to specify a **Start Output** value, which determines when this job will start producing resulting output. If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required, in order to produce the first output event at the specified time. There are three options: **Job Start Time**, **Custom**, and **Last Stopped Time**. The default setting is **Job Start Time**. For cases when a job has been stopped temporarily, the best practice is to choose **Last Stopped Time** for the **Start Output** value in order to resume the job from the last output time and avoid data loss. For the **Custom** option, you must specify a date and time. This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time. \n\n### Configure jobs\nYou can adjust the following top-level settings for a Stream Analytics job:\n\n- **Start output** - Use this setting to specify when this job will start producing resulting output. If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required in order to produce the first output event at the specified time. There are two options, **Job Start Time** and **Custom**. The default setting is **Job Start Time**. For the **Custom** option, you must specify a date and time. This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time, such as when a job was last stopped. \n- **Out of order policy** - Settings for handling events that do not arrive to the Stream Analytics job sequentially. You can designate a time threshold to reorder events within by specifying a tolerance window and also determine an action to take on events outside this window: **Drop** or **Adjust**. **Drop** will drop all events received out of order, and **Adjust** will change the System. Timestamp of out-of-order events to the timestamp of the most recently received ordered event. \n- **Late arrival policy** - When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system. Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data. This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical. User can also decide to only wait for a limited time, ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later. This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.\n- **Locale** - Use this setting to specify the internationalization preference for the Stream Analytics job. While timestamps of data are locale neutral, settings here impact how the job will parse, compare, and sort data. For the preview release, only **en-US** is supported.\n\n### Status\n\nThe status of Stream Analytics jobs can be inspected in the Azure portal. Running jobs can be in one of two states: **Running**, or **Degraded**. The definition for each of these states is below:\n\n- **Running** - The job is allocated, processing input, or waiting to process input. If the job shows a Running state without producing output, it is likely that the data processing time window is large or the query logic is complicated. Another reason may be that currently there isn't any data being sent to the job.\n- **Degraded** - This state indicates that a Stream Analytics job is encountering one of the following errors: input/output communication errors, query errors, or retry-able run-time errors. To distinguish what type of error(s) the job is encountering, view the operation logs.\n\n\n## Get support\nFor further assistance, try our [Azure Stream Analytics forum](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics). \n\n\n## Next steps\n\nNow that you're familiar with the key concepts of Stream Analytics, try:\n\n- [Introduction to Azure Stream Analytics](stream-analytics-introduction.md)\n- [Get started using Azure Stream Analytics](stream-analytics-get-started.md)\n- [Scale Azure Stream Analytics jobs](stream-analytics-scale-jobs.md)\n- [Azure Stream Analytics Query Language Reference](https://msdn.microsoft.com/library/azure/dn834998.aspx)\n- [Azure Stream Analytics Management REST API Reference](https://msdn.microsoft.com/library/azure/dn835031.aspx)\n "
}