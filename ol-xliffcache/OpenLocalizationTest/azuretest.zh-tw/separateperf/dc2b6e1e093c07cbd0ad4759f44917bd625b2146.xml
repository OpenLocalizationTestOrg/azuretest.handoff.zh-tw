{
  "nodes": [
    {
      "content": "Develop C# Hadoop streaming programs for HDInsight | Microsoft Azure",
      "pos": [
        28,
        96
      ]
    },
    {
      "content": "Learn how to develop Hadoop streaming MapReduce programs in C#, and how to deploy them to Azure HDInsight.",
      "pos": [
        115,
        221
      ]
    },
    {
      "content": "Develop C# Hadoop streaming programs for HDInsight",
      "pos": [
        547,
        597
      ]
    },
    {
      "content": "Hadoop provides a streaming API for MapReduce that enables you to write map and reduce functions in languages other than Java.",
      "pos": [
        599,
        725
      ]
    },
    {
      "content": "This tutorial walks you through creating a C# word-count program, which counts the occurrences of a given word in the input data you provide.",
      "pos": [
        726,
        867
      ]
    },
    {
      "content": "The following illustration shows how the MapReduce framework does a word count:",
      "pos": [
        868,
        947
      ]
    },
    {
      "content": "HDI.WordCountDiagram",
      "pos": [
        951,
        971
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The steps in this article apply only to Windows-based Azure HDInsight clusters.",
      "pos": [
        1004,
        1096
      ]
    },
    {
      "content": "For an example of streaming for Linux-based HDInsight, see <bpt id=\"p1\">[</bpt>Develop Python streaming programs for HDInsight<ept id=\"p1\">](hdinsight-hadoop-streaming-python.md)</ept>.",
      "pos": [
        1097,
        1244
      ]
    },
    {
      "content": "This tutorial shows you how to:",
      "pos": [
        1246,
        1277
      ]
    },
    {
      "content": "Develop and test a Hadoop streaming MapReduce program by using C# on the HDInsight Emulator for Azure",
      "pos": [
        1281,
        1382
      ]
    },
    {
      "content": "Run the same MapReduce job on Azure HDInsight",
      "pos": [
        1385,
        1430
      ]
    },
    {
      "content": "Retrieve the results of the MapReduce job",
      "pos": [
        1433,
        1474
      ]
    },
    {
      "pos": [
        1478,
        1519
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"prerequisites\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Prerequisites"
    },
    {
      "content": "Before you begin this tutorial, you must have done the following:",
      "pos": [
        1521,
        1586
      ]
    },
    {
      "content": "Install the HDInsight Emulator.",
      "pos": [
        1590,
        1621
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Get started using HDInsight Emulator<ept id=\"p1\">][hdinsight-get-started-emulator]</ept>.",
      "pos": [
        1622,
        1715
      ]
    },
    {
      "content": "Install Azure PowerShell on the emulator computer.",
      "pos": [
        1718,
        1768
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Install and configure Azure PowerShell<ept id=\"p1\">][powershell-install]</ept>.",
      "pos": [
        1769,
        1852
      ]
    },
    {
      "content": "Obtain an Azure subscription.",
      "pos": [
        1855,
        1884
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Purchase Options<ept id=\"p1\">][azure-purchase-options]</ept>, <bpt id=\"p2\">[</bpt>Member Offers<ept id=\"p2\">][azure-member-offers]</ept>, or <bpt id=\"p3\">[</bpt>Free Trial<ept id=\"p3\">][azure-free-trial]</ept>.",
      "pos": [
        1885,
        2023
      ]
    },
    {
      "pos": [
        2028,
        2105
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"develop\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Develop a word-count Hadoop streaming program in C&amp;#35;"
    },
    {
      "content": "The word-count solution contains two console application projects: mapper and reducer.",
      "pos": [
        2107,
        2193
      ]
    },
    {
      "content": "The mapper application streams each word into the console, and the reducer application counts the number of words that are streamed from a document.",
      "pos": [
        2194,
        2342
      ]
    },
    {
      "content": "Both the mapper and the reducer read characters, line by line, from the standard input stream (stdin) and write to the standard output stream (stdout).",
      "pos": [
        2343,
        2494
      ]
    },
    {
      "content": "To create a C# console application",
      "pos": [
        2498,
        2532
      ]
    },
    {
      "content": "Open Visual Studio 2013.",
      "pos": [
        2539,
        2563
      ]
    },
    {
      "pos": [
        2567,
        2625
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>FILE<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>New<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>Project<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Type or select the following values:",
      "pos": [
        2629,
        2665
      ]
    },
    {
      "content": "Field",
      "pos": [
        2668,
        2673
      ]
    },
    {
      "content": "Value",
      "pos": [
        2674,
        2679
      ]
    },
    {
      "content": "Template",
      "pos": [
        2688,
        2696
      ]
    },
    {
      "content": "Visual C#/Windows/Console Application",
      "pos": [
        2697,
        2734
      ]
    },
    {
      "content": "Name",
      "pos": [
        2735,
        2739
      ]
    },
    {
      "content": "WordCountMapper",
      "pos": [
        2740,
        2755
      ]
    },
    {
      "content": "Location",
      "pos": [
        2756,
        2764
      ]
    },
    {
      "content": "C:\\Tutorials",
      "pos": [
        2765,
        2777
      ]
    },
    {
      "content": "Solution name",
      "pos": [
        2778,
        2791
      ]
    },
    {
      "content": "WordCount",
      "pos": [
        2792,
        2801
      ]
    },
    {
      "pos": [
        2807,
        2842
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>OK<ept id=\"p1\">**</ept> to create the project."
    },
    {
      "content": "To create the mapper program",
      "pos": [
        2846,
        2874
      ]
    },
    {
      "pos": [
        2881,
        2957
      ],
      "content": "In Solution Explorer, right-click <bpt id=\"p1\">**</bpt>Program.cs<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Rename<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        2961,
        3029
      ],
      "content": "Rename the file to <bpt id=\"p1\">**</bpt>WordCountMapper.cs<ept id=\"p1\">**</ept>, and then press <bpt id=\"p2\">**</bpt>ENTER<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        3033,
        3082
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Yes<ept id=\"p1\">**</ept> to confirm renaming all references."
    },
    {
      "pos": [
        3086,
        3133
      ],
      "content": "Double-click <bpt id=\"p1\">**</bpt>WordCountMapper.cs<ept id=\"p1\">**</ept> to open it."
    },
    {
      "pos": [
        3137,
        3175
      ],
      "content": "Add the following <bpt id=\"p1\">**</bpt>using<ept id=\"p1\">**</ept> statement:"
    },
    {
      "pos": [
        3207,
        3258
      ],
      "content": "Replace the <bpt id=\"p1\">**</bpt>Main()<ept id=\"p1\">**</ept> function with the following:"
    },
    {
      "pos": [
        3727,
        3808
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>BUILD<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Build Solution<ept id=\"p2\">**</ept> to compile the mapper program."
    },
    {
      "content": "To create the reducer program",
      "pos": [
        3813,
        3842
      ]
    },
    {
      "pos": [
        3849,
        3936
      ],
      "content": "From Visual Studio 2013, click <bpt id=\"p1\">**</bpt>FILE<ept id=\"p1\">**</ept>, click <bpt id=\"p2\">**</bpt>Add<ept id=\"p2\">**</ept>, and then click <bpt id=\"p3\">**</bpt>New Project<ept id=\"p3\">**</ept>."
    },
    {
      "content": "Type or select the following values:",
      "pos": [
        3940,
        3976
      ]
    },
    {
      "content": "Field",
      "pos": [
        3978,
        3983
      ]
    },
    {
      "content": "Value",
      "pos": [
        3984,
        3989
      ]
    },
    {
      "content": "Template",
      "pos": [
        3998,
        4006
      ]
    },
    {
      "content": "Visual C#/Windows/Console Application",
      "pos": [
        4007,
        4044
      ]
    },
    {
      "content": "Name",
      "pos": [
        4045,
        4049
      ]
    },
    {
      "content": "WordCountReducer",
      "pos": [
        4050,
        4066
      ]
    },
    {
      "content": "Location",
      "pos": [
        4067,
        4075
      ]
    },
    {
      "content": "C:\\Tutorials\\WordCount",
      "pos": [
        4076,
        4098
      ]
    },
    {
      "pos": [
        4103,
        4205
      ],
      "content": "Clear the check box to <bpt id=\"p1\">**</bpt>Create directory for solution<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>OK<ept id=\"p2\">**</ept> to create the project."
    },
    {
      "pos": [
        4209,
        4287
      ],
      "content": "From Solution Explorer, right-click <bpt id=\"p1\">**</bpt>Program.cs<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Rename<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        4291,
        4360
      ],
      "content": "Rename the file to <bpt id=\"p1\">**</bpt>WordCountReducer.cs<ept id=\"p1\">**</ept>, and then press <bpt id=\"p2\">**</bpt>ENTER<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        4364,
        4413
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>Yes<ept id=\"p1\">**</ept> to confirm renaming all references."
    },
    {
      "pos": [
        4417,
        4465
      ],
      "content": "Double-click <bpt id=\"p1\">**</bpt>WordCountReducer.cs<ept id=\"p1\">**</ept> to open it."
    },
    {
      "pos": [
        4469,
        4507
      ],
      "content": "Add the following <bpt id=\"p1\">**</bpt>using<ept id=\"p1\">**</ept> statement:"
    },
    {
      "pos": [
        4539,
        4590
      ],
      "content": "Replace the <bpt id=\"p1\">**</bpt>Main()<ept id=\"p1\">**</ept> function with the following:"
    },
    {
      "pos": [
        5314,
        5396
      ],
      "content": "Click <bpt id=\"p1\">**</bpt>BUILD<ept id=\"p1\">**</ept>, and then click <bpt id=\"p2\">**</bpt>Build Solution<ept id=\"p2\">**</ept> to compile the reducer program."
    },
    {
      "content": "The mapper and reducer executables are located at:",
      "pos": [
        5398,
        5448
      ]
    },
    {
      "content": "C:\\Tutorials\\WordCount\\WordCountMapper\\bin\\Debug\\WordCountMapper.exe",
      "pos": [
        5452,
        5520
      ]
    },
    {
      "content": "C:\\Tutorials\\WordCount\\WordCountReducer\\bin\\Debug\\WordCountReducer.exe",
      "pos": [
        5523,
        5593
      ]
    },
    {
      "pos": [
        5598,
        5649
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"test\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Test the program on the emulator"
    },
    {
      "content": "Do the following to test the program on the HDInsight Emulator:",
      "pos": [
        5651,
        5714
      ]
    },
    {
      "content": "Upload data to the emulator's file system",
      "pos": [
        5719,
        5760
      ]
    },
    {
      "content": "Upload the mapper and reducer applications to the emulator's file system",
      "pos": [
        5764,
        5836
      ]
    },
    {
      "content": "Submit a word-count MapReduce job",
      "pos": [
        5840,
        5873
      ]
    },
    {
      "content": "Check the job status",
      "pos": [
        5877,
        5897
      ]
    },
    {
      "content": "Retrieve the job results",
      "pos": [
        5901,
        5925
      ]
    },
    {
      "content": "By default, HDInsight emulator uses Hadoop Distributed File System (HDFS) as the file system.",
      "pos": [
        5927,
        6020
      ]
    },
    {
      "content": "Optionally, you can configure the HDInsight Emulator to use Azure Blob storage.",
      "pos": [
        6021,
        6100
      ]
    },
    {
      "content": "For details, see <bpt id=\"p1\">[</bpt>Get started with HDInsight Emulator<ept id=\"p1\">][hdinsight-emulator-wasb]</ept>.",
      "pos": [
        6101,
        6181
      ]
    },
    {
      "content": "In this section, you will use the HDFS <bpt id=\"p1\">**</bpt>copyFromLocal<ept id=\"p1\">**</ept> command to upload the files.",
      "pos": [
        6182,
        6267
      ]
    },
    {
      "content": "The next section shows you how to upload files by using Azure PowerShell.",
      "pos": [
        6268,
        6341
      ]
    },
    {
      "content": "For other methods, see <bpt id=\"p1\">[</bpt>Upload data to HDInsight<ept id=\"p1\">][hdinsight-upload-data]</ept>.",
      "pos": [
        6342,
        6415
      ]
    },
    {
      "content": "This tutorial uses the following folder structure:",
      "pos": [
        6417,
        6467
      ]
    },
    {
      "content": "Folder",
      "pos": [
        6469,
        6475
      ]
    },
    {
      "content": "Note",
      "pos": [
        6476,
        6480
      ]
    },
    {
      "content": "\\WordCount",
      "pos": [
        6489,
        6499
      ]
    },
    {
      "content": "The root folder for the word-count project.",
      "pos": [
        6500,
        6543
      ]
    },
    {
      "content": "\\WordCount\\Apps",
      "pos": [
        6544,
        6559
      ]
    },
    {
      "content": "The folder for the mapper and reducer executables.",
      "pos": [
        6560,
        6610
      ]
    },
    {
      "content": "\\WordCount\\Input",
      "pos": [
        6611,
        6627
      ]
    },
    {
      "content": "The MapReduce source file folder.",
      "pos": [
        6628,
        6661
      ]
    },
    {
      "content": "\\WordCount\\Output",
      "pos": [
        6662,
        6679
      ]
    },
    {
      "content": "The MapReduce output file folder.",
      "pos": [
        6680,
        6713
      ]
    },
    {
      "content": "\\WordCount\\MRStatusOutput",
      "pos": [
        6714,
        6739
      ]
    },
    {
      "content": "The job output folder.",
      "pos": [
        6740,
        6762
      ]
    },
    {
      "content": "This tutorial uses the .txt files located in the %hadoop_home% directory.",
      "pos": [
        6765,
        6838
      ]
    },
    {
      "pos": [
        6842,
        6899
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The Hadoop HDFS commands are case sensitive."
    },
    {
      "content": "To copy the text files to the emulator's file system",
      "pos": [
        6903,
        6955
      ]
    },
    {
      "content": "From the Hadoop command-line window, run the following command to make a directory for the input files:",
      "pos": [
        6962,
        7065
      ]
    },
    {
      "content": "The path used here is the relative path.",
      "pos": [
        7151,
        7191
      ]
    },
    {
      "content": "It is equivalent to the following:",
      "pos": [
        7192,
        7226
      ]
    },
    {
      "content": "Run the following command to copy some text files to the input folder on HDFS:",
      "pos": [
        7295,
        7373
      ]
    },
    {
      "content": "Run the following command to list the uploaded files:",
      "pos": [
        7473,
        7526
      ]
    },
    {
      "content": "To deploy the mapper and the reducer to the emulator's file system",
      "pos": [
        7573,
        7639
      ]
    },
    {
      "content": "Open a Hadoop command line from your desktop and create the /Apps folder in HDFS:",
      "pos": [
        7646,
        7727
      ]
    },
    {
      "content": "Run the following commands:",
      "pos": [
        7774,
        7801
      ]
    },
    {
      "content": "Run the following command to list the uploaded files:",
      "pos": [
        8086,
        8139
      ]
    },
    {
      "content": "You shall see the two .exe files.",
      "pos": [
        8184,
        8217
      ]
    },
    {
      "content": "To run the MapReduce job by using Azure PowerShell",
      "pos": [
        8222,
        8272
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        8279,
        8301
      ]
    },
    {
      "content": "For instructions, see <bpt id=\"p1\">[</bpt>Install and configure Azure PowerShell<ept id=\"p1\">][powershell-install]</ept>.",
      "pos": [
        8302,
        8385
      ]
    },
    {
      "content": "Run the following commands to set variables:",
      "pos": [
        8389,
        8433
      ]
    },
    {
      "content": "The HDInsight emulator cluster name is \"http://localhost:50111\".",
      "pos": [
        8834,
        8898
      ]
    },
    {
      "content": "Run the following commands to define the streaming job:",
      "pos": [
        8905,
        8960
      ]
    },
    {
      "content": "Run the following command to create a credential object:",
      "pos": [
        9266,
        9322
      ]
    },
    {
      "content": "You will get a prompt to enter the password.",
      "pos": [
        9406,
        9450
      ]
    },
    {
      "content": "The password can be any string.",
      "pos": [
        9451,
        9482
      ]
    },
    {
      "content": "The user name must be \"hadoop\".",
      "pos": [
        9483,
        9514
      ]
    },
    {
      "content": "Run the following commands to submit the MapReduce job and wait for the job to finish:",
      "pos": [
        9519,
        9605
      ]
    },
    {
      "content": "When the job finishes, you will get an output similar to the following:",
      "pos": [
        9808,
        9879
      ]
    },
    {
      "pos": [
        10269,
        10344
      ],
      "content": "You can see the job ID in the output, for example, <bpt id=\"p1\">*</bpt>job-201311132317-0034<ept id=\"p1\">*</ept>."
    },
    {
      "content": "To check the job status",
      "pos": [
        10348,
        10371
      ]
    },
    {
      "pos": [
        10378,
        10481
      ],
      "content": "From the desktop, click <bpt id=\"p1\">**</bpt>Hadoop YARN Status<ept id=\"p1\">**</ept>, or browse to <bpt id=\"p2\">**</bpt>http://localhost:50030/jobtracker.jsp<ept id=\"p2\">**</ept>."
    },
    {
      "pos": [
        10485,
        10572
      ],
      "content": "Find the job by using the job ID under either the <bpt id=\"p1\">**</bpt>RUNNING<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>FINISHED<ept id=\"p2\">**</ept> category."
    },
    {
      "content": "If a job failed, you can find it under the <bpt id=\"p1\">**</bpt>FAILED<ept id=\"p1\">**</ept> category.",
      "pos": [
        10576,
        10639
      ]
    },
    {
      "content": "You can also open the job details and find some helpful information for debugging.",
      "pos": [
        10640,
        10722
      ]
    },
    {
      "content": "To display the output from HDFS",
      "pos": [
        10727,
        10758
      ]
    },
    {
      "content": "Open the Hadoop command line.",
      "pos": [
        10765,
        10794
      ]
    },
    {
      "content": "Run the following commands to display the output:",
      "pos": [
        10798,
        10847
      ]
    },
    {
      "content": "You can append \"|more\" at the end of the command to get the page view.",
      "pos": [
        10947,
        11017
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"upload\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Upload data to Azure Blob storage",
      "pos": [
        11021,
        11073
      ]
    },
    {
      "content": "Azure HDInsight uses Azure Blob storage as the default file system.",
      "pos": [
        11074,
        11141
      ]
    },
    {
      "content": "You can configure an HDInsight cluster to use additional Blob storage for the data files.",
      "pos": [
        11142,
        11231
      ]
    },
    {
      "content": "In this section, you will create an Azure Storage account and upload the data files to the Blob storage.",
      "pos": [
        11232,
        11336
      ]
    },
    {
      "content": "The data files are the .txt files in the %hadoop_home%\\share\\doc\\hadoop\\common directory.",
      "pos": [
        11337,
        11426
      ]
    },
    {
      "content": "To create a Storage account and a container",
      "pos": [
        11431,
        11474
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        11481,
        11503
      ]
    },
    {
      "content": "Set the variables, and then run the commands:",
      "pos": [
        11507,
        11552
      ]
    },
    {
      "content": "Run the following commands to create a Storage account and a Blob storage container on the account:",
      "pos": [
        11785,
        11884
      ]
    },
    {
      "content": "Run the following commands to verify the Storage account and the container:",
      "pos": [
        12442,
        12517
      ]
    },
    {
      "content": "To upload the data files",
      "pos": [
        12650,
        12674
      ]
    },
    {
      "content": "In the Azure PowerShell window, set the values for the local and destination folders:",
      "pos": [
        12681,
        12766
      ]
    },
    {
      "content": "Notice the local source file folder is <bpt id=\"p1\">**</bpt>C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common<ept id=\"p1\">**</ept>, and the destination folder is <bpt id=\"p2\">**</bpt>WordCount/Input<ept id=\"p2\">**</ept>.",
      "pos": [
        12895,
        13046
      ]
    },
    {
      "content": "The source location is the location of the .txt files on the HDInsight Emulator.",
      "pos": [
        13047,
        13127
      ]
    },
    {
      "content": "The destination is the folder structure that will be reflected under the Azure Blob container.",
      "pos": [
        13128,
        13222
      ]
    },
    {
      "content": "Run the following commands to get a list of the .txt files in the source file folder:",
      "pos": [
        13227,
        13312
      ]
    },
    {
      "content": "Run the following snippet to copy the files:",
      "pos": [
        13468,
        13512
      ]
    },
    {
      "content": "Run the following command to list the uploaded files:",
      "pos": [
        13908,
        13961
      ]
    },
    {
      "content": "To upload the word-count applications",
      "pos": [
        14129,
        14166
      ]
    },
    {
      "content": "In the Azure PowerShell window, set the following variables:",
      "pos": [
        14173,
        14233
      ]
    },
    {
      "pos": [
        14468,
        14595
      ],
      "content": "Notice the destination folder is <bpt id=\"p1\">**</bpt>WordCount/Apps<ept id=\"p1\">**</ept>, which is the structure that will be reflected in the Azure Blob container."
    },
    {
      "content": "Run the following commands to copy the applications:",
      "pos": [
        14600,
        14652
      ]
    },
    {
      "content": "Run the following command to list the uploaded files:",
      "pos": [
        14944,
        14997
      ]
    },
    {
      "content": "You shall see both application files listed there.",
      "pos": [
        15166,
        15216
      ]
    },
    {
      "pos": [
        15221,
        15279
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Run the MapReduce job on Azure HDInsight"
    },
    {
      "content": "This section provides an Azure PowerShell script that performs all the tasks related to running a MapReduce job.",
      "pos": [
        15281,
        15393
      ]
    },
    {
      "content": "The list of tasks includes:",
      "pos": [
        15394,
        15421
      ]
    },
    {
      "content": "Provision an HDInsight cluster",
      "pos": [
        15426,
        15456
      ]
    },
    {
      "content": "Create a Storage account that will be used as the default HDInsight cluster file system",
      "pos": [
        15465,
        15552
      ]
    },
    {
      "content": "Create a Blob storage container",
      "pos": [
        15560,
        15591
      ]
    },
    {
      "content": "Create an HDInsight cluster",
      "pos": [
        15599,
        15626
      ]
    },
    {
      "content": "Submit the MapReduce job",
      "pos": [
        15631,
        15655
      ]
    },
    {
      "content": "Create a streaming MapReduce job definition",
      "pos": [
        15664,
        15707
      ]
    },
    {
      "content": "Submit a MapReduce job",
      "pos": [
        15715,
        15737
      ]
    },
    {
      "content": "Wait for the job to finish",
      "pos": [
        15745,
        15771
      ]
    },
    {
      "content": "Display standard error",
      "pos": [
        15779,
        15801
      ]
    },
    {
      "content": "Display standard output",
      "pos": [
        15809,
        15832
      ]
    },
    {
      "content": "Delete the cluster",
      "pos": [
        15837,
        15855
      ]
    },
    {
      "content": "Delete the HDInsight cluster",
      "pos": [
        15864,
        15892
      ]
    },
    {
      "content": "Delete the Storage account used as the default HDInsight cluster file system",
      "pos": [
        15900,
        15976
      ]
    },
    {
      "content": "To run the Azure PowerShell script",
      "pos": [
        15981,
        16015
      ]
    },
    {
      "content": "Open Notepad.",
      "pos": [
        16022,
        16035
      ]
    },
    {
      "content": "Copy and paste the following code:",
      "pos": [
        16039,
        16073
      ]
    },
    {
      "content": "Set the first four variables in the script.",
      "pos": [
        20485,
        20528
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$stringPrefix<ept id=\"p1\">**</ept> variable is used to prefix the specified string to the HDInsight cluster name, the Storage account name, and the Blob storage container name.",
      "pos": [
        20529,
        20692
      ]
    },
    {
      "content": "Because the names for these must be 3 to 24 characters, make sure the string you specify and the names this script uses, together, do not exceed the character limit for the name.",
      "pos": [
        20693,
        20871
      ]
    },
    {
      "content": "You must use all lowercase for <bpt id=\"p1\">**</bpt>$stringPrefix<ept id=\"p1\">**</ept>.",
      "pos": [
        20872,
        20921
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$storageAccountName_Data<ept id=\"p1\">**</ept> and <bpt id=\"p2\">**</bpt>$containerName_Data<ept id=\"p2\">**</ept> variables are the Storage account and container that you already created in the previous steps.",
      "pos": [
        20927,
        21083
      ]
    },
    {
      "content": "So, you must provide the names for those.",
      "pos": [
        21084,
        21125
      ]
    },
    {
      "content": "These are used for storing the data files and the applications.",
      "pos": [
        21126,
        21189
      ]
    },
    {
      "content": "The <bpt id=\"p1\">**</bpt>$location<ept id=\"p1\">**</ept> variable must match the data storage account location.",
      "pos": [
        21190,
        21262
      ]
    },
    {
      "content": "Review the rest of the variables.",
      "pos": [
        21267,
        21300
      ]
    },
    {
      "content": "Save the script file.",
      "pos": [
        21304,
        21325
      ]
    },
    {
      "content": "Open Azure PowerShell.",
      "pos": [
        21329,
        21351
      ]
    },
    {
      "content": "Run the following command to set the execution policy to RemoteSigned:",
      "pos": [
        21355,
        21425
      ]
    },
    {
      "content": "When prompted, enter the user name and password for the HDInsight cluster.",
      "pos": [
        21497,
        21571
      ]
    },
    {
      "content": "Make sure the password is at least 10 characters and contains one uppercase letter, one lowercase letter, a number, and a special character.",
      "pos": [
        21572,
        21712
      ]
    },
    {
      "content": "If you don't want to get prompted for the credentials, see <bpt id=\"p1\">[</bpt>Working with Passwords, Secure Strings and Credentials in Windows PowerShell<ept id=\"p1\">][powershell-PSCredential]</ept>.",
      "pos": [
        21713,
        21876
      ]
    },
    {
      "pos": [
        21878,
        22013
      ],
      "content": "For an HDInsight .NET SDK sample on submitting Hadoop streaming jobs, see <bpt id=\"p1\">[</bpt>Submit Hadoop jobs programmatically<ept id=\"p1\">][hdinsight-submit-jobs]</ept>."
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"retrieve\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Retrieve the MapReduce job output",
      "pos": [
        22018,
        22074
      ]
    },
    {
      "content": "This section shows you how to download and display the output.",
      "pos": [
        22075,
        22137
      ]
    },
    {
      "content": "For information on displaying the results in Excel, see <bpt id=\"p1\">[</bpt>Connect Excel to HDInsight with the Microsoft Hive ODBC Driver<ept id=\"p1\">][hdinsight-ODBC]</ept> and <bpt id=\"p2\">[</bpt>Connect Excel to HDInsight with Power Query<ept id=\"p2\">][hdinsight-power-query]</ept>.",
      "pos": [
        22138,
        22348
      ]
    },
    {
      "content": "To retrieve the output",
      "pos": [
        22353,
        22375
      ]
    },
    {
      "content": "Open the Azure PowerShell window.",
      "pos": [
        22382,
        22415
      ]
    },
    {
      "content": "Set the values, and then run the commands:",
      "pos": [
        22419,
        22461
      ]
    },
    {
      "content": "Run the following commands to create an Azure Storage context object:",
      "pos": [
        22692,
        22761
      ]
    },
    {
      "content": "Run the following commands to download and display the output:",
      "pos": [
        23035,
        23097
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Next steps",
      "pos": [
        23258,
        23290
      ]
    },
    {
      "content": "In this tutorial, you have learned how to develop a Hadoop streaming MapReduce job, how to test the application on the HDInsight Emulator, and how to write an Azure PowerShell script to provision an HDInsight cluster and run a MapReduce job on the cluster.",
      "pos": [
        23291,
        23547
      ]
    },
    {
      "content": "To learn more, see the following articles:",
      "pos": [
        23548,
        23590
      ]
    },
    {
      "content": "Get started with Azure HDInsight",
      "pos": [
        23595,
        23627
      ]
    },
    {
      "content": "Get started with the HDInsight Emulator",
      "pos": [
        23661,
        23700
      ]
    },
    {
      "content": "Develop Java MapReduce programs for HDInsight",
      "pos": [
        23737,
        23782
      ]
    },
    {
      "content": "Use Azure Blob storage with HDInsight",
      "pos": [
        23816,
        23853
      ]
    },
    {
      "content": "Administer HDInsight using Azure PowerShell",
      "pos": [
        23877,
        23920
      ]
    },
    {
      "content": "Upload data to HDInsight",
      "pos": [
        23953,
        23977
      ]
    },
    {
      "content": "Use Hive with HDInsight",
      "pos": [
        24005,
        24028
      ]
    },
    {
      "content": "Use Pig with HDInsight",
      "pos": [
        24053,
        24075
      ]
    },
    {
      "content": "test",
      "pos": [
        25380,
        25384
      ]
    }
  ],
  "content": "\n<properties\n    pageTitle=\"Develop C# Hadoop streaming programs for HDInsight | Microsoft Azure\"\n    description=\"Learn how to develop Hadoop streaming MapReduce programs in C#, and how to deploy them to Azure HDInsight.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"07/08/2015\"\n    ms.author=\"jgao\"/>\n\n\n\n# Develop C# Hadoop streaming programs for HDInsight\n\nHadoop provides a streaming API for MapReduce that enables you to write map and reduce functions in languages other than Java. This tutorial walks you through creating a C# word-count program, which counts the occurrences of a given word in the input data you provide. The following illustration shows how the MapReduce framework does a word count:\n\n![HDI.WordCountDiagram][image-hdi-wordcountdiagram]\n\n> [AZURE.NOTE] The steps in this article apply only to Windows-based Azure HDInsight clusters. For an example of streaming for Linux-based HDInsight, see [Develop Python streaming programs for HDInsight](hdinsight-hadoop-streaming-python.md).\n\nThis tutorial shows you how to:\n\n- Develop and test a Hadoop streaming MapReduce program by using C# on the HDInsight Emulator for Azure\n- Run the same MapReduce job on Azure HDInsight\n- Retrieve the results of the MapReduce job\n\n##<a name=\"prerequisites\"></a>Prerequisites\n\nBefore you begin this tutorial, you must have done the following:\n\n- Install the HDInsight Emulator. For instructions, see [Get started using HDInsight Emulator][hdinsight-get-started-emulator].\n- Install Azure PowerShell on the emulator computer. For instructions, see [Install and configure Azure PowerShell][powershell-install].\n- Obtain an Azure subscription. For instructions, see [Purchase Options][azure-purchase-options], [Member Offers][azure-member-offers], or [Free Trial][azure-free-trial].\n\n\n##<a name=\"develop\"></a>Develop a word-count Hadoop streaming program in C&#35;\n\nThe word-count solution contains two console application projects: mapper and reducer. The mapper application streams each word into the console, and the reducer application counts the number of words that are streamed from a document. Both the mapper and the reducer read characters, line by line, from the standard input stream (stdin) and write to the standard output stream (stdout).\n\n**To create a C# console application**\n\n1. Open Visual Studio 2013.\n2. Click **FILE**, click **New**, and then click **Project**.\n3. Type or select the following values:\n\n\nField|Value\n---|---\nTemplate|Visual C#/Windows/Console Application\nName|WordCountMapper\nLocation|C:\\Tutorials\nSolution name|WordCount\n\n\n4. Click **OK** to create the project.\n\n**To create the mapper program**\n\n5. In Solution Explorer, right-click **Program.cs**, and then click **Rename**.\n6. Rename the file to **WordCountMapper.cs**, and then press **ENTER**.\n7. Click **Yes** to confirm renaming all references.\n8. Double-click **WordCountMapper.cs** to open it.\n9. Add the following **using** statement:\n\n        using System.IO;\n\n10. Replace the **Main()** function with the following:\n\n        static void Main(string[] args)\n        {\n            if (args.Length > 0)\n            {\n                Console.SetIn(new StreamReader(args[0]));\n            }\n\n            string line;\n            string[] words;\n\n            while ((line = Console.ReadLine()) != null)\n            {\n                words = line.Split(' ');\n\n                foreach (string word in words)\n                    Console.WriteLine(word.ToLower());\n            }\n        }\n\n11. Click **BUILD**, and then click **Build Solution** to compile the mapper program.\n\n\n**To create the reducer program**\n\n1. From Visual Studio 2013, click **FILE**, click **Add**, and then click **New Project**.\n2. Type or select the following values:\n\nField|Value\n---|---\nTemplate|Visual C#/Windows/Console Application\nName|WordCountReducer\nLocation|C:\\Tutorials\\WordCount\n\n3. Clear the check box to **Create directory for solution**, and then click **OK** to create the project.\n4. From Solution Explorer, right-click **Program.cs**, and then click **Rename**.\n5. Rename the file to **WordCountReducer.cs**, and then press **ENTER**.\n7. Click **Yes** to confirm renaming all references.\n8. Double-click **WordCountReducer.cs** to open it.\n9. Add the following **using** statement:\n\n        using System.IO;\n\n10. Replace the **Main()** function with the following:\n\n        static void Main(string[] args)\n        {\n            string word, lastWord = null;\n            int count = 0;\n\n            if (args.Length > 0)\n            {\n                Console.SetIn(new StreamReader(args[0]));\n            }\n\n            while ((word = Console.ReadLine()) != null)\n            {\n                if (word != lastWord)\n                {\n                    if(lastWord != null)\n                        Console.WriteLine(\"{0}[{1}]\", lastWord, count);\n\n                    count = 1;\n                    lastWord = word;\n                }\n                else\n                {\n                    count += 1;\n                }\n            }\n            Console.WriteLine(count);\n        }\n\n11. Click **BUILD**, and then click **Build Solution** to compile the reducer program.\n\nThe mapper and reducer executables are located at:\n\n- C:\\Tutorials\\WordCount\\WordCountMapper\\bin\\Debug\\WordCountMapper.exe\n- C:\\Tutorials\\WordCount\\WordCountReducer\\bin\\Debug\\WordCountReducer.exe\n\n\n##<a name=\"test\"></a>Test the program on the emulator\n\nDo the following to test the program on the HDInsight Emulator:\n\n1. Upload data to the emulator's file system\n2. Upload the mapper and reducer applications to the emulator's file system\n3. Submit a word-count MapReduce job\n4. Check the job status\n5. Retrieve the job results\n\nBy default, HDInsight emulator uses Hadoop Distributed File System (HDFS) as the file system. Optionally, you can configure the HDInsight Emulator to use Azure Blob storage. For details, see [Get started with HDInsight Emulator][hdinsight-emulator-wasb]. In this section, you will use the HDFS **copyFromLocal** command to upload the files. The next section shows you how to upload files by using Azure PowerShell. For other methods, see [Upload data to HDInsight][hdinsight-upload-data].\n\nThis tutorial uses the following folder structure:\n\nFolder|Note\n---|---\n\\WordCount|The root folder for the word-count project.\n\\WordCount\\Apps|The folder for the mapper and reducer executables.\n\\WordCount\\Input|The MapReduce source file folder.\n\\WordCount\\Output|The MapReduce output file folder.\n\\WordCount\\MRStatusOutput|The job output folder.\n\n\nThis tutorial uses the .txt files located in the %hadoop_home% directory.\n\n> [AZURE.NOTE] The Hadoop HDFS commands are case sensitive.\n\n**To copy the text files to the emulator's file system**\n\n1. From the Hadoop command-line window, run the following command to make a directory for the input files:\n\n        hadoop fs -mkdir /WordCount/\n        hadoop fs -mkdir /WordCount/Input\n\n    The path used here is the relative path. It is equivalent to the following:\n\n        hadoop fs -mkdir hdfs://localhost:8020/WordCount/Input\n\n2. Run the following command to copy some text files to the input folder on HDFS:\n\n        hadoop fs -copyFromLocal %hadoop_home%\\share\\doc\\hadoop\\common\\*.txt \\WordCount\\Input\n\n3. Run the following command to list the uploaded files:\n\n        hadoop fs -ls \\WordCount\\Input\n\n\n\n\n**To deploy the mapper and the reducer to the emulator's file system**\n\n1. Open a Hadoop command line from your desktop and create the /Apps folder in HDFS:\n\n        hadoop fs -mkdir /WordCount/Apps\n\n2. Run the following commands:\n\n        hadoop fs -copyFromLocal C:\\Tutorials\\WordCount\\WordCountMapper\\bin\\Debug\\WordCountMapper.exe /WordCount/Apps/WordCountMapper.exe\n        hadoop fs -copyFromLocal C:\\Tutorials\\WordCount\\WordCountReducer\\bin\\Debug\\WordCountReducer.exe /WordCount/Apps/WordCountReducer.exe\n\n3. Run the following command to list the uploaded files:\n\n        hadoop fs -ls /WordCount/Apps\n\n    You shall see the two .exe files.\n\n\n**To run the MapReduce job by using Azure PowerShell**\n\n1. Open Azure PowerShell. For instructions, see [Install and configure Azure PowerShell][powershell-install].\n3. Run the following commands to set variables:\n\n        $clusterName = \"http://localhost:50111\"\n\n        $mrMapper = \"WordCountMapper.exe\"\n        $mrReducer = \"WordCountReducer.exe\"\n        $mrMapperFile = \"/WordCount/Apps/WordCountMapper.exe\"\n        $mrReducerFile = \"/WordCount/Apps/WordCountReducer.exe\"\n        $mrInput = \"/WordCount/Input/\"\n        $mrOutput = \"/WordCount/Output\"\n        $mrStatusOutput = \"/WordCount/MRStatusOutput\"\n\n    The HDInsight emulator cluster name is \"http://localhost:50111\".  \n\n4. Run the following commands to define the streaming job:\n\n        $mrJobDef = New-AzureHDInsightStreamingMapReduceJobDefinition -JobName mrWordCountStreamingJob -StatusFolder $mrStatusOutput -Mapper $mrMapper -Reducer $mrReducer -InputPath $mrInput -OutputPath $mrOutput\n        $mrJobDef.Files.Add($mrMapperFile)\n        $mrJobDef.Files.Add($mrReducerFile)\n\n5. Run the following command to create a credential object:\n\n        $creds = Get-Credential -Message \"Enter password\" -UserName \"hadoop\"\n\n    You will get a prompt to enter the password. The password can be any string. The user name must be \"hadoop\".\n\n6. Run the following commands to submit the MapReduce job and wait for the job to finish:\n\n        $mrJob = Start-AzureHDInsightJob -Cluster $clusterName -Credential $creds -JobDefinition $mrJobDef\n        Wait-AzureHDInsightJob -Credential $creds -job $mrJob -WaitTimeoutInSeconds 3600\n\n    When the job finishes, you will get an output similar to the following:\n\n        StatusDirectory : /WordCount/MRStatusOutput\n        ExitCode        :\n        Name            : mrWordCountStreamingJob\n        Query           :\n        State           : Completed\n        SubmissionTime  : 11/15/2013 7:18:16 PM\n        Cluster         : http://localhost:50111\n        PercentComplete : map 100%  reduce 100%\n        JobId           : job_201311132317_0034\n\n    You can see the job ID in the output, for example, *job-201311132317-0034*.\n\n**To check the job status**\n\n1. From the desktop, click **Hadoop YARN Status**, or browse to **http://localhost:50030/jobtracker.jsp**.\n2. Find the job by using the job ID under either the **RUNNING** or **FINISHED** category.\n3. If a job failed, you can find it under the **FAILED** category. You can also open the job details and find some helpful information for debugging.\n\n\n**To display the output from HDFS**\n\n1. Open the Hadoop command line.\n2. Run the following commands to display the output:\n\n        hadoop fs -ls /WordCount/Output/\n        hadoop fs -cat /WordCount/Output/part-00000\n\n    You can append \"|more\" at the end of the command to get the page view.\n\n##<a id=\"upload\"></a>Upload data to Azure Blob storage\nAzure HDInsight uses Azure Blob storage as the default file system. You can configure an HDInsight cluster to use additional Blob storage for the data files. In this section, you will create an Azure Storage account and upload the data files to the Blob storage. The data files are the .txt files in the %hadoop_home%\\share\\doc\\hadoop\\common directory.\n\n\n**To create a Storage account and a container**\n\n1. Open Azure PowerShell.\n2. Set the variables, and then run the commands:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName = \"<AzureStorageAccountName>\"  \n        $containerName = \"<ContainerName>\"\n        $location = \"<MicrosoftDataCenter>\"  # For example, \"East US\"\n\n3. Run the following commands to create a Storage account and a Blob storage container on the account:\n\n        # Select an Azure subscription\n        Select-AzureSubscription $subscriptionName\n\n        # Create a Storage account\n        New-AzureStorageAccount -StorageAccountName $storageAccountName -location $location\n\n        # Create a Blob storage container\n        $storageAccountKey = Get-AzureStorageKey $storageAccountName | %{ $_.Primary }\n        $destContext = New-AzureStorageContext –StorageAccountName $storageAccountName –StorageAccountKey $storageAccountKey  \n        New-AzureStorageContainer -Name $containerName -Context $destContext\n\n4. Run the following commands to verify the Storage account and the container:\n\n        Get-AzureStorageAccount -StorageAccountName $storageAccountName\n        Get-AzureStorageContainer -Context $destContext\n\n**To upload the data files**\n\n1. In the Azure PowerShell window, set the values for the local and destination folders:\n\n        $localFolder = \"C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common\"\n        $destFolder = \"WordCount/Input\"\n\n    Notice the local source file folder is **C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common**, and the destination folder is **WordCount/Input**. The source location is the location of the .txt files on the HDInsight Emulator. The destination is the folder structure that will be reflected under the Azure Blob container.\n\n3. Run the following commands to get a list of the .txt files in the source file folder:\n\n        # Get a list of the .txt files\n        $filesAll = Get-ChildItem $localFolder\n        $filesTxt = $filesAll | where {$_.Extension -eq \".txt\"}\n\n5. Run the following snippet to copy the files:\n\n        # Copy the files from the local workstation to the Blob container\n        foreach ($file in $filesTxt){\n\n            $fileName = \"$localFolder\\$file\"\n            $blobName = \"$destFolder/$file\"\n\n            write-host \"Copying $fileName to $blobName\"\n\n            Set-AzureStorageBlobContent -File $fileName -Container $containerName -Blob $blobName -Context $destContext\n        }\n\n6. Run the following command to list the uploaded files:\n\n        # List the uploaded files in the Blob storage container\n        Get-AzureStorageBlob -Container $containerName  -Context $destContext -Prefix $destFolder\n\n\n**To upload the word-count applications**\n\n1. In the Azure PowerShell window, set the following variables:\n\n        $mapperFile = \"C:\\Tutorials\\WordCount\\WordCountMapper\\bin\\Debug\\WordCountMapper.exe\"\n        $reducerFile = \"C:\\Tutorials\\WordCount\\WordCountReducer\\bin\\Debug\\WordCountReducer.exe\"\n        $blobFolder = \"WordCount/Apps\"\n\n    Notice the destination folder is **WordCount/Apps**, which is the structure that will be reflected in the Azure Blob container.\n\n2. Run the following commands to copy the applications:\n\n        Set-AzureStorageBlobContent -File $mapperFile -Container $containerName -Blob \"$blobFolder/WordCountMapper.exe\" -Context $destContext\n        Set-AzureStorageBlobContent -File $reducerFile -Container $containerName -Blob \"$blobFolder/WordCountReducer.exe\" -Context $destContext\n\n3. Run the following command to list the uploaded files:\n\n        # List the uploaded files in the Blob storage container\n        Get-AzureStorageBlob -Container $containerName  -Context $destContext -Prefix $blobFolder\n\n    You shall see both application files listed there.\n\n\n##<a name=\"run\"></a>Run the MapReduce job on Azure HDInsight\n\nThis section provides an Azure PowerShell script that performs all the tasks related to running a MapReduce job. The list of tasks includes:\n\n1. Provision an HDInsight cluster\n\n    1. Create a Storage account that will be used as the default HDInsight cluster file system\n    2. Create a Blob storage container\n    3. Create an HDInsight cluster\n\n2. Submit the MapReduce job\n\n    1. Create a streaming MapReduce job definition\n    2. Submit a MapReduce job\n    3. Wait for the job to finish\n    4. Display standard error\n    5. Display standard output\n\n3. Delete the cluster\n\n    1. Delete the HDInsight cluster\n    2. Delete the Storage account used as the default HDInsight cluster file system\n\n\n**To run the Azure PowerShell script**\n\n1. Open Notepad.\n2. Copy and paste the following code:\n\n        # ====== STORAGE ACCOUNT AND HDINSIGHT CLUSTER VARIABLES ======\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $stringPrefix = \"<StringForPrefix>\"     ### Prefix to cluster, Storage account, and container names\n        $storageAccountName_Data = \"<TheDataStorageAccountName>\"\n        $containerName_Data = \"<TheDataBlobStorageContainerName>\"\n        $location = \"<MicrosoftDataCenter>\"     ### Must match the data storage account location\n        $clusterNodes = 1\n\n        $clusterName = $stringPrefix + \"hdicluster\"\n\n        $storageAccountName_Default = $stringPrefix + \"hdistore\"\n        $containerName_Default =  $stringPrefix + \"hdicluster\"\n\n        # ====== THE STREAMING MAPREDUCE JOB VARIABLES ======\n        $mrMapper = \"WordCountMapper.exe\"\n        $mrReducer = \"WordCountReducer.exe\"\n        $mrMapperFile = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Apps/WordCountMapper.exe\"\n        $mrReducerFile = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Apps/WordCountReducer.exe\"\n        $mrInput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Input/\"\n        $mrOutput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/Output/\"\n        $mrStatusOutput = \"wasb://$containerName_Data@$storageAccountName_Data.blob.core.windows.net/WordCount/MRStatusOutput/\"\n\n        Select-AzureSubscription $subscriptionName\n\n        #====== CREATE A STORAGE ACCOUNT ======\n        Write-Host \"Create a storage account\" -ForegroundColor Green\n        New-AzureStorageAccount -StorageAccountName $storageAccountName_Default -location $location\n\n        #====== CREATE A BLOB STORAGE CONTAINER ======\n        Write-Host \"Create a Blob storage container\" -ForegroundColor Green\n        $storageAccountKey_Default = Get-AzureStorageKey $storageAccountName_Default | %{ $_.Primary }\n        $destContext = New-AzureStorageContext –StorageAccountName $storageAccountName_Default –StorageAccountKey $storageAccountKey_Default\n\n        New-AzureStorageContainer -Name $containerName_Default -Context $destContext\n\n        #====== CREATE AN HDINSIGHT CLUSTER ======\n        Write-Host \"Create an HDInsight cluster\" -ForegroundColor Green\n        $storageAccountKey_Data = Get-AzureStorageKey $storageAccountName_Data | %{ $_.Primary }\n\n        $config = New-AzureHDInsightClusterConfig -ClusterSizeInNodes $clusterNodes |\n            Set-AzureHDInsightDefaultStorage -StorageAccountName \"$storageAccountName_Default.blob.core.windows.net\" -StorageAccountKey $storageAccountKey_Default -StorageContainerName $containerName_Default |\n            Add-AzureHDInsightStorage -StorageAccountName \"$storageAccountName_Data.blob.core.windows.net\" -StorageAccountKey $storageAccountKey_Data\n\n        Select-AzureSubscription $subscriptionName\n        New-AzureHDInsightCluster -Name $clusterName -Location $location -Config $config\n\n        #====== CREATE A STREAMING MAPREDUCE JOB DEFINITION ======\n        Write-Host \"Create a streaming MapReduce job definition\" -ForegroundColor Green\n\n        $mrJobDef = New-AzureHDInsightStreamingMapReduceJobDefinition -JobName mrWordCountStreamingJob -StatusFolder $mrStatusOutput -Mapper $mrMapper -Reducer $mrReducer -InputPath $mrInput -OutputPath $mrOutput\n        $mrJobDef.Files.Add($mrMapperFile)\n        $mrJobDef.Files.Add($mrReducerFile)\n\n        #====== RUN A STREAMING MAPREDUCE JOB ======\n        Write-Host \"Run a streaming MapReduce job\" -ForegroundColor Green\n        Select-AzureSubscription $subscriptionName\n        $mrJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $mrJobDef\n        Wait-AzureHDInsightJob -Job $mrJob -WaitTimeoutInSeconds 3600\n\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardError\n        Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardOutput\n\n        #====== DELETE THE HDINSIGHT CLUSTER ======\n        Write-Host \"Delete the HDInsight cluster\" -ForegroundColor Green\n        Select-AzureSubscription $subscriptionName\n        Remove-AzureHDInsightCluster -Name $clusterName\n\n        #====== DELETE THE STORAGE ACCOUNT ======\n        Write-Host \"Delete the storage account\" -ForegroundColor Green\n        Remove-AzureStorageAccount -StorageAccountName $storageAccountName_Default\n\n3. Set the first four variables in the script. The **$stringPrefix** variable is used to prefix the specified string to the HDInsight cluster name, the Storage account name, and the Blob storage container name. Because the names for these must be 3 to 24 characters, make sure the string you specify and the names this script uses, together, do not exceed the character limit for the name. You must use all lowercase for **$stringPrefix**.\n\n    The **$storageAccountName_Data** and **$containerName_Data** variables are the Storage account and container that you already created in the previous steps. So, you must provide the names for those. These are used for storing the data files and the applications. The **$location** variable must match the data storage account location.\n\n4. Review the rest of the variables.\n5. Save the script file.\n6. Open Azure PowerShell.\n7. Run the following command to set the execution policy to RemoteSigned:\n\n        PowerShell -File <FileName> -ExecutionPolicy RemoteSigned\n\n8. When prompted, enter the user name and password for the HDInsight cluster. Make sure the password is at least 10 characters and contains one uppercase letter, one lowercase letter, a number, and a special character. If you don't want to get prompted for the credentials, see [Working with Passwords, Secure Strings and Credentials in Windows PowerShell][powershell-PSCredential].\n\nFor an HDInsight .NET SDK sample on submitting Hadoop streaming jobs, see [Submit Hadoop jobs programmatically][hdinsight-submit-jobs].\n\n\n##<a name=\"retrieve\"></a>Retrieve the MapReduce job output\nThis section shows you how to download and display the output. For information on displaying the results in Excel, see [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-ODBC] and [Connect Excel to HDInsight with Power Query][hdinsight-power-query].\n\n\n**To retrieve the output**\n\n1. Open the Azure PowerShell window.\n2. Set the values, and then run the commands:\n\n        $subscriptionName = \"<AzureSubscriptionName>\"\n        $storageAccountName = \"<TheDataStorageAccountName>\"\n        $containerName = \"<TheDataBlobStorageContainerName>\"\n        $blobName = \"WordCount/Output/part-00000\"\n\n3. Run the following commands to create an Azure Storage context object:\n\n        Select-AzureSubscription $subscriptionName\n        $storageAccountKey = Get-AzureStorageKey $storageAccountName | %{ $_.Primary }\n        $storageContext = New-AzureStorageContext –StorageAccountName $storageAccountName –StorageAccountKey $storageAccountKey  \n\n4. Run the following commands to download and display the output:\n\n        Get-AzureStorageBlobContent -Container $ContainerName -Blob $blobName -Context $storageContext -Force\n        cat \"./$blobName\" | findstr \"there\"\n\n\n\n##<a id=\"nextsteps\"></a>Next steps\nIn this tutorial, you have learned how to develop a Hadoop streaming MapReduce job, how to test the application on the HDInsight Emulator, and how to write an Azure PowerShell script to provision an HDInsight cluster and run a MapReduce job on the cluster. To learn more, see the following articles:\n\n- [Get started with Azure HDInsight](../hdinsight-get-started.md)\n- [Get started with the HDInsight Emulator][hdinsight-get-started-emulator]\n- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]\n- [Use Azure Blob storage with HDInsight][hdinsight-storage]\n- [Administer HDInsight using Azure PowerShell][hdinsight-admin-powershell]\n- [Upload data to HDInsight][hdinsight-upload-data]\n- [Use Hive with HDInsight][hdinsight-use-hive]\n- [Use Pig with HDInsight][hdinsight-use-pig]\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[hdinsight-develop-mapreduce]: hdinsight-develop-deploy-java-mapreduce.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n\n[hdinsight-get-started-emulator]: ../hdinsight-get-started-emulator.md\n[hdinsight-emulator-wasb]: ../hdinsight-get-started-emulator.md#blobstorage\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-storage]: ../hdinsight-use-blob-storage.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-ODBC]: hdinsight-connect-excel-hive-ODBC-driver.md\n[hdinsight-power-query]: hdinsight-connect-excel-power-query.md\n\n[powershell-PSCredential]: http://social.technet.microsoft.com/wiki/contents/articles/4546.working-with-passwords-secure-strings-and-credentials-in-windows-powershell.aspx\n[powershell-install]: ../powershell-install-configure.md\n\n[image-hdi-wordcountdiagram]: ./media/hdinsight-hadoop-develop-deploy-streaming-jobs/HDI.WordCountDiagram.gif \"MapReduce wordcount application flow\"\n\ntest\n"
}