<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Tips for using Hadoop on Linux-based HDInsight | Microsoft Azure</source>
          <target state="new">Tips for using Hadoop on Linux-based HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Get implementation tips for using Linux-based HDInsight (Hadoop) clusters on a familiar Linux environment running in the Azure cloud.</source>
          <target state="new">Get implementation tips for using Linux-based HDInsight (Hadoop) clusters on a familiar Linux environment running in the Azure cloud.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Information about using HDInsight on Linux (preview)</source>
          <target state="new">Information about using HDInsight on Linux (preview)</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Linux-based Azure HDInsight clusters provide Hadoop on a familiar Linux environment, running in the Azure cloud.</source>
          <target state="new">Linux-based Azure HDInsight clusters provide Hadoop on a familiar Linux environment, running in the Azure cloud.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>For most things, it should work exactly as any other Hadoop-on-Linux installation.</source>
          <target state="new">For most things, it should work exactly as any other Hadoop-on-Linux installation.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>This document calls out specific differences that you should be aware of.</source>
          <target state="new">This document calls out specific differences that you should be aware of.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Domain names</source>
          <target state="new">Domain names</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>The fully qualified domain name (FQDN) to use when connecting to the cluster is <bpt id="p1">**</bpt>&amp;lt;clustername&gt;.azurehdinsight.net<ept id="p1">**</ept> or (for SSH only) <bpt id="p2">**</bpt>&amp;lt;clustername-ssh&gt;.azurehdinsight.net<ept id="p2">**</ept>.</source>
          <target state="new">The fully qualified domain name (FQDN) to use when connecting to the cluster is <bpt id="p1">**</bpt>&amp;lt;clustername&gt;.azurehdinsight.net<ept id="p1">**</ept> or (for SSH only) <bpt id="p2">**</bpt>&amp;lt;clustername-ssh&gt;.azurehdinsight.net<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Remote access to services</source>
          <target state="new">Remote access to services</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Ambari (web)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net</source>
          <target state="new"><bpt id="p1">**</bpt>Ambari (web)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password, and then log in to Ambari.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password, and then log in to Ambari.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>This also uses the cluster administrator user and password.</source>
          <target state="new">This also uses the cluster administrator user and password.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</source>
          <target state="new">Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>While Ambari for your cluster is accessible directly over the Internet, some functionality relies on accessing nodes by the internal domain name used by the cluster.</source>
          <target state="new">While Ambari for your cluster is accessible directly over the Internet, some functionality relies on accessing nodes by the internal domain name used by the cluster.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Since this is an internal domain name, and not public, you will receive "server not found" errors when trying to access some features over the Internet.</source>
          <target state="new">Since this is an internal domain name, and not public, you will receive "server not found" errors when trying to access some features over the Internet.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>To work around this problem, use an SSH tunnel to proxy web traffic to the cluster head node.</source>
          <target state="new">To work around this problem, use an SSH tunnel to proxy web traffic to the cluster head node.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">**</bpt>SSH tunneling<ept id="p1">**</ept> section of the following articles to create an SSH tunnel from a port on your local machine to the cluster:</source>
          <target state="new">Use the <bpt id="p1">**</bpt>SSH tunneling<ept id="p1">**</ept> section of the following articles to create an SSH tunnel from a port on your local machine to the cluster:</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>: Steps on creating an SSH tunnel by using the <ph id="ph1">`ssh`</ph> command.</source>
          <target state="new"><bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>: Steps on creating an SSH tunnel by using the <ph id="ph1">`ssh`</ph> command.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows)</ept>: Steps on using PuTTY to create an SSH tunnel.</source>
          <target state="new"><bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows)</ept>: Steps on using PuTTY to create an SSH tunnel.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Ambari (REST)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/ambari</source>
          <target state="new"><bpt id="p1">**</bpt>Ambari (REST)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/ambari</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</source>
          <target state="new">Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>WebHCat (Templeton)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/templeton</source>
          <target state="new"><bpt id="p1">**</bpt>WebHCat (Templeton)<ept id="p1">**</ept> - https://&amp;lt;clustername&gt;.azurehdinsight.net/templeton</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Authenticate by using the cluster administrator user and password.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</source>
          <target state="new">Authentication is plaintext - always use HTTPS to help ensure that the connection is secure.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>SSH<ept id="p1">**</ept> - &amp;lt;clustername&gt;-ssh.azurehdinsight.net on port 22 or 23.</source>
          <target state="new"><bpt id="p1">**</bpt>SSH<ept id="p1">**</ept> - &amp;lt;clustername&gt;-ssh.azurehdinsight.net on port 22 or 23.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Port 22 is used to connect to headnode0, while 23 is used to connect to headnode1.</source>
          <target state="new">Port 22 is used to connect to headnode0, while 23 is used to connect to headnode1.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For more information on the head nodes, see <bpt id="p1">[</bpt>Availability and reliability of Hadoop clusters in HDInsight<ept id="p1">](hdinsight-high-availability-linux.md)</ept>.</source>
          <target state="new">For more information on the head nodes, see <bpt id="p1">[</bpt>Availability and reliability of Hadoop clusters in HDInsight<ept id="p1">](hdinsight-high-availability-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> You can only access the cluster head nodes through SSH from a client machine.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> You can only access the cluster head nodes through SSH from a client machine.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Once connected, you can then access the worker nodes by using SSH from the head node.</source>
          <target state="new">Once connected, you can then access the worker nodes by using SSH from the head node.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>File locations</source>
          <target state="new">File locations</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Hadoop-related files can be found on the cluster nodes at <ph id="ph1">`/usr/hdp`</ph>.</source>
          <target state="new">Hadoop-related files can be found on the cluster nodes at <ph id="ph1">`/usr/hdp`</ph>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>This directory contains the following subdirectories:</source>
          <target state="new">This directory contains the following subdirectories:</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>2.2.4.9-1<ept id="p1">__</ept>: This directory is named for the version of the Hortonworks Data Platform used by HDInsight, so the number on your cluster may be different than the one listed here.</source>
          <target state="new"><bpt id="p1">__</bpt>2.2.4.9-1<ept id="p1">__</ept>: This directory is named for the version of the Hortonworks Data Platform used by HDInsight, so the number on your cluster may be different than the one listed here.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>current<ept id="p1">__</ept>: This directory contains links to directories under the <bpt id="p2">__</bpt>2.2.4.9-1<ept id="p2">__</ept> directory, and exists so that you don't have to type a version number (that might change,) every time you want to access a file.</source>
          <target state="new"><bpt id="p1">__</bpt>current<ept id="p1">__</ept>: This directory contains links to directories under the <bpt id="p2">__</bpt>2.2.4.9-1<ept id="p2">__</ept> directory, and exists so that you don't have to type a version number (that might change,) every time you want to access a file.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Example data and JAR files can be found on Hadoop Distributed File System (HDFS) or Azure Blob storage at '/example' or 'wasb:///example'.</source>
          <target state="new">Example data and JAR files can be found on Hadoop Distributed File System (HDFS) or Azure Blob storage at '/example' or 'wasb:///example'.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>HDFS, Azure Blob storage, and storage best practices</source>
          <target state="new">HDFS, Azure Blob storage, and storage best practices</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>In most Hadoop distributions, HDFS is backed by local storage on the machines in the cluster.</source>
          <target state="new">In most Hadoop distributions, HDFS is backed by local storage on the machines in the cluster.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>While this is efficient, it can be costly for a cloud-based solution where you are charged hourly for compute resources.</source>
          <target state="new">While this is efficient, it can be costly for a cloud-based solution where you are charged hourly for compute resources.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>HDInsight uses Azure Blob storage as the default store, which provides the following benefits:</source>
          <target state="new">HDInsight uses Azure Blob storage as the default store, which provides the following benefits:</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Cheap long-term storage</source>
          <target state="new">Cheap long-term storage</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Accessibility from external services such as websites, file upload/download utilities, various language SDKs, and web browsers</source>
          <target state="new">Accessibility from external services such as websites, file upload/download utilities, various language SDKs, and web browsers</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Since it is the default store for HDInsight, you normally don't have to do anything to use it.</source>
          <target state="new">Since it is the default store for HDInsight, you normally don't have to do anything to use it.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>For example, the following command will list files in the <bpt id="p1">**</bpt>/example/data<ept id="p1">**</ept> folder, which is stored on Azure Blob storage:</source>
          <target state="new">For example, the following command will list files in the <bpt id="p1">**</bpt>/example/data<ept id="p1">**</ept> folder, which is stored on Azure Blob storage:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Some commands may require you to specify that you are using Blob storage.</source>
          <target state="new">Some commands may require you to specify that you are using Blob storage.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>For these, you can prefix the command with <bpt id="p1">**</bpt>WASB://<ept id="p1">**</ept>.</source>
          <target state="new">For these, you can prefix the command with <bpt id="p1">**</bpt>WASB://<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>HDInsight also allows you to associate multiple Blob storage accounts with a cluster.</source>
          <target state="new">HDInsight also allows you to associate multiple Blob storage accounts with a cluster.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>To access data on a non-default Blob storage account, you can use the format <bpt id="p1">**</bpt>WASB://&amp;lt;container-name&gt;@&amp;lt;account-name&gt;.blob.core.windows.net/<ept id="p1">**</ept>.</source>
          <target state="new">To access data on a non-default Blob storage account, you can use the format <bpt id="p1">**</bpt>WASB://&amp;lt;container-name&gt;@&amp;lt;account-name&gt;.blob.core.windows.net/<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>For example, the following will list the contents of the <bpt id="p1">**</bpt>/example/data<ept id="p1">**</ept> directory for the specified container and Blob storage account:</source>
          <target state="new">For example, the following will list the contents of the <bpt id="p1">**</bpt>/example/data<ept id="p1">**</ept> directory for the specified container and Blob storage account:</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>What Blob storage is the cluster using?</source>
          <target state="new">What Blob storage is the cluster using?</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>During cluster creation, you selected to either use an existing Azure Storage account and container, or create a new one.</source>
          <target state="new">During cluster creation, you selected to either use an existing Azure Storage account and container, or create a new one.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Then, you probably forgot about it.</source>
          <target state="new">Then, you probably forgot about it.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>You can find the default storage account and container by using the Ambari REST API.</source>
          <target state="new">You can find the default storage account and container by using the Ambari REST API.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Use the following command to retrieve HDFS configuration information:</source>
          <target state="new">Use the following command to retrieve HDFS configuration information:</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>In the JSON data returned, find the <ph id="ph1">`fs.defaultFS`</ph> entry.</source>
          <target state="new">In the JSON data returned, find the <ph id="ph1">`fs.defaultFS`</ph> entry.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>This will contain default container and storage account name in a format similar to the following:</source>
          <target state="new">This will contain default container and storage account name in a format similar to the following:</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.TIP]</ph> If you have installed <bpt id="p1">[</bpt>jq<ept id="p1">](http://stedolan.github.io/jq/)</ept>, you can use the following to return just the <ph id="ph2">`fs.defaultFS`</ph> entry:</source>
          <target state="new"><ph id="ph1">[AZURE.TIP]</ph> If you have installed <bpt id="p1">[</bpt>jq<ept id="p1">](http://stedolan.github.io/jq/)</ept>, you can use the following to return just the <ph id="ph2">`fs.defaultFS`</ph> entry:</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>To find the key used to authenticate to the storage account, or to find any secondary storage accounts associated with the cluster, use the following:</source>
          <target state="new">To find the key used to authenticate to the storage account, or to find any secondary storage accounts associated with the cluster, use the following:</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>In the JSON data returned, find the entries that begin with <ph id="ph1">`fs.azure.account.key`</ph>.</source>
          <target state="new">In the JSON data returned, find the entries that begin with <ph id="ph1">`fs.azure.account.key`</ph>.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>The remainder of the entry name is the storage account name.</source>
          <target state="new">The remainder of the entry name is the storage account name.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph1">`fs.azure.account.key.mystorage.blob.core.windows.net`</ph>.</source>
          <target state="new">For example, <ph id="ph1">`fs.azure.account.key.mystorage.blob.core.windows.net`</ph>.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>The value stored in this entry is the key used to authenticate to the storage account.</source>
          <target state="new">The value stored in this entry is the key used to authenticate to the storage account.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.TIP]</ph> If you have installed <bpt id="p1">[</bpt>jq<ept id="p1">](http://stedolan.github.io/jq/)</ept>, you can use the following to return a list of the keys and values:</source>
          <target state="new"><ph id="ph1">[AZURE.TIP]</ph> If you have installed <bpt id="p1">[</bpt>jq<ept id="p1">](http://stedolan.github.io/jq/)</ept>, you can use the following to return a list of the keys and values:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>How do I access Blob storage?</source>
          <target state="new">How do I access Blob storage?</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Other than through the Hadoop command from the cluster, there are a variety of ways to access blobs:</source>
          <target state="new">Other than through the Hadoop command from the cluster, there are a variety of ways to access blobs:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Azure CLI for Mac, Linux and Windows<ept id="p1">](../xplat-cli.md)</ept>: Command-Line interface commands for working with Azure.</source>
          <target state="new"><bpt id="p1">[</bpt>Azure CLI for Mac, Linux and Windows<ept id="p1">](../xplat-cli.md)</ept>: Command-Line interface commands for working with Azure.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>After installing, use the <ph id="ph1">`azure storage`</ph> command for help on using storage, or <ph id="ph2">`azure blob`</ph> for blob-specific commands.</source>
          <target state="new">After installing, use the <ph id="ph1">`azure storage`</ph> command for help on using storage, or <ph id="ph2">`azure blob`</ph> for blob-specific commands.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>blobxfer.py<ept id="p1">](https://github.com/Azure/azure-batch-samples/tree/master/Python/Storage)</ept>: A python script for working with blobs in Azure Storage.</source>
          <target state="new"><bpt id="p1">[</bpt>blobxfer.py<ept id="p1">](https://github.com/Azure/azure-batch-samples/tree/master/Python/Storage)</ept>: A python script for working with blobs in Azure Storage.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>A variety of SDKs:</source>
          <target state="new">A variety of SDKs:</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Java</source>
          <target state="new">Java</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Node.js</source>
          <target state="new">Node.js</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>PHP</source>
          <target state="new">PHP</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Python</source>
          <target state="new">Python</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Ruby</source>
          <target state="new">Ruby</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>.NET</source>
          <target state="new">.NET</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Storage REST API</source>
          <target state="new">Storage REST API</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Use Hive with HDInsight</source>
          <target state="new">Use Hive with HDInsight</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Use Pig with HDInsight</source>
          <target state="new">Use Pig with HDInsight</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Use MapReduce jobs with HDInsight<ept id="p1">](hdinsight-use-mapreduce.md)</ept></source>
          <target state="new"><bpt id="p1">[</bpt>Use MapReduce jobs with HDInsight<ept id="p1">](hdinsight-use-mapreduce.md)</ept></target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1bbc0687ae4e2ac72916a1e3301a1e24aad9c275</xliffext:olfilehash>
  </header>
</xliff>