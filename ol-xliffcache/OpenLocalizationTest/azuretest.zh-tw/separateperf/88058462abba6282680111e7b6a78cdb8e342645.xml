{
  "nodes": [
    {
      "content": "Create and load data into Hive tables from Blob storage | Microsoft Azure",
      "pos": [
        28,
        101
      ]
    },
    {
      "content": "Create Hive tables and load data in blob to hive tables",
      "pos": [
        121,
        176
      ]
    },
    {
      "content": "Create and load data into Hive tables from Azure blob storage",
      "pos": [
        544,
        605
      ]
    },
    {
      "content": "In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.",
      "pos": [
        608,
        723
      ]
    },
    {
      "content": "These Hive queries are shared in the GitHub repository.",
      "pos": [
        724,
        779
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Github repository<ept id=\"p1\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept>.",
      "pos": [
        780,
        964
      ]
    },
    {
      "content": "If you create an Azure virtual machine by following the instructions in \"Set Up an Azure Virtual Machine with IPython Notebook Server\", this script file has been downloaded to the <ph id=\"ph1\">`C:\\Users\\&lt;user name&gt;\\Documents\\Data Science Scripts`</ph> directory on the virtual machine.",
      "pos": [
        968,
        1235
      ]
    },
    {
      "content": "You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission.",
      "pos": [
        1236,
        1413
      ]
    },
    {
      "content": "We assume that the data for Hive tables is in <bpt id=\"p1\">**</bpt>uncompressed<ept id=\"p1\">**</ept> tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster.",
      "pos": [
        1416,
        1615
      ]
    },
    {
      "content": "If users want to practice on the <bpt id=\"p1\">_</bpt>NYC Taxi Trip Data<ept id=\"p1\">_</ept>, they need to first <bpt id=\"p2\">[</bpt>download all 24 files<ept id=\"p2\">](http://www.andresmh.com/nyctaxitrips/)</ept> (12 Trip files, and 12 Fair files), <bpt id=\"p3\">**</bpt>unzip<ept id=\"p3\">**</ept> all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the <bpt id=\"p4\">[</bpt>Azure HDInsight Hadoop cluster is customized<ept id=\"p4\">](machine-learning-data-science-customize-hadoop-cluster.html)</ept>.",
      "pos": [
        1616,
        2041
      ]
    },
    {
      "content": "Hive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster.",
      "pos": [
        2044,
        2140
      ]
    },
    {
      "content": "You need to:",
      "pos": [
        2141,
        2153
      ]
    },
    {
      "pos": [
        2158,
        2308
      ],
      "content": "<bpt id=\"p1\">[</bpt>Enable remote access to the head node of the Hadoop cluster, and log on to the head node<ept id=\"p1\">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>."
    },
    {
      "pos": [
        2312,
        2429
      ],
      "content": "<bpt id=\"p1\">[</bpt>Submit the Hive queries in the Hadoop Command Line on the head node<ept id=\"p1\">](machine-learning-data-science-hive-queries.md)</ept>."
    },
    {
      "content": "Users can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://",
      "pos": [
        2431,
        2526
      ]
    },
    {
      "content": ".azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can <bpt id=\"p1\">[</bpt>submit Hive jobs using PowerShell<ept id=\"p1\">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept>.",
      "pos": [
        2547,
        2747
      ]
    },
    {
      "content": "Step 1: Create Hive database and tables",
      "pos": [
        2753,
        2792
      ]
    },
    {
      "content": "Step 2: Load data to Hive tables",
      "pos": [
        2813,
        2845
      ]
    },
    {
      "content": "Advanced topics: partitioned table and store Hive data in ORC format",
      "pos": [
        2862,
        2930
      ]
    },
    {
      "pos": [
        2952,
        3011
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"create-tables\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Create Hive database and tables"
    },
    {
      "content": "Here are the descriptions of the fields that users need to plug in and other configurations:",
      "pos": [
        3478,
        3570
      ]
    },
    {
      "content": "<ph id=\"ph1\">`&lt;database name&gt;`</ph>: the name of the database users want to create.",
      "pos": [
        3574,
        3639
      ]
    },
    {
      "content": "If users just want to use the default database, the query <ph id=\"ph1\">`create database...`</ph> can be omitted.",
      "pos": [
        3640,
        3734
      ]
    },
    {
      "content": "<ph id=\"ph1\">`&lt;table name&gt;`</ph>: the name of the table users want to create within the specified database.",
      "pos": [
        3738,
        3827
      ]
    },
    {
      "content": "If users want to use the default database, the table can be directly referred by <ph id=\"ph1\">`&lt;table name&gt;`</ph> without <ph id=\"ph2\">`&lt;database name&gt;.`</ph>.",
      "pos": [
        3828,
        3951
      ]
    },
    {
      "pos": [
        3954,
        4061
      ],
      "content": "<ph id=\"ph1\">`&lt;field separator&gt;`</ph>: the separator that separates fields in the data file to be uploaded to the Hive table."
    },
    {
      "pos": [
        4065,
        4137
      ],
      "content": "<ph id=\"ph1\">`&lt;line separator&gt;`</ph>: the separator that separates lines in the data file."
    },
    {
      "content": "<ph id=\"ph1\">`&lt;storage location&gt;`</ph>: the Azure storage location to save the data of Hive tables.",
      "pos": [
        4141,
        4222
      ]
    },
    {
      "content": "If users do not specify <ph id=\"ph1\">`LOCATION '&lt;storage location&gt;'`</ph>, by default the database and the tables are stored in <ph id=\"ph2\">`hive/warehouse/`</ph> directory in the default container of the Hive cluster.",
      "pos": [
        4223,
        4406
      ]
    },
    {
      "content": "If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables.",
      "pos": [
        4407,
        4545
      ]
    },
    {
      "content": "This location has to be referred as relative location to the default container of the cluster in the format of <ph id=\"ph1\">`'wasb:///&lt;directory 1&gt;/'`</ph> or <ph id=\"ph2\">`'wasb:///&lt;directory 1&gt;/&lt;directory 2&gt;/'`</ph>, etc. After the query is executed, the relative directories will be created within the default container.",
      "pos": [
        4546,
        4833
      ]
    },
    {
      "content": "<ph id=\"ph1\">`TBLPROPERTIES(\"skip.header.line.count\"=\"1\")`</ph>: If the data file has a header line, users have to add this property at the <bpt id=\"p1\">**</bpt>end<ept id=\"p1\">**</ept> of the <ph id=\"ph2\">`create table`</ph> query.",
      "pos": [
        4837,
        4995
      ]
    },
    {
      "content": "Otherwise, the header line will be loaded as a record to the table.",
      "pos": [
        4996,
        5063
      ]
    },
    {
      "content": "If the data file does not have a header line, this configuration can be omitted in the query.",
      "pos": [
        5064,
        5157
      ]
    },
    {
      "pos": [
        5163,
        5211
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"load-data\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Load data to Hive tables"
    },
    {
      "content": "<ph id=\"ph1\">`&lt;path to blob data&gt;`</ph>: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the <ph id=\"ph2\">`&lt;path to blob data&gt;`</ph> should be in the format <ph id=\"ph3\">`'wasb:///&lt;directory in this container&gt;/&lt;blob file name&gt;'`</ph>.",
      "pos": [
        5300,
        5539
      ]
    },
    {
      "content": "The blob file can also be in the additional container of the HDInsight Hadoop cluster.",
      "pos": [
        5540,
        5626
      ]
    },
    {
      "content": "In this case, <ph id=\"ph1\">`&lt;path to blob data&gt;`</ph> should be in the format <ph id=\"ph2\">`'wasb://&lt;container name&gt;@&lt;storage account name&gt;.blob.windows.core.net/&lt;blob file name&gt;'`</ph>.",
      "pos": [
        5627,
        5777
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.",
      "pos": [
        5781,
        5932
      ]
    },
    {
      "content": "Otherwise, the <ph id=\"ph1\">`LOAD DATA`</ph> query will fail complaining that it cannot access the data.",
      "pos": [
        5933,
        6019
      ]
    },
    {
      "pos": [
        6026,
        6122
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"partition-orc\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Advanced topics: partitioned table and store Hive data in ORC format"
    },
    {
      "content": "If the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table.",
      "pos": [
        6124,
        6253
      ]
    },
    {
      "content": "For instance,  it is reasonable to partition the log data of a web site by dates.",
      "pos": [
        6254,
        6335
      ]
    },
    {
      "content": "In addition to partition the table, it is also beneficial to store the Hive data in ORC format.",
      "pos": [
        6338,
        6433
      ]
    },
    {
      "content": "ORC stands for \"Optimized Row Columnar\".",
      "pos": [
        6434,
        6474
      ]
    },
    {
      "content": "Please refer to <bpt id=\"p1\">_</bpt>\"<bpt id=\"p2\">[</bpt>Using ORC files improves performance when Hive is reading, writing, and processing data<ept id=\"p2\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles)</ept>.\"<ept id=\"p1\">_</ept> for more details.",
      "pos": [
        6476,
        6700
      ]
    },
    {
      "content": "Partitioned table",
      "pos": [
        6706,
        6723
      ]
    },
    {
      "content": "The queries of creating a partitioned table and loading data to it are as follows:",
      "pos": [
        6725,
        6807
      ]
    },
    {
      "pos": [
        7302,
        7489
      ],
      "content": "When querying partitioned tables, it is recommended to add the partition condition in the <bpt id=\"p1\">**</bpt>beginning<ept id=\"p1\">**</ept> of the <ph id=\"ph1\">`where`</ph> clause so that the searching efficacy can be significantly improved."
    },
    {
      "pos": [
        7658,
        7705
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"orc\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>Store Hive data in ORC format"
    },
    {
      "content": "Users cannot directly load data in blob to Hive tables in ORC storage format.",
      "pos": [
        7707,
        7784
      ]
    },
    {
      "content": "Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.",
      "pos": [
        7785,
        7907
      ]
    },
    {
      "pos": [
        7913,
        8006
      ],
      "content": "Create an external table <bpt id=\"p1\">**</bpt>STORED AS TEXTFILE<ept id=\"p1\">**</ept> and load data from blob storage to the table."
    },
    {
      "content": "Create an internal table with the same schema as the external table in step 1, and the same field delimiter.",
      "pos": [
        8553,
        8661
      ]
    },
    {
      "content": "And store the Hive data in the ORC format",
      "pos": [
        8662,
        8703
      ]
    },
    {
      "content": "Select data from the external table in step 1 and insert into the ORC table",
      "pos": [
        8975,
        9050
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> If the TEXTFILE table <ph id=\"ph2\">`&lt;database name&gt;.&lt;external textfile table name&gt;`</ph> has partitions, in STEP 3, <ph id=\"ph3\">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph> will select the partition variable as a field in the returned data set.",
      "pos": [
        9181,
        9426
      ]
    },
    {
      "content": "Inserting it to the <ph id=\"ph1\">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> will fail since <ph id=\"ph2\">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> does not have the partition variable as a field in the table schema.",
      "pos": [
        9427,
        9601
      ]
    },
    {
      "content": "In this case, users need to specifically select the fields to be inserted to <ph id=\"ph1\">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph> like follows:",
      "pos": [
        9602,
        9727
      ]
    },
    {
      "pos": [
        10024,
        10179
      ],
      "content": "It is safe to drop the <ph id=\"ph1\">`&lt;external textfile table name&gt;`</ph> using the following query after all data has been inserted into <ph id=\"ph2\">`&lt;database name&gt;.&lt;ORC table name&gt;`</ph>:"
    },
    {
      "content": "Now we have a table with data in the ORC format ready to use.",
      "pos": [
        10259,
        10320
      ]
    },
    {
      "content": "test",
      "pos": [
        10325,
        10329
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Create and load data into Hive tables from Blob storage | Microsoft Azure\" \n    description=\"Create Hive tables and load data in blob to hive tables\" \n    services=\"machine-learning\" \n    solutions=\"\" \n    documentationCenter=\"\" \n    authors=\"hangzh-msft\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"  />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"07/02/2015\" \n    ms.author=\"hangzh;bradsev\" />\n\n \n#Create and load data into Hive tables from Azure blob storage\n \nIn this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented. These Hive queries are shared in the GitHub repository. [Github repository](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql). \n\n\nIf you create an Azure virtual machine by following the instructions in \"Set Up an Azure Virtual Machine with IPython Notebook Server\", this script file has been downloaded to the `C:\\Users\\<user name>\\Documents\\Data Science Scripts` directory on the virtual machine. You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission. \n\nWe assume that the data for Hive tables is in **uncompressed** tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster. If users want to practice on the _NYC Taxi Trip Data_, they need to first [download all 24 files](http://www.andresmh.com/nyctaxitrips/) (12 Trip files, and 12 Fair files), **unzip** all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the [Azure HDInsight Hadoop cluster is customized](machine-learning-data-science-customize-hadoop-cluster.html). \n\nHive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster. You need to:\n\n1. [Enable remote access to the head node of the Hadoop cluster, and log on to the head node](machine-learning-data-science-customize-hadoop-cluster.md).\n2. [Submit the Hive queries in the Hadoop Command Line on the head node](machine-learning-data-science-hive-queries.md).\n\nUsers can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://<Hadoop cluster name>.azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can [submit Hive jobs using PowerShell](hdinsight-submit-hadoop-jobs-programmatically.md). \n\n- [Step 1: Create Hive database and tables](#create-tables)\n- [Step 2: Load data to Hive tables](#load-data)\n- [Advanced topics: partitioned table and store Hive data in ORC format](#partition-orc)\n\n## <a name=\"create-tables\"></a>Create Hive database and tables\n\n    create database if not exists <database name>;\n    CREATE EXTERNAL TABLE if not exists <database name>.<table name>\n    (\n        field1 string, \n        field2 int, \n        field3 float, \n        field4 double, \n        ...,\n        fieldN string\n    ) \n    ROW FORMAT DELIMITED FIELDS TERMINATED BY '<field separator>' lines terminated by '<line separator>' \n    STORED AS TEXTFILE LOCATION '<storage location>' TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\nHere are the descriptions of the fields that users need to plug in and other configurations:\n\n- `<database name>`: the name of the database users want to create. If users just want to use the default database, the query `create database...` can be omitted. \n- `<table name>`: the name of the table users want to create within the specified database. If users want to use the default database, the table can be directly referred by `<table name>` without `<database name>.`.\n- `<field separator>`: the separator that separates fields in the data file to be uploaded to the Hive table. \n- `<line separator>`: the separator that separates lines in the data file. \n- `<storage location>`: the Azure storage location to save the data of Hive tables. If users do not specify `LOCATION '<storage location>'`, by default the database and the tables are stored in `hive/warehouse/` directory in the default container of the Hive cluster. If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables. This location has to be referred as relative location to the default container of the cluster in the format of `'wasb:///<directory 1>/'` or `'wasb:///<directory 1>/<directory 2>/'`, etc. After the query is executed, the relative directories will be created within the default container. \n- `TBLPROPERTIES(\"skip.header.line.count\"=\"1\")`: If the data file has a header line, users have to add this property at the **end** of the `create table` query. Otherwise, the header line will be loaded as a record to the table. If the data file does not have a header line, this configuration can be omitted in the query. \n\n## <a name=\"load-data\"></a>Load data to Hive tables\n\n    LOAD DATA INPATH '<path to blob data>' INTO TABLE <database name>.<table name>;\n\n- `<path to blob data>`: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the `<path to blob data>` should be in the format `'wasb:///<directory in this container>/<blob file name>'`. The blob file can also be in the additional container of the HDInsight Hadoop cluster. In this case, `<path to blob data>` should be in the format `'wasb://<container name>@<storage account name>.blob.windows.core.net/<blob file name>'`.\n\n> [AZURE.NOTE] The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster. Otherwise, the `LOAD DATA` query will fail complaining that it cannot access the data. \n\n\n## <a name=\"partition-orc\"></a>Advanced topics: partitioned table and store Hive data in ORC format\n\nIf the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table. For instance,  it is reasonable to partition the log data of a web site by dates. \n\nIn addition to partition the table, it is also beneficial to store the Hive data in ORC format. ORC stands for \"Optimized Row Columnar\".  Please refer to _\"[Using ORC files improves performance when Hive is reading, writing, and processing data](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles).\"_ for more details.\n\n### Partitioned table\n\nThe queries of creating a partitioned table and loading data to it are as follows:\n\n    CREATE EXTERNAL TABLE IF NOT EXISTS <database name>.<table name>\n    (field1 string,\n    ...\n    fieldN string\n    )\n    PARTITIONED BY (<partitionfieldname> vartype) ROW FORMAT DELIMITED FIELDS TERMINATED BY '<field separator>'\n         lines terminated by '<line separator>' TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n    LOAD DATA INPATH '<path to the source file>' INTO TABLE <database name>.<partitioned table name> \n        PARTITION (<partitionfieldname>=<partitionfieldvalue>);\n\nWhen querying partitioned tables, it is recommended to add the partition condition in the **beginning** of the `where` clause so that the searching efficacy can be significantly improved. \n\n    select \n        field1, field2, ..., fieldN\n    from <database name>.<partitioned table name> \n    where <partitionfieldname>=<partitionfieldvalue> and ...;\n\n### <a name=\"orc\"></a>Store Hive data in ORC format\n\nUsers cannot directly load data in blob to Hive tables in ORC storage format. Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format. \n\n1. Create an external table **STORED AS TEXTFILE** and load data from blob storage to the table.\n\n        CREATE EXTERNAL TABLE IF NOT EXISTS <database name>.<external textfile table name>\n        (\n            field1 string,\n            field2 int,\n            ...\n            fieldN date\n        )\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '<field separator>' \n            lines terminated by '<line separator>' STORED AS TEXTFILE \n            LOCATION 'wasb:///<directory in Azure blob>' TBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n        LOAD DATA INPATH '<path to the source file>' INTO TABLE <database name>.<table name>;\n\n2. Create an internal table with the same schema as the external table in step 1, and the same field delimiter. And store the Hive data in the ORC format\n\n        CREATE TABLE IF NOT EXISTS <database name>.<ORC table name> \n        (\n            field1 string,\n            field2 int,\n            ...\n            fieldN date\n        ) \n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '<field separator>' STORED AS ORC;\n\n3. Select data from the external table in step 1 and insert into the ORC table\n\n        INSERT OVERWRITE TABLE <database name>.<ORC table name> SELECT * FROM <database name>.<external textfile table name>;\n\n> [AZURE.NOTE] If the TEXTFILE table `<database name>.<external textfile table name>` has partitions, in STEP 3, `SELECT * FROM <database name>.<external textfile table name>` will select the partition variable as a field in the returned data set. Inserting it to the `<database name>.<ORC table name>` will fail since `<database name>.<ORC table name>` does not have the partition variable as a field in the table schema. In this case, users need to specifically select the fields to be inserted to `<database name>.<ORC table name>` like follows:\n\n        INSERT OVERWRITE TABLE <database name>.<ORC table name> PARTITION (<partition variable>=<partition value>)\n              SELECT field1, field2, ..., fieldN\n              FROM <database name>.<external textfile table name> \n              WHERE <partition variable>=<partition value>;\n\n4. It is safe to drop the `<external textfile table name>` using the following query after all data has been inserted into `<database name>.<ORC table name>`:\n\n        DROP TABLE IF EXISTS <database name>.<external textfile table name>;\n\nNow we have a table with data in the ORC format ready to use. \n \n\ntest\n"
}