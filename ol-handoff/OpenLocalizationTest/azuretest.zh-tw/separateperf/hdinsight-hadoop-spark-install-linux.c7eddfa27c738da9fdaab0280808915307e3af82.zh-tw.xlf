<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Script Action to install Spark on Hadoop cluster | Microsoft Azure</source>
          <target state="new">Use Script Action to install Spark on Hadoop cluster | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to customize an HDInsight cluster with Spark.</source>
          <target state="new">Learn how to customize an HDInsight cluster with Spark.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>You'll use a Script Action configuration option to use a script to install Spark.</source>
          <target state="new">You'll use a Script Action configuration option to use a script to install Spark.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Install and use Spark on HDInsight Hadoop clusters</source>
          <target state="new">Install and use Spark on HDInsight Hadoop clusters</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In this document, you will learn how to install Spark by using Script Action.</source>
          <target state="new">In this document, you will learn how to install Spark by using Script Action.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Script Action lets you run scripts to customize a cluster, only when the cluster is being created.</source>
          <target state="new">Script Action lets you run scripts to customize a cluster, only when the cluster is being created.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Customize HDInsight cluster using Script Action<ept id="p1">][hdinsight-cluster-customize]</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Customize HDInsight cluster using Script Action<ept id="p1">][hdinsight-cluster-customize]</ept>.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.</source>
          <target state="new">Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>However, this is limited to Windows-based clusters currently.</source>
          <target state="new">However, this is limited to Windows-based clusters currently.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1.</source>
          <target state="new">Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p1">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p1">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is Spark?</source>
          <target state="new"><ph id="ph1">&lt;a name="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is Spark?</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a href="http://spark.apache.org/docs/latest/index.html" target="_blank"&gt;</ph>Apache Spark<ph id="ph2">&lt;/a&gt;</ph> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</source>
          <target state="new"><ph id="ph1">&lt;a href="http://spark.apache.org/docs/latest/index.html" target="_blank"&gt;</ph>Apache Spark<ph id="ph2">&lt;/a&gt;</ph> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.</source>
          <target state="new">Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Spark can also be used to perform conventional disk-based data processing.</source>
          <target state="new">Spark can also be used to perform conventional disk-based data processing.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.</source>
          <target state="new">Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.</source>
          <target state="new">Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>This topic provides instructions on how to customize an HDInsight cluster to install Spark.</source>
          <target state="new">This topic provides instructions on how to customize an HDInsight cluster to install Spark.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Which version of Spark can I install?</source>
          <target state="new"><ph id="ph1">&lt;a name="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Which version of Spark can I install?</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster.</source>
          <target state="new">In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>This script installs Spark 1.3.1.</source>
          <target state="new">This script installs Spark 1.3.1.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>You can modify this script or create your own script to install other versions of Spark.</source>
          <target state="new">You can modify this script or create your own script to install other versions of Spark.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>What the script does</source>
          <target state="new">What the script does</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>This script installs Spark version 1.3.1 into <ph id="ph1">`/usr/hdp/current/spark`</ph>.</source>
          <target state="new">This script installs Spark version 1.3.1 into <ph id="ph1">`/usr/hdp/current/spark`</ph>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="install"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Install Spark using Script Actions</source>
          <target state="new"><ph id="ph1">&lt;a name="install"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Install Spark using Script Actions</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id="p1">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh<ept id="p1">](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh)</ept>.</source>
          <target state="new">A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id="p1">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh<ept id="p1">](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh)</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal.</source>
          <target state="new">This section provides instructions on how to use the sample script while provisioning the cluster by using the Azure portal.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>For more information on using these methods, see <bpt id="p1">[</bpt>Customize HDInsight clusters with Script Actions<ept id="p1">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.</source>
          <target state="new">For more information on using these methods, see <bpt id="p1">[</bpt>Customize HDInsight clusters with Script Actions<ept id="p1">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Start provisioning a cluster by using the steps in <bpt id="p1">[</bpt>Provision Linux-based HDInsight clusters<ept id="p1">](hdinsight-provision-linux-clusters.md#portal)</ept>, but do not complete provisioning.</source>
          <target state="new">Start provisioning a cluster by using the steps in <bpt id="p1">[</bpt>Provision Linux-based HDInsight clusters<ept id="p1">](hdinsight-provision-linux-clusters.md#portal)</ept>, but do not complete provisioning.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p1">**</bpt>Optional Configuration<ept id="p1">**</ept> blade, select <bpt id="p2">**</bpt>Script Actions<ept id="p2">**</ept>, and provide the information below:</source>
          <target state="new">On the <bpt id="p1">**</bpt>Optional Configuration<ept id="p1">**</ept> blade, select <bpt id="p2">**</bpt>Script Actions<ept id="p2">**</ept>, and provide the information below:</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>NAME<ept id="p1">__</ept>: Enter a friendly name for the script action.</source>
          <target state="new"><bpt id="p1">__</bpt>NAME<ept id="p1">__</ept>: Enter a friendly name for the script action.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>SCRIPT URI<ept id="p1">__</ept>: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh</source>
          <target state="new"><bpt id="p1">__</bpt>SCRIPT URI<ept id="p1">__</ept>: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv01/spark-installer-v01.sh</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>HEAD<ept id="p1">__</ept>: Check this option</source>
          <target state="new"><bpt id="p1">__</bpt>HEAD<ept id="p1">__</ept>: Check this option</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>WORKER<ept id="p1">__</ept>: Check this option</source>
          <target state="new"><bpt id="p1">__</bpt>WORKER<ept id="p1">__</ept>: Check this option</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>ZOOKEEPER<ept id="p1">__</ept>: Check this option to install on the Zookeeper node.</source>
          <target state="new"><bpt id="p1">__</bpt>ZOOKEEPER<ept id="p1">__</ept>: Check this option to install on the Zookeeper node.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><bpt id="p1">__</bpt>PARAMETERS<ept id="p1">__</ept>: Leave this field blank</source>
          <target state="new"><bpt id="p1">__</bpt>PARAMETERS<ept id="p1">__</ept>: Leave this field blank</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>At the bottom of the <bpt id="p1">**</bpt>Script Actions<ept id="p1">**</ept>, use the <bpt id="p2">**</bpt>Select<ept id="p2">**</ept> button to save the configuration.</source>
          <target state="new">At the bottom of the <bpt id="p1">**</bpt>Script Actions<ept id="p1">**</ept>, use the <bpt id="p2">**</bpt>Select<ept id="p2">**</ept> button to save the configuration.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Finally, use the <bpt id="p1">**</bpt>Select<ept id="p1">**</ept> button at the bottom of the <bpt id="p2">**</bpt>Optional Configuration<ept id="p2">**</ept> blade to save the optional configuration information.</source>
          <target state="new">Finally, use the <bpt id="p1">**</bpt>Select<ept id="p1">**</ept> button at the bottom of the <bpt id="p2">**</bpt>Optional Configuration<ept id="p2">**</ept> blade to save the optional configuration information.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Continue provisining the cluster as described in <bpt id="p1">[</bpt>Provision Linux-based HDInsight clusters<ept id="p1">](hdinsight-provision-linux-clusters.md#portal)</ept>.</source>
          <target state="new">Continue provisining the cluster as described in <bpt id="p1">[</bpt>Provision Linux-based HDInsight clusters<ept id="p1">](hdinsight-provision-linux-clusters.md#portal)</ept>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="usespark"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>How do I use Spark in HDInsight?</source>
          <target state="new"><ph id="ph1">&lt;a name="usespark"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>How do I use Spark in HDInsight?</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Spark provides APIs in Scala, Python, and Java.</source>
          <target state="new">Spark provides APIs in Scala, Python, and Java.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>You can also use the interactive Spark shell to run Spark queries.</source>
          <target state="new">You can also use the interactive Spark shell to run Spark queries.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Once your cluster has finished provisioning, use the following to connect to your HDInsight cluster:</source>
          <target state="new">Once your cluster has finished provisioning, use the following to connect to your HDInsight cluster:</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>For more information on using SSH with HDInsight, see the following:</source>
          <target state="new">For more information on using SSH with HDInsight, see the following:</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Windows</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Windows</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Once connected, use the following sections for specific steps on using Spark:</source>
          <target state="new">Once connected, use the following sections for specific steps on using Spark:</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Using the Spark shell to run interactive queries</source>
          <target state="new">Using the Spark shell to run interactive queries</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Using the Spark shell to run Spark SQL queries</source>
          <target state="new">Using the Spark shell to run Spark SQL queries</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Using a standalone Scala program</source>
          <target state="new">Using a standalone Scala program</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="sparkshell"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using the Spark shell to run interactive queries</source>
          <target state="new"><ph id="ph1">&lt;a name="sparkshell"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using the Spark shell to run interactive queries</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Run the following command to start the Spark shell:</source>
          <target state="new">Run the following command to start the Spark shell:</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>After the command finishes running, you should get a Scala prompt:</source>
          <target state="new">After the command finishes running, you should get a Scala prompt:</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>On the Scala prompt, enter the Spark query shown below.</source>
          <target state="new">On the Scala prompt, enter the Spark query shown below.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.</source>
          <target state="new">This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The output should resemble the following:</source>
          <target state="new">The output should resemble the following:</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Enter :q to exit the Scala prompt.</source>
          <target state="new">Enter :q to exit the Scala prompt.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="sparksql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using the Spark shell to run Spark SQL queries</source>
          <target state="new"><ph id="ph1">&lt;a name="sparksql"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using the Spark shell to run Spark SQL queries</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.</source>
          <target state="new">Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>In this section, we look at using Spark to run a Hive query on a sample Hive table.</source>
          <target state="new">In this section, we look at using Spark to run a Hive query on a sample Hive table.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>The Hive table used in this section (called <bpt id="p1">**</bpt>hivesampletable<ept id="p1">**</ept>) is available by default when you provision a cluster.</source>
          <target state="new">The Hive table used in this section (called <bpt id="p1">**</bpt>hivesampletable<ept id="p1">**</ept>) is available by default when you provision a cluster.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Run the following command to start the Spark shell:</source>
          <target state="new">Run the following command to start the Spark shell:</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>After the command finishes running, you should get a Scala prompt:</source>
          <target state="new">After the command finishes running, you should get a Scala prompt:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>On the Scala prompt, set the Hive context.</source>
          <target state="new">On the Scala prompt, set the Hive context.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>This is required to work with Hive queries by using Spark.</source>
          <target state="new">This is required to work with Hive queries by using Spark.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph>  <ph id="ph2">`sc`</ph> in this statement is the default Spark context that is set when you start the Spark shell.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph>  <ph id="ph2">`sc`</ph> in this statement is the default Spark context that is set when you start the Spark shell.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Run a Hive query by using the Hive context and print the output to the console.</source>
          <target state="new">Run a Hive query by using the Hive context and print the output to the console.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.</source>
          <target state="new">The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>You should see an output like the following:</source>
          <target state="new">You should see an output like the following:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Enter :q to exit the Scala prompt.</source>
          <target state="new">Enter :q to exit the Scala prompt.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="standalone"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using a standalone Scala program</source>
          <target state="new"><ph id="ph1">&lt;a name="standalone"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Using a standalone Scala program</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>In this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.)</source>
          <target state="new">In this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.)</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Use the following commands to install the Scala Build Tool:</source>
          <target state="new">Use the following commands to install the Scala Build Tool:</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>When prompted, select <bpt id="p1">__</bpt>Y<ept id="p1">__</ept> to continue.</source>
          <target state="new">When prompted, select <bpt id="p1">__</bpt>Y<ept id="p1">__</ept> to continue.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Create the directory structure for the Scala project:</source>
          <target state="new">Create the directory structure for the Scala project:</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Create a new file named <bpt id="p1">__</bpt>simple.sbt<ept id="p1">__</ept>, which contains the configuration information for this project.</source>
          <target state="new">Create a new file named <bpt id="p1">__</bpt>simple.sbt<ept id="p1">__</ept>, which contains the configuration information for this project.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Use the following as the contents of the file:</source>
          <target state="new">Use the following as the contents of the file:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Make sure you retain the empty lines between each entry.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Make sure you retain the empty lines between each entry.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Use <bpt id="p1">__</bpt>Ctrl+X<ept id="p1">__</ept>, then <bpt id="p2">__</bpt>Y<ept id="p2">__</ept> and <bpt id="p3">__</bpt>Enter<ept id="p3">__</ept> to save the file.</source>
          <target state="new">Use <bpt id="p1">__</bpt>Ctrl+X<ept id="p1">__</ept>, then <bpt id="p2">__</bpt>Y<ept id="p2">__</ept> and <bpt id="p3">__</bpt>Enter<ept id="p3">__</ept> to save the file.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Use the following command to create a new file named <bpt id="p1">__</bpt>SimpleApp.scala<ept id="p1">__</ept> in the <bpt id="p2">__</bpt>SimpleScalaApp/src/main/scala<ept id="p2">__</ept> directory:</source>
          <target state="new">Use the following command to create a new file named <bpt id="p1">__</bpt>SimpleApp.scala<ept id="p1">__</ept> in the <bpt id="p2">__</bpt>SimpleScalaApp/src/main/scala<ept id="p2">__</ept> directory:</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Use the following as the contents of the file:</source>
          <target state="new">Use the following as the contents of the file:</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>Use <bpt id="p1">__</bpt>Ctrl+X<ept id="p1">__</ept>, then <bpt id="p2">__</bpt>Y<ept id="p2">__</ept>, and <bpt id="p3">__</bpt>Enter<ept id="p3">__</ept> to save the file.</source>
          <target state="new">Use <bpt id="p1">__</bpt>Ctrl+X<ept id="p1">__</ept>, then <bpt id="p2">__</bpt>Y<ept id="p2">__</ept>, and <bpt id="p3">__</bpt>Enter<ept id="p3">__</ept> to save the file.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>From the <bpt id="p1">__</bpt>SimpleScalaApp<ept id="p1">__</ept> directory, use the following command to build the application, and store it in a jar file:</source>
          <target state="new">From the <bpt id="p1">__</bpt>SimpleScalaApp<ept id="p1">__</ept> directory, use the following command to build the application, and store it in a jar file:</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Once the application is compiled, you will see a <bpt id="p1">**</bpt>simpleapp_2.10-1.0.jar<ept id="p1">**</ept> file created in the __SimpleScalaApp/target/scala-2.10** directory.</source>
          <target state="new">Once the application is compiled, you will see a <bpt id="p1">**</bpt>simpleapp_2.10-1.0.jar<ept id="p1">**</ept> file created in the __SimpleScalaApp/target/scala-2.10** directory.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Use the following command to run the SimpleApp.scala program:</source>
          <target state="new">Use the following command to run the SimpleApp.scala program:</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>When the program finishes running, the output is displayed on the console.</source>
          <target state="new">When the program finishes running, the output is displayed on the console.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Install and use Hue on HDInsight clusters<ept id="p1">](hdinsight-hadoop-hue-linux.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Install and use Hue on HDInsight clusters<ept id="p1">](hdinsight-hadoop-hue-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.</source>
          <target state="new">Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Install R on HDInsight clusters<ept id="p1">][hdinsight-install-r]</ept> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.</source>
          <target state="new"><bpt id="p1">[</bpt>Install R on HDInsight clusters<ept id="p1">][hdinsight-install-r]</ept> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>R is an open-source language and environment for statistical computing.</source>
          <target state="new">R is an open-source language and environment for statistical computing.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.</source>
          <target state="new">It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>It also provides extensive graphical capabilities.</source>
          <target state="new">It also provides extensive graphical capabilities.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Install Giraph on HDInsight clusters<ept id="p1">](hdinsight-hadoop-giraph-install-linux.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Install Giraph on HDInsight clusters<ept id="p1">](hdinsight-hadoop-giraph-install-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Use cluster customization to install Giraph on HDInsight Hadoop clusters.</source>
          <target state="new">Use cluster customization to install Giraph on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.</source>
          <target state="new">Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Install Solr on HDInsight clusters<ept id="p1">](hdinsight-hadoop-solr-install-linux.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Install Solr on HDInsight clusters<ept id="p1">](hdinsight-hadoop-solr-install-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Use cluster customization to install Solr on HDInsight Hadoop clusters.</source>
          <target state="new">Use cluster customization to install Solr on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Solr allows you to perform powerful search operations on data stored.</source>
          <target state="new">Solr allows you to perform powerful search operations on data stored.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Install Hue on HDInsight clusters<ept id="p1">](hdinsight-hadoop-hue-linux.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Install Hue on HDInsight clusters<ept id="p1">](hdinsight-hadoop-hue-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Use cluster customization to install Hue on HDInsight Hadoop clusters.</source>
          <target state="new">Use cluster customization to install Hue on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Hue is a set of Web applications used to interact with a Hadoop cluster.</source>
          <target state="new">Hue is a set of Web applications used to interact with a Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">03db151ee11ab9800a7ac1c54029f318ccff2022</xliffext:olfilehash>
  </header>
</xliff>