<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>MapReduce with Hadoop on HDInsight | Microsoft Azure</source>
          <target state="new">MapReduce with Hadoop on HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to run MapReduce jobs on Hadoop in HDInsight clusters.</source>
          <target state="new">Learn how to run MapReduce jobs on Hadoop in HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>You'll run a basic word count operation implemented as a Java MapReduce job.</source>
          <target state="new">You'll run a basic word count operation implemented as a Java MapReduce job.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Use MapReduce in Hadoop on HDInsight</source>
          <target state="new">Use MapReduce in Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In this article, you will learn how to run MapReduce jobs on Hadoop in HDInsight clusters.</source>
          <target state="new">In this article, you will learn how to run MapReduce jobs on Hadoop in HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>We run a basic word count operation implemented as a Java MapReduce job.</source>
          <target state="new">We run a basic word count operation implemented as a Java MapReduce job.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is MapReduce?</source>
          <target state="new"><ph id="ph1">&lt;a id="whatis"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is MapReduce?</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Hadoop MapReduce is a software framework for writing jobs that process vast amounts of data.</source>
          <target state="new">Hadoop MapReduce is a software framework for writing jobs that process vast amounts of data.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Input data is split into independent chunks, which are then processed in parallel across the nodes in your cluster.</source>
          <target state="new">Input data is split into independent chunks, which are then processed in parallel across the nodes in your cluster.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>A MapReduce job consist of two functions:</source>
          <target state="new">A MapReduce job consist of two functions:</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Mapper<ept id="p1">**</ept>: Consumes input data, analyzes it (usually with filter and sorting operations), and emits tuples (key-value pairs)</source>
          <target state="new"><bpt id="p1">**</bpt>Mapper<ept id="p1">**</ept>: Consumes input data, analyzes it (usually with filter and sorting operations), and emits tuples (key-value pairs)</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Reducer<ept id="p1">**</ept>: Consumes tuples emitted by the Mapper and performs a summary operation that creates a smaller, combined result from the Mapper data</source>
          <target state="new"><bpt id="p1">**</bpt>Reducer<ept id="p1">**</ept>: Consumes tuples emitted by the Mapper and performs a summary operation that creates a smaller, combined result from the Mapper data</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>A basic word count MapReduce job example is illustrated in the following diagram:</source>
          <target state="new">A basic word count MapReduce job example is illustrated in the following diagram:</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>HDI.WordCountDiagram</source>
          <target state="new">HDI.WordCountDiagram</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The output of this job is a count of how many times each word occurred in the text that was analyzed.</source>
          <target state="new">The output of this job is a count of how many times each word occurred in the text that was analyzed.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>The mapper takes each line from the input text as an input and breaks it into words.</source>
          <target state="new">The mapper takes each line from the input text as an input and breaks it into words.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>It emits a key/value pair each time a word occurs of the word is followed by a 1.</source>
          <target state="new">It emits a key/value pair each time a word occurs of the word is followed by a 1.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The output is sorted before sending it to reducer.</source>
          <target state="new">The output is sorted before sending it to reducer.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The reducer sums these individual counts for each word and emits a single key/value pair that contains the word followed by the sum of its occurrences.</source>
          <target state="new">The reducer sums these individual counts for each word and emits a single key/value pair that contains the word followed by the sum of its occurrences.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>MapReduce can be implemented in a variety of languages.</source>
          <target state="new">MapReduce can be implemented in a variety of languages.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Java is the most common implementation, and is used for demonstration purposes in this document.</source>
          <target state="new">Java is the most common implementation, and is used for demonstration purposes in this document.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Hadoop Streaming</source>
          <target state="new">Hadoop Streaming</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Languages or frameworks that are based on Java and the Java Virtual Machine (for example, Scalding or Cascading,) can be ran directly as a MapReduce job, similar to a Java application.</source>
          <target state="new">Languages or frameworks that are based on Java and the Java Virtual Machine (for example, Scalding or Cascading,) can be ran directly as a MapReduce job, similar to a Java application.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Others, such as C# or Python, or standalone executables, must use Hadoop streaming.</source>
          <target state="new">Others, such as C# or Python, or standalone executables, must use Hadoop streaming.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Hadoop streaming communicates with the mapper and reducer over STDIN and STDOUT - the mapper and reducer read data a line at a time from STDIN, and write the output to STDOUT.</source>
          <target state="new">Hadoop streaming communicates with the mapper and reducer over STDIN and STDOUT - the mapper and reducer read data a line at a time from STDIN, and write the output to STDOUT.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Each line read or emitted by the mapper and reducer must be in the format of a key/value pair, delimited by a tab charaacter:</source>
          <target state="new">Each line read or emitted by the mapper and reducer must be in the format of a key/value pair, delimited by a tab charaacter:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Hadoop Streaming<ept id="p1">](http://hadoop.apache.org/docs/r1.2.1/streaming.html)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Hadoop Streaming<ept id="p1">](http://hadoop.apache.org/docs/r1.2.1/streaming.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For examples of using Hadoop streaming with HDInsight, see the following:</source>
          <target state="new">For examples of using Hadoop streaming with HDInsight, see the following:</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Develop C# Hadoop streaming programs</source>
          <target state="new">Develop C# Hadoop streaming programs</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Develop Python MapReduce jobs</source>
          <target state="new">Develop Python MapReduce jobs</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the sample data</source>
          <target state="new"><ph id="ph1">&lt;a id="data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the sample data</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>In this example, for sample data, you will use the notebooks of Leonardo Da Vinci, which are provided as a text document in your HDInsight cluster.</source>
          <target state="new">In this example, for sample data, you will use the notebooks of Leonardo Da Vinci, which are provided as a text document in your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters.</source>
          <target state="new">The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>HDInsight can access files stored in Blob storage by using the <bpt id="p1">**</bpt>wasb<ept id="p1">**</ept> prefix.</source>
          <target state="new">HDInsight can access files stored in Blob storage by using the <bpt id="p1">**</bpt>wasb<ept id="p1">**</ept> prefix.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>For example, to access the sample.log file, you would use the following syntax:</source>
          <target state="new">For example, to access the sample.log file, you would use the following syntax:</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id="p1">**</bpt>/example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</source>
          <target state="new">Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id="p1">**</bpt>/example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> In the previous syntax, <bpt id="p1">**</bpt>wasb:///<ept id="p1">**</ept> is used to access files that are stored in the default storage container for your HDInsight cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> In the previous syntax, <bpt id="p1">**</bpt>wasb:///<ept id="p1">**</ept> is used to access files that are stored in the default storage container for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address.</source>
          <target state="new">If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>For example, <bpt id="p1">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</source>
          <target state="new">For example, <bpt id="p1">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/gutenberg/davinci.txt<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="job"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the example MapReduce</source>
          <target state="new"><ph id="ph1">&lt;a id="job"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the example MapReduce</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The MapReduce job that is used in this example is located at <bpt id="p1">**</bpt>wasb://example/jars/hadoop-mapreduce-examples.jar<ept id="p1">**</ept>, and it is provided with your HDInsight cluster.</source>
          <target state="new">The MapReduce job that is used in this example is located at <bpt id="p1">**</bpt>wasb://example/jars/hadoop-mapreduce-examples.jar<ept id="p1">**</ept>, and it is provided with your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>This contains a word count example that you will run against <bpt id="p1">**</bpt>davinci.txt<ept id="p1">**</ept>.</source>
          <target state="new">This contains a word count example that you will run against <bpt id="p1">**</bpt>davinci.txt<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> On HDInsight 2.1 clusters, the file location is <bpt id="p1">**</bpt>wasb:///example/jars/hadoop-examples.jar<ept id="p1">**</ept>.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> On HDInsight 2.1 clusters, the file location is <bpt id="p1">**</bpt>wasb:///example/jars/hadoop-examples.jar<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>For reference, the following is the Java code for the word count MapReduce job:</source>
          <target state="new">For reference, the following is the Java code for the word count MapReduce job:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>For instructions to write your own MapReduce job, see <bpt id="p1">[</bpt>Develop Java MapReduce programs for HDInsight<ept id="p1">](hdinsight-develop-deploy-java-mapreduce.md)</ept>.</source>
          <target state="new">For instructions to write your own MapReduce job, see <bpt id="p1">[</bpt>Develop Java MapReduce programs for HDInsight<ept id="p1">](hdinsight-develop-deploy-java-mapreduce.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="run"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Run the MapReduce</source>
          <target state="new"><ph id="ph1">&lt;a id="run"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Run the MapReduce</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>HDInsight can run HiveQL jobs by using a variety of methods.</source>
          <target state="new">HDInsight can run HiveQL jobs by using a variety of methods.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Use the following table to decide which method is right for you, then follow the link for a walkthrough.</source>
          <target state="new">Use the following table to decide which method is right for you, then follow the link for a walkthrough.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Use this<ept id="p1">**</ept>...</source>
          <target state="new"><bpt id="p1">**</bpt>Use this<ept id="p1">**</ept>...</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>...to do this</source>
          <target state="new">...to do this</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>...with this <bpt id="p1">**</bpt>cluster operating system<ept id="p1">**</ept></source>
          <target state="new">...with this <bpt id="p1">**</bpt>cluster operating system<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>...from this <bpt id="p1">**</bpt>client operating system<ept id="p1">**</ept></source>
          <target state="new">...from this <bpt id="p1">**</bpt>client operating system<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>SSH</source>
          <target state="new">SSH</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Use the Hadoop command through <bpt id="p1">**</bpt>SSH<ept id="p1">**</ept></source>
          <target state="new">Use the Hadoop command through <bpt id="p1">**</bpt>SSH<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Linux</source>
          <target state="new">Linux</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Linux, Unix, Mac OS X, or Windows</source>
          <target state="new">Linux, Unix, Mac OS X, or Windows</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Curl</source>
          <target state="new">Curl</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Submit the job remotely by using <bpt id="p1">**</bpt>REST<ept id="p1">**</ept></source>
          <target state="new">Submit the job remotely by using <bpt id="p1">**</bpt>REST<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Linux or Windows</source>
          <target state="new">Linux or Windows</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Linux, Unix, Mac OS X, or Windows</source>
          <target state="new">Linux, Unix, Mac OS X, or Windows</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Windows PowerShell</source>
          <target state="new">Windows PowerShell</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Submit the job remotely by using <bpt id="p1">**</bpt>Windows PowerShell<ept id="p1">**</ept></source>
          <target state="new">Submit the job remotely by using <bpt id="p1">**</bpt>Windows PowerShell<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Linux or Windows</source>
          <target state="new">Linux or Windows</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Remote Desktop</source>
          <target state="new">Remote Desktop</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Use the Hadoop command through <bpt id="p1">**</bpt>Remote Desktop<ept id="p1">**</ept></source>
          <target state="new">Use the Hadoop command through <bpt id="p1">**</bpt>Remote Desktop<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</source>
          <target state="new"><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Although MapReduce provides powerful diagnostic abilities, it can be a bit challenging to master.</source>
          <target state="new">Although MapReduce provides powerful diagnostic abilities, it can be a bit challenging to master.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>There are several Java-based frameworks that make it easier to define MapReduce applications, as well as technologies such as Pig and Hive, which provide an easier way to work with data in HDInsight.</source>
          <target state="new">There are several Java-based frameworks that make it easier to define MapReduce applications, as well as technologies such as Pig and Hive, which provide an easier way to work with data in HDInsight.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>To learn more, see the following articles:</source>
          <target state="new">To learn more, see the following articles:</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Develop Java MapReduce programs for HDInsight</source>
          <target state="new">Develop Java MapReduce programs for HDInsight</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Develop Python streaming MapReduce programs for HDInsight</source>
          <target state="new">Develop Python streaming MapReduce programs for HDInsight</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Develop C# Hadoop streaming MapReduce programs for HDInsight</source>
          <target state="new">Develop C# Hadoop streaming MapReduce programs for HDInsight</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Develop Scalding MapReduce jobs with Apache Hadoop on HDInsight</source>
          <target state="new">Develop Scalding MapReduce jobs with Apache Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Use Hive with HDInsight</source>
          <target state="new">Use Hive with HDInsight</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Use Pig with HDInsight</source>
          <target state="new">Use Pig with HDInsight</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Run the HDInsight Samples</source>
          <target state="new">Run the HDInsight Samples</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">5126fd65ea5ef92476b1a04b4cfa6e3142be6a96</xliffext:olfilehash>
  </header>
</xliff>