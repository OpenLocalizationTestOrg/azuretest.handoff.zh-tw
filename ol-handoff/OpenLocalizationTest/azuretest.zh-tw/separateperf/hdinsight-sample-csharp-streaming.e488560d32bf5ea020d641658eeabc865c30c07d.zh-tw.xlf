<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>C# streaming wordcount Hadoop sample | Microsoft Azure</source>
          <target state="new">C# streaming wordcount Hadoop sample | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>How to write MapReduce programs in C# that use the Hadoop Streaming interface, and how to run them on HDInsight using PowerShell cmdlets.</source>
          <target state="new">How to write MapReduce programs in C# that use the Hadoop Streaming interface, and how to run them on HDInsight using PowerShell cmdlets.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>The C# streaming word count MapReduce sample in Hadoop on HDInsight</source>
          <target state="new">The C# streaming word count MapReduce sample in Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Hadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java.</source>
          <target state="new">Hadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>This tutorial shows how to write MapReduce programs in C# by using the Hadoop Streaming interface and how to run the programs in Azure HDInsight by using Azure PowerShell cmdlets.</source>
          <target state="new">This tutorial shows how to write MapReduce programs in C# by using the Hadoop Streaming interface and how to run the programs in Azure HDInsight by using Azure PowerShell cmdlets.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The steps in this tutorial apply only to Windows-based HDInsight clusters.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The steps in this tutorial apply only to Windows-based HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>For an example of streaming for Linux-based HDInsight clusters, see <bpt id="p1">[</bpt>Develop Python streaming programs for HDInsight<ept id="p1">](hdinsight-hadoop-streaming-python.md)</ept>.</source>
          <target state="new">For an example of streaming for Linux-based HDInsight clusters, see <bpt id="p1">[</bpt>Develop Python streaming programs for HDInsight<ept id="p1">](hdinsight-hadoop-streaming-python.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>In the example, the mapper and the reducer are executables that read the input from <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> (line-by-line) and emit the output to <bpt id="p2">[</bpt>stdout<ept id="p2">][stdin-stdout-stderr]</ept>.</source>
          <target state="new">In the example, the mapper and the reducer are executables that read the input from <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> (line-by-line) and emit the output to <bpt id="p2">[</bpt>stdout<ept id="p2">][stdin-stdout-stderr]</ept>.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>The program counts all of the words in the text.</source>
          <target state="new">The program counts all of the words in the text.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>When an executable is specified for <bpt id="p1">**</bpt>mappers<ept id="p1">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized.</source>
          <target state="new">When an executable is specified for <bpt id="p1">**</bpt>mappers<ept id="p1">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>As the mapper task runs, it converts its input into lines, and feeds the lines to the <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</source>
          <target state="new">As the mapper task runs, it converts its input into lines, and feeds the lines to the <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>In the meantime, the mapper collects the line-oriented output from the stdout of the process.</source>
          <target state="new">In the meantime, the mapper collects the line-oriented output from the stdout of the process.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>It converts each line into a key/value pair, which is collected as the output of the mapper.</source>
          <target state="new">It converts each line into a key/value pair, which is collected as the output of the mapper.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</source>
          <target state="new">By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>If there is no Tab character in the line, entire line is considered as the key, and the value is null.</source>
          <target state="new">If there is no Tab character in the line, entire line is considered as the key, and the value is null.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>When an executable is specified for <bpt id="p1">**</bpt>reducers<ept id="p1">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized.</source>
          <target state="new">When an executable is specified for <bpt id="p1">**</bpt>reducers<ept id="p1">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</source>
          <target state="new">As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the <bpt id="p1">[</bpt>stdin<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>In the meantime, the reducer collects the line-oriented output from the <bpt id="p1">[</bpt>stdout<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</source>
          <target state="new">In the meantime, the reducer collects the line-oriented output from the <bpt id="p1">[</bpt>stdout<ept id="p1">][stdin-stdout-stderr]</ept> of the process.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>It converts each line to a key/value pair, which is collected as the output of the reducer.</source>
          <target state="new">It converts each line to a key/value pair, which is collected as the output of the reducer.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</source>
          <target state="new">By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>For more information about the Hadoop Streaming interface, see <bpt id="p1">[</bpt>Hadoop Streaming<ept id="p1">][hadoop-streaming]</ept>.</source>
          <target state="new">For more information about the Hadoop Streaming interface, see <bpt id="p1">[</bpt>Hadoop Streaming<ept id="p1">][hadoop-streaming]</ept>.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>In this tutorial, you will learn how to:</source>
          <target state="new">In this tutorial, you will learn how to:</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Use Azure PowerShell to run a C# streaming program to analyze data contained in a file in HDInsight.</source>
          <target state="new">Use Azure PowerShell to run a C# streaming program to analyze data contained in a file in HDInsight.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Write C# code that uses the Hadoop Streaming interface.</source>
          <target state="new">Write C# code that uses the Hadoop Streaming interface.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Prerequisites<ept id="p1">**</ept>:</source>
          <target state="new"><bpt id="p1">**</bpt>Prerequisites<ept id="p1">**</ept>:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Before you begin, you must have the following:</source>
          <target state="new">Before you begin, you must have the following:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>An Azure subscription<ept id="p1">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>An Azure subscription<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>Get Azure free trial<ept id="p1">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</source>
          <target state="new">See <bpt id="p1">[</bpt>Get Azure free trial<ept id="p1">](http://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>an HDInsight cluster<ept id="p1">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>an HDInsight cluster<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>For instructions on the various ways in which such clusters can be created, see <bpt id="p1">[</bpt>Provision HDInsight Clusters<ept id="p1">](hdinsight-provision-clusters.md)</ept>.</source>
          <target state="new">For instructions on the various ways in which such clusters can be created, see <bpt id="p1">[</bpt>Provision HDInsight Clusters<ept id="p1">](hdinsight-provision-clusters.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>A workstation with Azure PowerShell<ept id="p1">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>A workstation with Azure PowerShell<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>Install and use Azure PowerShell<ept id="p1">](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>.</source>
          <target state="new">See <bpt id="p1">[</bpt>Install and use Azure PowerShell<ept id="p1">](http://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="run-sample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Run the sample with Azure PowerShell</source>
          <target state="new"><ph id="ph1">&lt;a id="run-sample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Run the sample with Azure PowerShell</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>To run the MapReduce job</source>
          <target state="new">To run the MapReduce job</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Open <bpt id="p1">**</bpt>Azure PowerShell<ept id="p1">**</ept>.</source>
          <target state="new">Open <bpt id="p1">**</bpt>Azure PowerShell<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>For instructions to open the Azure PowerShell console window, see <bpt id="p1">[</bpt>Install and configure Azure PowerShell<ept id="p1">][powershell-install-configure]</ept>.</source>
          <target state="new">For instructions to open the Azure PowerShell console window, see <bpt id="p1">[</bpt>Install and configure Azure PowerShell<ept id="p1">][powershell-install-configure]</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Set the two variables in the following commands, and then run them:</source>
          <target state="new">Set the two variables in the following commands, and then run them:</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Run the following command to define the MapReduce job:</source>
          <target state="new">Run the following command to define the MapReduce job:</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The parameters, specify the mapper and reducer functions and the input file and output files.</source>
          <target state="new">The parameters, specify the mapper and reducer functions and the input file and output files.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Run the following commands to run the MapReduce job, wait for the job to complete, and then print the standard error message:</source>
          <target state="new">Run the following commands to run the MapReduce job, wait for the job to complete, and then print the standard error message:</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Run the following commands to display the results of the word count:</source>
          <target state="new">Run the following commands to display the results of the word count:</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>$storageAccountKey = Get-AzureStorageKey -StorageAccountName $storageAccountName | %{ $_.Primary }</source>
          <target state="new">$storageAccountKey = Get-AzureStorageKey -StorageAccountName $storageAccountName | %{ $_.Primary }</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>$storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey</source>
          <target state="new">$storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Note that the output files of a MapReduce job are immutable.</source>
          <target state="new">Note that the output files of a MapReduce job are immutable.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>So if you rerun this sample, you need to change the name of the output file.</source>
          <target state="new">So if you rerun this sample, you need to change the name of the output file.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="java-code"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>The C# code for Hadoop Streaming</source>
          <target state="new"><ph id="ph1">&lt;a id="java-code"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>The C# code for Hadoop Streaming</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document.</source>
          <target state="new">The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).</source>
          <target state="new">Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>The mapper code in the cat.cs file uses a <bpt id="p1">[</bpt>StreamReader<ept id="p1">][streamreader]</ept> object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static <bpt id="p2">[</bpt>Console.Writeline<ept id="p2">][console-writeline]</ept> method.</source>
          <target state="new">The mapper code in the cat.cs file uses a <bpt id="p1">[</bpt>StreamReader<ept id="p1">][streamreader]</ept> object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static <bpt id="p2">[</bpt>Console.Writeline<ept id="p2">][console-writeline]</ept> method.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>The reducer code in the wc.cs file uses a <bpt id="p1">[</bpt>StreamReader<ept id="p1">][streamreader]</ept>   object to read characters from the standard input stream that have been output by the cat.exe mapper.</source>
          <target state="new">The reducer code in the wc.cs file uses a <bpt id="p1">[</bpt>StreamReader<ept id="p1">][streamreader]</ept>   object to read characters from the standard input stream that have been output by the cat.exe mapper.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>As it reads the characters with the <bpt id="p1">[</bpt>Console.Writeline<ept id="p1">][console-writeline]</ept> method, it counts the words by counting spaces and end-of-line characters at the end of each word.</source>
          <target state="new">As it reads the characters with the <bpt id="p1">[</bpt>Console.Writeline<ept id="p1">][console-writeline]</ept> method, it counts the words by counting spaces and end-of-line characters at the end of each word.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>It then writes the total to the standard output stream with the <bpt id="p1">[</bpt>Console.Writeline<ept id="p1">][console-writeline]</ept> method.</source>
          <target state="new">It then writes the total to the standard output stream with the <bpt id="p1">[</bpt>Console.Writeline<ept id="p1">][console-writeline]</ept> method.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</source>
          <target state="new"><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>In this tutorial, you saw how to deploy a MapReduce job in HDInsight by using Hadoop Streaming.</source>
          <target state="new">In this tutorial, you saw how to deploy a MapReduce job in HDInsight by using Hadoop Streaming.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="next-steps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</source>
          <target state="new"><ph id="ph1">&lt;a id="next-steps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>For tutorials that run other samples and provide instructions for running Pig, Hive, and MapReduce jobs in Azure HDInsight with Azure PowerShell, see the following articles:</source>
          <target state="new">For tutorials that run other samples and provide instructions for running Pig, Hive, and MapReduce jobs in Azure HDInsight with Azure PowerShell, see the following articles:</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Get started with Azure HDInsight</source>
          <target state="new">Get started with Azure HDInsight</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Sample: Pi Estimator</source>
          <target state="new">Sample: Pi Estimator</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Sample: Word count</source>
          <target state="new">Sample: Word count</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Sample: 10GB GraySort</source>
          <target state="new">Sample: 10GB GraySort</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Use Pig with HDInsight</source>
          <target state="new">Use Pig with HDInsight</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Use Hive with HDInsight</source>
          <target state="new">Use Hive with HDInsight</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Azure HDInsight SDK documentation</source>
          <target state="new">Azure HDInsight SDK documentation</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ee85d8df36467e0d6dcdc88ca2661e1e29f02656</xliffext:olfilehash>
  </header>
</xliff>