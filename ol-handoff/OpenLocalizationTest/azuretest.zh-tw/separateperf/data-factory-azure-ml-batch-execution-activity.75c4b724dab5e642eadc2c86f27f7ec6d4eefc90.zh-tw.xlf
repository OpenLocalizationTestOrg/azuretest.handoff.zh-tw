<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Create predictive pipelines using Azure Machine Learning Bach Execution activity | Microsoft Azure</source>
          <target state="new">Create predictive pipelines using Azure Machine Learning Bach Execution activity | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning</source>
          <target state="new">Describes how to create create predictive pipelines using Azuer Data Factory and Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create predictive pipelines using Azure Machine Learning Batch Execution activity</source>
          <target state="new">Create predictive pipelines using Azure Machine Learning Batch Execution activity</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Overview</source>
          <target state="new">Overview</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> See <bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">](data-factory-introduction.md)</ept> and <bpt id="p2">[</bpt>Build your first pipeline<ept id="p2">](data-factory-build-your-first-pipeline.md)</ept> articles to quickly get started with the Azure Data Factory service.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> See <bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">](data-factory-introduction.md)</ept> and <bpt id="p2">[</bpt>Build your first pipeline<ept id="p2">](data-factory-build-your-first-pipeline.md)</ept> articles to quickly get started with the Azure Data Factory service.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Azure Data Factory enables you to easily create pipelines that leverage a published <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">][azure-machine-learning]</ept> web service for predictive analytics.</source>
          <target state="new">Azure Data Factory enables you to easily create pipelines that leverage a published <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">][azure-machine-learning]</ept> web service for predictive analytics.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Using Azure Data Factory, you can make use of big data pipelines (e.g. Pig and Hive) to process the data that you have ingested from various data sources, and use the Azure Machine Learning web services to make predictions on the data in batch.</source>
          <target state="new">Using Azure Data Factory, you can make use of big data pipelines (e.g. Pig and Hive) to process the data that you have ingested from various data sources, and use the Azure Machine Learning web services to make predictions on the data in batch.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>You use Azure Data Factory to orchestrate  data movement and processing, and then perform batch execution using Azure Machine Learning.</source>
          <target state="new">You use Azure Data Factory to orchestrate  data movement and processing, and then perform batch execution using Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>To achieve this, you will need to do the following:</source>
          <target state="new">To achieve this, you will need to do the following:</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Create an Azure Machne Learning linked service.</source>
          <target state="new">Create an Azure Machne Learning linked service.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>You will require the following:</source>
          <target state="new">You will require the following:</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Request URI<ept id="p1">**</ept> for the Batch Execution API.</source>
          <target state="new"><bpt id="p1">**</bpt>Request URI<ept id="p1">**</ept> for the Batch Execution API.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>You can find the Request URI by clicking on the <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> link in the web services page (shown below).</source>
          <target state="new">You can find the Request URI by clicking on the <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> link in the web services page (shown below).</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>API key<ept id="p1">**</ept> for the published Azure Machine Learning web service.</source>
          <target state="new"><bpt id="p1">**</bpt>API key<ept id="p1">**</ept> for the published Azure Machine Learning web service.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>You can find the API key by clicking on the web service that you have published.</source>
          <target state="new">You can find the API key by clicking on the web service that you have published.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">**</bpt>AzureMLBatchExecution<ept id="p1">**</ept> activity.</source>
          <target state="new">Use the <bpt id="p1">**</bpt>AzureMLBatchExecution<ept id="p1">**</ept> activity.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Machine Learning Dashboard</source>
          <target state="new">Machine Learning Dashboard</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Batch URI</source>
          <target state="new">Batch URI</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Scenario: Experiments using Web service inputs/outputs that refer to data in Azure Blob Storage</source>
          <target state="new">Scenario: Experiments using Web service inputs/outputs that refer to data in Azure Blob Storage</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>In this scenario, the Azure Machine Learning Web service makes predictions using data from a file in an Azure blob storage and stores the prediction results in the blob storage.</source>
          <target state="new">In this scenario, the Azure Machine Learning Web service makes predictions using data from a file in an Azure blob storage and stores the prediction results in the blob storage.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>The following JSON defines a Azure Data Factory pipeline with an AzureMLBatchExecution activity.</source>
          <target state="new">The following JSON defines a Azure Data Factory pipeline with an AzureMLBatchExecution activity.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>The activity has the dataset <bpt id="p1">**</bpt>DecisionTreeInputBlob<ept id="p1">**</ept> as input and <bpt id="p2">**</bpt>DecisionTreeResultBlob<ept id="p2">**</ept> as the output.</source>
          <target state="new">The activity has the dataset <bpt id="p1">**</bpt>DecisionTreeInputBlob<ept id="p1">**</ept> as input and <bpt id="p2">**</bpt>DecisionTreeResultBlob<ept id="p2">**</ept> as the output.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>DecisionTreeInputBlob<ept id="p1">**</ept> is passed as an input to the Web service by using the <bpt id="p2">**</bpt>webServiceInput<ept id="p2">**</ept> JSON property and <bpt id="p3">**</bpt>DecisionTreeResultBlob<ept id="p3">**</ept> as an output to the Web service by using the <bpt id="p4">**</bpt>webServiceOutputs<ept id="p4">**</ept> JSON property.</source>
          <target state="new">The <bpt id="p1">**</bpt>DecisionTreeInputBlob<ept id="p1">**</ept> is passed as an input to the Web service by using the <bpt id="p2">**</bpt>webServiceInput<ept id="p2">**</ept> JSON property and <bpt id="p3">**</bpt>DecisionTreeResultBlob<ept id="p3">**</ept> as an output to the Web service by using the <bpt id="p4">**</bpt>webServiceOutputs<ept id="p4">**</ept> JSON property.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Only datasets that are inputs/outputs for the activity can be passed as Web service inputs and outputs.</source>
          <target state="new">Only datasets that are inputs/outputs for the activity can be passed as Web service inputs and outputs.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Only inputs and outputs of the AzureMLBatchExecution activity can be passed as parameters to the Web service.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Only inputs and outputs of the AzureMLBatchExecution activity can be passed as parameters to the Web service.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>For example, in the above JSON snippet, DecisionTreeInputBlob is an input to the AzureMLBatchExecution activity, which is passed as an input to the Web service via webServiceInput parameter.</source>
          <target state="new">For example, in the above JSON snippet, DecisionTreeInputBlob is an input to the AzureMLBatchExecution activity, which is passed as an input to the Web service via webServiceInput parameter.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="new">Example</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>This example uses Azure Storage to hold both the input and output data.</source>
          <target state="new">This example uses Azure Storage to hold both the input and output data.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>We recommend that you go through the <bpt id="p1">[</bpt>Build your first pipeline with Data Factory<ept id="p1">][adf-build-1st-pipeline]</ept> tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, datasets, pipeline) in this example.</source>
          <target state="new">We recommend that you go through the <bpt id="p1">[</bpt>Build your first pipeline with Data Factory<ept id="p1">][adf-build-1st-pipeline]</ept> tutorial prior to going through this example and use the Data Factory Editor to create Data Factory artifacts (linked services, datasets, pipeline) in this example.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for your <bpt id="p2">**</bpt>Azure Storage<ept id="p2">**</ept>.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for your <bpt id="p2">**</bpt>Azure Storage<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>If the input and output files will be in different storage accounts, you will need two linked services.</source>
          <target state="new">If the input and output files will be in different storage accounts, you will need two linked services.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Here is a JSON example:</source>
          <target state="new">Here is a JSON example:</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Create the <bpt id="p1">**</bpt>input<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>dataset<ept id="p2">**</ept>.</source>
          <target state="new">Create the <bpt id="p1">**</bpt>input<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>dataset<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Note that unlike some other Data Factory datasets, these must both contain both <bpt id="p1">**</bpt>folderPath<ept id="p1">**</ept> and <bpt id="p2">**</bpt>fileName<ept id="p2">**</ept> values.</source>
          <target state="new">Note that unlike some other Data Factory datasets, these must both contain both <bpt id="p1">**</bpt>folderPath<ept id="p1">**</ept> and <bpt id="p2">**</bpt>fileName<ept id="p2">**</ept> values.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files.</source>
          <target state="new">You can use partitioning to cause each batch execution (each data slice) to process or produce unique input and output files.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice.</source>
          <target state="new">You will likely need to include some upstream activity to transform the input into the CSV file format and place it in the storage account for each slice.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>In that case, you would not include the <bpt id="p1">**</bpt>external<ept id="p1">**</ept> and <bpt id="p2">**</bpt>externalData<ept id="p2">**</ept> settings shown in the example below, and your DecisionTreeInputBlob would be the output dataset of a different Activity.</source>
          <target state="new">In that case, you would not include the <bpt id="p1">**</bpt>external<ept id="p1">**</ept> and <bpt id="p2">**</bpt>externalData<ept id="p2">**</ept> settings shown in the example below, and your DecisionTreeInputBlob would be the output dataset of a different Activity.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Your input csv file must have the column header row.</source>
          <target state="new">Your input csv file must have the column header row.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>If you are using the <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> to create/move the csv into the blob storage, you should set the sink property <bpt id="p2">**</bpt>blobWriterAddHeader<ept id="p2">**</ept> to <bpt id="p3">**</bpt>true<ept id="p3">**</ept>.</source>
          <target state="new">If you are using the <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> to create/move the csv into the blob storage, you should set the sink property <bpt id="p2">**</bpt>blobWriterAddHeader<ept id="p2">**</ept> to <bpt id="p3">**</bpt>true<ept id="p3">**</ept>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>If the csv file does not have the header row, you may see the following error: <bpt id="p1">**</bpt>Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1<ept id="p1">**</ept>.</source>
          <target state="new">If the csv file does not have the header row, you may see the following error: <bpt id="p1">**</bpt>Error in Activity: Error reading string. Unexpected token: StartObject. Path '', line 1, position 1<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Create the <bpt id="p1">**</bpt>output<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>dataset<ept id="p2">**</ept>.</source>
          <target state="new">Create the <bpt id="p1">**</bpt>output<ept id="p1">**</ept> Azure Data Factory <bpt id="p2">**</bpt>dataset<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>This example uses partitioning to create a unique output path for each slice execution.</source>
          <target state="new">This example uses partitioning to create a unique output path for each slice execution.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Without this, the activity would overwrite the file.</source>
          <target state="new">Without this, the activity would overwrite the file.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> of type: <bpt id="p2">**</bpt>AzureMLLinkedService<ept id="p2">**</ept>, providing the API key and model batch execution URL.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> of type: <bpt id="p2">**</bpt>AzureMLLinkedService<ept id="p2">**</ept>, providing the API key and model batch execution URL.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Finally, author a pipeline containing an <bpt id="p1">**</bpt>AzureMLBatchExecution<ept id="p1">**</ept> Activity.</source>
          <target state="new">Finally, author a pipeline containing an <bpt id="p1">**</bpt>AzureMLBatchExecution<ept id="p1">**</ept> Activity.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>It will get the location of the input file from your input datasets, call the Azure Machine Learning batch execution API, and copy the batch execution output to the blob given in your output dataset.</source>
          <target state="new">It will get the location of the input file from your input datasets, call the Azure Machine Learning batch execution API, and copy the batch execution output to the blob given in your output dataset.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> AzureMLBatchExecution activity can have zero or more inputs and one or more outputs.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> AzureMLBatchExecution activity can have zero or more inputs and one or more outputs.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</source>
          <target state="new">Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>For example: 2014-10-14T16:32:41Z.</source>
          <target state="new">For example: 2014-10-14T16:32:41Z.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</source>
          <target state="new">The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</source>
          <target state="new">If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</source>
          <target state="new">To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</source>
          <target state="new">See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Specifying input for the AzureMLBatchExecution activity is optional.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Specifying input for the AzureMLBatchExecution activity is optional.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Scenario: Experiments using Reader/Writer Modules to refer to data in various storages</source>
          <target state="new">Scenario: Experiments using Reader/Writer Modules to refer to data in various storages</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Another common scenario when creating Azure ML experiments is to use Reader and Writer modules.</source>
          <target state="new">Another common scenario when creating Azure ML experiments is to use Reader and Writer modules.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The reader module is used to load data into an experiment and the writer module is to save data from your experiments.</source>
          <target state="new">The reader module is used to load data into an experiment and the writer module is to save data from your experiments.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>For details about reader and writer modules, see <bpt id="p1">[</bpt>Reader<ept id="p1">](https://msdn.microsoft.com/library/azure/dn905997.aspx)</ept> and <bpt id="p2">[</bpt>Writer<ept id="p2">](https://msdn.microsoft.com/library/azure/dn905984.aspx)</ept> topics on MSDN Library.</source>
          <target state="new">For details about reader and writer modules, see <bpt id="p1">[</bpt>Reader<ept id="p1">](https://msdn.microsoft.com/library/azure/dn905997.aspx)</ept> and <bpt id="p2">[</bpt>Writer<ept id="p2">](https://msdn.microsoft.com/library/azure/dn905984.aspx)</ept> topics on MSDN Library.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>When using the reader and writer modules, it is good practice to use a Web service parameter for each property of these reader/writer modules.</source>
          <target state="new">When using the reader and writer modules, it is good practice to use a Web service parameter for each property of these reader/writer modules.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>These web parameters enable you to configure the values during runtime.</source>
          <target state="new">These web parameters enable you to configure the values during runtime.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>For example, you could create an experiment with a reader module that uses an Azure SQL Database: XXX.database.windows.net.</source>
          <target state="new">For example, you could create an experiment with a reader module that uses an Azure SQL Database: XXX.database.windows.net.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>After the web service has been deployed, you want to enable the consumers of the web service to specify another Azure SQL Server called YYY.database.windows.net.</source>
          <target state="new">After the web service has been deployed, you want to enable the consumers of the web service to specify another Azure SQL Server called YYY.database.windows.net.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>You can use a Web service parameter to allow this value to be configured.</source>
          <target state="new">You can use a Web service parameter to allow this value to be configured.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Web service input and output are different from Web service parameters.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Web service input and output are different from Web service parameters.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>In the first scenario, you have seen how an input and output can be specified for an Azure ML Web service.</source>
          <target state="new">In the first scenario, you have seen how an input and output can be specified for an Azure ML Web service.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>In this scenario, you will pass parameters for a Web service that correspond to properties of reader/writer modules.</source>
          <target state="new">In this scenario, you will pass parameters for a Web service that correspond to properties of reader/writer modules.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Let's look at a scenario for using Web service parameters.</source>
          <target state="new">Let's look at a scenario for using Web service parameters.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>You have a deployed Azure Machine Learning web service that is using a reader module to read data from one of the Azure Machine Learning supported data sources (for example: Azure SQL Database).</source>
          <target state="new">You have a deployed Azure Machine Learning web service that is using a reader module to read data from one of the Azure Machine Learning supported data sources (for example: Azure SQL Database).</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>After the batch execution is performed, the results are written using a Writer module (Azure SQL Database).</source>
          <target state="new">After the batch execution is performed, the results are written using a Writer module (Azure SQL Database).</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>No web service inputs and outputs are defined in the experiments.</source>
          <target state="new">No web service inputs and outputs are defined in the experiments.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>In this case, we recommend that you setup the relevant Web service parameters for the reader and writer modules.</source>
          <target state="new">In this case, we recommend that you setup the relevant Web service parameters for the reader and writer modules.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>This will allow the reader/writer modules to be configured when using the AzureMLBatchExecution activity.</source>
          <target state="new">This will allow the reader/writer modules to be configured when using the AzureMLBatchExecution activity.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>You specify Web service parameters in the <bpt id="p1">**</bpt>globalParameters<ept id="p1">**</ept> section in the activity JSON as follows.</source>
          <target state="new">You specify Web service parameters in the <bpt id="p1">**</bpt>globalParameters<ept id="p1">**</ept> section in the activity JSON as follows.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>You can also use <bpt id="p1">[</bpt>Data Factory Functions<ept id="p1">](https://msdn.microsoft.com/library/dn835056.aspx)</ept> in passing values for the Web service parameters as shown in the following example:</source>
          <target state="new">You can also use <bpt id="p1">[</bpt>Data Factory Functions<ept id="p1">](https://msdn.microsoft.com/library/dn835056.aspx)</ept> in passing values for the Web service parameters as shown in the following example:</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The Web service parameters are case-sensitive, so ensure that the names you specify in the activity JSON match the ones exposed by the Web service.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Using a Reader module to read data from multiple files in Azure Blob</source>
          <target state="new">Using a Reader module to read data from multiple files in Azure Blob</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Big data pipelines (Pig, Hive, etc...) can produce one or more output files with no extensions.</source>
          <target state="new">Big data pipelines (Pig, Hive, etc...) can produce one or more output files with no extensions.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>For example, when you specify an external Hive table, the data for the external Hive table can be stored in Azure blob storage with the following name 000000_0.</source>
          <target state="new">For example, when you specify an external Hive table, the data for the external Hive table can be stored in Azure blob storage with the following name 000000_0.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>You can use the reader module in an experiment to read multiple files, and use them for predictions.</source>
          <target state="new">You can use the reader module in an experiment to read multiple files, and use them for predictions.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>When using the reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input.</source>
          <target state="new">When using the reader module in an Azure Machine Learning experiment, you can specify Azure Blob as an input.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight.</source>
          <target state="new">The files in the Azure blob storage can be the output files (e.g. 000000_0) that are produced by a Pig and Hive script running on HDInsight.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>The reader module allows you to read files (with no extensions) by configuring the <bpt id="p1">**</bpt>Path to container, directory or blob<ept id="p1">**</ept> property of the reader module to point to the container/folder that contains the files as shown below.</source>
          <target state="new">The reader module allows you to read files (with no extensions) by configuring the <bpt id="p1">**</bpt>Path to container, directory or blob<ept id="p1">**</ept> property of the reader module to point to the container/folder that contains the files as shown below.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>Note, the asterisk (i.e. \*) <bpt id="p1">**</bpt>specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\*)<ept id="p1">**</ept> will be read as part of the experiment.</source>
          <target state="new">Note, the asterisk (i.e. \*) <bpt id="p1">**</bpt>specifies that all the files in the container/folder (i.e. data/aggregateddata/year=2014/month-6/\*)<ept id="p1">**</ept> will be read as part of the experiment.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>Azure Blob properties</source>
          <target state="new">Azure Blob properties</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="new">Example</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Pipeline with AzureMLBatchExecution activity with Web Service Parameters</source>
          <target state="new">Pipeline with AzureMLBatchExecution activity with Web Service Parameters</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>In the above JSON example:</source>
          <target state="new">In the above JSON example:</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>The deployed Azure Machine Learning Web service uses a reader and a writer module to read/write data from/to an Azure SQL Database.</source>
          <target state="new">The deployed Azure Machine Learning Web service uses a reader and a writer module to read/write data from/to an Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>This Web service exposes the following four parameters:  Database server name, Database name, Server user account name, and Server user account password.</source>
          <target state="new">This Web service exposes the following four parameters:  Database server name, Database name, Server user account name, and Server user account password.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</source>
          <target state="new">Both <bpt id="p1">**</bpt>start<ept id="p1">**</ept> and <bpt id="p2">**</bpt>end<ept id="p2">**</ept> datetimes must be in <bpt id="p3">[</bpt>ISO format<ept id="p3">](http://en.wikipedia.org/wiki/ISO_8601)</ept>.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>For example: 2014-10-14T16:32:41Z.</source>
          <target state="new">For example: 2014-10-14T16:32:41Z.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</source>
          <target state="new">The <bpt id="p1">**</bpt>end<ept id="p1">**</ept> time is optional.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</source>
          <target state="new">If you do not specify value for the <bpt id="p1">**</bpt>end<ept id="p1">**</ept> property, it is calculated as "<bpt id="p2">**</bpt>start + 48 hours<ept id="p2">**</ept>".</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</source>
          <target state="new">To run the pipeline indefinitely, specify <bpt id="p1">**</bpt>9999-09-09<ept id="p1">**</ept> as the value for the <bpt id="p2">**</bpt>end<ept id="p2">**</ept> property.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</source>
          <target state="new">See <bpt id="p1">[</bpt>JSON Scripting Reference<ept id="p1">](https://msdn.microsoft.com/library/dn835050.aspx)</ept> for details about JSON properties.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Frequently asked questions</source>
          <target state="new">Frequently asked questions</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Q:<ept id="p1">**</ept> I am using the AzureMLBatchScoring activity.</source>
          <target state="new"><bpt id="p1">**</bpt>Q:<ept id="p1">**</ept> I am using the AzureMLBatchScoring activity.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>Should I switch to using the AzureMLBatchExecution Activity?</source>
          <target state="new">Should I switch to using the AzureMLBatchExecution Activity?</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>A:<ept id="p1">**</ept> Yes.</source>
          <target state="new"><bpt id="p1">**</bpt>A:<ept id="p1">**</ept> Yes.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>If you are using the AzureMLBatchScoring activity to integrate with Azure Machine Learning, we recommend that you use the latest AzureMLBatchExecution activity.</source>
          <target state="new">If you are using the AzureMLBatchScoring activity to integrate with Azure Machine Learning, we recommend that you use the latest AzureMLBatchExecution activity.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>We are deprecating the AzureMLBatchScoring activity, and it will be removed in a future release.</source>
          <target state="new">We are deprecating the AzureMLBatchScoring activity, and it will be removed in a future release.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>The AzureMLBatchExecution activity is introduced in the August 2015 release of Azure SDK and Azure PowerShell.</source>
          <target state="new">The AzureMLBatchExecution activity is introduced in the August 2015 release of Azure SDK and Azure PowerShell.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>The AzureMLBatchExecution activity does not require an input (if input dependencies are not needed).</source>
          <target state="new">The AzureMLBatchExecution activity does not require an input (if input dependencies are not needed).</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>It also allows you to be explicit about whether you want to use the Web service input and Web service output or not use them.</source>
          <target state="new">It also allows you to be explicit about whether you want to use the Web service input and Web service output or not use them.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>If you do choose to use Web service input/output, it enables you to specify the relevant Azure Blob datasets to be used.In addition, it allows you to clearly specify the values for the web service parameters that are provided by the web service.</source>
          <target state="new">If you do choose to use Web service input/output, it enables you to specify the relevant Azure Blob datasets to be used.In addition, it allows you to clearly specify the values for the web service parameters that are provided by the web service.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>If you want to continue using the AzureMLBatchScoring activity, please refer to <bpt id="p1">[</bpt>Azure ML Batch Scoring Activity<ept id="p1">](data-factory-create-predictive-pipelines.md)</ept> article for details.</source>
          <target state="new">If you want to continue using the AzureMLBatchScoring activity, please refer to <bpt id="p1">[</bpt>Azure ML Batch Scoring Activity<ept id="p1">](data-factory-create-predictive-pipelines.md)</ept> article for details.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Q:<ept id="p1">**</ept> I have multiple files that are generated by my big data pipelines.</source>
          <target state="new"><bpt id="p1">**</bpt>Q:<ept id="p1">**</ept> I have multiple files that are generated by my big data pipelines.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Can I use the AzureMLBatchExecution Activity to work on all the files?</source>
          <target state="new">Can I use the AzureMLBatchExecution Activity to work on all the files?</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>A:<ept id="p1">**</ept> Yes.</source>
          <target state="new"><bpt id="p1">**</bpt>A:<ept id="p1">**</ept> Yes.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>See the <bpt id="p1">**</bpt>Using a Reader module to read data from multiple files in Azure Blob<ept id="p1">**</ept> section for details.</source>
          <target state="new">See the <bpt id="p1">**</bpt>Using a Reader module to read data from multiple files in Azure Blob<ept id="p1">**</ept> section for details.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ffb7915a8a74228f2b7fc7ff04f2dd2ce3bab91e</xliffext:olfilehash>
  </header>
</xliff>