<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Azure Data Factory - Terminology</source>
          <target state="new">Azure Data Factory - Terminology</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>This article introduces you to the terminology used in creating data factories using the Azure Data Factory service</source>
          <target state="new">This article introduces you to the terminology used in creating data factories using the Azure Data Factory service</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Azure Data Factory - Terminology</source>
          <target state="new">Azure Data Factory - Terminology</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Data factory</source>
          <target state="new">Data factory</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>An <bpt id="p1">**</bpt>Azure data factory<ept id="p1">**</ept> has one or more pipelines that process data in linked data stores (Azure Storage, Azure SQL Database, on-premises SQL Server etc...) by using linked compute services such as Azure HDInsight.</source>
          <target state="new">An <bpt id="p1">**</bpt>Azure data factory<ept id="p1">**</ept> has one or more pipelines that process data in linked data stores (Azure Storage, Azure SQL Database, on-premises SQL Server etc...) by using linked compute services such as Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>An Azure data factory itself does not contain data within it; The data is rather stored in data stores mentioned above.</source>
          <target state="new">An Azure data factory itself does not contain data within it; The data is rather stored in data stores mentioned above.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Linked Service</source>
          <target state="new">Linked Service</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> is a service that is linked to an Azure data factory.</source>
          <target state="new">A <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> is a service that is linked to an Azure data factory.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>A linked service can be one of the following:</source>
          <target state="new">A linked service can be one of the following:</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>data storage<ept id="p1">**</ept> service such as Azure Storage, Azure SQL Database or on-premises SQL Server database.</source>
          <target state="new">A <bpt id="p1">**</bpt>data storage<ept id="p1">**</ept> service such as Azure Storage, Azure SQL Database or on-premises SQL Server database.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>A data store is a container of input/output data sets.</source>
          <target state="new">A data store is a container of input/output data sets.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>compute<ept id="p1">**</ept> service such as Azure HDInsight, Azure Machine Learning, and Azure Batch.</source>
          <target state="new">A <bpt id="p1">**</bpt>compute<ept id="p1">**</ept> service such as Azure HDInsight, Azure Machine Learning, and Azure Batch.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>A compute service process the input data and produces the output data.</source>
          <target state="new">A compute service process the input data and produces the output data.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Data set</source>
          <target state="new">Data set</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>data set<ept id="p1">**</ept> is a named view of data.</source>
          <target state="new">A <bpt id="p1">**</bpt>data set<ept id="p1">**</ept> is a named view of data.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>The data being described can vary from simple bytes, semi-structured data like CSV files all the way up to relational tables or even models.</source>
          <target state="new">The data being described can vary from simple bytes, semi-structured data like CSV files all the way up to relational tables or even models.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>A  Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> is a data set that has a schema and is rectangular.</source>
          <target state="new">A  Data Factory <bpt id="p1">**</bpt>table<ept id="p1">**</ept> is a data set that has a schema and is rectangular.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>After creating a linked service in a data store that refers to a data store, you define data sets that represent input/output data that is stored in the data store.</source>
          <target state="new">After creating a linked service in a data store that refers to a data store, you define data sets that represent input/output data that is stored in the data store.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Pipeline</source>
          <target state="new">Pipeline</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>pipeline<ept id="p1">**</ept> in an Azure data factory processes data in linked storage services by using linked compute services.</source>
          <target state="new">A <bpt id="p1">**</bpt>pipeline<ept id="p1">**</ept> in an Azure data factory processes data in linked storage services by using linked compute services.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>It contains a sequence of activities where each activity performing a specific processing operation.</source>
          <target state="new">It contains a sequence of activities where each activity performing a specific processing operation.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>For example, a <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> copies data from a source storage to a destination storage and <bpt id="p2">**</bpt>HDInsight Activity<ept id="p2">**</ept> use an Azure HDInsight cluster to process data using Hive queries or Pig scripts.</source>
          <target state="new">For example, a <bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> copies data from a source storage to a destination storage and <bpt id="p2">**</bpt>HDInsight Activity<ept id="p2">**</ept> use an Azure HDInsight cluster to process data using Hive queries or Pig scripts.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>A data factory can have one or more pipelines.</source>
          <target state="new">A data factory can have one or more pipelines.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Typical steps for creating an Azure Data Factory instance are:</source>
          <target state="new">Typical steps for creating an Azure Data Factory instance are:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>data factory<ept id="p1">**</ept>.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>data factory<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for each data store or compute service.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>linked service<ept id="p1">**</ept> for each data store or compute service.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Create input and output <bpt id="p1">**</bpt>datasets<ept id="p1">**</ept>.</source>
          <target state="new">Create input and output <bpt id="p1">**</bpt>datasets<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Create a <bpt id="p1">**</bpt>pipeline<ept id="p1">**</ept>.</source>
          <target state="new">Create a <bpt id="p1">**</bpt>pipeline<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Activity</source>
          <target state="new">Activity</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>A data processing step in a pipeline that takes one or more input datasets and produces one or more output datasets.</source>
          <target state="new">A data processing step in a pipeline that takes one or more input datasets and produces one or more output datasets.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Activities run in an execution environment (for example: Azure HDInsight cluster) and read/write data to a data store associated/linked with the Azure data factory.</source>
          <target state="new">Activities run in an execution environment (for example: Azure HDInsight cluster) and read/write data to a data store associated/linked with the Azure data factory.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Azure Data Factory service supports the following activities in a pipeline:</source>
          <target state="new">Azure Data Factory service supports the following activities in a pipeline:</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> copies the data from a data store to another data store.</source>
          <target state="new"><bpt id="p1">**</bpt>Copy Activity<ept id="p1">**</ept> copies the data from a data store to another data store.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>HDInsight Activity<ept id="p1">**</ept> processes data by running Hive/Pig scripts or MapReduce programs on an HDInsight cluster.</source>
          <target state="new"><bpt id="p1">**</bpt>HDInsight Activity<ept id="p1">**</ept> processes data by running Hive/Pig scripts or MapReduce programs on an HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure Machine Learning Batch Scoring Activity<ept id="p1">**</ept> invokes the Azure Machine Learning batch scoring API.</source>
          <target state="new"><bpt id="p1">**</bpt>Azure Machine Learning Batch Scoring Activity<ept id="p1">**</ept> invokes the Azure Machine Learning batch scoring API.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning<ept id="p1">][azure-ml-adf]</ept> for details.</source>
          <target state="new">See <bpt id="p1">[</bpt>Create Predictive Pipelines using Azure Data Factory and Azure Machine Learning<ept id="p1">][azure-ml-adf]</ept> for details.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Stored Procedure Activity<ept id="p1">**</ept> invokes a stored procedure in an Azure SQL Database.</source>
          <target state="new"><bpt id="p1">**</bpt>Stored Procedure Activity<ept id="p1">**</ept> invokes a stored procedure in an Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>See the <bpt id="p1">[</bpt>Stored Procedure Activity<ept id="p1">][msdn-stored-procedure-activity]</ept> on MSDN Library for details.</source>
          <target state="new">See the <bpt id="p1">[</bpt>Stored Procedure Activity<ept id="p1">][msdn-stored-procedure-activity]</ept> on MSDN Library for details.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Slice</source>
          <target state="new">Slice</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>A table in an Azure data factory is composed of slices.</source>
          <target state="new">A table in an Azure data factory is composed of slices.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The width of a slice is determined by the schedule – hourly/daily.</source>
          <target state="new">The width of a slice is determined by the schedule – hourly/daily.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>When the schedule is “hourly”, a slice is produced hourly with in the start time and end time of a pipeline.</source>
          <target state="new">When the schedule is “hourly”, a slice is produced hourly with in the start time and end time of a pipeline.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>For example, if the pipeline start date-time is 03/03/2015 06:00:00 (6 AM) and end date-time is 03-03/2015 09:00:00 (9 AM on the same day), three data slices are produced, a slice for each 1 hour interval: 6-7 AM, 7-8 AM, and 8-9 AM.</source>
          <target state="new">For example, if the pipeline start date-time is 03/03/2015 06:00:00 (6 AM) and end date-time is 03-03/2015 09:00:00 (9 AM on the same day), three data slices are produced, a slice for each 1 hour interval: 6-7 AM, 7-8 AM, and 8-9 AM.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Slices provide the ability to work with a subset of overall data for a specific time window (for example: the slice that is produced for the duration (hour): 1:00 PM to 2:00 PM).</source>
          <target state="new">Slices provide the ability to work with a subset of overall data for a specific time window (for example: the slice that is produced for the duration (hour): 1:00 PM to 2:00 PM).</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Activity run for a slice</source>
          <target state="new">Activity run for a slice</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>run<ept id="p1">**</ept> or an activity run is a unit of processing for a slice.</source>
          <target state="new">The <bpt id="p1">**</bpt>run<ept id="p1">**</ept> or an activity run is a unit of processing for a slice.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>There could be one or more runs for a slice in case of retries or if you rerun your slice in case of a failure.</source>
          <target state="new">There could be one or more runs for a slice in case of retries or if you rerun your slice in case of a failure.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>A slice is identified by its start time.</source>
          <target state="new">A slice is identified by its start time.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Data Management Gateway</source>
          <target state="new">Data Management Gateway</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Microsoft <bpt id="p1">**</bpt>Data Management Gateway<ept id="p1">**</ept> is software that connects on-premises data sources to cloud services for consumption.</source>
          <target state="new">Microsoft <bpt id="p1">**</bpt>Data Management Gateway<ept id="p1">**</ept> is software that connects on-premises data sources to cloud services for consumption.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>You must have at least one gateway installed in your corporate environment and register it with the Azure Data Factory portal before adding on-premises data sources as linked services.</source>
          <target state="new">You must have at least one gateway installed in your corporate environment and register it with the Azure Data Factory portal before adding on-premises data sources as linked services.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Data Hub</source>
          <target state="new">Data Hub</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">**</bpt>Data Hub<ept id="p1">**</ept> is a logical container for data storage and compute services.</source>
          <target state="new">A <bpt id="p1">**</bpt>Data Hub<ept id="p1">**</ept> is a logical container for data storage and compute services.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>For example, a Hadoop cluster with HDFS as storage and Hive/Pig as compute (processing) is a data hub.</source>
          <target state="new">For example, a Hadoop cluster with HDFS as storage and Hive/Pig as compute (processing) is a data hub.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Similarly, an enterprise data warehouse (EDW) can be modeled as a data hub (database as storage, stored procedures and/or ETL tool as compute services).</source>
          <target state="new">Similarly, an enterprise data warehouse (EDW) can be modeled as a data hub (database as storage, stored procedures and/or ETL tool as compute services).</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Pipelines use data stores and run on compute resources in a data hub.</source>
          <target state="new">Pipelines use data stores and run on compute resources in a data hub.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Only HDInsight hub is supported at this moment.</source>
          <target state="new">Only HDInsight hub is supported at this moment.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The Data Hub allows a data factory to be divided into logical or domain specific groupings, such as the “West US Azure Hub” which manages all of the linked services (data stores and compute) and pipelines focused in the West US data center, or the “Sales EDW Hub” which manages all the linked services and pipelines concerned with populating and processing data for the Sales Enterprise Data Warehouse.</source>
          <target state="new">The Data Hub allows a data factory to be divided into logical or domain specific groupings, such as the “West US Azure Hub” which manages all of the linked services (data stores and compute) and pipelines focused in the West US data center, or the “Sales EDW Hub” which manages all the linked services and pipelines concerned with populating and processing data for the Sales Enterprise Data Warehouse.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>An important characteristic of Hub is that a pipeline runs on a single hub.</source>
          <target state="new">An important characteristic of Hub is that a pipeline runs on a single hub.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>This means that when defining a pipeline, all of the linked services referenced by tables or activities within that pipeline must have the same hub name as the pipeline itself.</source>
          <target state="new">This means that when defining a pipeline, all of the linked services referenced by tables or activities within that pipeline must have the same hub name as the pipeline itself.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>If the HubName property is not specified for a linked service, the linked service is placed in the “Default” Hub.</source>
          <target state="new">If the HubName property is not specified for a linked service, the linked service is placed in the “Default” Hub.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>See Also</source>
          <target state="new">See Also</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">][adf-intro]</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Introduction to Azure Data Factory<ept id="p1">][adf-intro]</ept>.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>This article provides an overview of the Azure Data Factory service, the value it provides, and the scenarios it supports.</source>
          <target state="new">This article provides an overview of the Azure Data Factory service, the value it provides, and the scenarios it supports.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Get started with Data Factory<ept id="p1">][datafactory-getstarted]</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Get started with Data Factory<ept id="p1">][datafactory-getstarted]</ept>.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>This article provides an end-to-end tutorial that shows you how to create a sample Azure data factory that copies data from an Azure blob to an Azure SQL database.</source>
          <target state="new">This article provides an end-to-end tutorial that shows you how to create a sample Azure data factory that copies data from an Azure blob to an Azure SQL database.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">8d7c5040475400e8a57083aa49589e1df910332e</xliffext:olfilehash>
  </header>
</xliff>