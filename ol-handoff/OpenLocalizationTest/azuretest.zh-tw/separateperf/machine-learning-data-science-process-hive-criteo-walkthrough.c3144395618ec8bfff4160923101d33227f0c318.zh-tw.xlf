<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Advanced Analytics Process and Technology in Action: using HDInsight Hadoop clusters on the 1 TB Criteo dataset | Microsoft Azure</source>
          <target state="new">Advanced Analytics Process and Technology in Action: using HDInsight Hadoop clusters on the 1 TB Criteo dataset | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Using the Advanced Analytics Process and Technology (ADAPT) for an end-to-end scenario employing an HDInsight Hadoop cluster to build and deploy a model using a large (1 TB) publicly available dataset</source>
          <target state="new">Using the Advanced Analytics Process and Technology (ADAPT) for an end-to-end scenario employing an HDInsight Hadoop cluster to build and deploy a model using a large (1 TB) publicly available dataset</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Advanced Analytics Process and Technology in Action - Using Azure HDInsight Hadoop Clusters on a 1 TB dataset</source>
          <target state="new">Advanced Analytics Process and Technology in Action - Using Azure HDInsight Hadoop Clusters on a 1 TB dataset</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In this walkthrough, we demonstrate using the Advanced Analytics Process and Technology (ADAPT) end-to-end with an <bpt id="p1">[</bpt>Azure HDInsight Hadoop cluster<ept id="p1">](http://azure.microsoft.com/services/hdinsight/)</ept> to store, explore, feature engineer, and down sample data from one of the publicly available <bpt id="p2">[</bpt>Criteo<ept id="p2">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept> datasets.</source>
          <target state="new">In this walkthrough, we demonstrate using the Advanced Analytics Process and Technology (ADAPT) end-to-end with an <bpt id="p1">[</bpt>Azure HDInsight Hadoop cluster<ept id="p1">](http://azure.microsoft.com/services/hdinsight/)</ept> to store, explore, feature engineer, and down sample data from one of the publicly available <bpt id="p2">[</bpt>Criteo<ept id="p2">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept> datasets.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>We use Azure Machine Learning to build a binary classification model on this data.</source>
          <target state="new">We use Azure Machine Learning to build a binary classification model on this data.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>We also show how to publish one of these models as a Web service.</source>
          <target state="new">We also show how to publish one of these models as a Web service.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>It is also possible to use an IPython notebook to accomplish the tasks presented in this walkthrough.</source>
          <target state="new">It is also possible to use an IPython notebook to accomplish the tasks presented in this walkthrough.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Users who would like to try this approach should consult the <bpt id="p1">[</bpt>Criteo walkthrough using a Hive ODBC connection<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/iPythonNotebooks/machine-Learning-data-science-process-hive-walkthrough-criteo.ipynb)</ept> topic.</source>
          <target state="new">Users who would like to try this approach should consult the <bpt id="p1">[</bpt>Criteo walkthrough using a Hive ODBC connection<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/iPythonNotebooks/machine-Learning-data-science-process-hive-walkthrough-criteo.ipynb)</ept> topic.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="dataset"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Criteo Dataset Description</source>
          <target state="new"><ph id="ph1">&lt;a name="dataset"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Criteo Dataset Description</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>The Criteo data is a click prediction dataset that is approximately 370GB of gzip compressed TSV files (~1.3TB uncompressed), comprising more than 4.3 billion records.</source>
          <target state="new">The Criteo data is a click prediction dataset that is approximately 370GB of gzip compressed TSV files (~1.3TB uncompressed), comprising more than 4.3 billion records.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>It is taken from 24 days of click data made available by <bpt id="p1">[</bpt>Criteo<ept id="p1">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept>.</source>
          <target state="new">It is taken from 24 days of click data made available by <bpt id="p1">[</bpt>Criteo<ept id="p1">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept>.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>For the convenience of data scientists, we have unzipped data available to us to experiment with.</source>
          <target state="new">For the convenience of data scientists, we have unzipped data available to us to experiment with.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Each record in this dataset contains 40 columns:</source>
          <target state="new">Each record in this dataset contains 40 columns:</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>the first column is a label column that indicates whether a user clicks on an add (value 1) or does not click (value 0)</source>
          <target state="new">the first column is a label column that indicates whether a user clicks on an add (value 1) or does not click (value 0)</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>next 13 columns are numeric, and</source>
          <target state="new">next 13 columns are numeric, and</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>last 26 are categorical columns</source>
          <target state="new">last 26 are categorical columns</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>The columns are anonymized and use a series of enumerated names: "Col1" (for the label column) to 'Col40" (for the last categorical column).</source>
          <target state="new">The columns are anonymized and use a series of enumerated names: "Col1" (for the label column) to 'Col40" (for the last categorical column).</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Here is an excerpt of the first 20 columns of two observations (rows) from this dataset:</source>
          <target state="new">Here is an excerpt of the first 20 columns of two observations (rows) from this dataset:</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>There are missing values in both the numeric and categorical columns in this dataset.</source>
          <target state="new">There are missing values in both the numeric and categorical columns in this dataset.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>We describe a simple method for handling the missing values below.</source>
          <target state="new">We describe a simple method for handling the missing values below.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Additional details of the data are explored below when we store them into Hive tables.</source>
          <target state="new">Additional details of the data are explored below when we store them into Hive tables.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Definition:<ept id="p1">**</ept> <bpt id="p2">*</bpt>Clickthrough rate (CTR):<ept id="p2">*</ept> This is the percentage of clicks in the data.</source>
          <target state="new"><bpt id="p1">**</bpt>Definition:<ept id="p1">**</ept> <bpt id="p2">*</bpt>Clickthrough rate (CTR):<ept id="p2">*</ept> This is the percentage of clicks in the data.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>In this Criteo dataset, the CTR is about 3.3% or 0.033.</source>
          <target state="new">In this Criteo dataset, the CTR is about 3.3% or 0.033.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="mltasks"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Examples of prediction tasks</source>
          <target state="new"><ph id="ph1">&lt;a name="mltasks"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Examples of prediction tasks</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Two sample prediction problems are addressed in this walkthrough:</source>
          <target state="new">Two sample prediction problems are addressed in this walkthrough:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Binary classification<ept id="p1">**</ept>: Predicts whether or not a user clicked on an add:</source>
          <target state="new"><bpt id="p1">**</bpt>Binary classification<ept id="p1">**</ept>: Predicts whether or not a user clicked on an add:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Class 0: No Click</source>
          <target state="new">Class 0: No Click</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Class 1: Click</source>
          <target state="new">Class 1: Click</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Regression<ept id="p1">**</ept>: Predicts the probability of an ad click from user features.</source>
          <target state="new"><bpt id="p1">**</bpt>Regression<ept id="p1">**</ept>: Predicts the probability of an ad click from user features.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="setup"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Set Up an HDInsight Hadoop cluster for data science</source>
          <target state="new"><ph id="ph1">&lt;a name="setup"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Set Up an HDInsight Hadoop cluster for data science</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically an <bpt id="p2">**</bpt>Admin<ept id="p2">**</ept> task.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically an <bpt id="p2">**</bpt>Admin<ept id="p2">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Set up your Azure Data Science environment for building predictive analytics solutions with HDInsight clusters in three steps:</source>
          <target state="new">Set up your Azure Data Science environment for building predictive analytics solutions with HDInsight clusters in three steps:</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Create a storage account<ept id="p1">](storage-whatis-account.md)</ept>: This storage account is used to store data in Azure Blob Storage.</source>
          <target state="new"><bpt id="p1">[</bpt>Create a storage account<ept id="p1">](storage-whatis-account.md)</ept>: This storage account is used to store data in Azure Blob Storage.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The data used in HDInsight clusters is stored here.</source>
          <target state="new">The data used in HDInsight clusters is stored here.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop Clusters for Data Science<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>: This step creates an Azure HDInsight Hadoop cluster with 64-bit Anaconda Python 2.7 installed on all nodes.</source>
          <target state="new"><bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop Clusters for Data Science<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>: This step creates an Azure HDInsight Hadoop cluster with 64-bit Anaconda Python 2.7 installed on all nodes.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>There are two important steps (described in this topic) to complete when customizing the HDInsight cluster.</source>
          <target state="new">There are two important steps (described in this topic) to complete when customizing the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>You must link the storage account created in step 1 with your HDInsight cluster when it is created.</source>
          <target state="new">You must link the storage account created in step 1 with your HDInsight cluster when it is created.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>This storage account is used for accessing data that can be processed within the cluster.</source>
          <target state="new">This storage account is used for accessing data that can be processed within the cluster.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>You must enable Remote Access to the head node of the cluster after it is created.</source>
          <target state="new">You must enable Remote Access to the head node of the cluster after it is created.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Remember the remote access credentials you specify here (different from those specified for the cluster at its creation): you will need them below.</source>
          <target state="new">Remember the remote access credentials you specify here (different from those specified for the cluster at its creation): you will need them below.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Create an Azure ML workspace<ept id="p1">](machine-learning-create-workspace.md)</ept>: This Azure Machine Learning workspace is used for building machine learning models after an initial data exploration and down sampling on the HDInsight cluster.</source>
          <target state="new"><bpt id="p1">[</bpt>Create an Azure ML workspace<ept id="p1">](machine-learning-create-workspace.md)</ept>: This Azure Machine Learning workspace is used for building machine learning models after an initial data exploration and down sampling on the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="getdata"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Get and consume data from a public source</source>
          <target state="new"><ph id="ph1">&lt;a name="getdata"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Get and consume data from a public source</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt>Criteo<ept id="p1">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept> dataset can be accessed by clicking on the link, accepting the terms of use, and providing a name.</source>
          <target state="new">The <bpt id="p1">[</bpt>Criteo<ept id="p1">](http://labs.criteo.com/downloads/download-terabyte-click-logs/)</ept> dataset can be accessed by clicking on the link, accepting the terms of use, and providing a name.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>A snapshot of what this looks like is shown below:</source>
          <target state="new">A snapshot of what this looks like is shown below:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Accept Criteo terms</source>
          <target state="new">Accept Criteo terms</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Click on <bpt id="p1">**</bpt>Continue to Download<ept id="p1">**</ept> to read more about the dataset and its availability.</source>
          <target state="new">Click on <bpt id="p1">**</bpt>Continue to Download<ept id="p1">**</ept> to read more about the dataset and its availability.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>The data resides in a public <bpt id="p1">[</bpt>Azure blob storage<ept id="p1">](storage-dotnet-how-to-use-blobs.md)</ept> location: wasb://criteo@azuremlsampleexperiments.blob.core.windows.net/raw/.</source>
          <target state="new">The data resides in a public <bpt id="p1">[</bpt>Azure blob storage<ept id="p1">](storage-dotnet-how-to-use-blobs.md)</ept> location: wasb://criteo@azuremlsampleexperiments.blob.core.windows.net/raw/.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>The "wasb" refers to Azure Blob Storage location.</source>
          <target state="new">The "wasb" refers to Azure Blob Storage location.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>The data in this public blob storage consists of three sub-folders of unzipped data.</source>
          <target state="new">The data in this public blob storage consists of three sub-folders of unzipped data.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>The sub-folder <bpt id="p1">*</bpt>raw/count/<ept id="p1">*</ept> contains the first 21 days of data - from day\_00 to day\_20</source>
          <target state="new">The sub-folder <bpt id="p1">*</bpt>raw/count/<ept id="p1">*</ept> contains the first 21 days of data - from day\_00 to day\_20</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>The sub-folder <bpt id="p1">*</bpt>raw/train/<ept id="p1">*</ept> consists of a single day of data, day\_21</source>
          <target state="new">The sub-folder <bpt id="p1">*</bpt>raw/train/<ept id="p1">*</ept> consists of a single day of data, day\_21</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>The sub-folder <bpt id="p1">*</bpt>raw/test/<ept id="p1">*</ept> consists of two days of data, day\_22 and day\_23</source>
          <target state="new">The sub-folder <bpt id="p1">*</bpt>raw/test/<ept id="p1">*</ept> consists of two days of data, day\_22 and day\_23</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>For those who want to start with the raw gzip data, these are also available in the main folder <bpt id="p1">*</bpt>raw/<ept id="p1">*</ept> as day_NN.gz, where NN goes from 00 to 23.</source>
          <target state="new">For those who want to start with the raw gzip data, these are also available in the main folder <bpt id="p1">*</bpt>raw/<ept id="p1">*</ept> as day_NN.gz, where NN goes from 00 to 23.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>An alternative approach to access, explore, and model this data that does not require any local downloads is explained later in this walkthrough when we create Hive tables.</source>
          <target state="new">An alternative approach to access, explore, and model this data that does not require any local downloads is explained later in this walkthrough when we create Hive tables.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="login"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Log into the cluster headnode</source>
          <target state="new"><ph id="ph1">&lt;a name="login"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Log into the cluster headnode</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>To login to the headnode of the cluster, use the <bpt id="p1">[</bpt>Azure Management<ept id="p1">](manage.windowsazure.com)</ept> portal to locate the cluster.</source>
          <target state="new">To login to the headnode of the cluster, use the <bpt id="p1">[</bpt>Azure Management<ept id="p1">](manage.windowsazure.com)</ept> portal to locate the cluster.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Click on the HDInsight elephant icon on the left and then double click on the name of your cluster.</source>
          <target state="new">Click on the HDInsight elephant icon on the left and then double click on the name of your cluster.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Navigate to the <bpt id="p1">**</bpt>Configuration<ept id="p1">**</ept> tab, double click on the CONNECT icon on the bottom of the page, and enter your remote access credentials when prompted.</source>
          <target state="new">Navigate to the <bpt id="p1">**</bpt>Configuration<ept id="p1">**</ept> tab, double click on the CONNECT icon on the bottom of the page, and enter your remote access credentials when prompted.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>This takes you to the headnode of the cluster.</source>
          <target state="new">This takes you to the headnode of the cluster.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Here is what a typical first login to the cluster headnode looks like:</source>
          <target state="new">Here is what a typical first login to the cluster headnode looks like:</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Login to cluster</source>
          <target state="new">Login to cluster</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>On the left, we see the "Hadoop Command Line", which will be our workhorse for the data exploration.</source>
          <target state="new">On the left, we see the "Hadoop Command Line", which will be our workhorse for the data exploration.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>We also see two useful URLs - "Hadoop Yarn Status" and "Hadoop Name Node".</source>
          <target state="new">We also see two useful URLs - "Hadoop Yarn Status" and "Hadoop Name Node".</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The yarn status URL shows job progress and the name node URL gives details on the cluster configuration.</source>
          <target state="new">The yarn status URL shows job progress and the name node URL gives details on the cluster configuration.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Now we are set up and ready to begin first part of the walkthrough: data exploration using Hive and getting data ready for Azure Machine Learning.</source>
          <target state="new">Now we are set up and ready to begin first part of the walkthrough: data exploration using Hive and getting data ready for Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="hive-db-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Create Hive database and tables</source>
          <target state="new"><ph id="ph1">&lt;a name="hive-db-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Create Hive database and tables</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>To create Hive tables for our Criteo dataset, open the <bpt id="p1">***</bpt>Hadoop Command Line<ept id="p1">***</ept> on the desktop of the head node, and enter the Hive directory by entering the command</source>
          <target state="new">To create Hive tables for our Criteo dataset, open the <bpt id="p1">***</bpt>Hadoop Command Line<ept id="p1">***</ept> on the desktop of the head node, and enter the Hive directory by entering the command</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>IMPORTANT NOTE<ept id="p1">**</ept>: <bpt id="p2">**</bpt>Run all Hive commands in this walkthrough from the above Hive bin/ directory prompt. This will take care of any path issues automatically. We will use the terms "Hive directory prompt", "Hive bin/ directory prompt",  and "Hadoop Command Line" interchangeably.<ept id="p2">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>IMPORTANT NOTE<ept id="p1">**</ept>: <bpt id="p2">**</bpt>Run all Hive commands in this walkthrough from the above Hive bin/ directory prompt. This will take care of any path issues automatically. We will use the terms "Hive directory prompt", "Hive bin/ directory prompt",  and "Hadoop Command Line" interchangeably.<ept id="p2">**</ept></target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>IMPORTANT NOTE 2<ept id="p1">**</ept>: <bpt id="p2">**</bpt>To execute any Hive query, one can always do the following<ept id="p2">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>IMPORTANT NOTE 2<ept id="p1">**</ept>: <bpt id="p2">**</bpt>To execute any Hive query, one can always do the following<ept id="p2">**</ept></target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>cd %hive_home%\bin</source>
          <target state="new">cd %hive_home%\bin</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>hive</source>
          <target state="new">hive</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>After the Hive REPL appears with a "hive &gt;"sign, simply cut and paste the query to execute it.</source>
          <target state="new">After the Hive REPL appears with a "hive &gt;"sign, simply cut and paste the query to execute it.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>The code below creates a database "criteo" and then generates 4 tables:</source>
          <target state="new">The code below creates a database "criteo" and then generates 4 tables:</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>a <bpt id="p1">*</bpt>table for generating counts<ept id="p1">*</ept> built on days day\_00 to day\_20,</source>
          <target state="new">a <bpt id="p1">*</bpt>table for generating counts<ept id="p1">*</ept> built on days day\_00 to day\_20,</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>a <bpt id="p1">*</bpt>table for use as the train dataset<ept id="p1">*</ept> built on day\_21, and</source>
          <target state="new">a <bpt id="p1">*</bpt>table for use as the train dataset<ept id="p1">*</ept> built on day\_21, and</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>two <bpt id="p1">*</bpt>tables for use as the test datasets<ept id="p1">*</ept> built on day\_22 and day\_23 respectively.</source>
          <target state="new">two <bpt id="p1">*</bpt>tables for use as the test datasets<ept id="p1">*</ept> built on day\_22 and day\_23 respectively.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>We split our test dataset into two different tables because one of the days is a holiday, and we want to determine if the model can detect differences between a holiday and non-holiday from the clickthrough rate.</source>
          <target state="new">We split our test dataset into two different tables because one of the days is a holiday, and we want to determine if the model can detect differences between a holiday and non-holiday from the clickthrough rate.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>The script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;create&amp;#95;criteo&amp;#95;database&amp;#95;and&amp;#95;tables.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_criteo_database_and_tables.hql)</ept> is displayed below for convenience:</source>
          <target state="new">The script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;create&amp;#95;criteo&amp;#95;database&amp;#95;and&amp;#95;tables.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_criteo_database_and_tables.hql)</ept> is displayed below for convenience:</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>We note that all these tables are external as we simply point to Azure Blob Storage (wasb) locations.</source>
          <target state="new">We note that all these tables are external as we simply point to Azure Blob Storage (wasb) locations.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>There are two ways to execute ANY Hive query that we now mention.</source>
          <target state="new">There are two ways to execute ANY Hive query that we now mention.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Using the Hive REPL command-line<ept id="p1">**</ept>: The first is to issue a "hive" command and copy and paste the above query at the Hive REPL command-line.</source>
          <target state="new"><bpt id="p1">**</bpt>Using the Hive REPL command-line<ept id="p1">**</ept>: The first is to issue a "hive" command and copy and paste the above query at the Hive REPL command-line.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>To do this, do:</source>
          <target state="new">To do this, do:</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Now at the REPL command-line, cutting and pasting the query executes it.</source>
          <target state="new">Now at the REPL command-line, cutting and pasting the query executes it.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Saving queries to a file and executing the command<ept id="p1">**</ept>: The second is to save the queries to a .hql file (<bpt id="p2">[</bpt>sample&amp;#95;hive&amp;#95;create&amp;#95;criteo&amp;#95;database&amp;#95;and&amp;#95;tables.hql<ept id="p2">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_criteo_database_and_tables.hql)</ept>) and then issue the following command to execute the query:</source>
          <target state="new"><bpt id="p1">**</bpt>Saving queries to a file and executing the command<ept id="p1">**</ept>: The second is to save the queries to a .hql file (<bpt id="p2">[</bpt>sample&amp;#95;hive&amp;#95;create&amp;#95;criteo&amp;#95;database&amp;#95;and&amp;#95;tables.hql<ept id="p2">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_criteo_database_and_tables.hql)</ept>) and then issue the following command to execute the query:</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>Confirm database and table creation</source>
          <target state="new">Confirm database and table creation</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Next, we confirm the creation of the database by issuing the command below from the Hive bin/ directory prompt:</source>
          <target state="new">Next, we confirm the creation of the database by issuing the command below from the Hive bin/ directory prompt:</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>This gives:</source>
          <target state="new">This gives:</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>This confirms the creation of the new database, "criteo".</source>
          <target state="new">This confirms the creation of the new database, "criteo".</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>To see what tables we created, we simply issue the command below from the Hive bin/ directory prompt:</source>
          <target state="new">To see what tables we created, we simply issue the command below from the Hive bin/ directory prompt:</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>We then see the following output:</source>
          <target state="new">We then see the following output:</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="exploration"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Data exploration in Hive</source>
          <target state="new"><ph id="ph1">&lt;a name="exploration"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Data exploration in Hive</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Now we are ready to do some basic data exploration in Hive.</source>
          <target state="new">Now we are ready to do some basic data exploration in Hive.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>We begin by counting the number of examples in the train and test data tables.</source>
          <target state="new">We begin by counting the number of examples in the train and test data tables.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Number of train examples</source>
          <target state="new">Number of train examples</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;train&amp;#95;table&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_train_table_examples.hql)</ept> are shown below:</source>
          <target state="new">The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;train&amp;#95;table&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_train_table_examples.hql)</ept> are shown below:</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Alternatively, one may also issue the command below from the Hive bin/ directory prompt:</source>
          <target state="new">Alternatively, one may also issue the command below from the Hive bin/ directory prompt:</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Number of test examples in the two test datasets</source>
          <target state="new">Number of test examples in the two test datasets</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>We now count the number of examples in the two test datasets.</source>
          <target state="new">We now count the number of examples in the two test datasets.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;criteo&amp;#95;test&amp;#95;day&amp;#95;22&amp;#95;table&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_criteo_test_day_22_table_examples.hql)</ept> are below:</source>
          <target state="new">The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;criteo&amp;#95;test&amp;#95;day&amp;#95;22&amp;#95;table&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_criteo_test_day_22_table_examples.hql)</ept> are below:</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>As usual, we may also call the script from the Hive bin/ directory prompt by issuing the below command:</source>
          <target state="new">As usual, we may also call the script from the Hive bin/ directory prompt by issuing the below command:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Finally, we examine the number of test examples in the test dataset based on day\_23.</source>
          <target state="new">Finally, we examine the number of test examples in the test dataset based on day\_23.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>The command to do this is similar to the above (refer to <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;criteo&amp;#95;test&amp;#95;day&amp;#95;23&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_criteo_test_day_23_examples.hql)</ept>):</source>
          <target state="new">The command to do this is similar to the above (refer to <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;count&amp;#95;criteo&amp;#95;test&amp;#95;day&amp;#95;23&amp;#95;examples.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_count_criteo_test_day_23_examples.hql)</ept>):</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>This gives:</source>
          <target state="new">This gives:</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Label distribution in the train dataset</source>
          <target state="new">Label distribution in the train dataset</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>The label distribution in the train dataset is of interest.</source>
          <target state="new">The label distribution in the train dataset is of interest.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>To see this, we show contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;label&amp;#95;distribution&amp;#95;train&amp;#95;table.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_label_distribution_train_table.hql)</ept>:</source>
          <target state="new">To see this, we show contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;label&amp;#95;distribution&amp;#95;train&amp;#95;table.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_label_distribution_train_table.hql)</ept>:</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>This yields the label distribution:</source>
          <target state="new">This yields the label distribution:</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Note that the percentage of positive labels is about 3.3% (consistent with the original dataset).</source>
          <target state="new">Note that the percentage of positive labels is about 3.3% (consistent with the original dataset).</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Histogram distributions of some numeric variables in the train dataset</source>
          <target state="new">Histogram distributions of some numeric variables in the train dataset</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>We can use Hive's native "histogram\_numeric" function to find out what the distribution of the numeric variables looks like.</source>
          <target state="new">We can use Hive's native "histogram\_numeric" function to find out what the distribution of the numeric variables looks like.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;histogram&amp;#95;numeric.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_histogram_numeric.hql)</ept> are as below:</source>
          <target state="new">The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;histogram&amp;#95;numeric.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_histogram_numeric.hql)</ept> are as below:</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>This yields the following:</source>
          <target state="new">This yields the following:</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>The LATERAL VIEW - explode combination in Hive serves to produce a SQL-like output instead of the usual list.</source>
          <target state="new">The LATERAL VIEW - explode combination in Hive serves to produce a SQL-like output instead of the usual list.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Note that in the above table, the first column corresponds to the bin center and the second to the bin frequency.</source>
          <target state="new">Note that in the above table, the first column corresponds to the bin center and the second to the bin frequency.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Approximate percentiles of some numeric variables in the train dataset</source>
          <target state="new">Approximate percentiles of some numeric variables in the train dataset</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>Also of interest with numeric variables is the computation of approximate percentiles.</source>
          <target state="new">Also of interest with numeric variables is the computation of approximate percentiles.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>Hive's native "percentile\_approx" does this for us.</source>
          <target state="new">Hive's native "percentile\_approx" does this for us.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;approximate&amp;#95;percentiles.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_approximate_percentiles.hql)</ept> are:</source>
          <target state="new">The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;approximate&amp;#95;percentiles.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_approximate_percentiles.hql)</ept> are:</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>We remark that the distribution of percentiles is closely related to the histogram distribution of any numeric variable usually.</source>
          <target state="new">We remark that the distribution of percentiles is closely related to the histogram distribution of any numeric variable usually.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Find number of unique values for some categorical columns in the train dataset</source>
          <target state="new">Find number of unique values for some categorical columns in the train dataset</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>Continuing the data exploration, we now find, for some categorical columns, the number of unique values they take.</source>
          <target state="new">Continuing the data exploration, we now find, for some categorical columns, the number of unique values they take.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>To do this, we show contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;unique&amp;#95;values&amp;#95;categoricals.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_unique_values_categoricals.hql)</ept>:</source>
          <target state="new">To do this, we show contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;unique&amp;#95;values&amp;#95;categoricals.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_unique_values_categoricals.hql)</ept>:</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>We note that Col15 has 19M unique values!</source>
          <target state="new">We note that Col15 has 19M unique values!</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Using naive techniques like "one-hot encoding" to encode such high-dimensional categorical variables is infeasible.</source>
          <target state="new">Using naive techniques like "one-hot encoding" to encode such high-dimensional categorical variables is infeasible.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>In particular, we explain and demonstrate a powerful, robust technique called <bpt id="p1">[</bpt>Learning With Counts<ept id="p1">](http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx)</ept> for tackling this problem efficiently.</source>
          <target state="new">In particular, we explain and demonstrate a powerful, robust technique called <bpt id="p1">[</bpt>Learning With Counts<ept id="p1">](http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx)</ept> for tackling this problem efficiently.</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>We end this sub-section by looking at the number of unique values for some other categorical columns as well.</source>
          <target state="new">We end this sub-section by looking at the number of unique values for some other categorical columns as well.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;unique&amp;#95;values&amp;#95;multiple&amp;#95;categoricals.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_unique_values_multiple_categoricals.hql)</ept> are:</source>
          <target state="new">The contents of <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;unique&amp;#95;values&amp;#95;multiple&amp;#95;categoricals.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_unique_values_multiple_categoricals.hql)</ept> are:</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>Again we see that except for Col20, all the other columns have many unique values.</source>
          <target state="new">Again we see that except for Col20, all the other columns have many unique values.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>Co-occurence counts of pairs of categorical variables in the train dataset</source>
          <target state="new">Co-occurence counts of pairs of categorical variables in the train dataset</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>The co-occurence counts of pairs of categorical variables is also of interest.</source>
          <target state="new">The co-occurence counts of pairs of categorical variables is also of interest.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>This can be determined using the code in <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;paired&amp;#95;categorical&amp;#95;counts.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_paired_categorical_counts.hql)</ept>:</source>
          <target state="new">This can be determined using the code in <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;paired&amp;#95;categorical&amp;#95;counts.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_paired_categorical_counts.hql)</ept>:</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>We reverse order the counts by their occurrence and look at the top 15 in this case.</source>
          <target state="new">We reverse order the counts by their occurrence and look at the top 15 in this case.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>This gives us:</source>
          <target state="new">This gives us:</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="downsample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Down sample the datasets for Azure Machine Learning</source>
          <target state="new"><ph id="ph1">&lt;a name="downsample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Down sample the datasets for Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>Having explored the datasets and demonstrated how we may do this type of exploration for any variables (including combinations), we now down sample the data sets so that we can build models in Azure Machine Learning.</source>
          <target state="new">Having explored the datasets and demonstrated how we may do this type of exploration for any variables (including combinations), we now down sample the data sets so that we can build models in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>Recall that the problem we focus on is: given a set of example attributes (feature values from Col2 - Col40), we predict if Col1 is a 0 (no click) or a 1 (click).</source>
          <target state="new">Recall that the problem we focus on is: given a set of example attributes (feature values from Col2 - Col40), we predict if Col1 is a 0 (no click) or a 1 (click).</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>To down sample our train and test datasets to 1% of the original size, we use Hive's native RAND() function.</source>
          <target state="new">To down sample our train and test datasets to 1% of the original size, we use Hive's native RAND() function.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>The next script, <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;train&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_train_dataset.hql)</ept> does this for the train dataset:</source>
          <target state="new">The next script, <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;train&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_train_dataset.hql)</ept> does this for the train dataset:</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>The script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;test&amp;#95;day&amp;#95;22&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_test_day_22_dataset.hql)</ept> does it for test data, day\_22:</source>
          <target state="new">The script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;test&amp;#95;day&amp;#95;22&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_test_day_22_dataset.hql)</ept> does it for test data, day\_22:</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>Finally, the script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;test&amp;#95;day&amp;#95;23&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_test_day_23_dataset.hql)</ept> does it for test data, day\_23:</source>
          <target state="new">Finally, the script <bpt id="p1">[</bpt>sample&amp;#95;hive&amp;#95;criteo&amp;#95;downsample&amp;#95;test&amp;#95;day&amp;#95;23&amp;#95;dataset.hql<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_criteo_downsample_test_day_23_dataset.hql)</ept> does it for test data, day\_23:</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>With this, we are ready to use our down sampled train and test datasets for building models in Azure Machine Learning.</source>
          <target state="new">With this, we are ready to use our down sampled train and test datasets for building models in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>There is a final important component before we move on to Azure Machine Learning, which is concerns the count table.</source>
          <target state="new">There is a final important component before we move on to Azure Machine Learning, which is concerns the count table.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>In the next sub-section, we discuss this in some detail.</source>
          <target state="new">In the next sub-section, we discuss this in some detail.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="count"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> A brief discussion on the count table</source>
          <target state="new"><ph id="ph1">&lt;a name="count"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> A brief discussion on the count table</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>As we saw, several categorical variables have a very high dimensionality.</source>
          <target state="new">As we saw, several categorical variables have a very high dimensionality.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>In our walkthrough, we present a powerful technique called <bpt id="p1">[</bpt>Learning With Counts<ept id="p1">](http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx)</ept> to encode these variables in an efficient, robust manner.</source>
          <target state="new">In our walkthrough, we present a powerful technique called <bpt id="p1">[</bpt>Learning With Counts<ept id="p1">](http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx)</ept> to encode these variables in an efficient, robust manner.</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>More information on this technique is in the link provided.</source>
          <target state="new">More information on this technique is in the link provided.</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> In this walkthrough, we focus on using count tables to produce compact representations of high-dimensional categorical features.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> In this walkthrough, we focus on using count tables to produce compact representations of high-dimensional categorical features.</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>This is certainly not the only way to encode categorical features ; for more information on other techniques, interested users can check out <bpt id="p1">[</bpt>one-hot-encoding<ept id="p1">](http://en.wikipedia.org/wiki/One-hot)</ept> and <bpt id="p2">[</bpt>feature hashing<ept id="p2">](http://en.wikipedia.org/wiki/Feature_hashing)</ept>.</source>
          <target state="new">This is certainly not the only way to encode categorical features ; for more information on other techniques, interested users can check out <bpt id="p1">[</bpt>one-hot-encoding<ept id="p1">](http://en.wikipedia.org/wiki/One-hot)</ept> and <bpt id="p2">[</bpt>feature hashing<ept id="p2">](http://en.wikipedia.org/wiki/Feature_hashing)</ept>.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>To build count tables on the count data, we use the data in the folder raw/count.</source>
          <target state="new">To build count tables on the count data, we use the data in the folder raw/count.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>In the modeling section, we show users how to build these count tables for categorical features from scratch, or alternatively to use a pre-built count table for their explorations.</source>
          <target state="new">In the modeling section, we show users how to build these count tables for categorical features from scratch, or alternatively to use a pre-built count table for their explorations.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>In what follows, when we refer to "pre-built count tables", we mean using the count tables that we provide.</source>
          <target state="new">In what follows, when we refer to "pre-built count tables", we mean using the count tables that we provide.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Detailed instructions on how to access these tables are below.</source>
          <target state="new">Detailed instructions on how to access these tables are below.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="aml"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Build a model with Azure Machine Learning</source>
          <target state="new"><ph id="ph1">&lt;a name="aml"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Build a model with Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Our model building process in Azure Machine Learning will follow these steps:</source>
          <target state="new">Our model building process in Azure Machine Learning will follow these steps:</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>Get the data from Hive tables into Azure Machine Learning</source>
          <target state="new">Get the data from Hive tables into Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>Create the experiment: clean the data, choose a learner, and featurize with count tables</source>
          <target state="new">Create the experiment: clean the data, choose a learner, and featurize with count tables</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>Train the model</source>
          <target state="new">Train the model</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>Score the model on test data</source>
          <target state="new">Score the model on test data</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>Evaluate the model</source>
          <target state="new">Evaluate the model</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>Publish the model as a web-service to be consumed</source>
          <target state="new">Publish the model as a web-service to be consumed</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>Now we are ready to build models in Azure Machine Learning studio.</source>
          <target state="new">Now we are ready to build models in Azure Machine Learning studio.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>Our down sampled data is saved as Hive tables in the cluster.</source>
          <target state="new">Our down sampled data is saved as Hive tables in the cluster.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>We will use the Azure Machine Learning <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module to read this data.</source>
          <target state="new">We will use the Azure Machine Learning <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module to read this data.</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>The credentials to access the storage account of this cluster are provided below.</source>
          <target state="new">The credentials to access the storage account of this cluster are provided below.</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="step1"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 1: Get data from Hive tables into Azure Machine Learning using the Reader module and select it for a machine learning experiment</source>
          <target state="new"><ph id="ph1">&lt;a name="step1"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 1: Get data from Hive tables into Azure Machine Learning using the Reader module and select it for a machine learning experiment</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>Start by selecting a <bpt id="p1">**</bpt>+NEW<ept id="p1">**</ept> -&gt; <bpt id="p2">**</bpt>EXPERIMENT<ept id="p2">**</ept> -&gt; <bpt id="p3">**</bpt>Blank Experiment<ept id="p3">**</ept>.</source>
          <target state="new">Start by selecting a <bpt id="p1">**</bpt>+NEW<ept id="p1">**</ept> -&gt; <bpt id="p2">**</bpt>EXPERIMENT<ept id="p2">**</ept> -&gt; <bpt id="p3">**</bpt>Blank Experiment<ept id="p3">**</ept>.</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>Then, from the <bpt id="p1">**</bpt>Search<ept id="p1">**</ept> box on the top left, search for "Reader".</source>
          <target state="new">Then, from the <bpt id="p1">**</bpt>Search<ept id="p1">**</ept> box on the top left, search for "Reader".</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>Drag and drop the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module on to the experiment canvas (the middle portion of the screen) to use the module for data access.</source>
          <target state="new">Drag and drop the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module on to the experiment canvas (the middle portion of the screen) to use the module for data access.</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>This is what the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> looks like while getting data from the Hive table:</source>
          <target state="new">This is what the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> looks like while getting data from the Hive table:</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>Reader gets data</source>
          <target state="new">Reader gets data</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>For the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module, the values of the parameters that are provided in the graphic are just examples of the sort of values you will need to provide.</source>
          <target state="new">For the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module, the values of the parameters that are provided in the graphic are just examples of the sort of values you will need to provide.</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source>Here is some general guidance on how to fill out the parameter set for the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module.</source>
          <target state="new">Here is some general guidance on how to fill out the parameter set for the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>Choose "Hive query" for <bpt id="p1">**</bpt>Data Source<ept id="p1">**</ept></source>
          <target state="new">Choose "Hive query" for <bpt id="p1">**</bpt>Data Source<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p1">**</bpt>Hive database query<ept id="p1">**</ept> box, a simple SELECT * FROM &lt;your\_database\_name.your\_table\_name&gt; - is enough.</source>
          <target state="new">In the <bpt id="p1">**</bpt>Hive database query<ept id="p1">**</ept> box, a simple SELECT * FROM &lt;your\_database\_name.your\_table\_name&gt; - is enough.</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Hcatalog server URI<ept id="p1">**</ept>: If your cluster is "abc", then this is simply: https://abc.azurehdinsight.net</source>
          <target state="new"><bpt id="p1">**</bpt>Hcatalog server URI<ept id="p1">**</ept>: If your cluster is "abc", then this is simply: https://abc.azurehdinsight.net</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Hadoop user account name<ept id="p1">**</ept>: The user name chosen at the time of commissioning the cluster.</source>
          <target state="new"><bpt id="p1">**</bpt>Hadoop user account name<ept id="p1">**</ept>: The user name chosen at the time of commissioning the cluster.</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>(NOT the Remote Access user name!)</source>
          <target state="new">(NOT the Remote Access user name!)</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Hadoop user account password<ept id="p1">**</ept>: The password for the above user name chosen at the time of commissioning the cluster.</source>
          <target state="new"><bpt id="p1">**</bpt>Hadoop user account password<ept id="p1">**</ept>: The password for the above user name chosen at the time of commissioning the cluster.</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>(NOT the Remote Access password!)</source>
          <target state="new">(NOT the Remote Access password!)</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Location of output data<ept id="p1">**</ept>: Choose "Azure"</source>
          <target state="new"><bpt id="p1">**</bpt>Location of output data<ept id="p1">**</ept>: Choose "Azure"</target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure storage account name<ept id="p1">**</ept>: The storage account associated with the cluster</source>
          <target state="new"><bpt id="p1">**</bpt>Azure storage account name<ept id="p1">**</ept>: The storage account associated with the cluster</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure storage account key<ept id="p1">**</ept>: The key of the storage account associated with the cluster.</source>
          <target state="new"><bpt id="p1">**</bpt>Azure storage account key<ept id="p1">**</ept>: The key of the storage account associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure container name<ept id="p1">**</ept>: If the cluster name is "abc", then this is simply "abc", usually.</source>
          <target state="new"><bpt id="p1">**</bpt>Azure container name<ept id="p1">**</ept>: If the cluster name is "abc", then this is simply "abc", usually.</target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source>Once the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> finishes getting data (you see the green tick on the Module), save this data as a Dataset (with a name of your choice).</source>
          <target state="new">Once the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> finishes getting data (you see the green tick on the Module), save this data as a Dataset (with a name of your choice).</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source>What this looks like:</source>
          <target state="new">What this looks like:</target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source>Reader save data</source>
          <target state="new">Reader save data</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source>Right click on the output port of the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module.</source>
          <target state="new">Right click on the output port of the <bpt id="p1">**</bpt>Reader<ept id="p1">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>This reveals a <bpt id="p1">**</bpt>Save as dataset<ept id="p1">**</ept> option and a <bpt id="p2">**</bpt>Visualize<ept id="p2">**</ept> option.</source>
          <target state="new">This reveals a <bpt id="p1">**</bpt>Save as dataset<ept id="p1">**</ept> option and a <bpt id="p2">**</bpt>Visualize<ept id="p2">**</ept> option.</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Visualize<ept id="p1">**</ept> option, if clicked, displays 100 rows of the data, along with a right panel that is useful for some summary statistics.</source>
          <target state="new">The <bpt id="p1">**</bpt>Visualize<ept id="p1">**</ept> option, if clicked, displays 100 rows of the data, along with a right panel that is useful for some summary statistics.</target>
        </trans-unit>
        <trans-unit id="299" translate="yes" xml:space="preserve">
          <source>To save data, simply select <bpt id="p1">**</bpt>Save as dataset<ept id="p1">**</ept> and follow instructions.</source>
          <target state="new">To save data, simply select <bpt id="p1">**</bpt>Save as dataset<ept id="p1">**</ept> and follow instructions.</target>
        </trans-unit>
        <trans-unit id="300" translate="yes" xml:space="preserve">
          <source>To select the saved dataset for use in a machine learning experiment, locate the datasets using the <bpt id="p1">**</bpt>Search<ept id="p1">**</ept> box shown below.</source>
          <target state="new">To select the saved dataset for use in a machine learning experiment, locate the datasets using the <bpt id="p1">**</bpt>Search<ept id="p1">**</ept> box shown below.</target>
        </trans-unit>
        <trans-unit id="301" translate="yes" xml:space="preserve">
          <source>Then simply type out the name you gave the dataset partially to access it and drag the dataset onto the main panel.</source>
          <target state="new">Then simply type out the name you gave the dataset partially to access it and drag the dataset onto the main panel.</target>
        </trans-unit>
        <trans-unit id="302" translate="yes" xml:space="preserve">
          <source>Dropping it onto the main panel selects it for use in machine learning modeling.</source>
          <target state="new">Dropping it onto the main panel selects it for use in machine learning modeling.</target>
        </trans-unit>
        <trans-unit id="303" translate="yes" xml:space="preserve">
          <source>Do this for both the train and the test datasets.</source>
          <target state="new">Do this for both the train and the test datasets.</target>
        </trans-unit>
        <trans-unit id="304" translate="yes" xml:space="preserve">
          <source>Also, remember to use the database name and table names that you gave for this purpose.</source>
          <target state="new">Also, remember to use the database name and table names that you gave for this purpose.</target>
        </trans-unit>
        <trans-unit id="305" translate="yes" xml:space="preserve">
          <source>The values used in the figure are solely for illustration purposes.</source>
          <target state="new">The values used in the figure are solely for illustration purposes.</target>
        </trans-unit>
        <trans-unit id="307" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="step2"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 2: Create a simple experiment in Azure Machine Learning to predict clicks / no clicks</source>
          <target state="new"><ph id="ph1">&lt;a name="step2"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 2: Create a simple experiment in Azure Machine Learning to predict clicks / no clicks</target>
        </trans-unit>
        <trans-unit id="308" translate="yes" xml:space="preserve">
          <source>Our Azure ML experiment looks like the below:</source>
          <target state="new">Our Azure ML experiment looks like the below:</target>
        </trans-unit>
        <trans-unit id="309" translate="yes" xml:space="preserve">
          <source>We now examine the key components of this experiment.</source>
          <target state="new">We now examine the key components of this experiment.</target>
        </trans-unit>
        <trans-unit id="310" translate="yes" xml:space="preserve">
          <source>As a reminder, we need to drag our saved train and test datasets on to our experiment canvas first.</source>
          <target state="new">As a reminder, we need to drag our saved train and test datasets on to our experiment canvas first.</target>
        </trans-unit>
        <trans-unit id="311" translate="yes" xml:space="preserve">
          <source>Clean Missing Data</source>
          <target state="new">Clean Missing Data</target>
        </trans-unit>
        <trans-unit id="312" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Clean Missing Data<ept id="p1">**</ept> module does what its name suggests:  it cleans missing data in ways that can be user-specified.</source>
          <target state="new">The <bpt id="p1">**</bpt>Clean Missing Data<ept id="p1">**</ept> module does what its name suggests:  it cleans missing data in ways that can be user-specified.</target>
        </trans-unit>
        <trans-unit id="313" translate="yes" xml:space="preserve">
          <source>Looking into this module, we see this:</source>
          <target state="new">Looking into this module, we see this:</target>
        </trans-unit>
        <trans-unit id="314" translate="yes" xml:space="preserve">
          <source>Clean missing data</source>
          <target state="new">Clean missing data</target>
        </trans-unit>
        <trans-unit id="315" translate="yes" xml:space="preserve">
          <source>Here, we chose to replace all missing values with a 0.</source>
          <target state="new">Here, we chose to replace all missing values with a 0.</target>
        </trans-unit>
        <trans-unit id="316" translate="yes" xml:space="preserve">
          <source>There are other options as well, which can be seen by looking at the dropdowns in the module.</source>
          <target state="new">There are other options as well, which can be seen by looking at the dropdowns in the module.</target>
        </trans-unit>
        <trans-unit id="317" translate="yes" xml:space="preserve">
          <source>Feature engineering on the data</source>
          <target state="new">Feature engineering on the data</target>
        </trans-unit>
        <trans-unit id="318" translate="yes" xml:space="preserve">
          <source>There can be millions of unique values for some categorical features of large datasets.</source>
          <target state="new">There can be millions of unique values for some categorical features of large datasets.</target>
        </trans-unit>
        <trans-unit id="319" translate="yes" xml:space="preserve">
          <source>Using naive methods such as one-hot encoding for representing such high-dimensional categorical features is entirely infeasible.</source>
          <target state="new">Using naive methods such as one-hot encoding for representing such high-dimensional categorical features is entirely infeasible.</target>
        </trans-unit>
        <trans-unit id="320" translate="yes" xml:space="preserve">
          <source>In this walkthrough, we demonstrate how to use count features using built-in Azure Machine Learning modules to generate compact representations of these high-dimensional categorical variables.</source>
          <target state="new">In this walkthrough, we demonstrate how to use count features using built-in Azure Machine Learning modules to generate compact representations of these high-dimensional categorical variables.</target>
        </trans-unit>
        <trans-unit id="321" translate="yes" xml:space="preserve">
          <source>The end-result is a smaller model size, faster training times, and performance metrics that are quite comparable to using other techniques.</source>
          <target state="new">The end-result is a smaller model size, faster training times, and performance metrics that are quite comparable to using other techniques.</target>
        </trans-unit>
        <trans-unit id="322" translate="yes" xml:space="preserve">
          <source>Building counting transforms</source>
          <target state="new">Building counting transforms</target>
        </trans-unit>
        <trans-unit id="323" translate="yes" xml:space="preserve">
          <source>To build count features, we use the <bpt id="p1">**</bpt>Build Counting Transform<ept id="p1">**</ept> module that is available in Azure Machine Learning.</source>
          <target state="new">To build count features, we use the <bpt id="p1">**</bpt>Build Counting Transform<ept id="p1">**</ept> module that is available in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="324" translate="yes" xml:space="preserve">
          <source>The module looks like this :</source>
          <target state="new">The module looks like this :</target>
        </trans-unit>
        <trans-unit id="325" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Important Note<ept id="p1">**</ept> : In the <bpt id="p2">**</bpt>Count columns<ept id="p2">**</ept> box, we enter those columns that we wish to perform counts on.</source>
          <target state="new"><bpt id="p1">**</bpt>Important Note<ept id="p1">**</ept> : In the <bpt id="p2">**</bpt>Count columns<ept id="p2">**</ept> box, we enter those columns that we wish to perform counts on.</target>
        </trans-unit>
        <trans-unit id="326" translate="yes" xml:space="preserve">
          <source>Typically, these are (as mentioned) high-dimensional categorical columns.</source>
          <target state="new">Typically, these are (as mentioned) high-dimensional categorical columns.</target>
        </trans-unit>
        <trans-unit id="327" translate="yes" xml:space="preserve">
          <source>At the start, we mentioned that the Criteo dataset has 26 categorical columns : from Col15 to Col40.</source>
          <target state="new">At the start, we mentioned that the Criteo dataset has 26 categorical columns : from Col15 to Col40.</target>
        </trans-unit>
        <trans-unit id="328" translate="yes" xml:space="preserve">
          <source>Here, we count on all of them and give their indices (from 15 to 40 separated by commas as shown).</source>
          <target state="new">Here, we count on all of them and give their indices (from 15 to 40 separated by commas as shown).</target>
        </trans-unit>
        <trans-unit id="329" translate="yes" xml:space="preserve">
          <source>To use the module in the MapReduce mode (appropriate for large datasets), we need access to an HDInsight Hadoop cluster (the one used for feature exploration above can be reused for this purpose as well) and its credentials.</source>
          <target state="new">To use the module in the MapReduce mode (appropriate for large datasets), we need access to an HDInsight Hadoop cluster (the one used for feature exploration above can be reused for this purpose as well) and its credentials.</target>
        </trans-unit>
        <trans-unit id="330" translate="yes" xml:space="preserve">
          <source>The figures above illustrate what the filled-in values look like (replace the values provided for illustration with those relevant for your own use-case).</source>
          <target state="new">The figures above illustrate what the filled-in values look like (replace the values provided for illustration with those relevant for your own use-case).</target>
        </trans-unit>
        <trans-unit id="331" translate="yes" xml:space="preserve">
          <source>In the figure above, we show how to enter the input blob location.</source>
          <target state="new">In the figure above, we show how to enter the input blob location.</target>
        </trans-unit>
        <trans-unit id="332" translate="yes" xml:space="preserve">
          <source>This location has the data reserved for building count tables on.</source>
          <target state="new">This location has the data reserved for building count tables on.</target>
        </trans-unit>
        <trans-unit id="333" translate="yes" xml:space="preserve">
          <source>After this module finishes running, we can save the transform for later by right clicking on the module and selecting the <bpt id="p1">**</bpt>Save as Transform<ept id="p1">**</ept> option:</source>
          <target state="new">After this module finishes running, we can save the transform for later by right clicking on the module and selecting the <bpt id="p1">**</bpt>Save as Transform<ept id="p1">**</ept> option:</target>
        </trans-unit>
        <trans-unit id="334" translate="yes" xml:space="preserve">
          <source>In our experiment architecture shown above, the dataset "ytransform2" corresponds precisely to a saved count transform.</source>
          <target state="new">In our experiment architecture shown above, the dataset "ytransform2" corresponds precisely to a saved count transform.</target>
        </trans-unit>
        <trans-unit id="335" translate="yes" xml:space="preserve">
          <source>For the remainder of this experiment, we assume that the reader used a <bpt id="p1">**</bpt>Build Counting Transform<ept id="p1">**</ept> module on some data to generate counts, and can then use those counts to generate count features on the train and test datasets.</source>
          <target state="new">For the remainder of this experiment, we assume that the reader used a <bpt id="p1">**</bpt>Build Counting Transform<ept id="p1">**</ept> module on some data to generate counts, and can then use those counts to generate count features on the train and test datasets.</target>
        </trans-unit>
        <trans-unit id="336" translate="yes" xml:space="preserve">
          <source>Choosing what count features to include as part of the train and test datasets</source>
          <target state="new">Choosing what count features to include as part of the train and test datasets</target>
        </trans-unit>
        <trans-unit id="337" translate="yes" xml:space="preserve">
          <source>Once we have a count transform ready, the user can choose what features to include in their train and test datasets using the <bpt id="p1">**</bpt>Modify Count Table Parameters<ept id="p1">**</ept> module.</source>
          <target state="new">Once we have a count transform ready, the user can choose what features to include in their train and test datasets using the <bpt id="p1">**</bpt>Modify Count Table Parameters<ept id="p1">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="338" translate="yes" xml:space="preserve">
          <source>We just show this module below for completeness, but in interests of simplicity do not actually use it in our experiment.</source>
          <target state="new">We just show this module below for completeness, but in interests of simplicity do not actually use it in our experiment.</target>
        </trans-unit>
        <trans-unit id="339" translate="yes" xml:space="preserve">
          <source>In this case, as can be seen, we have chosen to use just the log-odds and to ignore the back off column.</source>
          <target state="new">In this case, as can be seen, we have chosen to use just the log-odds and to ignore the back off column.</target>
        </trans-unit>
        <trans-unit id="340" translate="yes" xml:space="preserve">
          <source>We can also set parameters such as the garbage bin threshold, how many pseudo-prior examples to add for smoothing, and whether to use any Laplacian noise or not.</source>
          <target state="new">We can also set parameters such as the garbage bin threshold, how many pseudo-prior examples to add for smoothing, and whether to use any Laplacian noise or not.</target>
        </trans-unit>
        <trans-unit id="341" translate="yes" xml:space="preserve">
          <source>All these are advanced features and it is to be noted that the default values are a good starting point for users who are new to this type of feature generation.</source>
          <target state="new">All these are advanced features and it is to be noted that the default values are a good starting point for users who are new to this type of feature generation.</target>
        </trans-unit>
        <trans-unit id="342" translate="yes" xml:space="preserve">
          <source>Data transformation before generating the count features</source>
          <target state="new">Data transformation before generating the count features</target>
        </trans-unit>
        <trans-unit id="343" translate="yes" xml:space="preserve">
          <source>Now we focus on an important point about transforming our train and test data prior to actually generating count features.</source>
          <target state="new">Now we focus on an important point about transforming our train and test data prior to actually generating count features.</target>
        </trans-unit>
        <trans-unit id="344" translate="yes" xml:space="preserve">
          <source>Note that there are two <bpt id="p1">**</bpt>Execute R Script<ept id="p1">**</ept> modules used before we apply the count transform to our data.</source>
          <target state="new">Note that there are two <bpt id="p1">**</bpt>Execute R Script<ept id="p1">**</ept> modules used before we apply the count transform to our data.</target>
        </trans-unit>
        <trans-unit id="345" translate="yes" xml:space="preserve">
          <source>Here is the first R script:</source>
          <target state="new">Here is the first R script:</target>
        </trans-unit>
        <trans-unit id="346" translate="yes" xml:space="preserve">
          <source>In this R script, we rename our columns to names "Col1" to "Col40".</source>
          <target state="new">In this R script, we rename our columns to names "Col1" to "Col40".</target>
        </trans-unit>
        <trans-unit id="347" translate="yes" xml:space="preserve">
          <source>This is because the count transform expects names of this format.</source>
          <target state="new">This is because the count transform expects names of this format.</target>
        </trans-unit>
        <trans-unit id="348" translate="yes" xml:space="preserve">
          <source>In the second R script, we balance the distribution between positive and negative classes (classes 1 ansd 0 respectively) by downsampling the negative class.</source>
          <target state="new">In the second R script, we balance the distribution between positive and negative classes (classes 1 ansd 0 respectively) by downsampling the negative class.</target>
        </trans-unit>
        <trans-unit id="349" translate="yes" xml:space="preserve">
          <source>The R script below shows how to do this :</source>
          <target state="new">The R script below shows how to do this :</target>
        </trans-unit>
        <trans-unit id="350" translate="yes" xml:space="preserve">
          <source>In this simple R script, we use "pos\_neg\_ratio" to set the amount of balance between the positive and the negative classes.</source>
          <target state="new">In this simple R script, we use "pos\_neg\_ratio" to set the amount of balance between the positive and the negative classes.</target>
        </trans-unit>
        <trans-unit id="351" translate="yes" xml:space="preserve">
          <source>This is important to do since improving class imbalance usually has performance benefits for classification problems where the class distribution is skewed (recall that in our case, we have 3.3% positive class and 96.7% negative class).</source>
          <target state="new">This is important to do since improving class imbalance usually has performance benefits for classification problems where the class distribution is skewed (recall that in our case, we have 3.3% positive class and 96.7% negative class).</target>
        </trans-unit>
        <trans-unit id="352" translate="yes" xml:space="preserve">
          <source>Applying the count transformation on our data</source>
          <target state="new">Applying the count transformation on our data</target>
        </trans-unit>
        <trans-unit id="353" translate="yes" xml:space="preserve">
          <source>Finally, we can use the <bpt id="p1">**</bpt>Apply Transformation<ept id="p1">**</ept> module to apply the count transforms on our train and test datasets.</source>
          <target state="new">Finally, we can use the <bpt id="p1">**</bpt>Apply Transformation<ept id="p1">**</ept> module to apply the count transforms on our train and test datasets.</target>
        </trans-unit>
        <trans-unit id="354" translate="yes" xml:space="preserve">
          <source>This module takes the saved count transform as one input and the train or test datasets as the other input, and returns data with count features.</source>
          <target state="new">This module takes the saved count transform as one input and the train or test datasets as the other input, and returns data with count features.</target>
        </trans-unit>
        <trans-unit id="355" translate="yes" xml:space="preserve">
          <source>It is shown below :</source>
          <target state="new">It is shown below :</target>
        </trans-unit>
        <trans-unit id="356" translate="yes" xml:space="preserve">
          <source>An excerpt of what the count features look like</source>
          <target state="new">An excerpt of what the count features look like</target>
        </trans-unit>
        <trans-unit id="357" translate="yes" xml:space="preserve">
          <source>It is instructive to see what the count features look like in our case.</source>
          <target state="new">It is instructive to see what the count features look like in our case.</target>
        </trans-unit>
        <trans-unit id="358" translate="yes" xml:space="preserve">
          <source>Below, we show an excerpt of this :</source>
          <target state="new">Below, we show an excerpt of this :</target>
        </trans-unit>
        <trans-unit id="359" translate="yes" xml:space="preserve">
          <source>In this excerpt, we show that for the columns that we counted on, we get the counts and log odds in addition to any relevant backoffs.</source>
          <target state="new">In this excerpt, we show that for the columns that we counted on, we get the counts and log odds in addition to any relevant backoffs.</target>
        </trans-unit>
        <trans-unit id="360" translate="yes" xml:space="preserve">
          <source>We are now ready to build an Azure Machine Learning model using these transformed datasets.</source>
          <target state="new">We are now ready to build an Azure Machine Learning model using these transformed datasets.</target>
        </trans-unit>
        <trans-unit id="361" translate="yes" xml:space="preserve">
          <source>In the next section, we show how this can be done.</source>
          <target state="new">In the next section, we show how this can be done.</target>
        </trans-unit>
        <trans-unit id="362" translate="yes" xml:space="preserve">
          <source>Azure Machine Learning model building</source>
          <target state="new">Azure Machine Learning model building</target>
        </trans-unit>
        <trans-unit id="363" translate="yes" xml:space="preserve">
          <source>Choice of learner</source>
          <target state="new">Choice of learner</target>
        </trans-unit>
        <trans-unit id="364" translate="yes" xml:space="preserve">
          <source>First, we need to choose a learner.</source>
          <target state="new">First, we need to choose a learner.</target>
        </trans-unit>
        <trans-unit id="365" translate="yes" xml:space="preserve">
          <source>We are going to use a two class boosted decision tree as our learner.</source>
          <target state="new">We are going to use a two class boosted decision tree as our learner.</target>
        </trans-unit>
        <trans-unit id="366" translate="yes" xml:space="preserve">
          <source>Here are the default options for this learner:</source>
          <target state="new">Here are the default options for this learner:</target>
        </trans-unit>
        <trans-unit id="367" translate="yes" xml:space="preserve">
          <source>For our experiment, we simply going to choose the default values.</source>
          <target state="new">For our experiment, we simply going to choose the default values.</target>
        </trans-unit>
        <trans-unit id="368" translate="yes" xml:space="preserve">
          <source>We note that the defaults are usually meaningful and a good way to get quick baselines on performance.</source>
          <target state="new">We note that the defaults are usually meaningful and a good way to get quick baselines on performance.</target>
        </trans-unit>
        <trans-unit id="369" translate="yes" xml:space="preserve">
          <source>You can improve on performance by sweeping parameters if you choose to once you have a baseline.</source>
          <target state="new">You can improve on performance by sweeping parameters if you choose to once you have a baseline.</target>
        </trans-unit>
        <trans-unit id="370" translate="yes" xml:space="preserve">
          <source>Train the model</source>
          <target state="new">Train the model</target>
        </trans-unit>
        <trans-unit id="371" translate="yes" xml:space="preserve">
          <source>For training, we simply invoke a <bpt id="p1">**</bpt>Train Model<ept id="p1">**</ept> module.</source>
          <target state="new">For training, we simply invoke a <bpt id="p1">**</bpt>Train Model<ept id="p1">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="372" translate="yes" xml:space="preserve">
          <source>The two inputs to it are the Two-Class Boosted Decision Tree learner and our train dataset.</source>
          <target state="new">The two inputs to it are the Two-Class Boosted Decision Tree learner and our train dataset.</target>
        </trans-unit>
        <trans-unit id="373" translate="yes" xml:space="preserve">
          <source>This is shown below :</source>
          <target state="new">This is shown below :</target>
        </trans-unit>
        <trans-unit id="374" translate="yes" xml:space="preserve">
          <source>Score the model</source>
          <target state="new">Score the model</target>
        </trans-unit>
        <trans-unit id="375" translate="yes" xml:space="preserve">
          <source>Once we have a trained model, we are ready to score on the test dataset and to evaluate its performance.</source>
          <target state="new">Once we have a trained model, we are ready to score on the test dataset and to evaluate its performance.</target>
        </trans-unit>
        <trans-unit id="376" translate="yes" xml:space="preserve">
          <source>We do this by using the <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module shown below, along with an <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module :</source>
          <target state="new">We do this by using the <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module shown below, along with an <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module :</target>
        </trans-unit>
        <trans-unit id="377" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="step5"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 5: Evaluate the model</source>
          <target state="new"><ph id="ph1">&lt;a name="step5"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 5: Evaluate the model</target>
        </trans-unit>
        <trans-unit id="378" translate="yes" xml:space="preserve">
          <source>Finally, we would like to analyze model performance.</source>
          <target state="new">Finally, we would like to analyze model performance.</target>
        </trans-unit>
        <trans-unit id="379" translate="yes" xml:space="preserve">
          <source>Usually, for two class (binary) classification problems, a good measure is the AUC.</source>
          <target state="new">Usually, for two class (binary) classification problems, a good measure is the AUC.</target>
        </trans-unit>
        <trans-unit id="380" translate="yes" xml:space="preserve">
          <source>To visualize this, we hook up the <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module to an <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module for this.</source>
          <target state="new">To visualize this, we hook up the <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module to an <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module for this.</target>
        </trans-unit>
        <trans-unit id="381" translate="yes" xml:space="preserve">
          <source>Clicking <bpt id="p1">**</bpt>Visualize<ept id="p1">**</ept> on the <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module yields a graphic like the below:</source>
          <target state="new">Clicking <bpt id="p1">**</bpt>Visualize<ept id="p1">**</ept> on the <bpt id="p2">**</bpt>Evaluate Model<ept id="p2">**</ept> module yields a graphic like the below:</target>
        </trans-unit>
        <trans-unit id="382" translate="yes" xml:space="preserve">
          <source>Evaluate module BDT model</source>
          <target state="new">Evaluate module BDT model</target>
        </trans-unit>
        <trans-unit id="383" translate="yes" xml:space="preserve">
          <source>In binary (or two class) classification problems, a good measure of prediction accuracy is the Area Under Curve (AUC).</source>
          <target state="new">In binary (or two class) classification problems, a good measure of prediction accuracy is the Area Under Curve (AUC).</target>
        </trans-unit>
        <trans-unit id="384" translate="yes" xml:space="preserve">
          <source>Below, we show our results using this model on our test dataset.</source>
          <target state="new">Below, we show our results using this model on our test dataset.</target>
        </trans-unit>
        <trans-unit id="385" translate="yes" xml:space="preserve">
          <source>To get this, right click the output port of the <bpt id="p1">**</bpt>Evaluate Model<ept id="p1">**</ept> module and then <bpt id="p2">**</bpt>Visualize<ept id="p2">**</ept>.</source>
          <target state="new">To get this, right click the output port of the <bpt id="p1">**</bpt>Evaluate Model<ept id="p1">**</ept> module and then <bpt id="p2">**</bpt>Visualize<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="386" translate="yes" xml:space="preserve">
          <source>Visualize Evaluate Model module</source>
          <target state="new">Visualize Evaluate Model module</target>
        </trans-unit>
        <trans-unit id="387" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="step6"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 6: Publish the model as a Web service</source>
          <target state="new"><ph id="ph1">&lt;a name="step6"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph> Step 6: Publish the model as a Web service</target>
        </trans-unit>
        <trans-unit id="388" translate="yes" xml:space="preserve">
          <source>The ability to publish an Azure Machine Learning model as web services with a minimum of fuss is a valuable feature for making it widely available.</source>
          <target state="new">The ability to publish an Azure Machine Learning model as web services with a minimum of fuss is a valuable feature for making it widely available.</target>
        </trans-unit>
        <trans-unit id="389" translate="yes" xml:space="preserve">
          <source>Once that is done, anyone can make calls to the web service with input data that they need predictions for, and the web service uses the model to return those predictions.</source>
          <target state="new">Once that is done, anyone can make calls to the web service with input data that they need predictions for, and the web service uses the model to return those predictions.</target>
        </trans-unit>
        <trans-unit id="390" translate="yes" xml:space="preserve">
          <source>To do this, we first save our trained model as a Trained Model object.</source>
          <target state="new">To do this, we first save our trained model as a Trained Model object.</target>
        </trans-unit>
        <trans-unit id="391" translate="yes" xml:space="preserve">
          <source>This is done by right clicking on the <bpt id="p1">**</bpt>Train Model<ept id="p1">**</ept> module and using the <bpt id="p2">**</bpt>Save as Trained Model<ept id="p2">**</ept> option.</source>
          <target state="new">This is done by right clicking on the <bpt id="p1">**</bpt>Train Model<ept id="p1">**</ept> module and using the <bpt id="p2">**</bpt>Save as Trained Model<ept id="p2">**</ept> option.</target>
        </trans-unit>
        <trans-unit id="392" translate="yes" xml:space="preserve">
          <source>Next, we need to create input and output ports for our web service:</source>
          <target state="new">Next, we need to create input and output ports for our web service:</target>
        </trans-unit>
        <trans-unit id="393" translate="yes" xml:space="preserve">
          <source>an input port takes data in the same form as the data that we need predictions for</source>
          <target state="new">an input port takes data in the same form as the data that we need predictions for</target>
        </trans-unit>
        <trans-unit id="394" translate="yes" xml:space="preserve">
          <source>an output port returns the Scored Labels and the associated probabilities.</source>
          <target state="new">an output port returns the Scored Labels and the associated probabilities.</target>
        </trans-unit>
        <trans-unit id="395" translate="yes" xml:space="preserve">
          <source>Select a few rows of data for the input port</source>
          <target state="new">Select a few rows of data for the input port</target>
        </trans-unit>
        <trans-unit id="396" translate="yes" xml:space="preserve">
          <source>It is convenient to use an <bpt id="p1">**</bpt>Apply SQL Transformation<ept id="p1">**</ept> module to select just 10 rows to serve as the input port data.</source>
          <target state="new">It is convenient to use an <bpt id="p1">**</bpt>Apply SQL Transformation<ept id="p1">**</ept> module to select just 10 rows to serve as the input port data.</target>
        </trans-unit>
        <trans-unit id="397" translate="yes" xml:space="preserve">
          <source>Select just these rows of data for our input port using the SQL query shown below.</source>
          <target state="new">Select just these rows of data for our input port using the SQL query shown below.</target>
        </trans-unit>
        <trans-unit id="398" translate="yes" xml:space="preserve">
          <source>Input port data</source>
          <target state="new">Input port data</target>
        </trans-unit>
        <trans-unit id="399" translate="yes" xml:space="preserve">
          <source>Web service</source>
          <target state="new">Web service</target>
        </trans-unit>
        <trans-unit id="400" translate="yes" xml:space="preserve">
          <source>Now we are ready to run a small experiment that can be used to publish our web service.</source>
          <target state="new">Now we are ready to run a small experiment that can be used to publish our web service.</target>
        </trans-unit>
        <trans-unit id="401" translate="yes" xml:space="preserve">
          <source>Generate input data for webservice</source>
          <target state="new">Generate input data for webservice</target>
        </trans-unit>
        <trans-unit id="402" translate="yes" xml:space="preserve">
          <source>As a zeroth step, since the count table is large, we take a few lines of test data and generate output data from it with count features.</source>
          <target state="new">As a zeroth step, since the count table is large, we take a few lines of test data and generate output data from it with count features.</target>
        </trans-unit>
        <trans-unit id="403" translate="yes" xml:space="preserve">
          <source>This can serve as the input data format for our webservice.</source>
          <target state="new">This can serve as the input data format for our webservice.</target>
        </trans-unit>
        <trans-unit id="404" translate="yes" xml:space="preserve">
          <source>This is shown below:</source>
          <target state="new">This is shown below:</target>
        </trans-unit>
        <trans-unit id="405" translate="yes" xml:space="preserve">
          <source>Create BDT input data</source>
          <target state="new">Create BDT input data</target>
        </trans-unit>
        <trans-unit id="406" translate="yes" xml:space="preserve">
          <source>Note: for the input data format, we will now use the OUTPUT of the <bpt id="p1">**</bpt>Count Featurizer<ept id="p1">**</ept> module.</source>
          <target state="new">Note: for the input data format, we will now use the OUTPUT of the <bpt id="p1">**</bpt>Count Featurizer<ept id="p1">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="407" translate="yes" xml:space="preserve">
          <source>Once this experiment finishes running, save the output from the <bpt id="p1">**</bpt>Count Featurizer<ept id="p1">**</ept> module as a Dataset.</source>
          <target state="new">Once this experiment finishes running, save the output from the <bpt id="p1">**</bpt>Count Featurizer<ept id="p1">**</ept> module as a Dataset.</target>
        </trans-unit>
        <trans-unit id="408" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Important Note:<ept id="p1">**</ept> This Dataset will be used for the input data in the webservice.</source>
          <target state="new"><bpt id="p1">**</bpt>Important Note:<ept id="p1">**</ept> This Dataset will be used for the input data in the webservice.</target>
        </trans-unit>
        <trans-unit id="409" translate="yes" xml:space="preserve">
          <source>Scoring experiment for publishing webservice</source>
          <target state="new">Scoring experiment for publishing webservice</target>
        </trans-unit>
        <trans-unit id="410" translate="yes" xml:space="preserve">
          <source>First, we show what this looks like.</source>
          <target state="new">First, we show what this looks like.</target>
        </trans-unit>
        <trans-unit id="411" translate="yes" xml:space="preserve">
          <source>The essential structure is a <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module that accepts our trained model object and a few lines of input data that we generated in the previous steps using the <bpt id="p2">**</bpt>Count Featurizer<ept id="p2">**</ept> module.</source>
          <target state="new">The essential structure is a <bpt id="p1">**</bpt>Score Model<ept id="p1">**</ept> module that accepts our trained model object and a few lines of input data that we generated in the previous steps using the <bpt id="p2">**</bpt>Count Featurizer<ept id="p2">**</ept> module.</target>
        </trans-unit>
        <trans-unit id="412" translate="yes" xml:space="preserve">
          <source>We use "Project Columns" to project out the Scored labels and the Score probabilities.</source>
          <target state="new">We use "Project Columns" to project out the Scored labels and the Score probabilities.</target>
        </trans-unit>
        <trans-unit id="413" translate="yes" xml:space="preserve">
          <source>Project Columns</source>
          <target state="new">Project Columns</target>
        </trans-unit>
        <trans-unit id="414" translate="yes" xml:space="preserve">
          <source>Notice how the <bpt id="p1">**</bpt>Project Columns<ept id="p1">**</ept> module can be used for 'filtering' data out from a dataset.</source>
          <target state="new">Notice how the <bpt id="p1">**</bpt>Project Columns<ept id="p1">**</ept> module can be used for 'filtering' data out from a dataset.</target>
        </trans-unit>
        <trans-unit id="415" translate="yes" xml:space="preserve">
          <source>We show the contents below:</source>
          <target state="new">We show the contents below:</target>
        </trans-unit>
        <trans-unit id="416" translate="yes" xml:space="preserve">
          <source>Filtering with the Project Columns module</source>
          <target state="new">Filtering with the Project Columns module</target>
        </trans-unit>
        <trans-unit id="417" translate="yes" xml:space="preserve">
          <source>To get the blue input and output ports, you simply click <bpt id="p1">**</bpt>prepare webservice<ept id="p1">**</ept> at the bottom right.</source>
          <target state="new">To get the blue input and output ports, you simply click <bpt id="p1">**</bpt>prepare webservice<ept id="p1">**</ept> at the bottom right.</target>
        </trans-unit>
        <trans-unit id="418" translate="yes" xml:space="preserve">
          <source>Running this experiment also allows us to publish the web service  by clicking the <bpt id="p1">**</bpt>PUBLISH WEB SERVICE<ept id="p1">**</ept> icon at the bottom right, shown below.</source>
          <target state="new">Running this experiment also allows us to publish the web service  by clicking the <bpt id="p1">**</bpt>PUBLISH WEB SERVICE<ept id="p1">**</ept> icon at the bottom right, shown below.</target>
        </trans-unit>
        <trans-unit id="419" translate="yes" xml:space="preserve">
          <source>Publish Web service</source>
          <target state="new">Publish Web service</target>
        </trans-unit>
        <trans-unit id="420" translate="yes" xml:space="preserve">
          <source>Once the webservice is published, we get redirected to a page that looks thus:</source>
          <target state="new">Once the webservice is published, we get redirected to a page that looks thus:</target>
        </trans-unit>
        <trans-unit id="421" translate="yes" xml:space="preserve">
          <source>We see two links for webservices on the left side:</source>
          <target state="new">We see two links for webservices on the left side:</target>
        </trans-unit>
        <trans-unit id="422" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>REQUEST/RESPONSE<ept id="p1">**</ept> Service (or RRS) is meant for single predictions and is what we will utilize in this workshop.</source>
          <target state="new">The <bpt id="p1">**</bpt>REQUEST/RESPONSE<ept id="p1">**</ept> Service (or RRS) is meant for single predictions and is what we will utilize in this workshop.</target>
        </trans-unit>
        <trans-unit id="423" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> Service (BES) is used for batch predictions and requires that the input data used to make predictions reside in Azure Blob Storage.</source>
          <target state="new">The <bpt id="p1">**</bpt>BATCH EXECUTION<ept id="p1">**</ept> Service (BES) is used for batch predictions and requires that the input data used to make predictions reside in Azure Blob Storage.</target>
        </trans-unit>
        <trans-unit id="424" translate="yes" xml:space="preserve">
          <source>Clicking on the link <bpt id="p1">**</bpt>REQUEST/RESPONSE<ept id="p1">**</ept> takes us to a page that gives us pre-canned code in C#, python, and R. This code can be conveniently used for making calls to the webservice.</source>
          <target state="new">Clicking on the link <bpt id="p1">**</bpt>REQUEST/RESPONSE<ept id="p1">**</ept> takes us to a page that gives us pre-canned code in C#, python, and R. This code can be conveniently used for making calls to the webservice.</target>
        </trans-unit>
        <trans-unit id="425" translate="yes" xml:space="preserve">
          <source>Note that the API key on this page needs to be used for authentication.</source>
          <target state="new">Note that the API key on this page needs to be used for authentication.</target>
        </trans-unit>
        <trans-unit id="426" translate="yes" xml:space="preserve">
          <source>It is convenient to copy this python code over to a new cell in the IPython notebook.</source>
          <target state="new">It is convenient to copy this python code over to a new cell in the IPython notebook.</target>
        </trans-unit>
        <trans-unit id="427" translate="yes" xml:space="preserve">
          <source>Below, we show a segment of python code with the correct API key.</source>
          <target state="new">Below, we show a segment of python code with the correct API key.</target>
        </trans-unit>
        <trans-unit id="428" translate="yes" xml:space="preserve">
          <source>Python code</source>
          <target state="new">Python code</target>
        </trans-unit>
        <trans-unit id="429" translate="yes" xml:space="preserve">
          <source>Note that we replaced the default API key with our webservices's API key.</source>
          <target state="new">Note that we replaced the default API key with our webservices's API key.</target>
        </trans-unit>
        <trans-unit id="430" translate="yes" xml:space="preserve">
          <source>Clicking <bpt id="p1">**</bpt>Run<ept id="p1">**</ept> on this cell in an IPython notebook yields the following response:</source>
          <target state="new">Clicking <bpt id="p1">**</bpt>Run<ept id="p1">**</ept> on this cell in an IPython notebook yields the following response:</target>
        </trans-unit>
        <trans-unit id="431" translate="yes" xml:space="preserve">
          <source>IPython response</source>
          <target state="new">IPython response</target>
        </trans-unit>
        <trans-unit id="432" translate="yes" xml:space="preserve">
          <source>We see that for the two test examples we asked about (in the JSON framework of the python script), we get back answers in the form "Scored Labels, Scored Probabilities".</source>
          <target state="new">We see that for the two test examples we asked about (in the JSON framework of the python script), we get back answers in the form "Scored Labels, Scored Probabilities".</target>
        </trans-unit>
        <trans-unit id="433" translate="yes" xml:space="preserve">
          <source>Note that in this case, we chose the default values that the pre-canned code provides (0's for all numeric columns and the string "value" for all categorical columns).</source>
          <target state="new">Note that in this case, we chose the default values that the pre-canned code provides (0's for all numeric columns and the string "value" for all categorical columns).</target>
        </trans-unit>
        <trans-unit id="434" translate="yes" xml:space="preserve">
          <source>This concludes our end-to-end walkthrough showing how to handle large scale dataset using Azure Machine Learning.</source>
          <target state="new">This concludes our end-to-end walkthrough showing how to handle large scale dataset using Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="435" translate="yes" xml:space="preserve">
          <source>We started with a terabyte of data, constructed a prediction model and deployed it as a web service in the cloud.</source>
          <target state="new">We started with a terabyte of data, constructed a prediction model and deployed it as a web service in the cloud.</target>
        </trans-unit>
        <trans-unit id="436" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">23352e3a4cddb17aa28d309ebd5e302b29d62c0b</xliffext:olfilehash>
  </header>
</xliff>