<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Hadoop Hive and SSH in HDInsight | Microsoft Azure</source>
          <target state="new">Use Hadoop Hive and SSH in HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using the Hive command-line interface.</source>
          <target state="new">Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using the Hive command-line interface.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Use Hive with Hadoop in HDInsight with SSH</source>
          <target state="new">Use Hive with Hadoop in HDInsight with SSH</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In this article, you will learn how to use Secure Shell (SSH) to connect to a Hadoop on Azure HDInsight cluster and then interactively submit Hive queries by using the Hive command-line interface (CLI).</source>
          <target state="new">In this article, you will learn how to use Secure Shell (SSH) to connect to a Hadoop on Azure HDInsight cluster and then interactively submit Hive queries by using the Hive command-line interface (CLI).</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id="p1">[</bpt>What you need to know about Hadoop on Linux-based HDInsight<ept id="p1">](hdinsight-hadoop-linux-information.md)</ept>.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id="p1">[</bpt>What you need to know about Hadoop on Linux-based HDInsight<ept id="p1">](hdinsight-hadoop-linux-information.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="prereq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</source>
          <target state="new"><ph id="ph1">&lt;a id="prereq"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Prerequisites</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>To complete the steps in this article, you will need the following:</source>
          <target state="new">To complete the steps in this article, you will need the following:</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>A Linux-based Hadoop on HDInsight cluster.</source>
          <target state="new">A Linux-based Hadoop on HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>An SSH client.</source>
          <target state="new">An SSH client.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Linux, Unix, and Mac OS should come with an SSH client.</source>
          <target state="new">Linux, Unix, and Mac OS should come with an SSH client.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Windows users must download a client, such as <bpt id="p1">[</bpt>PuTTY<ept id="p1">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">Windows users must download a client, such as <bpt id="p1">[</bpt>PuTTY<ept id="p1">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="ssh"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Connect with SSH</source>
          <target state="new"><ph id="ph1">&lt;a id="ssh"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Connect with SSH</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</source>
          <target state="new">Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>The FQDN will be the name you gave the cluster, then <bpt id="p1">**</bpt>.azurehdinsight.net<ept id="p1">**</ept>.</source>
          <target state="new">The FQDN will be the name you gave the cluster, then <bpt id="p1">**</bpt>.azurehdinsight.net<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>For example, the following would connect to a cluster named <bpt id="p1">**</bpt>myhdinsight<ept id="p1">**</ept>:</source>
          <target state="new">For example, the following would connect to a cluster named <bpt id="p1">**</bpt>myhdinsight<ept id="p1">**</ept>:</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>If you provided a certificate key for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:</source>
          <target state="new"><bpt id="p1">**</bpt>If you provided a certificate key for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>If you provided a password for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you will need to provide the password when prompted.</source>
          <target state="new"><bpt id="p1">**</bpt>If you provided a password for SSH authentication<ept id="p1">**</ept> when you created the HDInsight cluster, you will need to provide the password when prompted.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>For more information on using SSH with HDInsight, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</source>
          <target state="new">For more information on using SSH with HDInsight, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p1">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>PuTTY (Windows-based clients)</source>
          <target state="new">PuTTY (Windows-based clients)</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Windows does not provide a built-in SSH client.</source>
          <target state="new">Windows does not provide a built-in SSH client.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>We recommend using <bpt id="p1">**</bpt>PuTTY<ept id="p1">**</ept>, which can be downloaded from <bpt id="p2">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p2">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">We recommend using <bpt id="p1">**</bpt>PuTTY<ept id="p1">**</ept>, which can be downloaded from <bpt id="p2">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p2">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>For more information on using PuTTY, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</source>
          <target state="new">For more information on using PuTTY, see <bpt id="p1">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p1">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="hive"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use the Hive command</source>
          <target state="new"><ph id="ph1">&lt;a id="hive"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use the Hive command</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Once connected, start the Hive CLI by using the following command:</source>
          <target state="new">Once connected, start the Hive CLI by using the following command:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Using the CLI, enter the following statements to create a new table named <bpt id="p1">**</bpt>log4jLogs<ept id="p1">**</ept> by using the sample data:</source>
          <target state="new">Using the CLI, enter the following statements to create a new table named <bpt id="p1">**</bpt>log4jLogs<ept id="p1">**</ept> by using the sample data:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>These statements perform the following actions:</source>
          <target state="new">These statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>DROP TABLE<ept id="p1">**</ept> - Deletes the table and the data file, in case the table already exists.</source>
          <target state="new"><bpt id="p1">**</bpt>DROP TABLE<ept id="p1">**</ept> - Deletes the table and the data file, in case the table already exists.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>CREATE EXTERNAL TABLE<ept id="p1">**</ept> - Creates a new 'external' table in Hive.</source>
          <target state="new"><bpt id="p1">**</bpt>CREATE EXTERNAL TABLE<ept id="p1">**</ept> - Creates a new 'external' table in Hive.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>External tables only store the table definition in Hive.</source>
          <target state="new">External tables only store the table definition in Hive.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>The data is left in the original location.</source>
          <target state="new">The data is left in the original location.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>ROW FORMAT<ept id="p1">**</ept> - Tells Hive how the data is formatted.</source>
          <target state="new"><bpt id="p1">**</bpt>ROW FORMAT<ept id="p1">**</ept> - Tells Hive how the data is formatted.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>In this case, the fields in each log are separated by a space.</source>
          <target state="new">In this case, the fields in each log are separated by a space.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p1">**</ept> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.</source>
          <target state="new"><bpt id="p1">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p1">**</ept> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>SELECT<ept id="p1">**</ept> - Selects a count of all rows where column <bpt id="p2">**</bpt>t4<ept id="p2">**</ept> contains the value <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>SELECT<ept id="p1">**</ept> - Selects a count of all rows where column <bpt id="p2">**</bpt>t4<ept id="p2">**</ept> contains the value <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>This should return a value of <bpt id="p1">**</bpt>3<ept id="p1">**</ept> as there are three rows that contain this value.</source>
          <target state="new">This should return a value of <bpt id="p1">**</bpt>3<ept id="p1">**</ept> as there are three rows that contain this value.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p1">**</ept> - Tells Hive that we should only return data from files ending in .log.</source>
          <target state="new"><bpt id="p1">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p1">**</ept> - Tells Hive that we should only return data from files ending in .log.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.</source>
          <target state="new">This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Dropping an external table does <bpt id="p1">**</bpt>not<ept id="p1">**</ept> delete the data, only the table definition.</source>
          <target state="new">Dropping an external table does <bpt id="p1">**</bpt>not<ept id="p1">**</ept> delete the data, only the table definition.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Use the following statements to create a new 'internal' table named <bpt id="p1">**</bpt>errorLogs<ept id="p1">**</ept>:</source>
          <target state="new">Use the following statements to create a new 'internal' table named <bpt id="p1">**</bpt>errorLogs<ept id="p1">**</ept>:</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>These statements perform the following actions:</source>
          <target state="new">These statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p1">**</ept> - Creates a table, if it does not already exist.</source>
          <target state="new"><bpt id="p1">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p1">**</ept> - Creates a table, if it does not already exist.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Since the <bpt id="p1">**</bpt>EXTERNAL<ept id="p1">**</ept> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</source>
          <target state="new">Since the <bpt id="p1">**</bpt>EXTERNAL<ept id="p1">**</ept> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>STORED AS ORC<ept id="p1">**</ept> - Stores the data in Optimized Row Columnar (ORC) format.</source>
          <target state="new"><bpt id="p1">**</bpt>STORED AS ORC<ept id="p1">**</ept> - Stores the data in Optimized Row Columnar (ORC) format.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>This is a highly optimized and efficient format for storing Hive data.</source>
          <target state="new">This is a highly optimized and efficient format for storing Hive data.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p1">**</ept> - Selects rows from the <bpt id="p2">**</bpt>log4jLogs<ept id="p2">**</ept> table that contain <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>, then inserts the data into the <bpt id="p4">**</bpt>errorLogs<ept id="p4">**</ept> table.</source>
          <target state="new"><bpt id="p1">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p1">**</ept> - Selects rows from the <bpt id="p2">**</bpt>log4jLogs<ept id="p2">**</ept> table that contain <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>, then inserts the data into the <bpt id="p4">**</bpt>errorLogs<ept id="p4">**</ept> table.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>To verify that only rows containing <bpt id="p1">**</bpt>[ERROR]<ept id="p1">**</ept> in column t4 were stored to the <bpt id="p2">**</bpt>errorLogs<ept id="p2">**</ept> table, use the following statement to return all the rows from <bpt id="p3">**</bpt>errorLogs<ept id="p3">**</ept>:</source>
          <target state="new">To verify that only rows containing <bpt id="p1">**</bpt>[ERROR]<ept id="p1">**</ept> in column t4 were stored to the <bpt id="p2">**</bpt>errorLogs<ept id="p2">**</ept> table, use the following statement to return all the rows from <bpt id="p3">**</bpt>errorLogs<ept id="p3">**</ept>:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Three rows of data should be returned, all containing <bpt id="p1">**</bpt>[ERROR]<ept id="p1">**</ept> in column t4.</source>
          <target state="new">Three rows of data should be returned, all containing <bpt id="p1">**</bpt>[ERROR]<ept id="p1">**</ept> in column t4.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Unlike external tables, dropping an internal table will delete the underlying data as well.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Unlike external tables, dropping an internal table will delete the underlying data as well.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</source>
          <target state="new"><ph id="ph1">&lt;a id="summary"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Summary</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>As you can see, the Hive command provides an easy way to interactively run Hive queries on an HDInsight cluster, monitor the job status, and retrieve the output.</source>
          <target state="new">As you can see, the Hive command provides an easy way to interactively run Hive queries on an HDInsight cluster, monitor the job status, and retrieve the output.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</source>
          <target state="new"><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>For general information on Hive in HDInsight:</source>
          <target state="new">For general information on Hive in HDInsight:</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Use Hive with Hadoop on HDInsight</source>
          <target state="new">Use Hive with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>For information on other ways you can work with Hadoop on HDInsight:</source>
          <target state="new">For information on other ways you can work with Hadoop on HDInsight:</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Use Pig with Hadoop on HDInsight</source>
          <target state="new">Use Pig with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Use MapReduce with Hadoop on HDInsight</source>
          <target state="new">Use MapReduce with Hadoop on HDInsight</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">7c8e670f46cf7825655e1e3097c4a096e3167901</xliffext:olfilehash>
  </header>
</xliff>