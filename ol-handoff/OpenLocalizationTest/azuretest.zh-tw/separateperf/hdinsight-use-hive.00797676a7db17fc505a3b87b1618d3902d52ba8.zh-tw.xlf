<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Learn what is Hive and how to use HiveQL | Microsoft Azure</source>
          <target state="new">Learn what is Hive and how to use HiveQL | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn about Apache Hive and how to use it with Hadoop in HDInsight.</source>
          <target state="new">Learn about Apache Hive and how to use it with Hadoop in HDInsight.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Choose how to run your Hive job, and use HiveQL to analyze a sample Apache log4j file.</source>
          <target state="new">Choose how to run your Hive job, and use HiveQL to analyze a sample Apache log4j file.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file</source>
          <target state="new">Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In this tutorial, you'll learn how to use Apache Hive in Hadoop on HDInsight, and choose how to run your Hive job.</source>
          <target state="new">In this tutorial, you'll learn how to use Apache Hive in Hadoop on HDInsight, and choose how to run your Hive job.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>You'll also learn about HiveQL and how to analyze a sample Apache log4j file.</source>
          <target state="new">You'll also learn about HiveQL and how to analyze a sample Apache log4j file.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="why"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is Hive and why use it?</source>
          <target state="new"><ph id="ph1">&lt;a id="why"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>What is Hive and why use it?</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Apache Hive<ept id="p1">](http://hive.apache.org/)</ept> is a data warehouse system for Hadoop, which enables data summarization, querying, and analysis of data by using HiveQL (a query language similar to SQL).</source>
          <target state="new"><bpt id="p1">[</bpt>Apache Hive<ept id="p1">](http://hive.apache.org/)</ept> is a data warehouse system for Hadoop, which enables data summarization, querying, and analysis of data by using HiveQL (a query language similar to SQL).</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Hive can be used to interactively explore your data or to create reusable batch processing jobs.</source>
          <target state="new">Hive can be used to interactively explore your data or to create reusable batch processing jobs.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Hive allows you to project structure on largely unstructured data.</source>
          <target state="new">Hive allows you to project structure on largely unstructured data.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>After you define the structure, you can use Hive to query that data without knowledge of Java or MapReduce.</source>
          <target state="new">After you define the structure, you can use Hive to query that data without knowledge of Java or MapReduce.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>HiveQL<ept id="p1">**</ept> (the Hive query language) allows you to write queries with statements that are similar to T-SQL.</source>
          <target state="new"><bpt id="p1">**</bpt>HiveQL<ept id="p1">**</ept> (the Hive query language) allows you to write queries with statements that are similar to T-SQL.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Hive understands how to work with structured and semi-structured data, such as text files where the fields are delimited by specific characters.</source>
          <target state="new">Hive understands how to work with structured and semi-structured data, such as text files where the fields are delimited by specific characters.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Hive also supports custom <bpt id="p1">**</bpt>serializer/deserializers (SerDe)<ept id="p1">**</ept> for complex or irregularly structured data.</source>
          <target state="new">Hive also supports custom <bpt id="p1">**</bpt>serializer/deserializers (SerDe)<ept id="p1">**</ept> for complex or irregularly structured data.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>How to use a custom JSON SerDe with HDInsight<ept id="p1">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>How to use a custom JSON SerDe with HDInsight<ept id="p1">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Hive can also be extended through <bpt id="p1">**</bpt>user-defined functions (UDF)<ept id="p1">**</ept>.</source>
          <target state="new">Hive can also be extended through <bpt id="p1">**</bpt>user-defined functions (UDF)<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL.</source>
          <target state="new">A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>For an example of using UDFs with Hive, see the following:</source>
          <target state="new">For an example of using UDFs with Hive, see the following:</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Using Python with Hive and Pig in HDInsight</source>
          <target state="new">Using Python with Hive and Pig in HDInsight</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Use C# with Hive and Pig in HDInsight</source>
          <target state="new">Use C# with Hive and Pig in HDInsight</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>How to add a custom Hive UDF to HDInsight</source>
          <target state="new">How to add a custom Hive UDF to HDInsight</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the sample data, an Apache log4j file</source>
          <target state="new"><ph id="ph1">&lt;a id="data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>About the sample data, an Apache log4j file</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>This example uses a <bpt id="p1">*</bpt>log4j<ept id="p1">*</ept> sample file, which is stored at <bpt id="p2">**</bpt>/example/data/sample.log<ept id="p2">**</ept> in your blob storage container.</source>
          <target state="new">This example uses a <bpt id="p1">*</bpt>log4j<ept id="p1">*</ept> sample file, which is stored at <bpt id="p2">**</bpt>/example/data/sample.log<ept id="p2">**</ept> in your blob storage container.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Each log inside the file consists of a line of fields that contains a <ph id="ph1">`[LOG LEVEL]`</ph> field to show the type and the severity, for example:</source>
          <target state="new">Each log inside the file consists of a line of fields that contains a <ph id="ph1">`[LOG LEVEL]`</ph> field to show the type and the severity, for example:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>In the previous example, the log level is ERROR.</source>
          <target state="new">In the previous example, the log level is ERROR.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> You can also generate a log4j file by using the <bpt id="p1">[</bpt>Apache Log4j<ept id="p1">](http://en.wikipedia.org/wiki/Log4j)</ept> logging tool and then upload that file to the blob container.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> You can also generate a log4j file by using the <bpt id="p1">[</bpt>Apache Log4j<ept id="p1">](http://en.wikipedia.org/wiki/Log4j)</ept> logging tool and then upload that file to the blob container.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>Upload Data to HDInsight<ept id="p1">](hdinsight-upload-data.md)</ept> for instructions.</source>
          <target state="new">See <bpt id="p1">[</bpt>Upload Data to HDInsight<ept id="p1">](hdinsight-upload-data.md)</ept> for instructions.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For more information about how Azure Blob storage is used with HDInsight, see <bpt id="p1">[</bpt>Use Azure Blob Storage with HDInsight<ept id="p1">](../hdinsight-use-blob-storage.md)</ept>.</source>
          <target state="new">For more information about how Azure Blob storage is used with HDInsight, see <bpt id="p1">[</bpt>Use Azure Blob Storage with HDInsight<ept id="p1">](../hdinsight-use-blob-storage.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system.</source>
          <target state="new">The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>HDInsight can access files stored in blobs by using the <bpt id="p1">**</bpt>wasb<ept id="p1">**</ept> prefix.</source>
          <target state="new">HDInsight can access files stored in blobs by using the <bpt id="p1">**</bpt>wasb<ept id="p1">**</ept> prefix.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>For example, to access the sample.log file, you would use the following syntax:</source>
          <target state="new">For example, to access the sample.log file, you would use the following syntax:</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id="p1">**</bpt>/example/data/sample.log<ept id="p1">**</ept> from HiveQL.</source>
          <target state="new">Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id="p1">**</bpt>/example/data/sample.log<ept id="p1">**</ept> from HiveQL.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The syntax, <bpt id="p1">**</bpt>wasb:///<ept id="p1">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The syntax, <bpt id="p1">**</bpt>wasb:///<ept id="p1">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example, <bpt id="p1">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id="p1">**</ept>.</source>
          <target state="new">If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example, <bpt id="p1">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="job"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Sample job: Project columns onto delimited data</source>
          <target state="new"><ph id="ph1">&lt;a id="job"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Sample job: Project columns onto delimited data</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>The following HiveQL statements will project columns onto delimited data that is stored in the <bpt id="p1">**</bpt>wasb:///example/data<ept id="p1">**</ept> directory:</source>
          <target state="new">The following HiveQL statements will project columns onto delimited data that is stored in the <bpt id="p1">**</bpt>wasb:///example/data<ept id="p1">**</ept> directory:</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>In the previous example, the HiveQL statements perform the following actions:</source>
          <target state="new">In the previous example, the HiveQL statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>DROP TABLE<ept id="p1">**</ept>: Deletes the table and the data file if the table already exists.</source>
          <target state="new"><bpt id="p1">**</bpt>DROP TABLE<ept id="p1">**</ept>: Deletes the table and the data file if the table already exists.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>CREATE EXTERNAL TABLE<ept id="p1">**</ept>: Creates a new <bpt id="p2">**</bpt>external<ept id="p2">**</ept> table in Hive.</source>
          <target state="new"><bpt id="p1">**</bpt>CREATE EXTERNAL TABLE<ept id="p1">**</ept>: Creates a new <bpt id="p2">**</bpt>external<ept id="p2">**</ept> table in Hive.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>External tables only store the table definition in Hive; the data is left in the original location and in the original format.</source>
          <target state="new">External tables only store the table definition in Hive; the data is left in the original location and in the original format.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>ROW FORMAT<ept id="p1">**</ept>: Tells Hive how the data is formatted.</source>
          <target state="new"><bpt id="p1">**</bpt>ROW FORMAT<ept id="p1">**</ept>: Tells Hive how the data is formatted.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>In this case, the fields in each log are separated by a space.</source>
          <target state="new">In this case, the fields in each log are separated by a space.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p1">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text.</source>
          <target state="new"><bpt id="p1">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p1">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>The data can be in one file or spread across multiple files within the directory.</source>
          <target state="new">The data can be in one file or spread across multiple files within the directory.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>SELECT<ept id="p1">**</ept>: Selects a count of all rows where the column <bpt id="p2">**</bpt>t4<ept id="p2">**</ept> contains the value <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>SELECT<ept id="p1">**</ept>: Selects a count of all rows where the column <bpt id="p2">**</bpt>t4<ept id="p2">**</ept> contains the value <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>This should return a value of <bpt id="p1">**</bpt>3<ept id="p1">**</ept> because there are three rows that contain this value.</source>
          <target state="new">This should return a value of <bpt id="p1">**</bpt>3<ept id="p1">**</ept> because there are three rows that contain this value.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p1">**</ept> - Tells Hive that we should only return data from files ending in .log.</source>
          <target state="new"><bpt id="p1">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p1">**</ept> - Tells Hive that we should only return data from files ending in .log.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.</source>
          <target state="new">This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, and you always want Hive queries to use the latest data.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, and you always want Hive queries to use the latest data.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Dropping an external table does <bpt id="p1">**</bpt>not<ept id="p1">**</ept> delete the data, it only deletes the table definition.</source>
          <target state="new">Dropping an external table does <bpt id="p1">**</bpt>not<ept id="p1">**</ept> delete the data, it only deletes the table definition.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>After creating the external table, the following statements are used to create an <bpt id="p1">**</bpt>internal<ept id="p1">**</ept> table.</source>
          <target state="new">After creating the external table, the following statements are used to create an <bpt id="p1">**</bpt>internal<ept id="p1">**</ept> table.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>These statements perform the following actions:</source>
          <target state="new">These statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p1">**</ept>: Creates a table, if it does not already exist.</source>
          <target state="new"><bpt id="p1">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p1">**</ept>: Creates a table, if it does not already exist.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Because the <bpt id="p1">**</bpt>EXTERNAL<ept id="p1">**</ept> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</source>
          <target state="new">Because the <bpt id="p1">**</bpt>EXTERNAL<ept id="p1">**</ept> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>STORED AS ORC<ept id="p1">**</ept>: Stores the data in Optimized Row Columnar (ORC) format.</source>
          <target state="new"><bpt id="p1">**</bpt>STORED AS ORC<ept id="p1">**</ept>: Stores the data in Optimized Row Columnar (ORC) format.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>This is a highly optimized and efficient format for storing Hive data.</source>
          <target state="new">This is a highly optimized and efficient format for storing Hive data.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p1">**</ept>: Selects rows from the <bpt id="p2">**</bpt>log4jLogs<ept id="p2">**</ept> table that contains <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>, and then inserts the data into the <bpt id="p4">**</bpt>errorLogs<ept id="p4">**</ept> table.</source>
          <target state="new"><bpt id="p1">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p1">**</ept>: Selects rows from the <bpt id="p2">**</bpt>log4jLogs<ept id="p2">**</ept> table that contains <bpt id="p3">**</bpt>[ERROR]<ept id="p3">**</ept>, and then inserts the data into the <bpt id="p4">**</bpt>errorLogs<ept id="p4">**</ept> table.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> Unlike external tables, dropping an internal table also deletes the underlying data.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> Unlike external tables, dropping an internal table also deletes the underlying data.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="usetez"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use Apache Tez for improved performance</source>
          <target state="new"><ph id="ph1">&lt;a id="usetez"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Use Apache Tez for improved performance</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Apache Tez<ept id="p1">](http://tez.apache.org)</ept> is a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale.</source>
          <target state="new"><bpt id="p1">[</bpt>Apache Tez<ept id="p1">](http://tez.apache.org)</ept> is a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>In the latest release of HDInsight, Hive supports running on Tez.</source>
          <target state="new">In the latest release of HDInsight, Hive supports running on Tez.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Tez is currently off by default for Windows-based HDInsight clusters and it must be enabled.</source>
          <target state="new">Tez is currently off by default for Windows-based HDInsight clusters and it must be enabled.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>To take advantage of Tez, the following value must be set for a Hive query:</source>
          <target state="new">To take advantage of Tez, the following value must be set for a Hive query:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>This can be submitted on a per-query basis by placing it at the beginning of your query.</source>
          <target state="new">This can be submitted on a per-query basis by placing it at the beginning of your query.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>You can also set this to be on by default on a cluster by setting the configuration value when you create the cluster.</source>
          <target state="new">You can also set this to be on by default on a cluster by setting the configuration value when you create the cluster.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>You can find more details in <bpt id="p1">[</bpt>Provisioning HDInsight Clusters<ept id="p1">](hdinsight-provision-clusters.md)</ept>.</source>
          <target state="new">You can find more details in <bpt id="p1">[</bpt>Provisioning HDInsight Clusters<ept id="p1">](hdinsight-provision-clusters.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Tez is on as default for Linux-based HDInsight clusters.</source>
          <target state="new">Tez is on as default for Linux-based HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt>Hive on Tez design documents<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)</ept> contain a number of details about the implementation choices and tuning configurations.</source>
          <target state="new">The <bpt id="p1">[</bpt>Hive on Tez design documents<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)</ept> contain a number of details about the implementation choices and tuning configurations.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="run"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Choose how to run the HiveQL job</source>
          <target state="new"><ph id="ph1">&lt;a id="run"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Choose how to run the HiveQL job</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>HDInsight can run HiveQL jobs using a variety of methods.</source>
          <target state="new">HDInsight can run HiveQL jobs using a variety of methods.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Use the following table to decide which method is right for you, then follow the link for a walkthrough.</source>
          <target state="new">Use the following table to decide which method is right for you, then follow the link for a walkthrough.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Use this<ept id="p1">**</ept> if you want...</source>
          <target state="new"><bpt id="p1">**</bpt>Use this<ept id="p1">**</ept> if you want...</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>...an <bpt id="p1">**</bpt>interactive<ept id="p1">**</ept> shell</source>
          <target state="new">...an <bpt id="p1">**</bpt>interactive<ept id="p1">**</ept> shell</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>...<bpt id="p1">**</bpt>batch<ept id="p1">**</ept> processing</source>
          <target state="new">...<bpt id="p1">**</bpt>batch<ept id="p1">**</ept> processing</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>...with this <bpt id="p1">**</bpt>cluster operating system<ept id="p1">**</ept></source>
          <target state="new">...with this <bpt id="p1">**</bpt>cluster operating system<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>...from this <bpt id="p1">**</bpt>client operating system<ept id="p1">**</ept></source>
          <target state="new">...from this <bpt id="p1">**</bpt>client operating system<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>SSH</source>
          <target state="new">SSH</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Linux</source>
          <target state="new">Linux</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Linux, Unix, Mac OS X, or Windows</source>
          <target state="new">Linux, Unix, Mac OS X, or Windows</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Curl</source>
          <target state="new">Curl</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>&amp;nbsp;</source>
          <target state="new">&amp;nbsp;</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>Linux or Windows</source>
          <target state="new">Linux or Windows</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Linux, Unix, Mac OS X, or Windows</source>
          <target state="new">Linux, Unix, Mac OS X, or Windows</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Query console</source>
          <target state="new">Query console</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>&amp;nbsp;</source>
          <target state="new">&amp;nbsp;</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Browser-based</source>
          <target state="new">Browser-based</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>HDInsight tools for Visual Studio</source>
          <target state="new">HDInsight tools for Visual Studio</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>&amp;nbsp;</source>
          <target state="new">&amp;nbsp;</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Linux or Windows</source>
          <target state="new">Linux or Windows</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Windows PowerShell</source>
          <target state="new">Windows PowerShell</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>&amp;nbsp;</source>
          <target state="new">&amp;nbsp;</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Linux or Windows</source>
          <target state="new">Linux or Windows</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Remote Desktop</source>
          <target state="new">Remote Desktop</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>✔</source>
          <target state="new">✔</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Windows</source>
          <target state="new">Windows</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Running Hive jobs on Azure HDInsight using on-premises SQL Server Integration Services</source>
          <target state="new">Running Hive jobs on Azure HDInsight using on-premises SQL Server Integration Services</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>You can also use SQL Server Integration Services (SSIS) to run a Hive job.</source>
          <target state="new">You can also use SQL Server Integration Services (SSIS) to run a Hive job.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.</source>
          <target state="new">The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Azure HDInsight Hive Task</source>
          <target state="new">Azure HDInsight Hive Task</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Azure Subscription Connection Manager</source>
          <target state="new">Azure Subscription Connection Manager</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Learn more about the Azure Feature Pack for SSIS <bpt id="p1">[</bpt>here<ept id="p1">][ssispack]</ept>.</source>
          <target state="new">Learn more about the Azure Feature Pack for SSIS <bpt id="p1">[</bpt>here<ept id="p1">][ssispack]</ept>.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</source>
          <target state="new"><ph id="ph1">&lt;a id="nextsteps"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Next steps</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Now that you've learned what Hive is and how to use it with Hadoop in HDInsight, use the following links to explore other ways to work with Azure HDInsight.</source>
          <target state="new">Now that you've learned what Hive is and how to use it with Hadoop in HDInsight, use the following links to explore other ways to work with Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Upload data to HDInsight</source>
          <target state="new">Upload data to HDInsight</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Use Pig with HDInsight</source>
          <target state="new">Use Pig with HDInsight</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Use MapReduce jobs with HDInsight</source>
          <target state="new">Use MapReduce jobs with HDInsight</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">0bd9a771c4830e03c292b83697bb80c3dcfd9127</xliffext:olfilehash>
  </header>
</xliff>