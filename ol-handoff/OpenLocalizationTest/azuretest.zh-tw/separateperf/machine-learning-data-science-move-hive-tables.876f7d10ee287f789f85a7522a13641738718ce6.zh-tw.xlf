<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Blob storage | Microsoft Azure</source>
          <target state="new">Create and load data into Hive tables from Blob storage | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Create Hive tables and load data in blob to hive tables</source>
          <target state="new">Create Hive tables and load data in blob to hive tables</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Azure blob storage</source>
          <target state="new">Create and load data into Hive tables from Azure blob storage</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Introduction</source>
          <target state="new">Introduction</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</source>
          <target state="new">In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Some guidance is also provided on partitioning Hive tables and on using the Optimized Row Columnar (ORC) formatting to improve query performance.</source>
          <target state="new">Some guidance is also provided on partitioning Hive tables and on using the Optimized Row Columnar (ORC) formatting to improve query performance.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>This article assumes that you have:</source>
          <target state="new">This article assumes that you have:</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Created an Azure storage account.</source>
          <target state="new">Created an Azure storage account.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p1">[</bpt>Create an Azure Storage account<ept id="p1">](../hdinsight-get-started.md#storage)</ept></source>
          <target state="new">If you need instructions, see <bpt id="p1">[</bpt>Create an Azure Storage account<ept id="p1">](../hdinsight-get-started.md#storage)</ept></target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Provisioned a customized Hadoop cluster with the HDInsight service.</source>
          <target state="new">Provisioned a customized Hadoop cluster with the HDInsight service.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop clusters for advanced analytics<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop clusters for advanced analytics<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Enabled remote access to the cluster, logged in, and opened the Hadoop Command Line console.</source>
          <target state="new">Enabled remote access to the cluster, logged in, and opened the Hadoop Command Line console.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p1">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p1">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Upload data to Azure blob storage</source>
          <target state="new">Upload data to Azure blob storage</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>If you created an Azure virtual machine by following the instructions provided in <bpt id="p1">[</bpt>Set up an Azure virtual machine for advanced analytics<ept id="p1">](machine-learning-data-science-setup-virtual-machine.md)</ept>, this script file should have been downloaded to the <bpt id="p2">*</bpt>C:\\Users\\\&lt;user name\&gt;\\Documents\\Data Science Scripts<ept id="p2">*</ept> directory on the virtual machine.</source>
          <target state="new">If you created an Azure virtual machine by following the instructions provided in <bpt id="p1">[</bpt>Set up an Azure virtual machine for advanced analytics<ept id="p1">](machine-learning-data-science-setup-virtual-machine.md)</ept>, this script file should have been downloaded to the <bpt id="p2">*</bpt>C:\\Users\\\&lt;user name\&gt;\\Documents\\Data Science Scripts<ept id="p2">*</ept> directory on the virtual machine.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>These Hive queries only require that you plug in your own data schema and Azure blob storage configuration in the appropriate fields to be ready for submission.</source>
          <target state="new">These Hive queries only require that you plug in your own data schema and Azure blob storage configuration in the appropriate fields to be ready for submission.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>We assume that the data for Hive tables is in an <bpt id="p1">**</bpt>uncompressed<ept id="p1">**</ept> tabular format, and that the data has been uploaded to the default (or to an additional) container of the storage account used by the Hadoop cluster.</source>
          <target state="new">We assume that the data for Hive tables is in an <bpt id="p1">**</bpt>uncompressed<ept id="p1">**</ept> tabular format, and that the data has been uploaded to the default (or to an additional) container of the storage account used by the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>If you want to practice on the <bpt id="p1">_</bpt>NYC Taxi Trip Data<ept id="p1">_</ept>, you need to first  download the 24 <ph id="ph1">&lt;a href="http://www.andresmh.com/nyctaxitrips/" target="_blank"&gt;</ph>NYC Taxi Trip Data<ph id="ph2">&lt;/a&gt;</ph> files (12 Trip files, and 12 Fare files), <bpt id="p2">**</bpt>unzip<ept id="p2">**</ept> all files into .csv files, and then upload them to the default (or appropriate container) of the Azure storage account that was created by the procedure outlined in the <bpt id="p3">[</bpt>Customize Azure HDInsight Hadoop clusters for Advanced Analytics Process and Technology<ept id="p3">](machine-learning-data-science-customize-hadoop-cluster.md)</ept> topic.</source>
          <target state="new">If you want to practice on the <bpt id="p1">_</bpt>NYC Taxi Trip Data<ept id="p1">_</ept>, you need to first  download the 24 <ph id="ph1">&lt;a href="http://www.andresmh.com/nyctaxitrips/" target="_blank"&gt;</ph>NYC Taxi Trip Data<ph id="ph2">&lt;/a&gt;</ph> files (12 Trip files, and 12 Fare files), <bpt id="p2">**</bpt>unzip<ept id="p2">**</ept> all files into .csv files, and then upload them to the default (or appropriate container) of the Azure storage account that was created by the procedure outlined in the <bpt id="p3">[</bpt>Customize Azure HDInsight Hadoop clusters for Advanced Analytics Process and Technology<ept id="p3">](machine-learning-data-science-customize-hadoop-cluster.md)</ept> topic.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>The process to upload the .csv files to the default container on the storage account can be found on this <bpt id="p1">[</bpt>page<ept id="p1">](machine-learning-data-science-process-hive-walkthrough/#upload)</ept>.</source>
          <target state="new">The process to upload the .csv files to the default container on the storage account can be found on this <bpt id="p1">[</bpt>page<ept id="p1">](machine-learning-data-science-process-hive-walkthrough/#upload)</ept>.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>How to submit a Hive query</source>
          <target state="new">How to submit a Hive query</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Hive queries can be submitted from the Hadoop Command Line console on the head node of the Hadoop cluster.</source>
          <target state="new">Hive queries can be submitted from the Hadoop Command Line console on the head node of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>To do this, log into the head node of the Hadoop cluster, open the Hadoop Command Line console, and submit the Hive queries from there.</source>
          <target state="new">To do this, log into the head node of the Hadoop cluster, open the Hadoop Command Line console, and submit the Hive queries from there.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>For instructions on how to do this, see <bpt id="p1">[</bpt>Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process<ept id="p1">](machine-learning-data-science-process-hive-tables.md)</ept>.</source>
          <target state="new">For instructions on how to do this, see <bpt id="p1">[</bpt>Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process<ept id="p1">](machine-learning-data-science-process-hive-tables.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Users can also use the Query Console (Hive Editor) by entering the URL</source>
          <target state="new">Users can also use the Query Console (Hive Editor) by entering the URL</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor</source>
          <target state="new">https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>into a web browser.</source>
          <target state="new">into a web browser.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Note that you will be prompted to input the Hadoop cluster credentials to log in, so you should have these credentials handy.</source>
          <target state="new">Note that you will be prompted to input the Hadoop cluster credentials to log in, so you should have these credentials handy.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Alternatively, you can <bpt id="p1">[</bpt>Submit Hive jobs using PowerShell<ept id="p1">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.</source>
          <target state="new">Alternatively, you can <bpt id="p1">[</bpt>Submit Hive jobs using PowerShell<ept id="p1">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="create-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables</source>
          <target state="new"><ph id="ph1">&lt;a name="create-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>The Hive queries are shared in the <bpt id="p1">[</bpt>Github repository<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept> and can be downloaded from there.</source>
          <target state="new">The Hive queries are shared in the <bpt id="p1">[</bpt>Github repository<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept> and can be downloaded from there.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that creates a Hive table.</source>
          <target state="new">Here is the Hive query that creates a Hive table.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Here are the descriptions of the fields that users need to plug in and other configurations:</source>
          <target state="new">Here are the descriptions of the fields that users need to plug in and other configurations:</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;database name&gt;<ept id="p1">**</ept>: the name of the database users want to create.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;database name&gt;<ept id="p1">**</ept>: the name of the database users want to create.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>If users just want to use the default database, the query <bpt id="p1">*</bpt>create database...<ept id="p1">*</ept> can be omitted.</source>
          <target state="new">If users just want to use the default database, the query <bpt id="p1">*</bpt>create database...<ept id="p1">*</ept> can be omitted.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;table name&gt;<ept id="p1">**</ept>: the name of the table users want to create within the specified database.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;table name&gt;<ept id="p1">**</ept>: the name of the table users want to create within the specified database.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>If users want to use the default database, the table can be directly referred by <bpt id="p1">*</bpt>&amp;#60;table name&gt;<ept id="p1">*</ept> without &amp;#60;database name&gt;.</source>
          <target state="new">If users want to use the default database, the table can be directly referred by <bpt id="p1">*</bpt>&amp;#60;table name&gt;<ept id="p1">*</ept> without &amp;#60;database name&gt;.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;field separator&gt;<ept id="p1">**</ept>: the separator that delimits fields in the data file to be uploaded to the Hive table.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;field separator&gt;<ept id="p1">**</ept>: the separator that delimits fields in the data file to be uploaded to the Hive table.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;line separator&gt;<ept id="p1">**</ept>: the separator that delimits lines in the data file.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;line separator&gt;<ept id="p1">**</ept>: the separator that delimits lines in the data file.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;storage location&gt;<ept id="p1">**</ept>: the Azure storage location to save the data of Hive tables.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;storage location&gt;<ept id="p1">**</ept>: the Azure storage location to save the data of Hive tables.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>If users do not specify <bpt id="p1">*</bpt>LOCATION &amp;#60;storage location&gt;<ept id="p1">*</ept>, the database and the tables are stored in <bpt id="p2">*</bpt>hive/warehouse/<ept id="p2">*</ept> directory in the default container of the Hive cluster by default.</source>
          <target state="new">If users do not specify <bpt id="p1">*</bpt>LOCATION &amp;#60;storage location&gt;<ept id="p1">*</ept>, the database and the tables are stored in <bpt id="p2">*</bpt>hive/warehouse/<ept id="p2">*</ept> directory in the default container of the Hive cluster by default.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>If a user wants to specify the storage location, the storage location has to be within the default container for the database and tables.</source>
          <target state="new">If a user wants to specify the storage location, the storage location has to be within the default container for the database and tables.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>This location has to be referred as location relative to the default container of the cluster in the format of <bpt id="p1">*</bpt>'wasb:///&amp;#60;directory 1&gt;/'<ept id="p1">*</ept> or <bpt id="p2">*</bpt>'wasb:///&amp;#60;directory 1&gt;/&amp;#60;directory 2&gt;/'<ept id="p2">*</ept>, etc. After the query is executed, the relative directories will be created within the default container.</source>
          <target state="new">This location has to be referred as location relative to the default container of the cluster in the format of <bpt id="p1">*</bpt>'wasb:///&amp;#60;directory 1&gt;/'<ept id="p1">*</ept> or <bpt id="p2">*</bpt>'wasb:///&amp;#60;directory 1&gt;/&amp;#60;directory 2&gt;/'<ept id="p2">*</ept>, etc. After the query is executed, the relative directories will be created within the default container.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="p1">**</ept>: If the data file has a header line, users have to add this property <bpt id="p2">**</bpt>at the end<ept id="p2">**</ept> of the <bpt id="p3">*</bpt>create table<ept id="p3">*</ept> query.</source>
          <target state="new"><bpt id="p1">**</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="p1">**</ept>: If the data file has a header line, users have to add this property <bpt id="p2">**</bpt>at the end<ept id="p2">**</ept> of the <bpt id="p3">*</bpt>create table<ept id="p3">*</ept> query.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Otherwise, the header line will be loaded as a record to the table.</source>
          <target state="new">Otherwise, the header line will be loaded as a record to the table.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>If the data file does not have a header line, this configuration can be omitted in the query.</source>
          <target state="new">If the data file does not have a header line, this configuration can be omitted in the query.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load data to Hive tables</source>
          <target state="new"><ph id="ph1">&lt;a name="load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load data to Hive tables</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that loads data into a Hive table.</source>
          <target state="new">Here is the Hive query that loads data into a Hive table.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>&amp;#60;path to blob data&gt;<ept id="p1">**</ept>: If the blob file to be uploaded to the Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="p2">*</bpt>&amp;#60;path to blob data&gt;<ept id="p2">*</ept> should be in the format <bpt id="p3">*</bpt>'wasb:///&amp;#60;directory in this container&gt;/&amp;#60;blob file name&gt;'<ept id="p3">*</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>&amp;#60;path to blob data&gt;<ept id="p1">**</ept>: If the blob file to be uploaded to the Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="p2">*</bpt>&amp;#60;path to blob data&gt;<ept id="p2">*</ept> should be in the format <bpt id="p3">*</bpt>'wasb:///&amp;#60;directory in this container&gt;/&amp;#60;blob file name&gt;'<ept id="p3">*</ept>.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>The blob file can also be in an additional container of the HDInsight Hadoop cluster.</source>
          <target state="new">The blob file can also be in an additional container of the HDInsight Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>In this case, <bpt id="p1">*</bpt>&amp;#60;path to blob data&gt;<ept id="p1">*</ept> should be in the format <bpt id="p2">*</bpt>'wasb://&amp;#60;container name&gt;@&amp;#60;storage account name&gt;.blob.core.windows.net/&amp;#60;blob file name&gt;'<ept id="p2">*</ept>.</source>
          <target state="new">In this case, <bpt id="p1">*</bpt>&amp;#60;path to blob data&gt;<ept id="p1">*</ept> should be in the format <bpt id="p2">*</bpt>'wasb://&amp;#60;container name&gt;@&amp;#60;storage account name&gt;.blob.core.windows.net/&amp;#60;blob file name&gt;'<ept id="p2">*</ept>.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Otherwise, the <bpt id="p1">*</bpt>LOAD DATA<ept id="p1">*</ept> query will fail complaining that it cannot access the data.</source>
          <target state="new">Otherwise, the <bpt id="p1">*</bpt>LOAD DATA<ept id="p1">*</ept> query will fail complaining that it cannot access the data.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="partition-orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Advanced topics: partitioned table and store Hive data in ORC format</source>
          <target state="new"><ph id="ph1">&lt;a name="partition-orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Advanced topics: partitioned table and store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>If the data is large, partitioning the table is beneficial for queries that only need to scan a few partitions of the table.</source>
          <target state="new">If the data is large, partitioning the table is beneficial for queries that only need to scan a few partitions of the table.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>For instance, it is reasonable to partition the log data of a web site by dates.</source>
          <target state="new">For instance, it is reasonable to partition the log data of a web site by dates.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>In addition to partitioning Hive tables, it is also beneficial to store the Hive data in the Optimized Row Columnar (ORC) format.</source>
          <target state="new">In addition to partitioning Hive tables, it is also beneficial to store the Hive data in the Optimized Row Columnar (ORC) format.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>For more information on ORC formatting, see <ph id="ph1">&lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles" target="_blank"&gt;</ph>Using ORC files improves performance when Hive is reading, writing, and processing data<ph id="ph2">&lt;/a&gt;</ph>.</source>
          <target state="new">For more information on ORC formatting, see <ph id="ph1">&lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles" target="_blank"&gt;</ph>Using ORC files improves performance when Hive is reading, writing, and processing data<ph id="ph2">&lt;/a&gt;</ph>.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Partitioned table</source>
          <target state="new">Partitioned table</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that creates a partitioned table and loads data into it.</source>
          <target state="new">Here is the Hive query that creates a partitioned table and loads data into it.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p1">**</bpt>beginning<ept id="p1">**</ept> of the <ph id="ph1">`where`</ph> clause as this improves the efficacy of searching significantly.</source>
          <target state="new">When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p1">**</bpt>beginning<ept id="p1">**</ept> of the <ph id="ph1">`where`</ph> clause as this improves the efficacy of searching significantly.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Store Hive data in ORC format</source>
          <target state="new"><ph id="ph1">&lt;a name="orc"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Users cannot directly load data from blob storage into Hive tables that is stored in the ORC format.</source>
          <target state="new">Users cannot directly load data from blob storage into Hive tables that is stored in the ORC format.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</source>
          <target state="new">Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Create an external table <bpt id="p1">**</bpt>STORED AS TEXTFILE<ept id="p1">**</ept> and load data from blob storage to the table.</source>
          <target state="new">Create an external table <bpt id="p1">**</bpt>STORED AS TEXTFILE<ept id="p1">**</ept> and load data from blob storage to the table.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Create an internal table with the same schema as the external table in step 1, with the same field delimiter, and store the Hive data in the ORC format.</source>
          <target state="new">Create an internal table with the same schema as the external table in step 1, with the same field delimiter, and store the Hive data in the ORC format.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Select data from the external table in step 1 and insert into the ORC table</source>
          <target state="new">Select data from the external table in step 1 and insert into the ORC table</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> If the TEXTFILE table <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;external textfile table name&gt;<ept id="p1">*</ept> has partitions, in STEP 3, the <ph id="ph2">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph> command will select the partition variable as a field in the returned data set.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> If the TEXTFILE table <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;external textfile table name&gt;<ept id="p1">*</ept> has partitions, in STEP 3, the <ph id="ph2">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph> command will select the partition variable as a field in the returned data set.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Inserting it into the <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p1">*</ept> will fail since <bpt id="p2">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p2">*</ept> does not have the partition variable as a field in the table schema.</source>
          <target state="new">Inserting it into the <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p1">*</ept> will fail since <bpt id="p2">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p2">*</ept> does not have the partition variable as a field in the table schema.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>In this case, users need to specifically select the fields to be inserted to <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p1">*</ept> as follows:</source>
          <target state="new">In this case, users need to specifically select the fields to be inserted to <bpt id="p1">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p1">*</ept> as follows:</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>It is safe to drop the <bpt id="p1">*</bpt>&amp;#60;external textfile table name&gt;<ept id="p1">*</ept> when using the following query after all data has been inserted into <bpt id="p2">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p2">*</ept>:</source>
          <target state="new">It is safe to drop the <bpt id="p1">*</bpt>&amp;#60;external textfile table name&gt;<ept id="p1">*</ept> when using the following query after all data has been inserted into <bpt id="p2">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p2">*</ept>:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>After following this procedure, you should have a table with data in the ORC format ready to use.</source>
          <target state="new">After following this procedure, you should have a table with data in the ORC format ready to use.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1a04f13db246a5cabc1e9f1e18e599a1c84464d3</xliffext:olfilehash>
  </header>
</xliff>