<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="zh-tw">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Advanced Analytics Process and Technology in Action: Use Hadoop clusters | Microsoft Azure</source>
          <target state="new">Advanced Analytics Process and Technology in Action: Use Hadoop clusters | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Using the Advanced Analytics Process and Technology (ADAPT) for an end-to-end scenario employing an HDInsight Hadoop cluster to build and deploy a model using a publicly available dataset.</source>
          <target state="new">Using the Advanced Analytics Process and Technology (ADAPT) for an end-to-end scenario employing an HDInsight Hadoop cluster to build and deploy a model using a publicly available dataset.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Advanced Analytics Process and Technology in Action: using HDInsight Hadoop clusters</source>
          <target state="new">Advanced Analytics Process and Technology in Action: using HDInsight Hadoop clusters</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>In this walkthrough, you use the Advanced Analytics Process  and Technology (ADAPT) in an end-to-end scenario using an <bpt id="p1">[</bpt>Azure HDInsight Hadoop cluster<ept id="p1">](http://azure.microsoft.com/services/hdinsight/)</ept> to store, explore and feature engineer data from the publicly available <bpt id="p2">[</bpt>NYC Taxi Trips<ept id="p2">](http://www.andresmh.com/nyctaxitrips/)</ept> dataset, and to down sample the data.</source>
          <target state="new">In this walkthrough, you use the Advanced Analytics Process  and Technology (ADAPT) in an end-to-end scenario using an <bpt id="p1">[</bpt>Azure HDInsight Hadoop cluster<ept id="p1">](http://azure.microsoft.com/services/hdinsight/)</ept> to store, explore and feature engineer data from the publicly available <bpt id="p2">[</bpt>NYC Taxi Trips<ept id="p2">](http://www.andresmh.com/nyctaxitrips/)</ept> dataset, and to down sample the data.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Models of the data are built with Azure Machine Learning to handle binary and multiclass classification and regression predictive tasks.</source>
          <target state="new">Models of the data are built with Azure Machine Learning to handle binary and multiclass classification and regression predictive tasks.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>For a walkthrough that shows how to handle a larger (1 terabyte) dataset for a similar scenario using HDInsight Hadoop clusters for data processing, see <bpt id="p1">[</bpt>Advanced Analytics Process and Technology in Action - Using Azure HDInsight Hadoop Clusters on a 1 TB dataset<ept id="p1">](machine-learning-data-science-process-hive-criteo-walkthrough.md)</ept>.</source>
          <target state="new">For a walkthrough that shows how to handle a larger (1 terabyte) dataset for a similar scenario using HDInsight Hadoop clusters for data processing, see <bpt id="p1">[</bpt>Advanced Analytics Process and Technology in Action - Using Azure HDInsight Hadoop Clusters on a 1 TB dataset<ept id="p1">](machine-learning-data-science-process-hive-criteo-walkthrough.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>It is also possible to use an IPython notebook to accomplish the tasks presented the walkthrough using the 1 TB dataset.</source>
          <target state="new">It is also possible to use an IPython notebook to accomplish the tasks presented the walkthrough using the 1 TB dataset.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Users who would like to try this approach should consult the <bpt id="p1">[</bpt>Criteo walkthrough using a Hive ODBC connection<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/iPythonNotebooks/machine-Learning-data-science-process-hive-walkthrough-criteo.ipynb)</ept> topic.</source>
          <target state="new">Users who would like to try this approach should consult the <bpt id="p1">[</bpt>Criteo walkthrough using a Hive ODBC connection<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/DataScienceProcess/iPythonNotebooks/machine-Learning-data-science-process-hive-walkthrough-criteo.ipynb)</ept> topic.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="dataset"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>NYC Taxi Trips Dataset description</source>
          <target state="new"><ph id="ph1">&lt;a name="dataset"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>NYC Taxi Trips Dataset description</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>The NYC Taxi Trip data is about 20GB of compressed comma-separated values (CSV) files (~48GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</source>
          <target state="new">The NYC Taxi Trip data is about 20GB of compressed comma-separated values (CSV) files (~48GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Each trip record includes the pickup and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</source>
          <target state="new">Each trip record includes the pickup and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</source>
          <target state="new">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>The 'trip_data' CSV files contain trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length.</source>
          <target state="new">The 'trip_data' CSV files contain trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Here are a few sample records:</source>
          <target state="new">Here are a few sample records:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</source>
          <target state="new">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Here are a few sample records:</source>
          <target state="new">Here are a few sample records:</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</source>
          <target state="new">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>To get all of the details relevant to a particular trip, it is sufficient to join with three keys: the "medallion", "hack\_license" and "pickup\_datetime".</source>
          <target state="new">To get all of the details relevant to a particular trip, it is sufficient to join with three keys: the "medallion", "hack\_license" and "pickup\_datetime".</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>We describe some more details of the data when we store them into Hive tables shortly.</source>
          <target state="new">We describe some more details of the data when we store them into Hive tables shortly.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="mltasks"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Examples of prediction tasks</source>
          <target state="new"><ph id="ph1">&lt;a name="mltasks"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Examples of prediction tasks</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>When approaching data, determining the kind of predictions you want to make based on its analysis helps clarify the tasks that you will need to include in your process.</source>
          <target state="new">When approaching data, determining the kind of predictions you want to make based on its analysis helps clarify the tasks that you will need to include in your process.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Here are three examples of prediction problems that we address in this walkthrough whose formulation is based on the <bpt id="p1">*</bpt>tip\_amount<ept id="p1">*</ept>:</source>
          <target state="new">Here are three examples of prediction problems that we address in this walkthrough whose formulation is based on the <bpt id="p1">*</bpt>tip\_amount<ept id="p1">*</ept>:</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Binary classification<ept id="p1">**</ept>: Predict whether or not a tip was paid for a trip, i.e. a <bpt id="p2">*</bpt>tip\_amount<ept id="p2">*</ept> that is greater than $0 is a positive example, while a <bpt id="p3">*</bpt>tip\_amount<ept id="p3">*</ept> of $0 is a negative example.</source>
          <target state="new"><bpt id="p1">**</bpt>Binary classification<ept id="p1">**</ept>: Predict whether or not a tip was paid for a trip, i.e. a <bpt id="p2">*</bpt>tip\_amount<ept id="p2">*</ept> that is greater than $0 is a positive example, while a <bpt id="p3">*</bpt>tip\_amount<ept id="p3">*</ept> of $0 is a negative example.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Multiclass classification<ept id="p1">**</ept>: To predict the range of tip amounts paid for the trip.</source>
          <target state="new"><bpt id="p1">**</bpt>Multiclass classification<ept id="p1">**</ept>: To predict the range of tip amounts paid for the trip.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>We divide the <bpt id="p1">*</bpt>tip\_amount<ept id="p1">*</ept> into five bins or classes:</source>
          <target state="new">We divide the <bpt id="p1">*</bpt>tip\_amount<ept id="p1">*</ept> into five bins or classes:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Regression task<ept id="p1">**</ept>: To predict the amount of the tip paid for a trip.</source>
          <target state="new"><bpt id="p1">**</bpt>Regression task<ept id="p1">**</ept>: To predict the amount of the tip paid for a trip.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="setup"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Set up an HDInsight Hadoop cluster for advanced analytics</source>
          <target state="new"><ph id="ph1">&lt;a name="setup"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Set up an HDInsight Hadoop cluster for advanced analytics</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>You can set up an Azure environment for advanced analytics that employs an HDInsight cluster in three steps:</source>
          <target state="new">You can set up an Azure environment for advanced analytics that employs an HDInsight cluster in three steps:</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Create a storage account<ept id="p1">](../storage-whatis-account.md)</ept>: This storage account is used for storing data in Azure Blob Storage.</source>
          <target state="new"><bpt id="p1">[</bpt>Create a storage account<ept id="p1">](../storage-whatis-account.md)</ept>: This storage account is used for storing data in Azure Blob Storage.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>The data used in HDInsight clusters also resides here.</source>
          <target state="new">The data used in HDInsight clusters also resides here.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop clusters for the Advanced Analytics Process and Technology<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</source>
          <target state="new"><bpt id="p1">[</bpt>Customize Azure HDInsight Hadoop clusters for the Advanced Analytics Process and Technology<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>This step creates an Azure HDInsight Hadoop cluster with 64-bit Anaconda Python 2.7 installed on all nodes.</source>
          <target state="new">This step creates an Azure HDInsight Hadoop cluster with 64-bit Anaconda Python 2.7 installed on all nodes.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>There are two important steps to remember while customizing your HDInsight cluster.</source>
          <target state="new">There are two important steps to remember while customizing your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Remember to link the storage account created in step 1 with your HDInsight cluster when creating it.</source>
          <target state="new">Remember to link the storage account created in step 1 with your HDInsight cluster when creating it.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>This storage account is used to access data that is processed within the cluster.</source>
          <target state="new">This storage account is used to access data that is processed within the cluster.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>After the cluster is created, enable Remote Access to the head node of the cluster.</source>
          <target state="new">After the cluster is created, enable Remote Access to the head node of the cluster.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Navigate to the <bpt id="p1">**</bpt>Configuration<ept id="p1">**</ept> tab and click <bpt id="p2">**</bpt>Enable Remote<ept id="p2">**</ept>.</source>
          <target state="new">Navigate to the <bpt id="p1">**</bpt>Configuration<ept id="p1">**</ept> tab and click <bpt id="p2">**</bpt>Enable Remote<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>This step specifies the user credentials used for remote login.</source>
          <target state="new">This step specifies the user credentials used for remote login.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Create an Azure Machine Learning workspace<ept id="p1">](machine-learning-create-workspace.md)</ept>: This Azure Machine Learning workspace is used to build machine learning models.</source>
          <target state="new"><bpt id="p1">[</bpt>Create an Azure Machine Learning workspace<ept id="p1">](machine-learning-create-workspace.md)</ept>: This Azure Machine Learning workspace is used to build machine learning models.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>This task is addressed after completing an initial data exploration and down sampling using the HDInsight cluster.</source>
          <target state="new">This task is addressed after completing an initial data exploration and down sampling using the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="getdata"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Get the data from a public source</source>
          <target state="new"><ph id="ph1">&lt;a name="getdata"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Get the data from a public source</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>To get the <bpt id="p1">[</bpt>NYC Taxi Trips<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept> dataset from its public location, you may use any of the methods described in <bpt id="p2">[</bpt>Move Data to and from Azure Blob Storage<ept id="p2">](machine-learning-data-science-move-azure-blob.md)</ept> to copy the data to your machine.</source>
          <target state="new">To get the <bpt id="p1">[</bpt>NYC Taxi Trips<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept> dataset from its public location, you may use any of the methods described in <bpt id="p2">[</bpt>Move Data to and from Azure Blob Storage<ept id="p2">](machine-learning-data-science-move-azure-blob.md)</ept> to copy the data to your machine.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Here we describe how use AzCopy to transfer the files containing data.</source>
          <target state="new">Here we describe how use AzCopy to transfer the files containing data.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>To download and install AzCopy follow the instructions at <bpt id="p1">[</bpt>Getting Started with the AzCopy Command-Line Utility<ept id="p1">](../storage-use-azcopy.md)</ept>.</source>
          <target state="new">To download and install AzCopy follow the instructions at <bpt id="p1">[</bpt>Getting Started with the AzCopy Command-Line Utility<ept id="p1">](../storage-use-azcopy.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>From a Command Prompt window, issue the following AzCopy commands, replacing <bpt id="p1">*</bpt>&lt;path_to_data_folder&gt;<ept id="p1">*</ept> with the desired destination:</source>
          <target state="new">From a Command Prompt window, issue the following AzCopy commands, replacing <bpt id="p1">*</bpt>&lt;path_to_data_folder&gt;<ept id="p1">*</ept> with the desired destination:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>When the copy completes, a total of 24 zipped files are in the data folder chosen.</source>
          <target state="new">When the copy completes, a total of 24 zipped files are in the data folder chosen.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Unzip the downloaded files to the same directory on your local machine.</source>
          <target state="new">Unzip the downloaded files to the same directory on your local machine.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Make a note of the folder where the uncompressed files reside.</source>
          <target state="new">Make a note of the folder where the uncompressed files reside.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>This folder will be referred to as the <bpt id="p1">*</bpt>&lt;path\_to\_unzipped_data\_files\&gt;<ept id="p1">*</ept> is what follows.</source>
          <target state="new">This folder will be referred to as the <bpt id="p1">*</bpt>&lt;path\_to\_unzipped_data\_files\&gt;<ept id="p1">*</ept> is what follows.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="upload"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Upload the data to the default container of Azure HDInsight Hadoop cluster</source>
          <target state="new"><ph id="ph1">&lt;a name="upload"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Upload the data to the default container of Azure HDInsight Hadoop cluster</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>In the following AzCopy commands, replace the following parameters with the actual values that you specified when creating the Hadoop cluster and unzipping the data files.</source>
          <target state="new">In the following AzCopy commands, replace the following parameters with the actual values that you specified when creating the Hadoop cluster and unzipping the data files.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><bpt id="p1">***</bpt>&amp;#60;path_to_data_folder&gt;<ept id="p1">***</ept> the directory (along with path) on your machine that contain the unzipped data files</source>
          <target state="new"><bpt id="p1">***</bpt>&amp;#60;path_to_data_folder&gt;<ept id="p1">***</ept> the directory (along with path) on your machine that contain the unzipped data files</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source><bpt id="p1">***</bpt>&amp;#60;storage account name of Hadoop cluster&gt;<ept id="p1">***</ept> the storage account associated with your HDInsight cluster</source>
          <target state="new"><bpt id="p1">***</bpt>&amp;#60;storage account name of Hadoop cluster&gt;<ept id="p1">***</ept> the storage account associated with your HDInsight cluster</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><bpt id="p1">***</bpt>&amp;#60;default container of Hadoop cluster&gt;<ept id="p1">***</ept> the default container used by your cluster.</source>
          <target state="new"><bpt id="p1">***</bpt>&amp;#60;default container of Hadoop cluster&gt;<ept id="p1">***</ept> the default container used by your cluster.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Note that the name of the default container is usually the same name as the cluster itself.</source>
          <target state="new">Note that the name of the default container is usually the same name as the cluster itself.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>For example, if the cluster is called "abc123.azurehdinsight.net", the default container is abc123.</source>
          <target state="new">For example, if the cluster is called "abc123.azurehdinsight.net", the default container is abc123.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><bpt id="p1">***</bpt>&amp;#60;storage account key&gt;<ept id="p1">***</ept> the key for the storage account used by your cluster</source>
          <target state="new"><bpt id="p1">***</bpt>&amp;#60;storage account key&gt;<ept id="p1">***</ept> the key for the storage account used by your cluster</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>From a Command Prompt or a Windows PowerShell window in your machine, run the following two AzCopy commands.</source>
          <target state="new">From a Command Prompt or a Windows PowerShell window in your machine, run the following two AzCopy commands.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>This command uploads the trip data to <bpt id="p1">***</bpt>nyctaxitripraw<ept id="p1">***</ept> directory in the default container of the Hadoop cluster.</source>
          <target state="new">This command uploads the trip data to <bpt id="p1">***</bpt>nyctaxitripraw<ept id="p1">***</ept> directory in the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>This command uploads the fare data to <bpt id="p1">***</bpt>nyctaxifareraw<ept id="p1">***</ept> directory in the default container of the Hadoop cluster.</source>
          <target state="new">This command uploads the fare data to <bpt id="p1">***</bpt>nyctaxifareraw<ept id="p1">***</ept> directory in the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The data should now in Azure Blob Storage and ready to be consumed within the HDInsight cluster.</source>
          <target state="new">The data should now in Azure Blob Storage and ready to be consumed within the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#download-hql-files"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Log into the head node of Hadoop cluster and and prepare for exploratory data analysis</source>
          <target state="new"><ph id="ph1">&lt;a name="#download-hql-files"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Log into the head node of Hadoop cluster and and prepare for exploratory data analysis</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>To access the head node of the cluster for exploratory data analysis and down sampling of the data, follow the procedure outlined in <bpt id="p1">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</source>
          <target state="new">To access the head node of the cluster for exploratory data analysis and down sampling of the data, follow the procedure outlined in <bpt id="p1">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p1">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>In this walkthrough, we primarily use queries written in <bpt id="p1">[</bpt>Hive<ept id="p1">](https://hive.apache.org/)</ept>, a SQL-like query language, to perform preliminary data explorations.</source>
          <target state="new">In this walkthrough, we primarily use queries written in <bpt id="p1">[</bpt>Hive<ept id="p1">](https://hive.apache.org/)</ept>, a SQL-like query language, to perform preliminary data explorations.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The Hive queries are stored in .hql files.</source>
          <target state="new">The Hive queries are stored in .hql files.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>We then down sample this data to be used within Azure Machine Learning for building models.</source>
          <target state="new">We then down sample this data to be used within Azure Machine Learning for building models.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>To prepare the cluster for exploratory data analysis, we download the .hql files containing the relevant Hive scripts from <bpt id="p1">[</bpt>github<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept> to a local directory (C:\temp) on the head node.</source>
          <target state="new">To prepare the cluster for exploratory data analysis, we download the .hql files containing the relevant Hive scripts from <bpt id="p1">[</bpt>github<ept id="p1">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept> to a local directory (C:\temp) on the head node.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>To do this, open the <bpt id="p1">**</bpt>Command Prompt<ept id="p1">**</ept> from within the head node of the cluster and issue the following two commands:</source>
          <target state="new">To do this, open the <bpt id="p1">**</bpt>Command Prompt<ept id="p1">**</ept> from within the head node of the cluster and issue the following two commands:</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>These two commands will download all .hql files needed in this walkthrough to the local directory <bpt id="p1">***</bpt>C:\temp&amp;#92;<ept id="p1">***</ept> in the head node.</source>
          <target state="new">These two commands will download all .hql files needed in this walkthrough to the local directory <bpt id="p1">***</bpt>C:\temp&amp;#92;<ept id="p1">***</ept> in the head node.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#hive-db-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables partitioned by month</source>
          <target state="new"><ph id="ph1">&lt;a name="#hive-db-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Create Hive database and tables partitioned by month</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>We are now ready to create Hive tables for our NYC taxi dataset.</source>
          <target state="new">We are now ready to create Hive tables for our NYC taxi dataset.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>In the head node of the Hadoop cluster, open the <bpt id="p1">***</bpt>Hadoop Command Line<ept id="p1">***</ept> on the desktop of the head node, and enter the Hive directory by entering the command</source>
          <target state="new">In the head node of the Hadoop cluster, open the <bpt id="p1">***</bpt>Hadoop Command Line<ept id="p1">***</ept> on the desktop of the head node, and enter the Hive directory by entering the command</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Run all Hive commands in this walkthrough from the above Hive bin/ directory prompt.</source>
          <target state="new">Run all Hive commands in this walkthrough from the above Hive bin/ directory prompt.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>This will take care of any path issues automatically.</source>
          <target state="new">This will take care of any path issues automatically.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>We use the terms "Hive directory prompt", "Hive bin/ directory prompt",  and "Hadoop Command Line" interchangeably in this walkthrough.</source>
          <target state="new">We use the terms "Hive directory prompt", "Hive bin/ directory prompt",  and "Hadoop Command Line" interchangeably in this walkthrough.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, enter the following command in Hadoop Command Line of the head node to submit the Hive query to create Hive database and tables:</source>
          <target state="new">From the Hive directory prompt, enter the following command in Hadoop Command Line of the head node to submit the Hive query to create Hive database and tables:</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Here is the content of the <bpt id="p1">***</bpt>C:\temp\sample\_hive\_create\_db\_and\_tables.hql<ept id="p1">***</ept> file which creates Hive database <bpt id="p2">***</bpt>nyctaxidb<ept id="p2">***</ept> and tables <bpt id="p3">***</bpt>trip<ept id="p3">***</ept> and <bpt id="p4">***</bpt>fare<ept id="p4">***</ept>.</source>
          <target state="new">Here is the content of the <bpt id="p1">***</bpt>C:\temp\sample\_hive\_create\_db\_and\_tables.hql<ept id="p1">***</ept> file which creates Hive database <bpt id="p2">***</bpt>nyctaxidb<ept id="p2">***</ept> and tables <bpt id="p3">***</bpt>trip<ept id="p3">***</ept> and <bpt id="p4">***</bpt>fare<ept id="p4">***</ept>.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>This Hive script creates two tables:</source>
          <target state="new">This Hive script creates two tables:</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>the "trip" table contains trip details of each ride (driver details, pickup time, trip distance and times)</source>
          <target state="new">the "trip" table contains trip details of each ride (driver details, pickup time, trip distance and times)</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>the "fare" table contains fare details (fare amount, tip amount, tolls and surcharges).</source>
          <target state="new">the "fare" table contains fare details (fare amount, tip amount, tolls and surcharges).</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>If you need any additional assistance with these procedures or want to investigate alternative ones, see the section <bpt id="p1">[</bpt>Submit Hive queries directly from the Hadoop Command Line <ept id="p1">](machine-learning-data-science-process-hive-tables.md#submit)</ept>.</source>
          <target state="new">If you need any additional assistance with these procedures or want to investigate alternative ones, see the section <bpt id="p1">[</bpt>Submit Hive queries directly from the Hadoop Command Line <ept id="p1">](machine-learning-data-science-process-hive-tables.md#submit)</ept>.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load Data to Hive tables by partitions</source>
          <target state="new"><ph id="ph1">&lt;a name="#load-data"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Load Data to Hive tables by partitions</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically an <bpt id="p1">**</bpt>Admin<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>The NYC taxi dataset has a natural partitioning by month, which we use to enable faster processing and query times.</source>
          <target state="new">The NYC taxi dataset has a natural partitioning by month, which we use to enable faster processing and query times.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>The PowerShell commands below (issued from the Hive directory using the <bpt id="p1">**</bpt>Hadoop Command Line<ept id="p1">**</ept>) load data to the "trip" and "fare" Hive tables partitioned by month.</source>
          <target state="new">The PowerShell commands below (issued from the Hive directory using the <bpt id="p1">**</bpt>Hadoop Command Line<ept id="p1">**</ept>) load data to the "trip" and "fare" Hive tables partitioned by month.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>sample\_hive\_load\_data\_by\_partitions.hql<ept id="p1">*</ept> file contains the following <bpt id="p2">**</bpt>LOAD<ept id="p2">**</ept> commands.</source>
          <target state="new">The <bpt id="p1">*</bpt>sample\_hive\_load\_data\_by\_partitions.hql<ept id="p1">*</ept> file contains the following <bpt id="p2">**</bpt>LOAD<ept id="p2">**</ept> commands.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Note that a number of Hive queries we use here in the exploration process involve looking at just a single partition or at only a couple of partitions.</source>
          <target state="new">Note that a number of Hive queries we use here in the exploration process involve looking at just a single partition or at only a couple of partitions.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>But these queries could be run across the entire data.</source>
          <target state="new">But these queries could be run across the entire data.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#show-db"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Show databases in the HDInsight Hadoop cluster</source>
          <target state="new"><ph id="ph1">&lt;a name="#show-db"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Show databases in the HDInsight Hadoop cluster</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>To show the databases created in HDInsight Hadoop cluster inside the Hadoop Command Line window, run the following command in Hadoop Command Line:</source>
          <target state="new">To show the databases created in HDInsight Hadoop cluster inside the Hadoop Command Line window, run the following command in Hadoop Command Line:</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#show-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Show the Hive tables in the nyctaxidb database</source>
          <target state="new"><ph id="ph1">&lt;a name="#show-tables"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Show the Hive tables in the nyctaxidb database</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>To show the tables in the nyctaxidb database, run the following command in Hadoop Command Line:</source>
          <target state="new">To show the tables in the nyctaxidb database, run the following command in Hadoop Command Line:</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>We can confirm that the tables are partitioned by issuing the command below:</source>
          <target state="new">We can confirm that the tables are partitioned by issuing the command below:</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>The expected output is shown below:</source>
          <target state="new">The expected output is shown below:</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Similarly, we can ensure that the fare table is partitioned by issuing the command below:</source>
          <target state="new">Similarly, we can ensure that the fare table is partitioned by issuing the command below:</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>The expected output is shown below:</source>
          <target state="new">The expected output is shown below:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#explore-hive"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Data exploration and feature engineering in Hive</source>
          <target state="new"><ph id="ph1">&lt;a name="#explore-hive"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Data exploration and feature engineering in Hive</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>The data exploration and feature engineering tasks for the data loaded into the Hive tables can be accomplished using Hive queries.</source>
          <target state="new">The data exploration and feature engineering tasks for the data loaded into the Hive tables can be accomplished using Hive queries.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Here are examples of such tasks that we walk you through in this section:</source>
          <target state="new">Here are examples of such tasks that we walk you through in this section:</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>View the top 10 records in both tables.</source>
          <target state="new">View the top 10 records in both tables.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Explore data distributions of a few fields in varying time windows.</source>
          <target state="new">Explore data distributions of a few fields in varying time windows.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Investigate data quality of the longitude and latitude fields.</source>
          <target state="new">Investigate data quality of the longitude and latitude fields.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Generate binary and multiclass classification labels based on the <bpt id="p1">**</bpt>tip\_amount<ept id="p1">**</ept>.</source>
          <target state="new">Generate binary and multiclass classification labels based on the <bpt id="p1">**</bpt>tip\_amount<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Generate features by computing the direct trip distances.</source>
          <target state="new">Generate features by computing the direct trip distances.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Exploration: View the top 10 records in table trip</source>
          <target state="new">Exploration: View the top 10 records in table trip</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>To see what the data looks like, we examine 10 records from each table.</source>
          <target state="new">To see what the data looks like, we examine 10 records from each table.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Run the following two queries separately from the Hive directory prompt in the Hadoop Command Line console to inspect the records.</source>
          <target state="new">Run the following two queries separately from the Hive directory prompt in the Hadoop Command Line console to inspect the records.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>To get the top 10 records in the table "trip" from the first month:</source>
          <target state="new">To get the top 10 records in the table "trip" from the first month:</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>To get the top 10 records in the table "fare" from the first month:</source>
          <target state="new">To get the top 10 records in the table "fare" from the first month:</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>It is often useful to save the records to a file for convenient viewing.</source>
          <target state="new">It is often useful to save the records to a file for convenient viewing.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>A small change to the above query accomplishes this:</source>
          <target state="new">A small change to the above query accomplishes this:</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>Exploration: View the number of records in each of the 12 partitions</source>
          <target state="new">Exploration: View the number of records in each of the 12 partitions</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>Of interest is the how the number of trips varies during the calendar year.</source>
          <target state="new">Of interest is the how the number of trips varies during the calendar year.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Grouping by month allows us to see what this distribution of trips looks like.</source>
          <target state="new">Grouping by month allows us to see what this distribution of trips looks like.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>This gives us the output :</source>
          <target state="new">This gives us the output :</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>Here, the first column is the month and the second is the number of trips for that month.</source>
          <target state="new">Here, the first column is the month and the second is the number of trips for that month.</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>We can also count the total number of records in our trip data set by issuing the following command at the Hive directory prompt.</source>
          <target state="new">We can also count the total number of records in our trip data set by issuing the following command at the Hive directory prompt.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>This yields:</source>
          <target state="new">This yields:</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Using commands similar to those shown for the trip data set, we can issue Hive queries from the Hive directory prompt for the fare data set to validate the number of records.</source>
          <target state="new">Using commands similar to those shown for the trip data set, we can issue Hive queries from the Hive directory prompt for the fare data set to validate the number of records.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>This gives us the output:</source>
          <target state="new">This gives us the output:</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>Note that the exact same number of trips per month is returned for both data sets.</source>
          <target state="new">Note that the exact same number of trips per month is returned for both data sets.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>This provides the first validation that the data has been loaded correctly.</source>
          <target state="new">This provides the first validation that the data has been loaded correctly.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>Counting the total number of records in the fare data set can be done using the command below from the Hive directory prompt:</source>
          <target state="new">Counting the total number of records in the fare data set can be done using the command below from the Hive directory prompt:</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>This yields :</source>
          <target state="new">This yields :</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>The total number of records in both tables is also the same.</source>
          <target state="new">The total number of records in both tables is also the same.</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>This provides a second validation that the data has been loaded correctly.</source>
          <target state="new">This provides a second validation that the data has been loaded correctly.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>Exploration: Trip distribution by medallion</source>
          <target state="new">Exploration: Trip distribution by medallion</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>This example identifies the medallion (taxi numbers) with more than 100 trips within a given time period.</source>
          <target state="new">This example identifies the medallion (taxi numbers) with more than 100 trips within a given time period.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>The query benefits from the partitioned table access since it is conditioned by the partition variable <bpt id="p1">**</bpt>month<ept id="p1">**</ept>.</source>
          <target state="new">The query benefits from the partitioned table access since it is conditioned by the partition variable <bpt id="p1">**</bpt>month<ept id="p1">**</ept>.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>The query results are written to a local file queryoutput.tsv in <ph id="ph1">`C:\temp`</ph> on the head node.</source>
          <target state="new">The query results are written to a local file queryoutput.tsv in <ph id="ph1">`C:\temp`</ph> on the head node.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>Here is the content of <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion.hql<ept id="p1">*</ept> file for inspection.</source>
          <target state="new">Here is the content of <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion.hql<ept id="p1">*</ept> file for inspection.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>The medallion in the NYC taxi data set identifies a unique cab.</source>
          <target state="new">The medallion in the NYC taxi data set identifies a unique cab.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>We can identify which cabs are "busy" by asking which ones made more than a certain number of trips in a particular time period.</source>
          <target state="new">We can identify which cabs are "busy" by asking which ones made more than a certain number of trips in a particular time period.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>The following example identifies cabs that made more than a hundred trips in the first three months, and saves the query results to a local file, C:\temp\queryoutput.tsv.</source>
          <target state="new">The following example identifies cabs that made more than a hundred trips in the first three months, and saves the query results to a local file, C:\temp\queryoutput.tsv.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>Here is the content of <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion.hql<ept id="p1">*</ept> file for inspection.</source>
          <target state="new">Here is the content of <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion.hql<ept id="p1">*</ept> file for inspection.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, issue the command below :</source>
          <target state="new">From the Hive directory prompt, issue the command below :</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>Exploration: Trip distribution by medallion and hack_license</source>
          <target state="new">Exploration: Trip distribution by medallion and hack_license</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>When exploring a dataset, we frequently want to examine the number of co-occurences of groups of values.</source>
          <target state="new">When exploring a dataset, we frequently want to examine the number of co-occurences of groups of values.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>This section provide an example of how to do this for cabs and drivers.</source>
          <target state="new">This section provide an example of how to do this for cabs and drivers.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion\_license.hql<ept id="p1">*</ept> file groups the fare data set on "medallion" and "hack_license" and returns counts of each combination.</source>
          <target state="new">The <bpt id="p1">*</bpt>sample\_hive\_trip\_count\_by\_medallion\_license.hql<ept id="p1">*</ept> file groups the fare data set on "medallion" and "hack_license" and returns counts of each combination.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>Below are its contents.</source>
          <target state="new">Below are its contents.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>This query returns cab and particular driver combinations ordered by descending number of trips.</source>
          <target state="new">This query returns cab and particular driver combinations ordered by descending number of trips.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, run :</source>
          <target state="new">From the Hive directory prompt, run :</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>The query results are written to a local file C:\temp\queryoutput.tsv.</source>
          <target state="new">The query results are written to a local file C:\temp\queryoutput.tsv.</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>Exploration: Assessing data quality by checking for invalid longitude/latitude records</source>
          <target state="new">Exploration: Assessing data quality by checking for invalid longitude/latitude records</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</source>
          <target state="new"><ph id="ph1">[AZURE.NOTE]</ph> This is typically a <bpt id="p1">**</bpt>Data Scientist<ept id="p1">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>A common objective of exploratory data analysis is to weed out invalid or bad records.</source>
          <target state="new">A common objective of exploratory data analysis is to weed out invalid or bad records.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>The example in this section determines whether either the longitude or latitude fields contain a value far outside the NYC area.</source>
          <target state="new">The example in this section determines whether either the longitude or latitude fields contain a value far outside the NYC area.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>Since it is likely that such records have an erroneous longitude-latitude values, we want to eliminate them from any data that is to be used for modeling.</source>
          <target state="new">Since it is likely that such records have an erroneous longitude-latitude values, we want to eliminate them from any data that is to be used for modeling.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Here is the content of <bpt id="p1">*</bpt>sample\_hive\_quality\_assessment.hql<ept id="p1">*</ept> file for inspection.</source>
          <target state="new">Here is the content of <bpt id="p1">*</bpt>sample\_hive\_quality\_assessment.hql<ept id="p1">*</ept> file for inspection.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, run :</source>
          <target state="new">From the Hive directory prompt, run :</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>-S<ept id="p1">*</ept> argument included in this command suppresses the status screen printout of the Hive Map/Reduce jobs.</source>
          <target state="new">The <bpt id="p1">*</bpt>-S<ept id="p1">*</ept> argument included in this command suppresses the status screen printout of the Hive Map/Reduce jobs.</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>This is useful because it makes the screen print of the Hive query output more readable.</source>
          <target state="new">This is useful because it makes the screen print of the Hive query output more readable.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>Exploration: Binary class distributions of trip tips</source>
          <target state="new">Exploration: Binary class distributions of trip tips</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>For the binary classification problem outlined in the <bpt id="p1">[</bpt>Examples of prediction tasks<ept id="p1">](machine-learning-data-science-process-hive-walkthrough.md#mltasks)</ept> section, it is useful to know whether a tip was given or not.</source>
          <target state="new">For the binary classification problem outlined in the <bpt id="p1">[</bpt>Examples of prediction tasks<ept id="p1">](machine-learning-data-science-process-hive-walkthrough.md#mltasks)</ept> section, it is useful to know whether a tip was given or not.</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>This distribution of tips is binary:</source>
          <target state="new">This distribution of tips is binary:</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>tip given(Class 1, tip\_amount &gt; $0)</source>
          <target state="new">tip given(Class 1, tip\_amount &gt; $0)</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>no tip (Class 0, tip\_amount = $0).</source>
          <target state="new">no tip (Class 0, tip\_amount = $0).</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">*</bpt>sample\_hive\_tipped\_frequencies.hql<ept id="p1">*</ept> file shown below does this.</source>
          <target state="new">The <bpt id="p1">*</bpt>sample\_hive\_tipped\_frequencies.hql<ept id="p1">*</ept> file shown below does this.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, run:</source>
          <target state="new">From the Hive directory prompt, run:</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>Exploration: Class distributions in the multiclass setting</source>
          <target state="new">Exploration: Class distributions in the multiclass setting</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>For the multiclass classification problem outlined in the <bpt id="p1">[</bpt>Examples of prediction tasks<ept id="p1">](machine-learning-data-science-process-hive-walkthrough.md#mltasks)</ept> section this data set also lends itself to a natural classification where we would like to predict the amount of the tips given.</source>
          <target state="new">For the multiclass classification problem outlined in the <bpt id="p1">[</bpt>Examples of prediction tasks<ept id="p1">](machine-learning-data-science-process-hive-walkthrough.md#mltasks)</ept> section this data set also lends itself to a natural classification where we would like to predict the amount of the tips given.</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>We can use bins to define tip ranges in the query.</source>
          <target state="new">We can use bins to define tip ranges in the query.</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>To get the class distributions for the various tip ranges, we use the <bpt id="p1">*</bpt>sample\_hive\_tip\_range\_frequencies.hql<ept id="p1">*</ept> file.</source>
          <target state="new">To get the class distributions for the various tip ranges, we use the <bpt id="p1">*</bpt>sample\_hive\_tip\_range\_frequencies.hql<ept id="p1">*</ept> file.</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>Below are its contents.</source>
          <target state="new">Below are its contents.</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>Run the following command from Hadoop Command Line console:</source>
          <target state="new">Run the following command from Hadoop Command Line console:</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>Exploration: Compute Direct Distance Between Two Longitude-Latitude Locations</source>
          <target state="new">Exploration: Compute Direct Distance Between Two Longitude-Latitude Locations</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>Having a measure of the direct distance allows us to find out the discrepancy between it and the actual trip distance.</source>
          <target state="new">Having a measure of the direct distance allows us to find out the discrepancy between it and the actual trip distance.</target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>We motivate this feature by pointing out that a passenger might be less likely to tip if they figure out that the driver has intentionally taken them by a much longer route.</source>
          <target state="new">We motivate this feature by pointing out that a passenger might be less likely to tip if they figure out that the driver has intentionally taken them by a much longer route.</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source>To see the comparison between actual trip distance and the <bpt id="p1">[</bpt>Haversine distance<ept id="p1">](http://en.wikipedia.org/wiki/Haversine_formula)</ept> between two longitude-latitude points (the "great circle" distance), we use the trigonometric functions available within Hive, thus :</source>
          <target state="new">To see the comparison between actual trip distance and the <bpt id="p1">[</bpt>Haversine distance<ept id="p1">](http://en.wikipedia.org/wiki/Haversine_formula)</ept> between two longitude-latitude points (the "great circle" distance), we use the trigonometric functions available within Hive, thus :</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source>In the query above, R is the radius of the Earth in miles, and pi is converted to radians.</source>
          <target state="new">In the query above, R is the radius of the Earth in miles, and pi is converted to radians.</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>Note that the longitude-latitude points are "filtered" to remove values that are far from the NYC area.</source>
          <target state="new">Note that the longitude-latitude points are "filtered" to remove values that are far from the NYC area.</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source>In this case, we write our results to a directory called "queryoutputdir".</source>
          <target state="new">In this case, we write our results to a directory called "queryoutputdir".</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>The sequence of commands shown below first creates this output directory, and then runs the Hive command.</source>
          <target state="new">The sequence of commands shown below first creates this output directory, and then runs the Hive command.</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, run:</source>
          <target state="new">From the Hive directory prompt, run:</target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>The query results are written to 9 Azure blobs <bpt id="p1">***</bpt>queryoutputdir/000000\_0<ept id="p1">***</ept> to  <bpt id="p2">***</bpt>queryoutputdir/000008\_0<ept id="p2">***</ept> under the default container of the Hadoop cluster.</source>
          <target state="new">The query results are written to 9 Azure blobs <bpt id="p1">***</bpt>queryoutputdir/000000\_0<ept id="p1">***</ept> to  <bpt id="p2">***</bpt>queryoutputdir/000008\_0<ept id="p2">***</ept> under the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source>To see the size of the individual blobs, we run the following command from the Hive directory prompt :</source>
          <target state="new">To see the size of the individual blobs, we run the following command from the Hive directory prompt :</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source>To see the contents of a given file, say 000000\_0, we use Hadoop's <ph id="ph1">`copyToLocal`</ph> command, thus.</source>
          <target state="new">To see the contents of a given file, say 000000\_0, we use Hadoop's <ph id="ph1">`copyToLocal`</ph> command, thus.</target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Warning:<ept id="p1">**</ept> <ph id="ph1">`copyToLocal`</ph> can be very slow for large files, and is not recommended for use with them.</source>
          <target state="new"><bpt id="p1">**</bpt>Warning:<ept id="p1">**</ept> <ph id="ph1">`copyToLocal`</ph> can be very slow for large files, and is not recommended for use with them.</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source>A key advantage of having this data reside in an Azure blob is that we may explore the data within Azure Machine Learning using the [Reader][reader] module.</source>
          <target state="new">A key advantage of having this data reside in an Azure blob is that we may explore the data within Azure Machine Learning using the [Reader][reader] module.</target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="#downsample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Down sample data and build models in Azure Machine Learning</source>
          <target state="new"><ph id="ph1">&lt;a name="#downsample"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Down sample data and build models in Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</source>
          <target state="new"><bpt id="p1">**</bpt>Note:<ept id="p1">**</ept> This is typically a <bpt id="p2">**</bpt>Data Scientist<ept id="p2">**</ept> task.</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>After the exploratory data analysis phase, we are now ready to down sample the data for building models in Azure Machine Learning.</source>
          <target state="new">After the exploratory data analysis phase, we are now ready to down sample the data for building models in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>In this section, we show how to use a Hive query to down sample the data, which is then accessed from the [Reader][reader] module in Azure Machine Learning.</source>
          <target state="new">In this section, we show how to use a Hive query to down sample the data, which is then accessed from the [Reader][reader] module in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="299" translate="yes" xml:space="preserve">
          <source>Down sampling the data</source>
          <target state="new">Down sampling the data</target>
        </trans-unit>
        <trans-unit id="300" translate="yes" xml:space="preserve">
          <source>There are two steps in this procedure.</source>
          <target state="new">There are two steps in this procedure.</target>
        </trans-unit>
        <trans-unit id="301" translate="yes" xml:space="preserve">
          <source>First we join the <bpt id="p1">**</bpt>nyctaxidb.trip<ept id="p1">**</ept> and <bpt id="p2">**</bpt>nyctaxidb.fare<ept id="p2">**</ept> tables on three keys that are present in all records : "medallion", "hack\_license", and "pickup\_datetime".</source>
          <target state="new">First we join the <bpt id="p1">**</bpt>nyctaxidb.trip<ept id="p1">**</ept> and <bpt id="p2">**</bpt>nyctaxidb.fare<ept id="p2">**</ept> tables on three keys that are present in all records : "medallion", "hack\_license", and "pickup\_datetime".</target>
        </trans-unit>
        <trans-unit id="302" translate="yes" xml:space="preserve">
          <source>We then generate a binary classification label <bpt id="p1">**</bpt>tipped<ept id="p1">**</ept> and a multi-class classification label <bpt id="p2">**</bpt>tip\_class<ept id="p2">**</ept>.</source>
          <target state="new">We then generate a binary classification label <bpt id="p1">**</bpt>tipped<ept id="p1">**</ept> and a multi-class classification label <bpt id="p2">**</bpt>tip\_class<ept id="p2">**</ept>.</target>
        </trans-unit>
        <trans-unit id="303" translate="yes" xml:space="preserve">
          <source>To be able to use the down sampled data directly from the [Reader][reader] module in Azure Machine Learning, it is necessary to store the results of the above query to an internal Hive table.</source>
          <target state="new">To be able to use the down sampled data directly from the [Reader][reader] module in Azure Machine Learning, it is necessary to store the results of the above query to an internal Hive table.</target>
        </trans-unit>
        <trans-unit id="304" translate="yes" xml:space="preserve">
          <source>In what follows, we create an internal Hive table and populate its contents with the joined and down sampled data.</source>
          <target state="new">In what follows, we create an internal Hive table and populate its contents with the joined and down sampled data.</target>
        </trans-unit>
        <trans-unit id="305" translate="yes" xml:space="preserve">
          <source>The query applies standard Hive functions directly to generate the hour of day, week of year, weekday (1 stands for Monday, and 7 stands for Sunday) from the "pickup\_datetime" field,  and the direct distance between the pickup and dropoff locations.</source>
          <target state="new">The query applies standard Hive functions directly to generate the hour of day, week of year, weekday (1 stands for Monday, and 7 stands for Sunday) from the "pickup\_datetime" field,  and the direct distance between the pickup and dropoff locations.</target>
        </trans-unit>
        <trans-unit id="306" translate="yes" xml:space="preserve">
          <source>Users can refer to <bpt id="p1">[</bpt>LanguageManual UDF<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)</ept> for a complete list of such functions.</source>
          <target state="new">Users can refer to <bpt id="p1">[</bpt>LanguageManual UDF<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)</ept> for a complete list of such functions.</target>
        </trans-unit>
        <trans-unit id="307" translate="yes" xml:space="preserve">
          <source>The query then down samples the data so that the query results can fit into the Azure Machine Learning Studio.</source>
          <target state="new">The query then down samples the data so that the query results can fit into the Azure Machine Learning Studio.</target>
        </trans-unit>
        <trans-unit id="308" translate="yes" xml:space="preserve">
          <source>Only about 1% of the original dataset is imported into the Studio.</source>
          <target state="new">Only about 1% of the original dataset is imported into the Studio.</target>
        </trans-unit>
        <trans-unit id="309" translate="yes" xml:space="preserve">
          <source>Below are the contents of <bpt id="p1">*</bpt>sample\_hive\_prepare\_for\_aml\_full.hql<ept id="p1">*</ept> file that prepares data for model building in Azure Machine Learning.</source>
          <target state="new">Below are the contents of <bpt id="p1">*</bpt>sample\_hive\_prepare\_for\_aml\_full.hql<ept id="p1">*</ept> file that prepares data for model building in Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="310" translate="yes" xml:space="preserve">
          <source>To run this query, from the Hive directory prompt :</source>
          <target state="new">To run this query, from the Hive directory prompt :</target>
        </trans-unit>
        <trans-unit id="311" translate="yes" xml:space="preserve">
          <source>We now have an internal table "nyctaxidb.nyctaxi_downsampled_dataset" which can be accessed using the [Reader][reader] module from Azure Machine Learning.</source>
          <target state="new">We now have an internal table "nyctaxidb.nyctaxi_downsampled_dataset" which can be accessed using the [Reader][reader] module from Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="312" translate="yes" xml:space="preserve">
          <source>Furthermore, we may use this dataset for building Machine Learning models.</source>
          <target state="new">Furthermore, we may use this dataset for building Machine Learning models.</target>
        </trans-unit>
        <trans-unit id="313" translate="yes" xml:space="preserve">
          <source>Use the Reader module in Azure Machine Learning to access the down sampled data</source>
          <target state="new">Use the Reader module in Azure Machine Learning to access the down sampled data</target>
        </trans-unit>
        <trans-unit id="314" translate="yes" xml:space="preserve">
          <source>As prerequisites for issuing Hive queries in the [Reader][reader] module of Azure Machine Learning, we need access to an Azure Machine Learning workspace and access to the credentials of the cluster and its associated storage account.</source>
          <target state="new">As prerequisites for issuing Hive queries in the [Reader][reader] module of Azure Machine Learning, we need access to an Azure Machine Learning workspace and access to the credentials of the cluster and its associated storage account.</target>
        </trans-unit>
        <trans-unit id="315" translate="yes" xml:space="preserve">
          <source>Some details on the [Reader][reader] module and the parameters to input :</source>
          <target state="new">Some details on the [Reader][reader] module and the parameters to input :</target>
        </trans-unit>
        <trans-unit id="316" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>HCatalog server URI<ept id="p1">**</ept>: If the cluster name is abc123, then this is simply : https://abc123.azurehdinsight.net</source>
          <target state="new"><bpt id="p1">**</bpt>HCatalog server URI<ept id="p1">**</ept>: If the cluster name is abc123, then this is simply : https://abc123.azurehdinsight.net</target>
        </trans-unit>
        <trans-unit id="317" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Hadoop user account name<ept id="p1">**</ept> : The user name chosen for the cluster (<bpt id="p2">**</bpt>not<ept id="p2">**</ept> the remote access user name)</source>
          <target state="new"><bpt id="p1">**</bpt>Hadoop user account name<ept id="p1">**</ept> : The user name chosen for the cluster (<bpt id="p2">**</bpt>not<ept id="p2">**</ept> the remote access user name)</target>
        </trans-unit>
        <trans-unit id="318" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Hadoop ser account password<ept id="p1">**</ept> : The password chosen for the cluster (<bpt id="p2">**</bpt>not<ept id="p2">**</ept> the remote access password)</source>
          <target state="new"><bpt id="p1">**</bpt>Hadoop ser account password<ept id="p1">**</ept> : The password chosen for the cluster (<bpt id="p2">**</bpt>not<ept id="p2">**</ept> the remote access password)</target>
        </trans-unit>
        <trans-unit id="319" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Location of output data<ept id="p1">**</ept> : This is chosen to be Azure.</source>
          <target state="new"><bpt id="p1">**</bpt>Location of output data<ept id="p1">**</ept> : This is chosen to be Azure.</target>
        </trans-unit>
        <trans-unit id="320" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure storage account name<ept id="p1">**</ept> : Name of the default storage account associated with the cluster.</source>
          <target state="new"><bpt id="p1">**</bpt>Azure storage account name<ept id="p1">**</ept> : Name of the default storage account associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="321" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Azure container name<ept id="p1">**</ept> : This is the default container name for the cluster, and is typically the same as the cluster name.</source>
          <target state="new"><bpt id="p1">**</bpt>Azure container name<ept id="p1">**</ept> : This is the default container name for the cluster, and is typically the same as the cluster name.</target>
        </trans-unit>
        <trans-unit id="322" translate="yes" xml:space="preserve">
          <source>For a cluster called "abc123", this is just abc123.</source>
          <target state="new">For a cluster called "abc123", this is just abc123.</target>
        </trans-unit>
        <trans-unit id="323" translate="yes" xml:space="preserve">
          <source>Any table we wish to query using the [Reader][reader] module in Azure Machine Learning must be an internal table.</source>
          <target state="new">Any table we wish to query using the [Reader][reader] module in Azure Machine Learning must be an internal table.</target>
        </trans-unit>
        <trans-unit id="324" translate="yes" xml:space="preserve">
          <source>A tip for determining if a table T in a database D.db is an internal table is as follows.</source>
          <target state="new">A tip for determining if a table T in a database D.db is an internal table is as follows.</target>
        </trans-unit>
        <trans-unit id="325" translate="yes" xml:space="preserve">
          <source>From the Hive directory prompt, issue the command :</source>
          <target state="new">From the Hive directory prompt, issue the command :</target>
        </trans-unit>
        <trans-unit id="326" translate="yes" xml:space="preserve">
          <source>If the table is an internal table and it is populated, its contents must show here.</source>
          <target state="new">If the table is an internal table and it is populated, its contents must show here.</target>
        </trans-unit>
        <trans-unit id="327" translate="yes" xml:space="preserve">
          <source>Another way to determine whether a table is an internal table is to use the Azure Storage Explorer.</source>
          <target state="new">Another way to determine whether a table is an internal table is to use the Azure Storage Explorer.</target>
        </trans-unit>
        <trans-unit id="328" translate="yes" xml:space="preserve">
          <source>Use it to navigate to the default container name of the cluster, and then filter by the table name.</source>
          <target state="new">Use it to navigate to the default container name of the cluster, and then filter by the table name.</target>
        </trans-unit>
        <trans-unit id="329" translate="yes" xml:space="preserve">
          <source>If the table and its contents show up, this confirms that it is an internal table.</source>
          <target state="new">If the table and its contents show up, this confirms that it is an internal table.</target>
        </trans-unit>
        <trans-unit id="330" translate="yes" xml:space="preserve">
          <source>Here is a snapshot of the Hive query and the [Reader][reader] module:</source>
          <target state="new">Here is a snapshot of the Hive query and the [Reader][reader] module:</target>
        </trans-unit>
        <trans-unit id="331" translate="yes" xml:space="preserve">
          <source>Note that since our down sampled data resides in the default container, the resulting Hive query from Azure Machine Learning is very simple and is just a "SELECT * FROM nyctaxidb.nyctaxi\_downsampled\_data".</source>
          <target state="new">Note that since our down sampled data resides in the default container, the resulting Hive query from Azure Machine Learning is very simple and is just a "SELECT * FROM nyctaxidb.nyctaxi\_downsampled\_data".</target>
        </trans-unit>
        <trans-unit id="332" translate="yes" xml:space="preserve">
          <source>The dataset may now be used as the starting point for building Machine Learning models.</source>
          <target state="new">The dataset may now be used as the starting point for building Machine Learning models.</target>
        </trans-unit>
        <trans-unit id="333" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;a name="mlmodel"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Build models in Azure Machine Learning</source>
          <target state="new"><ph id="ph1">&lt;a name="mlmodel"&gt;</ph><ph id="ph2">&lt;/a&gt;</ph>Build models in Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="334" translate="yes" xml:space="preserve">
          <source>We are now able to proceed to model building and model deployment in <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">](https://studio.azureml.net)</ept>.</source>
          <target state="new">We are now able to proceed to model building and model deployment in <bpt id="p1">[</bpt>Azure Machine Learning<ept id="p1">](https://studio.azureml.net)</ept>.</target>
        </trans-unit>
        <trans-unit id="335" translate="yes" xml:space="preserve">
          <source>The data is ready for us to use in addressing the prediction problems identified above:</source>
          <target state="new">The data is ready for us to use in addressing the prediction problems identified above:</target>
        </trans-unit>
        <trans-unit id="336" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>1. Binary classification<ept id="p1">**</ept>: To predict whether or not a tip was paid for a trip.</source>
          <target state="new"><bpt id="p1">**</bpt>1. Binary classification<ept id="p1">**</ept>: To predict whether or not a tip was paid for a trip.</target>
        </trans-unit>
        <trans-unit id="337" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Two-class logistic regression</source>
          <target state="new"><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Two-class logistic regression</target>
        </trans-unit>
        <trans-unit id="338" translate="yes" xml:space="preserve">
          <source>a.</source>
          <target state="new">a.</target>
        </trans-unit>
        <trans-unit id="339" translate="yes" xml:space="preserve">
          <source>For this problem, our target (or class) label is "tipped".</source>
          <target state="new">For this problem, our target (or class) label is "tipped".</target>
        </trans-unit>
        <trans-unit id="340" translate="yes" xml:space="preserve">
          <source>Our original down-sampled dataset has a few columns that are target leaks for this classification experiment.</source>
          <target state="new">Our original down-sampled dataset has a few columns that are target leaks for this classification experiment.</target>
        </trans-unit>
        <trans-unit id="341" translate="yes" xml:space="preserve">
          <source>In particular : tip\_class, tip\_amount, and total\_amount reveal information about the target label that is not available at testing time.</source>
          <target state="new">In particular : tip\_class, tip\_amount, and total\_amount reveal information about the target label that is not available at testing time.</target>
        </trans-unit>
        <trans-unit id="342" translate="yes" xml:space="preserve">
          <source>We remove these columns from consideration using the [Project Columns][project-columns] module.</source>
          <target state="new">We remove these columns from consideration using the [Project Columns][project-columns] module.</target>
        </trans-unit>
        <trans-unit id="343" translate="yes" xml:space="preserve">
          <source>The snapshot below shows our experiment to predict whether or not a tip was paid for a given trip.</source>
          <target state="new">The snapshot below shows our experiment to predict whether or not a tip was paid for a given trip.</target>
        </trans-unit>
        <trans-unit id="344" translate="yes" xml:space="preserve">
          <source>b.</source>
          <target state="new">b.</target>
        </trans-unit>
        <trans-unit id="345" translate="yes" xml:space="preserve">
          <source>For this experiment, our target label distributions were roughly 1:1.</source>
          <target state="new">For this experiment, our target label distributions were roughly 1:1.</target>
        </trans-unit>
        <trans-unit id="346" translate="yes" xml:space="preserve">
          <source>The snapshot below shows the distribution of tip class labels for the binary classification problem.</source>
          <target state="new">The snapshot below shows the distribution of tip class labels for the binary classification problem.</target>
        </trans-unit>
        <trans-unit id="347" translate="yes" xml:space="preserve">
          <source>As a result, we obtain an AUC of 0.987 as shown in the figure below.</source>
          <target state="new">As a result, we obtain an AUC of 0.987 as shown in the figure below.</target>
        </trans-unit>
        <trans-unit id="348" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>2. Multiclass classification<ept id="p1">**</ept>: To predict the range of tip amounts paid for the trip, using the previously defined classes.</source>
          <target state="new"><bpt id="p1">**</bpt>2. Multiclass classification<ept id="p1">**</ept>: To predict the range of tip amounts paid for the trip, using the previously defined classes.</target>
        </trans-unit>
        <trans-unit id="349" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Multiclass logistic regression</source>
          <target state="new"><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Multiclass logistic regression</target>
        </trans-unit>
        <trans-unit id="350" translate="yes" xml:space="preserve">
          <source>a.</source>
          <target state="new">a.</target>
        </trans-unit>
        <trans-unit id="351" translate="yes" xml:space="preserve">
          <source>For this problem, our target (or class) label is "tip\_class" which can take one of five values (0,1,2,3,4).</source>
          <target state="new">For this problem, our target (or class) label is "tip\_class" which can take one of five values (0,1,2,3,4).</target>
        </trans-unit>
        <trans-unit id="352" translate="yes" xml:space="preserve">
          <source>As in the binary classification case, we have a few columns that are target leaks for this experiment.</source>
          <target state="new">As in the binary classification case, we have a few columns that are target leaks for this experiment.</target>
        </trans-unit>
        <trans-unit id="353" translate="yes" xml:space="preserve">
          <source>In particular : tipped, tip\_amount, total\_amount reveal information about the target label that is not available at testing time.</source>
          <target state="new">In particular : tipped, tip\_amount, total\_amount reveal information about the target label that is not available at testing time.</target>
        </trans-unit>
        <trans-unit id="354" translate="yes" xml:space="preserve">
          <source>We remove these columns using the [Project Columns][project-columns] module.</source>
          <target state="new">We remove these columns using the [Project Columns][project-columns] module.</target>
        </trans-unit>
        <trans-unit id="355" translate="yes" xml:space="preserve">
          <source>The snapshot below shows our experiment to predict in which bin a tip is likely to fall ( Class 0: tip = $0, class 1 : tip &gt; $0 and tip &lt;= $5, Class 2 : tip &gt; $5 and tip &lt;= $10, Class 3 : tip &gt; $10 and tip &lt;= $20, Class 4 : tip &gt; $20)</source>
          <target state="new">The snapshot below shows our experiment to predict in which bin a tip is likely to fall ( Class 0: tip = $0, class 1 : tip &gt; $0 and tip &lt;= $5, Class 2 : tip &gt; $5 and tip &lt;= $10, Class 3 : tip &gt; $10 and tip &lt;= $20, Class 4 : tip &gt; $20)</target>
        </trans-unit>
        <trans-unit id="356" translate="yes" xml:space="preserve">
          <source>We now show what our actual test class distribution looks like.</source>
          <target state="new">We now show what our actual test class distribution looks like.</target>
        </trans-unit>
        <trans-unit id="357" translate="yes" xml:space="preserve">
          <source>We see that while Class 0 and Class 1 are prevalent, the other classes are rare.</source>
          <target state="new">We see that while Class 0 and Class 1 are prevalent, the other classes are rare.</target>
        </trans-unit>
        <trans-unit id="358" translate="yes" xml:space="preserve">
          <source>b.</source>
          <target state="new">b.</target>
        </trans-unit>
        <trans-unit id="359" translate="yes" xml:space="preserve">
          <source>For this experiment, we use a confusion matrix to look at our prediction accuracies.</source>
          <target state="new">For this experiment, we use a confusion matrix to look at our prediction accuracies.</target>
        </trans-unit>
        <trans-unit id="360" translate="yes" xml:space="preserve">
          <source>This is shown below.</source>
          <target state="new">This is shown below.</target>
        </trans-unit>
        <trans-unit id="361" translate="yes" xml:space="preserve">
          <source>Note that while our class accuracies on the prevalent classes is quite good, the model does not do a good job of "learning" on the rarer classes.</source>
          <target state="new">Note that while our class accuracies on the prevalent classes is quite good, the model does not do a good job of "learning" on the rarer classes.</target>
        </trans-unit>
        <trans-unit id="362" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>3. Regression task<ept id="p1">**</ept>: To predict the amount of tip paid for a trip.</source>
          <target state="new"><bpt id="p1">**</bpt>3. Regression task<ept id="p1">**</ept>: To predict the amount of tip paid for a trip.</target>
        </trans-unit>
        <trans-unit id="363" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Boosted decision tree</source>
          <target state="new"><bpt id="p1">**</bpt>Learner used:<ept id="p1">**</ept> Boosted decision tree</target>
        </trans-unit>
        <trans-unit id="364" translate="yes" xml:space="preserve">
          <source>a.</source>
          <target state="new">a.</target>
        </trans-unit>
        <trans-unit id="365" translate="yes" xml:space="preserve">
          <source>For this problem, our target (or class) label is "tip\_amount".</source>
          <target state="new">For this problem, our target (or class) label is "tip\_amount".</target>
        </trans-unit>
        <trans-unit id="366" translate="yes" xml:space="preserve">
          <source>Our target leaks in this case are : tipped, tip\_class, total\_amount ; all these variables reveal information about the tip amount that is typically unavailable at testing time.</source>
          <target state="new">Our target leaks in this case are : tipped, tip\_class, total\_amount ; all these variables reveal information about the tip amount that is typically unavailable at testing time.</target>
        </trans-unit>
        <trans-unit id="367" translate="yes" xml:space="preserve">
          <source>We remove these columns using the [Project Columns][project-columns] module.</source>
          <target state="new">We remove these columns using the [Project Columns][project-columns] module.</target>
        </trans-unit>
        <trans-unit id="368" translate="yes" xml:space="preserve">
          <source>The snapshot belows shows our experiment to predict the amount of the given tip.</source>
          <target state="new">The snapshot belows shows our experiment to predict the amount of the given tip.</target>
        </trans-unit>
        <trans-unit id="369" translate="yes" xml:space="preserve">
          <source>b.</source>
          <target state="new">b.</target>
        </trans-unit>
        <trans-unit id="370" translate="yes" xml:space="preserve">
          <source>For regression problems, we measure the accuracies of our prediction by looking at the squared error in the predictions, the coefficient of determination, and the like.</source>
          <target state="new">For regression problems, we measure the accuracies of our prediction by looking at the squared error in the predictions, the coefficient of determination, and the like.</target>
        </trans-unit>
        <trans-unit id="371" translate="yes" xml:space="preserve">
          <source>We show these below.</source>
          <target state="new">We show these below.</target>
        </trans-unit>
        <trans-unit id="372" translate="yes" xml:space="preserve">
          <source>We see that about the coefficient of determination is 0.709, implying about 71% of the variance is explained by our model coefficients.</source>
          <target state="new">We see that about the coefficient of determination is 0.709, implying about 71% of the variance is explained by our model coefficients.</target>
        </trans-unit>
        <trans-unit id="373" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Important note:<ept id="p1">**</ept> To learn more about Azure Machine Learning and how to access and use it, please refer to <bpt id="p2">[</bpt>What's Machine Learning?<ept id="p2">](machine-learning-what-is-machine-learning.md)</ept>.</source>
          <target state="new"><bpt id="p1">**</bpt>Important note:<ept id="p1">**</ept> To learn more about Azure Machine Learning and how to access and use it, please refer to <bpt id="p2">[</bpt>What's Machine Learning?<ept id="p2">](machine-learning-what-is-machine-learning.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="374" translate="yes" xml:space="preserve">
          <source>A very useful resource for playing with a bunch of Machine Learning experiments on Azure Machine Learning is the <bpt id="p1">[</bpt>gallery<ept id="p1">](https://gallery.azureml.net/)</ept>.</source>
          <target state="new">A very useful resource for playing with a bunch of Machine Learning experiments on Azure Machine Learning is the <bpt id="p1">[</bpt>gallery<ept id="p1">](https://gallery.azureml.net/)</ept>.</target>
        </trans-unit>
        <trans-unit id="375" translate="yes" xml:space="preserve">
          <source>The gallery covers a gamut of experiments and provides a thorough introduction into the range of capabilities of Azure Machine Learning.</source>
          <target state="new">The gallery covers a gamut of experiments and provides a thorough introduction into the range of capabilities of Azure Machine Learning.</target>
        </trans-unit>
        <trans-unit id="376" translate="yes" xml:space="preserve">
          <source>License Information</source>
          <target state="new">License Information</target>
        </trans-unit>
        <trans-unit id="377" translate="yes" xml:space="preserve">
          <source>This sample walkthrough and its accompanying scripts are shared by Microsoft under the MIT license.</source>
          <target state="new">This sample walkthrough and its accompanying scripts are shared by Microsoft under the MIT license.</target>
        </trans-unit>
        <trans-unit id="378" translate="yes" xml:space="preserve">
          <source>Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.</source>
          <target state="new">Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.</target>
        </trans-unit>
        <trans-unit id="379" translate="yes" xml:space="preserve">
          <source>References</source>
          <target state="new">References</target>
        </trans-unit>
        <trans-unit id="380" translate="yes" xml:space="preserve">
          <source>•   <bpt id="p1">[</bpt>Andrés Monroy NYC Taxi Trips Download Page<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept></source>
          <target state="new">•   <bpt id="p1">[</bpt>Andrés Monroy NYC Taxi Trips Download Page<ept id="p1">](http://www.andresmh.com/nyctaxitrips/)</ept></target>
        </trans-unit>
        <trans-unit id="381" translate="yes" xml:space="preserve">
          <source>•   <bpt id="p1">[</bpt>FOILing NYC’s Taxi Trip Data by Chris Whong<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph1"> </ph></source>
          <target state="new">•   <bpt id="p1">[</bpt>FOILing NYC’s Taxi Trip Data by Chris Whong<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph1"> </ph></target>
        </trans-unit>
        <trans-unit id="382" translate="yes" xml:space="preserve">
          <source>•   <bpt id="p1">[</bpt>NYC Taxi and Limousine Commission Research and Statistics<ept id="p1">](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)</ept></source>
          <target state="new">•   <bpt id="p1">[</bpt>NYC Taxi and Limousine Commission Research and Statistics<ept id="p1">](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)</ept></target>
        </trans-unit>
        <trans-unit id="383" translate="yes" xml:space="preserve">
          <source>test</source>
          <target state="new">test</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">905dfddcd56831e68d607c8439205f6e4f6415c4</xliffext:olfilehash>
  </header>
</xliff>